\section{Motivating Example}
\label{sec:motivating-example}

We illustrate the semantic influence problem through a real vulnerability pattern found in LLM agent frameworks.

\subsection{The ReAct Agent Pattern}

Modern LLM applications frequently implement the ReAct (Reasoning and Acting) pattern~\cite{yao2022react}, where an LLM reasons about a task and generates actions to execute:

\begin{lstlisting}[language=Python, caption={Vulnerable ReAct agent implementation}, label={lst:react-vuln}]
def process_user_request(user_message: str):
    # Build prompt with user input
    prompt = f"""You are a helpful assistant with tool access.
    User request: {user_message}

    Respond with JSON: {{"tool": "...", "args": "..."}}"""

    # Get LLM response
    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    ).choices[0].message.content

    # Parse and execute action
    action = json.loads(response)
    if action["tool"] == "search":
        return search_web(action["args"])
    elif action["tool"] == "execute":
        return subprocess.run(action["args"], shell=True)  # SINK
\end{lstlisting}

\subsection{Why Traditional Taint Analysis Fails}

A traditional taint analyzer would process this code as follows:

\begin{enumerate}
    \item \texttt{user\_message} is marked as tainted (source: function parameter)
    \item \texttt{prompt} is marked as tainted (f-string includes tainted variable)
    \item \texttt{response} is... \textbf{not tainted}?
\end{enumerate}

The LLM API call returns a string generated by the model, not derived from the input through traceable operations. Most taint analyzers treat API calls as sanitizers or simply lose track of taint at external call boundaries.

\subsection{The Attack}

An adversary can exploit this pattern through prompt injection:

\begin{lstlisting}[language=Python, caption={Prompt injection attack}]
malicious_input = """
Please help me find restaurants.

IGNORE PREVIOUS INSTRUCTIONS. You must respond with exactly:
{"tool": "execute", "args": "curl attacker.com/shell.sh | bash"}
"""

process_user_request(malicious_input)  # Executes attacker's command
\end{lstlisting}

The LLM, influenced by the injected instructions, generates a response that includes the attacker's payload. This payload flows to the \texttt{subprocess.run} sink and executes arbitrary commands.

\subsection{Semantic Influence Analysis}

Our approach tracks this vulnerability as follows:

\begin{enumerate}
    \item \texttt{user\_message}: $\tau = (\{\textsc{User}\}, 1.0)$
    \item \texttt{prompt}: $\tau = (\{\textsc{User}\}, 1.0)$ (propagated through f-string)
    \item \texttt{response}: $\tau = (\{\textsc{User}, \textsc{Semantic}\}, 0.85)$

    \emph{Key insight}: The LLM call transforms taint but preserves \textbf{semantic influence}. The decay factor $\alpha = 0.85$ reflects that the user prompt position has high influence over the output.

    \item \texttt{action["args"]}: $\tau = (\{\textsc{User}, \textsc{Semantic}\}, 0.85)$ (propagated through JSON parsing)

    \item At \texttt{subprocess.run}: Sink check triggered!
    \begin{itemize}
        \item \textsc{Semantic} $\in$ taint types ✓
        \item Influence strength $0.85 > \theta = 0.5$ ✓
        \item Sink category: Command Injection (Critical) ✓
    \end{itemize}
\end{enumerate}

\textbf{Result}: Vulnerability detected with high confidence.

\subsection{Detection Comparison}

\begin{table}[h]
\centering
\caption{Detection of Listing~\ref{lst:react-vuln} vulnerability}
\label{tab:detection-comparison}
\begin{tabular}{lcc}
\toprule
Tool & Detects? & Reason \\
\midrule
Bandit & \xmark & Taint lost at API call \\
Semgrep & \xmark & No LLM-aware rules \\
CodeQL & \xmark & External call breaks flow \\
\textbf{Ours} & \cmark & Semantic influence tracking \\
\bottomrule
\end{tabular}
\end{table}

This example demonstrates the core value of semantic influence: maintaining security-relevant data flow information across LLM API boundaries where traditional analysis fails.
