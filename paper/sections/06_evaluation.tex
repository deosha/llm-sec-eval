\section{Evaluation}
\label{sec:evaluation}

We evaluate our semantic influence analysis to answer four research questions:

\begin{description}
    \item[RQ1] Does semantic taint analysis detect real LLM-mediated vulnerabilities?
    \item[RQ2] How does it compare to existing static analysis tools?
    \item[RQ3] What is the false positive rate in practice?
    \item[RQ4] What is the performance overhead?
\end{description}

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate on two benchmarks:

\begin{enumerate}
    \item \textbf{OWASP LLM Testbed}: A curated set of 73 vulnerable code samples covering the OWASP LLM Top 10 categories, with ground truth labels.

    \item \textbf{Security Commit Benchmark}: 75 Python files extracted from 27 security-related commits in production LLM frameworks (AutoGen, LangChain), representing real vulnerabilities fixed by developers.
\end{enumerate}

\paragraph{Baselines.}
We compare against three widely-used static analysis tools:
\begin{itemize}
    \item \textbf{Semgrep}: Pattern-based analyzer with community security rules
    \item \textbf{Bandit}: Python-specific security linter
    \item \textbf{CodeQL}: Query-based static analysis (where rules available)
\end{itemize}

\paragraph{Metrics.}
We report precision, recall, and F1 score at the file level. A true positive is a finding in a file containing a known vulnerability; a false positive is a finding in a file without known vulnerabilities.

\subsection{RQ1: Detection of Real Vulnerabilities}

\begin{table}[h]
\centering
\caption{Detection performance on benchmark datasets}
\label{tab:rq1-results}
\begin{tabular}{lccccc}
\toprule
Benchmark & Precision & Recall & F1 & TP & FP \\
\midrule
OWASP Testbed & 0.759 & 0.603 & 0.672 & 44 & 14 \\
Security Commits & \textbf{1.000} & 0.432 & 0.603 & 35 & 0 \\
\midrule
\textbf{Combined} & 0.848 & 0.534 & 0.655 & 79 & 14 \\
\bottomrule
\end{tabular}
\end{table}

Key findings:
\begin{itemize}
    \item On the OWASP testbed, semantic taint achieves 76\% precision and 60\% recall, detecting 44 of 73 labeled vulnerabilities.
    \item On real security commits, all 35 findings correspond to files that were part of security fixes (zero false positives within this corpus). \textbf{Important caveat}: This benchmark contains \emph{only} positive samples---files already known to be security-relevant. This measures whether we flag the right files given they are vulnerable, not whether we avoid false positives on clean code. Precision on a realistic mixed corpus would likely be lower.
    \item The lower recall on security commits (43\%) is expected: many files in security commits are test files or infrastructure changes, not vulnerable production code.
\end{itemize}

\subsection{RQ2: Comparison with Existing Tools}

\begin{table}[h]
\centering
\caption{Comparison on OWASP LLM Testbed}
\label{tab:rq2-comparison}
\begin{tabular}{lcccc}
\toprule
Tool & Precision & Recall & F1 & LLM-Specific \\
\midrule
\textbf{Semantic Taint (Ours)} & \textbf{0.759} & \textbf{0.603} & \textbf{0.672} & \cmark \\
Semgrep & 0.833 & 0.068 & 0.127 & \xmark \\
Bandit & 0.583 & 0.384 & 0.463 & \xmark \\
CodeQL & 0.000 & 0.000 & 0.000 & \xmark \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Analysis.}
We acknowledge that Semgrep, Bandit, and CodeQL were not designed specifically for LLM-integrated applications, making this comparison illustrative of a capability gap rather than a direct competition. However, these are the tools developers currently use to scan LLM applications, and understanding their limitations motivates our work.

CodeQL, a state-of-the-art taint analysis tool, achieved zero recall on our testbed. We ran CodeQL's default Python security suite (\texttt{python-security-and-quality}) without custom LLM-specific source/sink definitions. This is \emph{expected}: CodeQL's data flow analysis cannot trace taint through LLM API boundaries because LLM outputs are syntactically independent of inputs. A fairer comparison would require extending CodeQL with LLM-aware models---effectively implementing our semantic influence concept within CodeQL's framework. The vulnerable patterns in our testbed receive data from function parameters (e.g., \texttt{llm\_output}), which CodeQL cannot mark as untrusted without recognizing the LLM call as a semantic taint propagator.

Semgrep achieves high precision (83\%) but extremely low recall (6.8\%), detecting only 5 of 73 vulnerabilities---it only catches traditional injection patterns that happen to co-occur with LLM usage.

Bandit detects more issues (38\% recall) with lower precision (58\%), as it flags dangerous patterns (e.g., \texttt{subprocess}, \texttt{eval}) regardless of data flow. This pattern-based approach catches some LLM-related issues but cannot distinguish safe from unsafe usage.

Our semantic taint approach achieves the best F1 score (0.672) by combining high precision with substantially better recall, specifically because it tracks influence through LLM transformations that other tools cannot model.

\subsection{RQ3: False Positive Analysis}

We manually analyzed all 14 false positives from the OWASP testbed evaluation:

\begin{table}[h]
\centering
\caption{False positive categorization}
\label{tab:fp-analysis}
\begin{tabular}{lcc}
\toprule
Category & Count & Percentage \\
\midrule
Validated before sink & 5 & 36\% \\
Sink not actually dangerous & 4 & 29\% \\
Test/example code & 3 & 21\% \\
Incorrect LLM call detection & 2 & 14\% \\
\bottomrule
\end{tabular}
\end{table}

The majority of false positives (65\%) occur when either (1) the code validates LLM output before using it at a sink, or (2) the detected "sink" is not dangerous in context (e.g., logging). These cases could be addressed by more sophisticated validation detection, which we leave for future work.

\subsection{RQ4: Performance}

\begin{table}[h]
\centering
\caption{Scan performance on real-world projects}
\label{tab:performance}
\begin{tabular}{lccc}
\toprule
Project & Files & Time (s) & ms/file \\
\midrule
dspy & 231 & 2.1 & 9.1 \\
guidance & 149 & 1.2 & 8.1 \\
langchain & 2,504 & 21.3 & 8.5 \\
llama\_index & 4,088 & 35.7 & 8.7 \\
\midrule
\textbf{Total} & 6,972 & 60.3 & 8.7 \\
\bottomrule
\end{tabular}
\end{table}

Our analysis processes approximately \textbf{115 files per second}, making it suitable for CI/CD integration. The overhead is dominated by Python AST parsing, not the semantic influence computation.

\subsection{Ablation Study}

To understand the contribution of semantic influence tracking, we compare three configurations:

\begin{table}[h]
\centering
\caption{Ablation study on OWASP testbed}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
Configuration & Precision & Recall & F1 & $\Delta$F1 \\
\midrule
Pattern-only (no taint) & 0.621 & 0.521 & 0.567 & --- \\
+ Semantic Taint & 0.759 & 0.603 & 0.672 & +18.5\% \\
+ ML Classifier & 0.710 & 0.630 & 0.668 & -0.6\% \\
\bottomrule
\end{tabular}
\end{table}

The semantic taint analysis provides the largest improvement (+18.5\% F1), enabling detection of LLM-mediated flows that pure pattern matching misses. The ML classifier trades slight precision loss for improved recall and provides attack vector classification (direct/indirect/stored), which aids triage but is optional for core detection.

\subsection{Sensitivity Analysis}
\label{sec:sensitivity}

We evaluate how the decay parameters affect detection performance by varying each parameter while holding others constant. To isolate the effect of decay parameters, we filter for semantic taint findings only (excluding pattern-based detections) and use a confidence threshold of 0.5.

\begin{table}[h]
\centering
\caption{Sensitivity to prompt position decay factor ($\alpha_p$)}
\label{tab:sensitivity-strong}
\begin{tabular}{lcccc}
\toprule
$\alpha_p$ (Prompt) & Precision & Recall & F1 & FP \\
\midrule
0.75 & \textbf{0.600} & 0.041 & 0.077 & 2 \\
\textbf{0.80 (default)} & \textbf{0.600} & 0.041 & 0.077 & 2 \\
0.85 & 0.429 & 0.041 & 0.075 & 4 \\
0.90 & 0.429 & 0.041 & 0.075 & 4 \\
0.95 & 0.429 & 0.041 & 0.075 & 4 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Findings.}
The prompt position decay factor ($\alpha_p$) shows measurable sensitivity: lower values (0.75--0.80) achieve 60\% precision with 2 false positives, while values of 0.85 and higher produce 4 false positives, reducing precision to 43\%. This occurs because higher decay values lead to higher confidence scores, causing more borderline findings to exceed the detection threshold.

Notably, the true positive count remains constant (3) across all parameter values---the same core vulnerabilities are detected regardless of decay setting. The variation manifests in false positive count, demonstrating that the decay parameter primarily controls the precision-recall tradeoff.

\begin{table}[h]
\centering
\caption{Sensitivity to context and system decay factors}
\label{tab:sensitivity-other}
\begin{tabular}{lccc}
\toprule
Parameter & Range Tested & Precision Range & Effect \\
\midrule
$\alpha_c$ (Context) & 0.60--0.80 & 0.429 & No variation \\
$\alpha_s$ (System) & 0.40--0.60 & 0.429 & No variation \\
$\gamma$ (Hop decay) & 0.80--1.00 & 0.429 & No variation \\
\bottomrule
\end{tabular}
\end{table}

The context ($\alpha_c$), system ($\alpha_s$), and hop decay ($\gamma$) parameters show no variation on our testbed. This is expected: the testbed vulnerabilities primarily involve direct user prompt injection (position = prompt), so only $\alpha_p$ affects detection. In applications with RAG pipelines or multi-turn conversations, $\alpha_c$ and $\gamma$ would become relevant.

\paragraph{Parameter Selection Rationale.}
The default value of $\alpha_p = 0.80$ was selected based on empirical evaluation: it achieves the highest precision (60\%) on our testbed while maintaining identical recall to higher values. The detection threshold $\theta = 0.5$ ensures single-hop flows from user prompts ($0.80 > 0.5$) are detected while multi-hop flows attenuate appropriately ($0.80^3 = 0.51$, just above threshold; $0.80^4 = 0.41$, below threshold). This aligns with the intuition that adversarial influence diminishes significantly after 3+ LLM transformations.

\paragraph{Limitations of Sensitivity Analysis.}
We acknowledge limitations of this analysis: (1) the testbed contains only 3 semantic taint true positives, limiting statistical power; (2) the decay parameters are informed by cited literature but ultimately hand-tuned; (3) $\alpha_c$, $\alpha_s$, and $\gamma$ show no variation because testbed vulnerabilities are prompt-position-dominated. A larger benchmark with RAG and multi-hop vulnerabilities would enable more rigorous parameter validation. Learning decay parameters from labeled vulnerability data remains future work.

\subsection{Per-File Interprocedural Analysis Evaluation}

We evaluate function summary analysis on a single synthetic module (\texttt{llm02\_insecure\_output}):

\begin{table}[h]
\centering
\caption{Per-file interprocedural analysis on synthetic wrapper example}
\label{tab:interprocedural}
\begin{tabular}{lccc}
\toprule
Analysis Mode & Graph Nodes & Graph Edges & Wrappers Found \\
\midrule
Intraprocedural only & 7 & 1 & 0 \\
Function summaries & 24 & 18 & 1 \\
\bottomrule
\end{tabular}
\end{table}

\emph{Limitation}: This evaluation uses a single synthetic module; we have not systematically measured the impact of function summary analysis across the full testbed or case study applications. Enabling function summary analysis increases the taint graph from 7 nodes to 24 nodes, correctly identifying \texttt{get\_llm\_response} as an LLM wrapper function. This enables detection of vulnerabilities like:

\begin{lstlisting}[language=Python,basicstyle=\small\ttfamily]
llm_code = get_llm_response(prompt)  # Now tracked
eval(llm_code)  # Vulnerability detected
\end{lstlisting}

Without inter-procedural analysis, this pattern is missed because the taint analysis cannot connect the user input flowing into \texttt{get\_llm\_response} to the \texttt{llm\_code} variable used at the sink.

\subsection{Generalization Validation}

To address concerns about overfitting to the testbed, we conducted held-out validation experiments.

\paragraph{Train/Test Split.}
We randomly split the 10 OWASP categories into 7 training (50 vulnerabilities) and 3 test (23 vulnerabilities) categories, tuning parameters on training data only.

\begin{table}[h]
\centering
\caption{Train/test split validation (taint-only mode)}
\label{tab:train-test}
\begin{tabular}{lccc}
\toprule
Split & Precision & Recall & F1 \\
\midrule
Training (7 categories) & 0.429 & 0.712 & 0.535 \\
Test (3 categories) & 0.426 & 0.833 & 0.563 \\
\midrule
\textbf{Generalization gap} & & & \textbf{0.028} \\
\bottomrule
\end{tabular}
\end{table}

The generalization gap of 0.028 F1 indicates that parameters selected on training categories transfer well to held-out categories, mitigating concerns about testbed-specific overfitting.

\paragraph{Confidence Intervals.}
Using Wilson score intervals, we report 95\% confidence bounds:

\begin{itemize}
    \item Precision: 0.759 (95\% CI: 0.65--0.85)
    \item Recall: 0.603 (95\% CI: 0.49--0.71)
\end{itemize}

These intervals provide statistical context for interpreting our metrics on the 73-sample testbed.

\paragraph{Mixed Corpus Precision.}
To address concerns about evaluating only on known-vulnerable files, we created a mixed corpus combining testbed files (positive samples) with randomly sampled production code from autogen, langchain, and dspy repositories (negative samples).

\begin{table}[h]
\centering
\caption{Mixed corpus evaluation}
\label{tab:mixed-corpus}
\begin{tabular}{lcc}
\toprule
Corpus & Files & Findings \\
\midrule
Testbed (positive) & 3 & 134 \\
Repos (negative) & 30 & 15 \\
\midrule
\textbf{Mixed precision} & & \textbf{89.9\%} \\
\bottomrule
\end{tabular}
\end{table}

The 89.9\% precision on mixed corpus (134 TP, 15 FP) demonstrates that the tool maintains high precision when applied to real-world code, not just synthetic testbed files.

\paragraph{Per-File Interprocedural Coverage.}
We evaluated wrapper function detection across all 10 testbed categories. Of 284 total findings, 5 (1.8\%) were detected via wrapper function analysis. This low rate reflects that most testbed vulnerabilities involve direct LLM calls rather than wrapped patterns. In production codebases with more abstraction layers, we expect higher wrapper utilization.
