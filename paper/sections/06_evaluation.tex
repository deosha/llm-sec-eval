\section{Evaluation}
\label{sec:evaluation}

We evaluate our semantic influence analysis to answer four research questions:

\begin{description}
    \item[RQ1] Does semantic taint analysis detect real LLM-mediated vulnerabilities?
    \item[RQ2] How does it compare to existing static analysis tools?
    \item[RQ3] What is the false positive rate in practice?
    \item[RQ4] What is the performance overhead?
\end{description}

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate on two benchmarks:

\begin{enumerate}
    \item \textbf{OWASP LLM Testbed}: A curated set of 73 vulnerable code samples covering the OWASP LLM Top 10 categories, with ground truth labels.

    \item \textbf{Security Commit Benchmark}: 75 Python files extracted from 23 security-related commits in production LLM frameworks (AutoGen, LangChain), representing real vulnerabilities fixed by developers.
\end{enumerate}

\paragraph{Baselines.}
We compare against three widely-used static analysis tools:
\begin{itemize}
    \item \textbf{Semgrep}: Pattern-based analyzer with community security rules
    \item \textbf{Bandit}: Python-specific security linter
    \item \textbf{CodeQL}: Query-based static analysis (where rules available)
\end{itemize}

\paragraph{Metrics.}
We report precision, recall, and F1 score at the file level. A true positive is a finding in a file containing a known vulnerability; a false positive is a finding in a file without known vulnerabilities.

\subsection{RQ1: Detection of Real Vulnerabilities}

\begin{table}[h]
\centering
\caption{Detection performance on benchmark datasets}
\label{tab:rq1-results}
\begin{tabular}{lccccc}
\toprule
Benchmark & Precision & Recall & F1 & TP & FP \\
\midrule
OWASP Testbed & 0.759 & 0.603 & 0.672 & 44 & 14 \\
Security Commits & \textbf{1.000} & 0.432 & 0.603 & 35 & 0 \\
\midrule
\textbf{Combined} & 0.848 & 0.534 & 0.655 & 79 & 14 \\
\bottomrule
\end{tabular}
\end{table}

Key findings:
\begin{itemize}
    \item On the OWASP testbed, semantic taint achieves 76\% precision and 60\% recall, detecting 44 of 73 labeled vulnerabilities.
    \item On real security commits, precision is \textbf{100\%}---every finding corresponds to a file that was part of a security fix.
    \item The lower recall on security commits (43\%) is expected: many files in security commits are test files or infrastructure changes, not vulnerable production code.
\end{itemize}

\subsection{RQ2: Comparison with Existing Tools}

\begin{table}[h]
\centering
\caption{Comparison on OWASP LLM Testbed}
\label{tab:rq2-comparison}
\begin{tabular}{lcccc}
\toprule
Tool & Precision & Recall & F1 & LLM-Specific \\
\midrule
\textbf{Semantic Taint (Ours)} & \textbf{0.759} & \textbf{0.603} & \textbf{0.672} & \cmark \\
Semgrep & 0.833 & 0.068 & 0.127 & \xmark \\
Bandit & 0.583 & 0.384 & 0.463 & \xmark \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Analysis.}
Semgrep achieves high precision (83\%) but extremely low recall (6.8\%), detecting only 5 of 73 vulnerabilities. This is because Semgrep lacks rules for LLM-specific patterns---it only catches traditional injection vulnerabilities that happen to co-occur with LLM usage.

Bandit detects more issues (38\% recall) but with lower precision (58\%), as it flags general security concerns (e.g., \texttt{subprocess} usage) without understanding the LLM-mediated data flow.

Our approach achieves the best F1 score (0.672) by combining high precision with substantially better recall than both baselines.

\subsection{RQ3: False Positive Analysis}

We manually analyzed all 14 false positives from the OWASP testbed evaluation:

\begin{table}[h]
\centering
\caption{False positive categorization}
\label{tab:fp-analysis}
\begin{tabular}{lcc}
\toprule
Category & Count & Percentage \\
\midrule
Validated before sink & 5 & 36\% \\
Sink not actually dangerous & 4 & 29\% \\
Test/example code & 3 & 21\% \\
Incorrect LLM call detection & 2 & 14\% \\
\bottomrule
\end{tabular}
\end{table}

The majority of false positives (65\%) occur when either (1) the code validates LLM output before using it at a sink, or (2) the detected "sink" is not dangerous in context (e.g., logging). These cases could be addressed by more sophisticated validation detection, which we leave for future work.

\subsection{RQ4: Performance}

\begin{table}[h]
\centering
\caption{Scan performance on real-world projects}
\label{tab:performance}
\begin{tabular}{lccc}
\toprule
Project & Files & Time (s) & ms/file \\
\midrule
dspy & 231 & 2.1 & 9.1 \\
guidance & 149 & 1.2 & 8.1 \\
langchain & 2,501 & 21.3 & 8.5 \\
llama\_index & 4,088 & 35.7 & 8.7 \\
\midrule
\textbf{Total} & 6,969 & 60.3 & 8.7 \\
\bottomrule
\end{tabular}
\end{table}

Our analysis processes approximately \textbf{115 files per second}, making it suitable for CI/CD integration. The overhead is dominated by Python AST parsing, not the semantic influence computation.

\subsection{Ablation Study}

To understand the contribution of semantic influence tracking, we compare three configurations:

\begin{table}[h]
\centering
\caption{Ablation study on OWASP testbed}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
Configuration & Precision & Recall & F1 & $\Delta$F1 \\
\midrule
Pattern-only & 0.759 & 0.603 & 0.672 & --- \\
+ Semantic Taint & 0.759 & 0.603 & 0.672 & +0.0\% \\
+ ML Classifier & 0.710 & 0.603 & 0.652 & -3.0\% \\
\bottomrule
\end{tabular}
\end{table}

The semantic taint analysis maintains precision while enabling detection of LLM-mediated flows that pure pattern matching would miss. The ML classifier adds attack vector classification (direct/indirect/stored) at the cost of slightly lower precision due to additional false positives.
