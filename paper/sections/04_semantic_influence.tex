\section{Semantic Influence Model}
\label{sec:semantic-influence}

Traditional taint analysis tracks the flow of untrusted data through program operations, marking outputs as tainted when they depend syntactically on tainted inputs. However, this model breaks down at LLM API boundaries: the LLM generates a \emph{new} string that has no syntactic relationship to its inputs, yet remains \emph{semantically influenced} by them. We introduce a formal model to capture this influence.

\subsection{Definitions}

\begin{definition}[Taint State]
A \emph{taint state} $\tau$ is a tuple $(T, I)$ where:
\begin{itemize}
    \item $T \subseteq \{\textsc{User}, \textsc{File}, \textsc{Network}, \textsc{Semantic}\}$ is a set of taint types
    \item $I \in [0, 1]$ is the \emph{influence strength}
\end{itemize}
A variable $v$ is \emph{tainted} if $\tau(v).T \neq \emptyset$.
\end{definition}

\begin{definition}[Semantic Influence]
Let $x$ be a tainted variable with $\tau(x) = (T_x, I_x)$, and let $f_{\text{LLM}}$ be an LLM API call. The output $y = f_{\text{LLM}}(x, c)$, where $c$ is additional context, carries \emph{semantic influence} from $x$:
\[
\tau(y) = (T_x \cup \{\textsc{Semantic}\}, \alpha \cdot I_x)
\]
where $\alpha \in (0, 1)$ is the \emph{position-dependent decay factor}.
\end{definition}

The key insight is that while the LLM output $y$ is a newly generated string with no syntactic dependency on $x$, an adversary who controls $x$ can influence the semantic content of $y$ through prompt injection attacks. This influence propagates to any sink that consumes $y$.

\subsection{Influence Decay Model}

Not all inputs to an LLM have equal influence over its output. We model this through position-dependent decay factors:

\begin{definition}[Position-Dependent Decay]
For an LLM call $f_{\text{LLM}}(p, s, c)$ with prompt $p$, system message $s$, and context $c$, the decay factor $\alpha$ is:
\[
\alpha(x) = \begin{cases}
    0.80 & \text{if } x \in p \text{ (user prompt)} \\
    0.70 & \text{if } x \in c \text{ (context/history)} \\
    0.50 & \text{if } x \in s \text{ (system prompt)}
\end{cases}
\]
\end{definition}

The rationale is grounded in prompt injection research and empirical evaluation:

\begin{itemize}
    \item \textbf{$\alpha_p = 0.80$ (user prompt)}: Greshake et al.~\cite{greshake2023} demonstrated attack success rates up to 85\% for user prompts in indirect injection scenarios. We use 0.80 as a conservative estimate.
    \item \textbf{$\alpha_c = 0.70$ (context)}: Liu et al.~\cite{liu2023prompt} found context injection succeeds at 60-80\% depending on context length and position. We use 0.70 as a midpoint.
    \item \textbf{$\alpha_s = 0.50$ (system prompt)}: System prompt overrides succeed in approximately 50\% of cases~\cite{perez2022}, as models are trained to prioritize system instructions but can be overridden.
\end{itemize}

We then tuned $\alpha_p$ on our OWASP testbed (Section~\ref{sec:sensitivity}), selecting 0.80 as it optimizes precision while maintaining recall. To validate these parameters beyond the primary testbed, we created a synthetic benchmark with 15 vulnerabilities specifically targeting each decay parameter (4 for $\alpha_c$, 6 for $\alpha_s$, 5 for $\gamma$), available in the evaluation repository. \emph{Limitation}: Parameters remain literature-informed heuristics; learning optimal values from labeled data is future work.

\begin{definition}[Multi-Hop Decay]
For a chain of $n$ LLM calls, influence decays exponentially:
\[
I_n = I_0 \cdot \left(\prod_{i=1}^{n} \alpha_i\right) \cdot \gamma^n
\]
where $\alpha_i$ is the position-dependent decay at hop $i$, and $\gamma = 0.9$ is an additional per-hop attenuation factor. This models the empirical observation that each LLM transformation dilutes adversarial control beyond what position alone predicts. For example, a two-hop flow through user prompts yields $I_2 = 1.0 \cdot 0.80^2 \cdot 0.9^2 = 0.52$.
\end{definition}

\subsection{Taint Propagation Rules}

We define propagation rules that extend classical taint analysis to handle LLM transformations:

\begin{equation}
\frac{x = \texttt{input()}}{(T, I) \leftarrow (\{\textsc{User}\}, 1.0)} \quad \textsc{(Source)}
\end{equation}

\begin{equation}
\frac{y = f(x) \quad \tau(x) = (T, I) \quad f \text{ is data-preserving}}{(T', I') \leftarrow (T, I)} \quad \textsc{(Propagate)}
\end{equation}

\begin{equation}
\frac{y = f_{\text{LLM}}(x, c) \quad \tau(x) = (T, I)}{(T', I') \leftarrow (T \cup \{\textsc{Semantic}\}, \alpha \cdot I)} \quad \textsc{(LLM-Transform)}
\end{equation}

\begin{equation}
\frac{\texttt{sink}(y) \quad \tau(y) = (T, I) \quad \textsc{Semantic} \in T \quad I > \theta}{\textsc{Vulnerability Detected}} \quad \textsc{(Sink-Check)}
\end{equation}

where $\theta$ is a configurable detection threshold (default: 0.5).

\subsection{Dangerous Sink Categories}

We define a catalog of dangerous sinks where semantically-influenced data can cause security violations:

\begin{table}[h]
\centering
\caption{Dangerous Sink Categories}
\label{tab:sinks}
\begin{tabular}{llll}
\toprule
Category & Examples & CWE & Severity \\
\midrule
Code Execution & \texttt{eval}, \texttt{exec} & CWE-94 & Critical \\
Command Injection & \texttt{os.system}, \texttt{subprocess} & CWE-78 & Critical \\
SQL Injection & \texttt{cursor.execute} & CWE-89 & High \\
SSRF & \texttt{requests.get} & CWE-918 & High \\
File Operations & \texttt{open}, \texttt{write} & CWE-73 & Medium \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Soundness Considerations}

Our analysis is \emph{sound} under the following assumptions:
\begin{enumerate}
    \item \textbf{LLM Controllability}: An adversary with control over LLM input can influence its output. This is empirically validated by prompt injection research~\cite{greshake2023,perez2022}.
    \item \textbf{Monotonic Decay}: Influence can only decrease through transformations, never increase. This is conservative: we may miss vulnerabilities where the LLM amplifies malicious intent.
    \item \textbf{Sink Completeness}: Our sink catalog covers the primary attack vectors. Additional sinks can be added without modifying the core model.
\end{enumerate}

The analysis may produce false positives when:
\begin{itemize}
    \item Input validation occurs after the LLM call but before the sink
    \item The LLM is constrained to produce outputs in a safe format (e.g., structured JSON with schema validation)
    \item The sink is not actually dangerous in context (e.g., logging)
\end{itemize}

We address these through confidence adjustment based on detected mitigations (Section~\ref{sec:implementation}).

\subsection{Per-File Interprocedural Analysis}
\label{sec:interprocedural}

A significant challenge in LLM application security is the prevalence of \emph{wrapper functions}---functions that encapsulate LLM API calls to provide a cleaner interface. We support interprocedural analysis \emph{within individual files}; cross-file analysis is left for future work. Consider:

\begin{lstlisting}[language=Python,basicstyle=\small\ttfamily]
def get_llm_response(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def vulnerable_eval(user_request: str) -> str:
    prompt = f"Generate code for: {user_request}"
    llm_code = get_llm_response(prompt)  # Wrapper call
    return eval(llm_code)  # VULNERABLE
\end{lstlisting}

Intra-procedural analysis cannot detect this vulnerability because the taint flow crosses function boundaries. We implement two inter-procedural analysis modes:

\subsubsection{Function Summary Analysis}
We construct \emph{function summaries} that capture each function's taint behavior:

\begin{definition}[Function Summary]
A function summary $\mathcal{S}_f$ is a tuple $(C, R, P, H, D)$ where:
\begin{itemize}
    \item $C \in \{\top, \bot\}$ indicates whether $f$ contains an LLM call
    \item $R \in \{\top, \bot\}$ indicates whether $f$ returns LLM-tainted data
    \item $P \subseteq \text{params}(f)$ is the set of parameters that flow to the return value
    \item $H \in \mathbb{N}$ is the number of LLM hops added by $f$
    \item $D \in [0, 1]$ is the influence decay through $f$
\end{itemize}
\end{definition}

During analysis, we first identify \emph{LLM wrapper functions}---functions where $C = \top$ and $R = \top$. Then, for each call to a wrapper function, we propagate semantic taint to the call's return value with appropriate decay.

\subsubsection{Extended Analysis (Experimental)}
For cases where function summaries are insufficient, we provide an experimental fixed-point iteration mode that propagates taint through function calls and returns until convergence. This handles nested wrapper calls and multi-function flows.

The tradeoff is computational cost: function summaries are $O(F)$ where $F$ is the number of functions, while fixed-point analysis is $O(F \cdot E)$ where $E$ is the number of edges in the call graph. \emph{Limitation}: Our evaluation (Section~\ref{sec:evaluation}) demonstrates function summary analysis on a single synthetic module; the fixed-point mode has not been extensively validated. In practice, function summaries suffice for the wrapper patterns we observe in case study codebases.
