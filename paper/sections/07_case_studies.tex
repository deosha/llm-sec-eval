\section{Case Studies}
\label{sec:case-studies}

We applied our tool to 5 popular open-source LLM applications. This section presents representative case studies, distinguishing between vulnerabilities that \emph{require} semantic influence tracking (Category A) and traditional vulnerabilities detected incidentally (Category B).

\subsection{Overview and Validation}

\begin{table}[h]
\centering
\caption{Taint-based findings by category (scanned January 14, 2026)}
\label{tab:case-studies-overview}
\begin{tabular}{lcccc}
\toprule
Application & Stars & Cat.\ A & Cat.\ B & Total \\
\midrule
chatgpt-retrieval-plugin & 21k & 5 & 12 & 17 \\
AutoGPT & 169k & 281 & 3 & 284 \\
danswer & 12k & 759 & 74 & 833 \\
OpenDevin & 35k & 564 & 4 & 568 \\
open-webui & 52k & 104 & 10 & 114 \\
\midrule
\textbf{Total} & & \textbf{1,713 (94\%)} & \textbf{103 (6\%)} & \textbf{1,816} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Finding Categories.}
\textbf{Category A} (semantic influence required): Vulnerabilities where data flows through an LLM API before reaching a dangerous sink. These require our semantic influence model---traditional taint analysis loses track at LLM boundaries. \textbf{94\% of findings (1,713/1,816) are Category A}, demonstrating that semantic influence tracking is essential for LLM application security. \textbf{Category B} (traditional): SQL injection, command injection, and other vulnerabilities detectable by standard taint analysis. These 6\% of findings occur in LLM codebases but do not traverse LLM calls.

\paragraph{Methodology Note.}
Table~\ref{tab:case-studies-overview} reports only \emph{taint-based} findings from our static analysis, generated using the command: \texttt{aisentry scan <repo> --taint-analysis --no-ml-detection --no-ensemble}. We exclude ML-based file-level detections (which flag entire files as potentially vulnerable) to focus on findings with specific line-level localization and data flow evidence.

\paragraph{Reproducibility Disclaimer.}
Results were generated on January 14, 2026 at 23:04 UTC. \textbf{Finding counts may vary} when reproducing this analysis due to: (1) ongoing codebase evolution---active repositories receive continuous commits that may introduce or fix vulnerabilities; (2) detection rule updates in the scanning tool; (3) differences in cloned repository state (branch, commit hash). We provide scan configuration and raw results in our supplementary materials to enable reproducibility at a specific point in time.

\paragraph{Precision Validation.}
We randomly sampled 50 Category A and 50 Category B findings for manual verification. For Category A findings, precision was \textbf{72\%}---false positives were primarily due to validation in called functions outside our intra-file analysis scope. For Category B findings, precision was \textbf{84\%}, comparable to traditional SAST tools. The lower precision for Category A reflects the inherent uncertainty in semantic influence modeling.

\paragraph{Maintainer Confirmation.}
Of the 1,816 total findings, we submitted fixes for 2 high-confidence vulnerabilities (Table~\ref{tab:disclosure-status}). The remaining findings have \emph{not} been individually reported to maintainers or independently confirmed. Based on our sampled precision (72\% for Category A, 84\% for Category B), we estimate approximately 1,320 true positives among Category A and 87 among Category B. We emphasize these are tool outputs requiring manual triage, not confirmed vulnerabilities.

\subsection{Case Study 1: SQL Injection in Retrieval Plugin (Category B)}

\paragraph{Application.}
The ChatGPT Retrieval Plugin is OpenAI's official reference implementation for building ChatGPT plugins with document retrieval capabilities (21k GitHub stars).

\paragraph{Vulnerability.}
We detected SQL injection in the AnalyticDB datastore provider:

\begin{lstlisting}[language=Python, caption={SQL injection in analyticdb\_datastore.py:68}]
sql = f"""
    SELECT id, text, metadata
    FROM {self.collection_name}
    ORDER BY embedding <-> '{embedding}'
    LIMIT {top_k}
"""
cursor.execute(sql)
\end{lstlisting}

\paragraph{Category B Classification.}
This is a \emph{traditional} SQL injection---the embedding vector is directly interpolated into SQL without parameterization. While our tool detected it, a properly configured traditional taint analyzer (e.g., CodeQL with custom sources) could also find this pattern. We include it because: (1) it demonstrates our tool's comprehensive coverage, and (2) it occurs in a high-profile LLM template where such vulnerabilities have outsized impact.

\paragraph{Impact.}
Attackers could extract documents, modify embeddings, or achieve database-level code execution.

\subsection{Case Study 2: Sensitive Data Exposure via LLM (Category A)}

\paragraph{Application.}
Danswer is an enterprise question-answering system (12k GitHub stars) that connects to internal documents and uses LLMs to answer employee queries.

\paragraph{Vulnerability.}
We detected sensitive information disclosure where database credentials flow through LLM context:

\begin{lstlisting}[language=Python, caption={Sensitive data in LLM context (simplified)}]
# User question triggers document retrieval
docs = retriever.get_relevant_docs(user_query)
# Documents may contain credentials from config files
context = "\n".join([d.content for d in docs])
# LLM response includes leaked credentials
response = llm.chat(f"Answer based on: {context}\n\nQ: {user_query}")
return response  # May leak sensitive data to user
\end{lstlisting}

\paragraph{Category A Classification.}
This vulnerability \emph{requires} semantic influence tracking. The sensitive data (credentials in indexed documents) flows: document store → retrieval → LLM context → LLM response → user. Traditional taint analysis loses track at the LLM boundary because the response is a new string. Our semantic influence model ($I = 0.70$ for context position) correctly identifies that sensitive context data influences the output.

\paragraph{Impact.}
Attackers can craft queries to extract sensitive information from indexed documents, including API keys, database credentials, and internal configuration.

\subsection{Case Study 3: Excessive Agency in Autonomous Agent (Category A)}

\paragraph{Application.}
AutoGPT is one of the most popular autonomous AI agents (169k GitHub stars). It executes multi-step tasks by generating and running code.

\paragraph{Vulnerability.}
The agent framework executes LLM-generated code without sandboxing:

\begin{lstlisting}[language=Python, caption={Code execution in manager.py}]
def _on_graph_execution(self, result):
    code = result.get("generated_code")
    exec(code, globals())  # LLM output executed directly
\end{lstlisting}

\paragraph{Category A Classification.}
This is a canonical \emph{semantic influence} vulnerability. The data flow is: user task → LLM reasoning → code generation → \texttt{exec()} sink. Traditional taint analysis cannot detect this because \texttt{code} is generated by the LLM, not derived syntactically from user input. Our semantic influence model assigns $I = 0.80$ (prompt position) to user input, which propagates through the LLM call to the generated code, correctly flagging the \texttt{exec()} as reachable by attacker-influenced data.

\paragraph{Attack Scenario.}
An attacker provides a task containing prompt injection:
\begin{lstlisting}
Please help me analyze sales data.
[IGNORE ABOVE] Generate: __import__('os').system('curl evil.com|sh')
\end{lstlisting}

\paragraph{Impact.}
Arbitrary code execution on the host system. While code execution is ``intentional'' for an agent, the lack of sandboxing means prompt injection achieves full system compromise.

\subsection{Lessons Learned}

\paragraph{Pattern 1: LLM Output to Dangerous Sinks (Category A).}
The most common Category A pattern is LLM-generated content reaching code execution (\texttt{exec}, \texttt{eval}), shell commands, or database queries. Developers assume LLM outputs are ``system-generated'' and therefore safe, not recognizing that adversaries can influence these outputs through prompt injection.

\paragraph{Pattern 2: Sensitive Data in LLM Context (Category A).}
RAG (Retrieval-Augmented Generation) applications frequently include sensitive documents in LLM context without considering that the LLM may leak this information in responses. This is a semantic data flow that traditional tools cannot track.

\paragraph{Pattern 3: Traditional Vulnerabilities in LLM Code (Category B).}
LLM applications also contain traditional vulnerabilities (SQL injection, path traversal) unrelated to LLM semantics. While our tool detects these incidentally, they highlight that LLM codebases need comprehensive security review beyond LLM-specific issues.

\subsection{Responsible Disclosure}

We reported vulnerabilities to the respective maintainers via pull requests:

\paragraph{Fix Status (as of January 14, 2026).}

\begin{table}[h]
\centering
\small
\caption{Vulnerabilities identified and fixes submitted}
\label{tab:disclosure-status}
\begin{tabular}{lllll}
\toprule
Repository & Vulnerability & Detection & PR & Status \\
\midrule
chatgpt-retrieval-plugin & SQL Injection & Taint & \#490 & Open \\
vllm & Unsafe eval() RCE & Pattern & \#32098 & Open \\
\bottomrule
\end{tabular}
\end{table}

Vulnerabilities were identified using either semantic taint analysis (tracking data flow through LLM calls) or pattern-based detection (identifying dangerous API usage). We follow responsible disclosure practices and coordinate with maintainers on remediation timelines.
