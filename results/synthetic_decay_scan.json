{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T19:03:13.919966Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation",
    "files_scanned": 3,
    "overall_score": 3.49,
    "confidence": 0.6,
    "duration_seconds": 0.485,
    "findings_count": 103,
    "severity_breakdown": {
      "CRITICAL": 60,
      "HIGH": 15,
      "MEDIUM": 25,
      "LOW": 0,
      "INFO": 3
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 21,
      "confidence": 0.42,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "1 jailbreak prevention mechanisms active"
      ],
      "gaps": [
        "No input validation detected",
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "model_protection": 0,
        "extraction_defense": 0,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.84,
      "subscores": {
        "LLM01": 0,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 100
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 11 critical",
        "Insecure Output Handling: 12 critical",
        "Model Denial of Service: 11 critical",
        "Supply Chain Vulnerabilities: 6 high, 24 medium",
        "Excessive Agency: 13 critical, 9 high, 1 medium",
        "Overreliance: 12 critical",
        "TAINT01: 1 critical"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_18",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_input' embedded in LLM prompt",
      "description": "User input parameter 'user_input' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 18,
      "code_snippet": "    # Hop 1: Summarization\n    summary_response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_41",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_request' embedded in LLM prompt",
      "description": "User input parameter 'user_request' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 41,
      "code_snippet": "    # Hop 1: Translation\n    translated = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_69",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_query' embedded in LLM prompt",
      "description": "User input parameter 'user_query' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 69,
      "code_snippet": "    # Hop 1\n    step1 = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_102",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_data' embedded in LLM prompt",
      "description": "User input parameter 'user_data' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 102,
      "code_snippet": "    # Branch 1: Analysis path\n    analysis = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_108",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_data' embedded in LLM prompt",
      "description": "User input parameter 'user_data' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 108,
      "code_snippet": "    # Branch 2: Extraction path\n    extracted = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_31",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'code' flows to 'exec' on line 31 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 31,
      "code_snippet": "\n    exec(code)  # SINK: 2-hop chain to code execution\n\n",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_59",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'command' flows to 'subprocess.run' on line 59 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 59,
      "code_snippet": "    import subprocess\n    subprocess.run(command, shell=True)  # SINK: 3-hop chain\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_92",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'sql' flows to 'db.execute' on line 92 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 92,
      "code_snippet": "\n    db.execute(sql)  # SINK: 4-hop chain (may be below threshold)\n\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_119",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'combined' flows to 'exec' on line 119 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 119,
      "code_snippet": "\n    exec(combined)  # SINK: Merged multi-hop\n\n",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_143",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'code' flows to 'exec' on line 143 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 143,
      "code_snippet": "\n    exec(code)  # SINK: After N+1 hops",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_12",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'two_hop_code_execution' on line 12 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 12,
      "code_snippet": "def two_hop_code_execution(user_input: str):\n    \"\"\"\n    Vulnerability: User input -> LLM1 (summarize) -> LLM2 (code gen) -> exec\n    Tests: 2-hop decay (\u03b3^2)\n    \"\"\"\n    # Hop 1: Summarization\n    summary_response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Summarize: {user_input}\"}]\n    )\n    summary = summary_response.choices[0].message.content",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_35",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'three_hop_command_injection' on line 35 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 35,
      "code_snippet": "def three_hop_command_injection(user_request: str):\n    \"\"\"\n    Vulnerability: User -> LLM1 (translate) -> LLM2 (plan) -> LLM3 (command) -> shell\n    Tests: 3-hop decay (\u03b3^3)\n    \"\"\"\n    # Hop 1: Translation\n    translated = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Translate to English: {user_request}\"}]\n    ).choices[0].message.content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_63",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'four_hop_sql_injection' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 63,
      "code_snippet": "def four_hop_sql_injection(user_query: str, db):\n    \"\"\"\n    Vulnerability: User -> LLM1 -> LLM2 -> LLM3 -> LLM4 -> SQL\n    Tests: 4-hop decay (should be below detection threshold)\n    \"\"\"\n    # Hop 1\n    step1 = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Rephrase: {user_query}\"}]\n    ).choices[0].message.content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_96",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'branching_multi_hop' on line 96 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 96,
      "code_snippet": "def branching_multi_hop(user_data: str):\n    \"\"\"\n    Vulnerability: User input branches into parallel LLM chains that merge\n    Tests: Complex multi-hop topology\n    \"\"\"\n    # Branch 1: Analysis path\n    analysis = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Analyze: {user_data}\"}]\n    ).choices[0].message.content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_123",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'iterative_refinement_chain' on line 123 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 123,
      "code_snippet": "def iterative_refinement_chain(user_spec: str, iterations: int = 3):\n    \"\"\"\n    Vulnerability: User spec refined through N iterations of LLM calls\n    Tests: Iterative hop decay\n    \"\"\"\n    current = user_spec\n\n    for i in range(iterations):\n        refined = openai.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": f\"Refine and improve: {current}\"}]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_18_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 18. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 18,
      "code_snippet": "    \"\"\"\n    # Hop 1: Summarization\n    summary_response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Summarize: {user_input}\"}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_25_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 25. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 25,
      "code_snippet": "\n    # Hop 2: Code generation from summary\n    code_response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Write Python for: {summary}\"}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_41_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 41. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 41,
      "code_snippet": "    \"\"\"\n    # Hop 1: Translation\n    translated = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Translate to English: {user_request}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_47_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 47. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 47,
      "code_snippet": "\n    # Hop 2: Task planning\n    plan = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Create action plan for: {translated}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_53_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 53. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 53,
      "code_snippet": "\n    # Hop 3: Command generation\n    command = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Generate shell command for: {plan}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_69_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 69. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 69,
      "code_snippet": "    \"\"\"\n    # Hop 1\n    step1 = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Rephrase: {user_query}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_75_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 75. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 75,
      "code_snippet": "\n    # Hop 2\n    step2 = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract entities from: {step1}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_81_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 81. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 81,
      "code_snippet": "\n    # Hop 3\n    step3 = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Structure as query spec: {step2}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_87_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 87. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 87,
      "code_snippet": "\n    # Hop 4\n    sql = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Generate SQL: {step3}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_102_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 102. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 102,
      "code_snippet": "    \"\"\"\n    # Branch 1: Analysis path\n    analysis = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Analyze: {user_data}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_108_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 108. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 108,
      "code_snippet": "\n    # Branch 2: Extraction path\n    extracted = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract key points: {user_data}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_114_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 114. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 114,
      "code_snippet": "\n    # Merge: Combine both branches\n    combined = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Combine:\\n{analysis}\\n{extracted}\\nGenerate code:\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_138_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 138. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 138,
      "code_snippet": "\n    # Final code generation\n    code = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": f\"Convert to Python: {current}\"}]\n    ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_131_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 131. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 131,
      "code_snippet": "\n    for i in range(iterations):\n        refined = openai.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": f\"Refine and improve: {current}\"}]\n        ).choices[0].message.content",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_31_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 31. ",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 31,
      "code_snippet": "    code = code_response.choices[0].message.content\n\n    exec(code)  # SINK: 2-hop chain to code execution\n\n\n# VULNERABLE: 3-hop chain (\u03b3^3 = 0.729 at default)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_119_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 119. ",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 119,
      "code_snippet": "    ).choices[0].message.content\n\n    exec(combined)  # SINK: Merged multi-hop\n\n\n# VULNERABLE: Recursive/iterative chain",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_143_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 143. ",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 143,
      "code_snippet": "    ).choices[0].message.content\n\n    exec(code)  # SINK: After N+1 hops",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_31_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to code execution in 'two_hop_code_execution'",
      "description": "In function 'two_hop_code_execution', LLM output variable 'code' flows to 'exec' on line 31 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 31,
      "code_snippet": "    code = code_response.choices[0].message.content\n\n    exec(code)  # SINK: 2-hop chain to code execution\n\n\n# VULNERABLE: 3-hop chain (\u03b3^3 = 0.729 at default)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_59_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'three_hop_command_injection'",
      "description": "In function 'three_hop_command_injection', LLM output variable 'command' flows to 'subprocess.run' on line 59 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 59,
      "code_snippet": "\n    import subprocess\n    subprocess.run(command, shell=True)  # SINK: 3-hop chain\n\n\n# VULNERABLE: 4-hop chain (\u03b3^4 = 0.656 at default, below threshold)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_92_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'four_hop_sql_injection'",
      "description": "In function 'four_hop_sql_injection', LLM output variable 'sql' flows to 'db.execute' on line 92 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 92,
      "code_snippet": "    ).choices[0].message.content\n\n    db.execute(sql)  # SINK: 4-hop chain (may be below threshold)\n\n\n# VULNERABLE: Branching multi-hop",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_119_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to code execution in 'branching_multi_hop'",
      "description": "In function 'branching_multi_hop', LLM output variable 'combined' flows to 'exec' on line 119 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 119,
      "code_snippet": "    ).choices[0].message.content\n\n    exec(combined)  # SINK: Merged multi-hop\n\n\n# VULNERABLE: Recursive/iterative chain",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_143_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to code execution in 'iterative_refinement_chain'",
      "description": "In function 'iterative_refinement_chain', LLM output variable 'code' flows to 'exec' on line 143 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 143,
      "code_snippet": "    ).choices[0].message.content\n\n    exec(code)  # SINK: After N+1 hops",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_12_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'two_hop_code_execution'",
      "description": "Function 'two_hop_code_execution' on line 12 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 12,
      "code_snippet": "\n# VULNERABLE: 2-hop chain (\u03b3^2 = 0.81 at default)\ndef two_hop_code_execution(user_input: str):\n    \"\"\"\n    Vulnerability: User input -> LLM1 (summarize) -> LLM2 (code gen) -> exec\n    Tests: 2-hop decay (\u03b3^2)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_35_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'three_hop_command_injection'",
      "description": "Function 'three_hop_command_injection' on line 35 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 35,
      "code_snippet": "\n# VULNERABLE: 3-hop chain (\u03b3^3 = 0.729 at default)\ndef three_hop_command_injection(user_request: str):\n    \"\"\"\n    Vulnerability: User -> LLM1 (translate) -> LLM2 (plan) -> LLM3 (command) -> shell\n    Tests: 3-hop decay (\u03b3^3)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_63_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'four_hop_sql_injection'",
      "description": "Function 'four_hop_sql_injection' on line 63 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 63,
      "code_snippet": "\n# VULNERABLE: 4-hop chain (\u03b3^4 = 0.656 at default, below threshold)\ndef four_hop_sql_injection(user_query: str, db):\n    \"\"\"\n    Vulnerability: User -> LLM1 -> LLM2 -> LLM3 -> LLM4 -> SQL\n    Tests: 4-hop decay (should be below detection threshold)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_96_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'branching_multi_hop'",
      "description": "Function 'branching_multi_hop' on line 96 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 96,
      "code_snippet": "\n# VULNERABLE: Branching multi-hop\ndef branching_multi_hop(user_data: str):\n    \"\"\"\n    Vulnerability: User input branches into parallel LLM chains that merge\n    Tests: Complex multi-hop topology",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_123_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'iterative_refinement_chain'",
      "description": "Function 'iterative_refinement_chain' on line 123 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 123,
      "code_snippet": "\n# VULNERABLE: Recursive/iterative chain\ndef iterative_refinement_chain(user_spec: str, iterations: int = 3):\n    \"\"\"\n    Vulnerability: User spec refined through N iterations of LLM calls\n    Tests: Iterative hop decay",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_12_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'two_hop_code_execution'",
      "description": "Function 'two_hop_code_execution' on line 12 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 12,
      "code_snippet": "\n# VULNERABLE: 2-hop chain (\u03b3^2 = 0.81 at default)\ndef two_hop_code_execution(user_input: str):\n    \"\"\"\n    Vulnerability: User input -> LLM1 (summarize) -> LLM2 (code gen) -> exec\n    Tests: 2-hop decay (\u03b3^2)",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_35_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'three_hop_command_injection'",
      "description": "Function 'three_hop_command_injection' on line 35 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 35,
      "code_snippet": "\n# VULNERABLE: 3-hop chain (\u03b3^3 = 0.729 at default)\ndef three_hop_command_injection(user_request: str):\n    \"\"\"\n    Vulnerability: User -> LLM1 (translate) -> LLM2 (plan) -> LLM3 (command) -> shell\n    Tests: 3-hop decay (\u03b3^3)",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_63_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'four_hop_sql_injection'",
      "description": "Function 'four_hop_sql_injection' on line 63 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 63,
      "code_snippet": "\n# VULNERABLE: 4-hop chain (\u03b3^4 = 0.656 at default, below threshold)\ndef four_hop_sql_injection(user_query: str, db):\n    \"\"\"\n    Vulnerability: User -> LLM1 -> LLM2 -> LLM3 -> LLM4 -> SQL\n    Tests: 4-hop decay (should be below detection threshold)",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_96_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'branching_multi_hop'",
      "description": "Function 'branching_multi_hop' on line 96 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 96,
      "code_snippet": "\n# VULNERABLE: Branching multi-hop\ndef branching_multi_hop(user_data: str):\n    \"\"\"\n    Vulnerability: User input branches into parallel LLM chains that merge\n    Tests: Complex multi-hop topology",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py_123_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'iterative_refinement_chain'",
      "description": "Function 'iterative_refinement_chain' on line 123 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/multi_hop_chains.py",
      "line_number": 123,
      "code_snippet": "\n# VULNERABLE: Recursive/iterative chain\ndef iterative_refinement_chain(user_spec: str, iterations: int = 3):\n    \"\"\"\n    Vulnerability: User spec refined through N iterations of LLM calls\n    Tests: Iterative hop decay",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_20",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_query' embedded in LLM prompt",
      "description": "User input parameter 'user_query' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 20,
      "code_snippet": "\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_93",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 93,
      "code_snippet": "\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_116",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_request' embedded in LLM prompt",
      "description": "User input parameter 'user_request' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 116,
      "code_snippet": "\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_136",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'context_docs' embedded in LLM prompt",
      "description": "User input parameter 'context_docs' is directly passed to LLM API call 'openai.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 136,
      "code_snippet": "\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_28",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'code' flows to 'exec' on line 28 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 28,
      "code_snippet": "    code = response.choices[0].message.content\n    exec(code)  # SINK: Code execution from system-influenced output\n\n",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_49",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to 'subprocess.run' on line 49 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 49,
      "code_snippet": "    import subprocess\n    subprocess.run(response.choices[0].message.content, shell=True)  # SINK\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_74",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'sql' flows to 'execute_sql' on line 74 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 74,
      "code_snippet": "    # SINK: SQL from system-template-influenced output\n    return execute_sql(sql)\n\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_143",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'response' flows to 'eval' on line 143 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 143,
      "code_snippet": "    )\n    eval(response.choices[0].message.content)  # SINK",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_13",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'configurable_system_prompt_v1' on line 13 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 13,
      "code_snippet": "def configurable_system_prompt_v1(user_persona: str, user_query: str):\n    \"\"\"\n    Vulnerability: User controls persona in system prompt\n    Tests: \u03b1_s decay (system position = 0.50)\n    \"\"\"\n    system_prompt = f\"You are a {user_persona}. Help the user with their request.\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # User-influenced system",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_84",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'dynamic_system_from_config' on line 84 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 84,
      "code_snippet": "def dynamic_system_from_config(config_override: str, query: str):\n    \"\"\"\n    Vulnerability: Config override influences system behavior\n    Tests: \u03b1_s for configuration injection\n    \"\"\"\n    base_system = \"You are a helpful assistant.\"\n    if config_override:\n        base_system = f\"{base_system}\\nConfig: {config_override}\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_107",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'multi_tenant_system_injection' on line 107 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 107,
      "code_snippet": "def multi_tenant_system_injection(tenant_config: str, user_request: str):\n    \"\"\"\n    Vulnerability: Tenant-specific config in system prompt\n    Tests: \u03b1_s for multi-tenant scenarios\n    \"\"\"\n    system = f\"\"\"You are an assistant for tenant.\nTenant configuration: {tenant_config}\nFollow tenant-specific rules when generating responses.\"\"\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_129",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'combined_system_context_injection' on line 129 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 129,
      "code_snippet": "def combined_system_context_injection(system_addon: str, context_docs: str, query: str):\n    \"\"\"\n    Vulnerability: User influences both system prompt AND context\n    Tests: Combined \u03b1_s and \u03b1_c effects\n    \"\"\"\n    system = f\"You are an assistant. Additional rules: {system_addon}\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system},",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_20_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 20. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 20,
      "code_snippet": "    system_prompt = f\"You are a {user_persona}. Help the user with their request.\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},  # User-influenced system",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_41_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 41. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 41,
      "code_snippet": "Always generate safe, validated code.\"\"\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_65_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 65,
      "code_snippet": "    system_prompt = system_template.format(**template_vars)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_93_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 93. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 93,
      "code_snippet": "        base_system = f\"{base_system}\\nConfig: {config_override}\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": base_system},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_116_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 116. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 116,
      "code_snippet": "Follow tenant-specific rules when generating responses.\"\"\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_136_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 136. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 136,
      "code_snippet": "    system = f\"You are an assistant. Additional rules: {system_addon}\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_143_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 143. ",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 143,
      "code_snippet": "        ]\n    )\n    eval(response.choices[0].message.content)  # SINK",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_28_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 28. ",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 28,
      "code_snippet": "    )\n    code = response.choices[0].message.content\n    exec(code)  # SINK: Code execution from system-influenced output\n\n\n# VULNERABLE: System prompt with instructions injection",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_28_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to code execution in 'configurable_system_prompt_v1'",
      "description": "In function 'configurable_system_prompt_v1', LLM output variable 'code' flows to 'exec' on line 28 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 28,
      "code_snippet": "    )\n    code = response.choices[0].message.content\n    exec(code)  # SINK: Code execution from system-influenced output\n\n\n# VULNERABLE: System prompt with instructions injection",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_49_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'system_instruction_injection'",
      "description": "In function 'system_instruction_injection', LLM output variable 'response' flows to 'subprocess.run' on line 49 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 49,
      "code_snippet": "    )\n    import subprocess\n    subprocess.run(response.choices[0].message.content, shell=True)  # SINK\n\n\n# VULNERABLE: System prompt template injection",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_74_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'system_template_injection'",
      "description": "In function 'system_template_injection', LLM output variable 'sql' flows to 'execute_sql' on line 74 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 74,
      "code_snippet": "    sql = response.choices[0].message.content\n    # SINK: SQL from system-template-influenced output\n    return execute_sql(sql)\n\n\ndef execute_sql(sql):",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_125_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'multi_tenant_system_injection'",
      "description": "In function 'multi_tenant_system_injection', LLM output variable 'response' flows to 'os.system' on line 125 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 125,
      "code_snippet": "    # SINK: Command execution from tenant-influenced output\n    import os\n    os.system(response.choices[0].message.content)\n\n\n# COMBINED: System + Context injection",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_143_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to code execution in 'combined_system_context_injection'",
      "description": "In function 'combined_system_context_injection', LLM output variable 'response' flows to 'eval' on line 143 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 143,
      "code_snippet": "        ]\n    )\n    eval(response.choices[0].message.content)  # SINK",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_84_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'dynamic_system_from_config'",
      "description": "Function 'dynamic_system_from_config' on line 84 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 84,
      "code_snippet": "\n# VULNERABLE: Dynamic system prompt from config\ndef dynamic_system_from_config(config_override: str, query: str):\n    \"\"\"\n    Vulnerability: Config override influences system behavior\n    Tests: \u03b1_s for configuration injection",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_13_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'configurable_system_prompt_v1'",
      "description": "Function 'configurable_system_prompt_v1' on line 13 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 13,
      "code_snippet": "\n# VULNERABLE: Direct system prompt injection\ndef configurable_system_prompt_v1(user_persona: str, user_query: str):\n    \"\"\"\n    Vulnerability: User controls persona in system prompt\n    Tests: \u03b1_s decay (system position = 0.50)",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_32_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'system_instruction_injection'",
      "description": "Function 'system_instruction_injection' on line 32 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 32,
      "code_snippet": "\n# VULNERABLE: System prompt with instructions injection\ndef system_instruction_injection(custom_instructions: str, task: str):\n    \"\"\"\n    Vulnerability: User provides custom instructions added to system prompt\n    Tests: \u03b1_s decay for instruction injection",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_107_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'multi_tenant_system_injection'",
      "description": "Function 'multi_tenant_system_injection' on line 107 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 107,
      "code_snippet": "\n# VULNERABLE: Multi-tenant system prompt\ndef multi_tenant_system_injection(tenant_config: str, user_request: str):\n    \"\"\"\n    Vulnerability: Tenant-specific config in system prompt\n    Tests: \u03b1_s for multi-tenant scenarios",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_129_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'combined_system_context_injection'",
      "description": "Function 'combined_system_context_injection' on line 129 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 129,
      "code_snippet": "\n# COMBINED: System + Context injection\ndef combined_system_context_injection(system_addon: str, context_docs: str, query: str):\n    \"\"\"\n    Vulnerability: User influences both system prompt AND context\n    Tests: Combined \u03b1_s and \u03b1_c effects",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_53_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'system_template_injection'",
      "description": "Function 'system_template_injection' on line 53 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 53,
      "code_snippet": "\n# VULNERABLE: System prompt template injection\ndef system_template_injection(template_vars: dict):\n    \"\"\"\n    Vulnerability: User-controlled template variables in system prompt\n    Tests: \u03b1_s with template-based injection",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_84_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'dynamic_system_from_config'",
      "description": "Function 'dynamic_system_from_config' on line 84 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 84,
      "code_snippet": "\n# VULNERABLE: Dynamic system prompt from config\ndef dynamic_system_from_config(config_override: str, query: str):\n    \"\"\"\n    Vulnerability: Config override influences system behavior\n    Tests: \u03b1_s for configuration injection",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "TAINT01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py_143_eval",
      "category": "Semantic Taint: LLM Output to Dangerous Sink",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output flows to code execution sink",
      "description": "User-controlled data flows through 1 LLM call(s) and reaches a code execution sink at `eval()`.\n\n**Why this is dangerous:** Even though the data is transformed by the LLM, an attacker can craft inputs that influence the LLM output to contain malicious payloads. This is known as 'indirect prompt injection'.\n\n**Taint path:** user_query (user input, line 13) \u2192 response (user input, line 20)\n\n**Influence strength:** 85% (based on input position and LLM hops)\n\n**Source:** Direct user input (highest risk)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/system_prompt_injection.py",
      "line_number": 143,
      "code_snippet": "    140:             {\"role\": \"user\", \"content\": f\"Context: {context_docs}\\n\\nQuery: {query}\"}\n    141:         ]\n    142:     )\n>>> 143:     eval(response.choices[0].message.content)  # SINK",
      "recommendation": "1. NEVER execute LLM output as code. Use structured outputs (JSON schema) instead.\n2. If code execution is required, use a sandboxed environment (e.g., Docker, gVisor).\n3. Implement strict output validation before any execution.\n4. Consider using ast.literal_eval() for simple data parsing instead of eval()."
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_24",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'user_query' embedded in LLM prompt",
      "description": "User input 'user_query' flows to LLM call via call in variable 'context'. Function 'rag_code_execution_v1' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 24,
      "code_snippet": "    \"\"\"\n    retrieved_docs = retrieve_documents(user_query)\n    context = \"\\n\".join(retrieved_docs)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_63",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'document_request' embedded in LLM prompt",
      "description": "User input 'document_request' flows to LLM call via call in variable 'context_docs'. Function 'rag_path_traversal_v1' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 63,
      "code_snippet": "    \"\"\"\n    context_docs = retrieve_documents(document_request)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_35",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'code' flows to 'exec' on line 35 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 35,
      "code_snippet": "    code = response.choices[0].message.content\n    exec(code)  # SINK: Code execution from RAG-influenced LLM output\n\n",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_54",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'sql' flows to 'db_cursor.execute' on line 54 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 54,
      "code_snippet": "    sql = response.choices[0].message.content\n    db_cursor.execute(sql)  # SINK: SQL injection via RAG context\n\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_96",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to 'subprocess.run' on line 96 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 96,
      "code_snippet": "    import subprocess\n    subprocess.run(response.choices[0].message.content, shell=True)  # SINK",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_18",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'rag_code_execution_v1' on line 18 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 18,
      "code_snippet": "def rag_code_execution_v1(user_query: str):\n    \"\"\"\n    Vulnerability: User query -> RAG retrieval -> LLM context -> code execution\n    Attack: User crafts query to inject malicious content into retrieved docs\n    Tests: \u03b1_c decay (context position)\n    \"\"\"\n    retrieved_docs = retrieve_documents(user_query)\n    context = \"\\n\".join(retrieved_docs)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_77",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'rag_multi_doc_injection' on line 77 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 77,
      "code_snippet": "def rag_multi_doc_injection(queries: List[str]):\n    \"\"\"\n    Vulnerability: Multiple user queries -> aggregated context -> LLM -> command\n    Tests: \u03b1_c with aggregated context\n    \"\"\"\n    all_docs = []\n    for q in queries:\n        all_docs.extend(retrieve_documents(q))\n\n    combined_context = \"\\n---\\n\".join(all_docs)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_27_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 27. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 27,
      "code_snippet": "    context = \"\\n\".join(retrieved_docs)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Generate Python code based on context.\"},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_46_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 46. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 46,
      "code_snippet": "    docs = retrieve_documents(search_term)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Convert natural language to SQL.\"},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_65_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 65,
      "code_snippet": "    context_docs = retrieve_documents(document_request)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Based on: {context_docs}\\nWhat file should I read?\"}",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_88_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 88. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 88,
      "code_snippet": "    combined_context = \"\\n---\\n\".join(all_docs)\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful shell assistant.\"},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_35_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 35. ",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 35,
      "code_snippet": "    )\n    code = response.choices[0].message.content\n    exec(code)  # SINK: Code execution from RAG-influenced LLM output\n\n\n# VULNERABLE: RAG context with SQL sink",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_35_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to code execution in 'rag_code_execution_v1'",
      "description": "In function 'rag_code_execution_v1', LLM output variable 'code' flows to 'exec' on line 35 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 35,
      "code_snippet": "    )\n    code = response.choices[0].message.content\n    exec(code)  # SINK: Code execution from RAG-influenced LLM output\n\n\n# VULNERABLE: RAG context with SQL sink",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_54_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'rag_sql_injection_v1'",
      "description": "In function 'rag_sql_injection_v1', LLM output variable 'sql' flows to 'db_cursor.execute' on line 54 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 54,
      "code_snippet": "    )\n    sql = response.choices[0].message.content\n    db_cursor.execute(sql)  # SINK: SQL injection via RAG context\n\n\n# VULNERABLE: RAG with file path sink",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_96_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'rag_multi_doc_injection'",
      "description": "In function 'rag_multi_doc_injection', LLM output variable 'response' flows to 'subprocess.run' on line 96 via direct flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 96,
      "code_snippet": "    )\n    import subprocess\n    subprocess.run(response.choices[0].message.content, shell=True)  # SINK",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_18_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'rag_code_execution_v1'",
      "description": "Function 'rag_code_execution_v1' on line 18 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 18,
      "code_snippet": "\n# VULNERABLE: User query influences context via RAG\ndef rag_code_execution_v1(user_query: str):\n    \"\"\"\n    Vulnerability: User query -> RAG retrieval -> LLM context -> code execution\n    Attack: User crafts query to inject malicious content into retrieved docs",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_39_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'rag_sql_injection_v1'",
      "description": "Function 'rag_sql_injection_v1' on line 39 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 39,
      "code_snippet": "\n# VULNERABLE: RAG context with SQL sink\ndef rag_sql_injection_v1(search_term: str, db_cursor):\n    \"\"\"\n    Vulnerability: Search term -> RAG -> LLM -> SQL query\n    Tests: \u03b1_c decay through context window",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_58_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'rag_path_traversal_v1'",
      "description": "Function 'rag_path_traversal_v1' on line 58 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 58,
      "code_snippet": "\n# VULNERABLE: RAG with file path sink\ndef rag_path_traversal_v1(document_request: str):\n    \"\"\"\n    Vulnerability: Document request -> RAG -> LLM -> file access\n    Tests: \u03b1_c decay for path traversal",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_77_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'rag_multi_doc_injection'",
      "description": "Function 'rag_multi_doc_injection' on line 77 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 77,
      "code_snippet": "\n# VULNERABLE: Multi-document RAG context\ndef rag_multi_doc_injection(queries: List[str]):\n    \"\"\"\n    Vulnerability: Multiple user queries -> aggregated context -> LLM -> command\n    Tests: \u03b1_c with aggregated context",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_18_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'rag_code_execution_v1'",
      "description": "Function 'rag_code_execution_v1' on line 18 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 18,
      "code_snippet": "\n# VULNERABLE: User query influences context via RAG\ndef rag_code_execution_v1(user_query: str):\n    \"\"\"\n    Vulnerability: User query -> RAG retrieval -> LLM context -> code execution\n    Attack: User crafts query to inject malicious content into retrieved docs",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_39_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'rag_sql_injection_v1'",
      "description": "Function 'rag_sql_injection_v1' on line 39 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 39,
      "code_snippet": "\n# VULNERABLE: RAG context with SQL sink\ndef rag_sql_injection_v1(search_term: str, db_cursor):\n    \"\"\"\n    Vulnerability: Search term -> RAG -> LLM -> SQL query\n    Tests: \u03b1_c decay through context window",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_77_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'rag_multi_doc_injection'",
      "description": "Function 'rag_multi_doc_injection' on line 77 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 77,
      "code_snippet": "\n# VULNERABLE: Multi-document RAG context\ndef rag_multi_doc_injection(queries: List[str]):\n    \"\"\"\n    Vulnerability: Multiple user queries -> aggregated context -> LLM -> command\n    Tests: \u03b1_c with aggregated context",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py_58_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'rag_path_traversal_v1'",
      "description": "Function 'rag_path_traversal_v1' on line 58 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/synthetic_decay_validation/rag_context_injection.py",
      "line_number": 58,
      "code_snippet": "\n# VULNERABLE: RAG with file path sink\ndef rag_path_traversal_v1(document_request: str):\n    \"\"\"\n    Vulnerability: Document request -> RAG -> LLM -> file access\n    Tests: \u03b1_c decay for path traversal",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 107,
      "kept": 103,
      "filtered": 4,
      "reduction_pct": 3.7,
      "avg_tp_probability": 0.537,
      "filter_reasons": {
        "high severity with context": 4
      }
    }
  }
}