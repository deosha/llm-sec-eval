{
  "audit_id": "12b9fc2e",
  "project_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack",
  "timestamp": "2026-01-08T23:44:54.591513",
  "overall_score": 18.8,
  "maturity_level": "Initial",
  "files_scanned": 1611,
  "scan_duration_seconds": 1.25,
  "detected_controls": 23,
  "total_controls": 61,
  "categories": {
    "prompt_security": {
      "category_id": "prompt_security",
      "category_name": "Prompt Security",
      "score": 28.1,
      "max_score": 100.0,
      "percentage": 28.1,
      "detected_count": 4,
      "total_count": 8,
      "controls": [
        {
          "control_id": "PS-01",
          "control_name": "Prompt Sanitization",
          "category": "prompt_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_preprocessing_pipeline.py",
              "line_number": 28,
              "snippet": "preprocessing_pipeline.add_component(instance=DocumentCleaner(), name=\"cleaner\")",
              "description": "Sanitization function call: DocumentCleaner",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_dense_doc_search.py",
              "line_number": 30,
              "snippet": "indexing_pipeline.add_component(instance=DocumentCleaner(), name=\"cleaner\")",
              "description": "Sanitization function call: DocumentCleaner",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_preprocessor.py",
              "line_number": 116,
              "snippet": "cleaner = DocumentCleaner(",
              "description": "Sanitization function call: DocumentCleaner",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/logging.py",
              "line_number": 360,
              "snippet": "wrapper_class=structlog.make_filtering_bound_logger(min_level=logging.root.getEffectiveLevel()),",
              "description": "Sanitization function call: structlog.make_filtering_bound_logger",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py",
              "line_number": 108,
              "snippet": "filtered_deps.append(dep)",
              "description": "Sanitization function call: filtered_deps.append",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_preprocessing_pipeline.py",
              "line_number": 77,
              "snippet": "stored_documents = filled_document_store.filter_documents()",
              "description": "Sanitization function call: filled_document_store.filter_documents",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/recursive_splitter.py",
              "line_number": 299,
              "snippet": "escaped_separator = re.escape(curr_separator)",
              "description": "Sanitization function call: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tool.py",
              "line_number": 108,
              "snippet": "match=re.escape(",
              "description": "Sanitization function call: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tool.py",
              "line_number": 167,
              "snippet": "match=re.escape(",
              "description": "Sanitization function call: re.escape",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-02",
          "control_name": "Rate Limiting",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "PS-03",
          "control_name": "Input Validation",
          "category": "prompt_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8666666666666667,
          "score": 75,
          "evidence": [
            {
              "type": "dependency",
              "file_path": "",
              "line_number": null,
              "snippet": "pydantic",
              "description": "Validation library pydantic found",
              "confidence": 0.95
            },
            {
              "type": "dependency",
              "file_path": "",
              "line_number": null,
              "snippet": "jsonschema",
              "description": "Validation library jsonschema found",
              "confidence": 0.95
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py",
              "line_number": 8,
              "snippet": "pydantic",
              "description": "Pydantic validation library imported",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py",
              "line_number": 102,
              "snippet": "class HFModelType(Enum):",
              "description": "Validation model class: HFModelType",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/utils/test_base_serialization.py",
              "line_number": 20,
              "snippet": "class CustomModel(pydantic.BaseModel):",
              "description": "Validation model class: CustomModel",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/components/readers/test_extractive.py",
              "line_number": 67,
              "snippet": "class MockModel(torch.nn.Module):",
              "description": "Validation model class: MockModel",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-04",
          "control_name": "Output Filtering",
          "category": "prompt_security",
          "detected": true,
          "level": "basic",
          "confidence": 0.8,
          "score": 25,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
              "line_number": 94,
              "snippet": "self._validate_output_mapping(all_possible_pipeline_outputs, resolved_output_mapping)",
              "description": "Output filtering function: self._validate_output_mapping",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-05",
          "control_name": "Context Window Protection",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "PS-06",
          "control_name": "Red Team Testing",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'"
          ]
        },
        {
          "control_id": "PS-07",
          "control_name": "Prompt Anomaly Detection",
          "category": "prompt_security",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
              "line_number": 188,
              "snippet": "self._check_lambda_threshold(self.lambda_threshold, self.strategy)",
              "description": "Statistical analysis: self._check_lambda_threshold",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
              "line_number": 420,
              "snippet": "self._check_lambda_threshold(lambda_threshold, self.strategy)",
              "description": "Statistical analysis: self._check_lambda_threshold",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-08",
          "control_name": "System Prompt Protection",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        }
      ]
    },
    "model_security": {
      "category_id": "model_security",
      "category_name": "Model Security",
      "score": 15.6,
      "max_score": 100.0,
      "percentage": 15.6,
      "detected_count": 2,
      "total_count": 8,
      "controls": [
        {
          "control_id": "MS-01",
          "control_name": "Access Control",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "MS-02",
          "control_name": "Model Versioning",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "MS-03",
          "control_name": "Dependency Scanning",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "MS-04",
          "control_name": "API Security",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "MS-05",
          "control_name": "Model Source Verification",
          "category": "model_security",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.9,
          "score": 50,
          "evidence": [
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docstrings_checksum.py",
              "line_number": 2,
              "snippet": "hashlib",
              "description": "Hash verification library: hashlib",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/document.py",
              "line_number": 5,
              "snippet": "hashlib",
              "description": "Hash verification library: hashlib",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "MS-06",
          "control_name": "Differential Privacy",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Opacus or TensorFlow Privacy for differential privacy",
            "Implement privacy budgets for model queries",
            "Monitor epsilon values for privacy guarantees"
          ]
        },
        {
          "control_id": "MS-07",
          "control_name": "Model Watermarking",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement watermarking for model outputs",
            "Use cryptographic watermarks for model weights",
            "Track watermark verification for model theft detection"
          ]
        },
        {
          "control_id": "MS-08",
          "control_name": "Secure Model Loading",
          "category": "model_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py",
              "line_number": 70,
              "snippet": "config = yaml.safe_load(config_file)",
              "description": "Safe loading: yaml.safe_load",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/openapi_functions.py",
              "line_number": 249,
              "snippet": "open_api_spec_content = yaml.safe_load(content)",
              "description": "Safe loading: yaml.safe_load",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    },
    "data_privacy": {
      "category_id": "data_privacy",
      "category_name": "Data Privacy",
      "score": 28.1,
      "max_score": 100.0,
      "percentage": 28.1,
      "detected_count": 4,
      "total_count": 8,
      "controls": [
        {
          "control_id": "DP-01",
          "control_name": "PII Detection",
          "category": "data_privacy",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py",
              "line_number": 148,
              "snippet": "poller = self.document_analysis_client.begin_analyze_document(",
              "description": "PII detection function: self.document_analysis_client.begin_analyze_document",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/components/converters/test_azure_ocr_doc_converter.py",
              "line_number": 98,
              "snippet": "return AnalyzeResult.from_dict(result)",
              "description": "PII detection function: AnalyzeResult.from_dict",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "DP-02",
          "control_name": "Data Redaction",
          "category": "data_privacy",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/readers/extractive.py",
              "line_number": 245,
              "snippet": "attention_mask = encodings_pt.attention_mask.to(first_device)",
              "description": "Redaction function: encodings_pt.attention_mask.to",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py",
              "line_number": 362,
              "snippet": "encoded_stop_words = tokenizer(stop_words, add_special_tokens=False, padding=True, return_tensors=\"pt\")",
              "description": "Redaction function: tokenizer",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py",
              "line_number": 361,
              "snippet": "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})",
              "description": "Redaction function: tokenizer.add_special_tokens",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/sentence_tokenizer.py",
              "line_number": 141,
              "snippet": "self.sentence_tokenizer = load_sentence_tokenizer(language, keep_white_spaces=keep_white_spaces)  # pylint: disable=possibly-used-before-assignment",
              "description": "Redaction function: load_sentence_tokenizer",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "DP-03",
          "control_name": "Data Encryption",
          "category": "data_privacy",
          "detected": true,
          "level": "basic",
          "confidence": 0.8,
          "score": 25,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/msg.py",
              "line_number": 90,
              "snippet": "if self._is_encrypted(msg):",
              "description": "Encryption function: self._is_encrypted",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "DP-04",
          "control_name": "Audit Logging",
          "category": "data_privacy",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8666666666666667,
          "score": 75,
          "evidence": [
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/test_logging.py",
              "line_number": 7,
              "snippet": "logging",
              "description": "Logging module imported: logging",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/test_logging.py",
              "line_number": 22,
              "snippet": "haystack.logging",
              "description": "Logging module imported: haystack.logging",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/test_logging.py",
              "line_number": 16,
              "snippet": "structlog",
              "description": "Logging module imported: structlog",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/logging.py",
              "line_number": 314,
              "snippet": "structlog",
              "description": "Logging module imported: structlog",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tools_utils.py",
              "line_number": 210,
              "snippet": "tool = WarmupTrackingTool(",
              "description": "Audit logging: WarmupTrackingTool",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tools_utils.py",
              "line_number": 226,
              "snippet": "tool1 = WarmupTrackingTool(",
              "description": "Audit logging: WarmupTrackingTool",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "DP-05",
          "control_name": "Consent Management",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "DP-06",
          "control_name": "NER PII Detection",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Presidio or SpaCy for NER-based PII detection",
            "Implement custom NER models for domain-specific PII",
            "Run PII detection on all inputs and outputs"
          ]
        },
        {
          "control_id": "DP-07",
          "control_name": "Data Retention Policy",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "DP-08",
          "control_name": "GDPR Compliance",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        }
      ]
    },
    "owasp_llm": {
      "category_id": "owasp_llm",
      "category_name": "OWASP LLM Top 10",
      "score": 35.0,
      "max_score": 100.0,
      "percentage": 35.0,
      "detected_count": 6,
      "total_count": 10,
      "controls": [
        {
          "control_id": "OWASP-01",
          "control_name": "LLM01: Prompt Injection Defense",
          "category": "owasp_llm",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8166666666666668,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/recursive_splitter.py",
              "line_number": 299,
              "snippet": "escaped_separator = re.escape(curr_separator)",
              "description": "Input sanitization: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tool.py",
              "line_number": 108,
              "snippet": "match=re.escape(",
              "description": "Input sanitization: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/logging.py",
              "line_number": 360,
              "snippet": "wrapper_class=structlog.make_filtering_bound_logger(min_level=logging.root.getEffectiveLevel()),",
              "description": "Input sanitization: structlog.make_filtering_bound_logger",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py",
              "line_number": 108,
              "snippet": "filtered_deps.append(dep)",
              "description": "Input sanitization: filtered_deps.append",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_preprocessing_pipeline.py",
              "line_number": 28,
              "snippet": "preprocessing_pipeline.add_component(instance=DocumentCleaner(), name=\"cleaner\")",
              "description": "Input sanitization: DocumentCleaner",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_dense_doc_search.py",
              "line_number": 30,
              "snippet": "indexing_pipeline.add_component(instance=DocumentCleaner(), name=\"cleaner\")",
              "description": "Input sanitization: DocumentCleaner",
              "confidence": 0.8
            },
            {
              "type": "dependency",
              "file_path": "",
              "line_number": null,
              "snippet": "pydantic",
              "description": "Input validation with Pydantic",
              "confidence": 0.95
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/jinja2_chat_extension.py",
              "line_number": 89,
              "snippet": "raise TemplateSyntaxError(f\"Role must be one of: {', '.join(self.SUPPORTED_ROLES)}\", lineno)",
              "description": "Prompt template usage: TemplateSyntaxError",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/jinja2_chat_extension.py",
              "line_number": 98,
              "snippet": "raise TemplateSyntaxError(\"name must be a string\", lineno)",
              "description": "Prompt template usage: TemplateSyntaxError",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-02",
          "control_name": "LLM02: Insecure Output Handling",
          "category": "owasp_llm",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/recursive_splitter.py",
              "line_number": 299,
              "snippet": "escaped_separator = re.escape(curr_separator)",
              "description": "Output escaping: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tool.py",
              "line_number": 108,
              "snippet": "match=re.escape(",
              "description": "Output escaping: re.escape",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-03",
          "control_name": "LLM03: Training Data Poisoning",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement data validation pipelines",
            "Verify data source integrity",
            "Monitor for anomalies in training data"
          ]
        },
        {
          "control_id": "OWASP-04",
          "control_name": "LLM04: Model Denial of Service",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "OWASP-05",
          "control_name": "LLM05: Supply Chain Vulnerabilities",
          "category": "owasp_llm",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.9,
          "score": 50,
          "evidence": [
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docstrings_checksum.py",
              "line_number": 2,
              "snippet": "hashlib",
              "description": "Integrity verification: hashlib",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/document.py",
              "line_number": 5,
              "snippet": "hashlib",
              "description": "Integrity verification: hashlib",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-06",
          "control_name": "LLM06: Sensitive Information Disclosure",
          "category": "owasp_llm",
          "detected": true,
          "level": "basic",
          "confidence": 0.8,
          "score": 25,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/readers/extractive.py",
              "line_number": 245,
              "snippet": "attention_mask = encodings_pt.attention_mask.to(first_device)",
              "description": "Data filtering: encodings_pt.attention_mask.to",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-07",
          "control_name": "LLM07: Insecure Plugin Design",
          "category": "owasp_llm",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/check_imports.py",
              "line_number": 64,
              "snippet": "imported, failed = validate_module_imports(root_dir=\"haystack\", exclude_subdirs=exclude_subdirs)",
              "description": "Plugin validation: validate_module_imports",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
              "line_number": 195,
              "snippet": "resolved_param_value = type_adapter.validate_python(param_value)",
              "description": "Plugin validation: type_adapter.validate_python",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/conditional_router.py",
              "line_number": 207,
              "snippet": "self._env = NativeEnvironment() if self._unsafe else SandboxedEnvironment()",
              "description": "Sandboxing: SandboxedEnvironment",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/builders/chat_prompt_builder.py",
              "line_number": 166,
              "snippet": "self._env = SandboxedEnvironment(extensions=[ChatMessageExtension])",
              "description": "Sandboxing: SandboxedEnvironment",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-08",
          "control_name": "LLM08: Excessive Agency",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement human-in-the-loop for critical actions",
            "Use principle of least privilege for LLM access",
            "Add approval workflows for sensitive operations"
          ]
        },
        {
          "control_id": "OWASP-09",
          "control_name": "LLM09: Overreliance",
          "category": "owasp_llm",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_evaluation_pipeline.py",
              "line_number": 228,
              "snippet": "assert list(aggregated_score_report_json.keys()) == [\"metrics\", \"score\"]",
              "description": "Confidence scoring: aggregated_score_report_json.keys",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/joiners/document_joiner.py",
              "line_number": 247,
              "snippet": "scores_list.append(doc.score if doc.score is not None else 0)",
              "description": "Confidence scoring: scores_list.append",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/readers/extractive.py",
              "line_number": 379,
              "snippet": "score=probability.item(),",
              "description": "Confidence scoring: probability.item",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
              "line_number": 94,
              "snippet": "self._validate_output_mapping(all_possible_pipeline_outputs, resolved_output_mapping)",
              "description": "Output verification: self._validate_output_mapping",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-10",
          "control_name": "LLM10: Model Theft",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement rate limiting on API endpoints",
            "Add query logging and anomaly detection",
            "Monitor for extraction patterns"
          ]
        }
      ]
    },
    "blue_team": {
      "category_id": "blue_team",
      "category_name": "Blue Team Operations",
      "score": 17.9,
      "max_score": 100.0,
      "percentage": 17.9,
      "detected_count": 2,
      "total_count": 7,
      "controls": [
        {
          "control_id": "BT-01",
          "control_name": "Model Monitoring",
          "category": "blue_team",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8000000000000002,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/core/test_type_utils.py",
              "line_number": 347,
              "snippet": "symmetric_cases = generate_symmetric_cases()",
              "description": "Metrics tracking: generate_symmetric_cases",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/core/test_type_utils.py",
              "line_number": 348,
              "snippet": "asymmetric_cases = generate_strict_asymmetric_cases()",
              "description": "Metrics tracking: generate_strict_asymmetric_cases",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
              "line_number": 114,
              "snippet": "_FREQ_VOCAB_FOR_IDF_STORAGES[self.index] = Counter()",
              "description": "Metrics tracking: Counter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
              "line_number": 160,
              "snippet": "return _FREQ_VOCAB_FOR_IDF_STORAGES.get(self.index, Counter())",
              "description": "Metrics tracking: Counter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tools_utils.py",
              "line_number": 210,
              "snippet": "tool = WarmupTrackingTool(",
              "description": "Metrics tracking: WarmupTrackingTool",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/test/tools/test_tools_utils.py",
              "line_number": 226,
              "snippet": "tool1 = WarmupTrackingTool(",
              "description": "Metrics tracking: WarmupTrackingTool",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "BT-02",
          "control_name": "Drift Detection",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement drift detection with evidently or alibi-detect",
            "Monitor input data distribution changes",
            "Set up automated alerts for drift events"
          ]
        },
        {
          "control_id": "BT-03",
          "control_name": "Anomaly Detection",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement anomaly detection on model inputs",
            "Monitor for unusual query patterns",
            "Use statistical methods or ML-based detection"
          ]
        },
        {
          "control_id": "BT-04",
          "control_name": "Adversarial Attack Detection",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement adversarial input detection",
            "Use adversarial robustness toolkits",
            "Add input perturbation analysis"
          ]
        },
        {
          "control_id": "BT-05",
          "control_name": "AI Incident Response",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "BT-06",
          "control_name": "Model Drift Monitoring",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Evidently or alibi-detect for drift monitoring",
            "Set up automated alerts for significant drift",
            "Implement automatic retraining pipelines"
          ]
        },
        {
          "control_id": "BT-07",
          "control_name": "Data Quality Monitoring",
          "category": "blue_team",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.95,
          "score": 50,
          "evidence": [
            {
              "type": "dependency",
              "file_path": "",
              "line_number": null,
              "snippet": "pydantic",
              "description": "Data quality library pydantic found",
              "confidence": 0.95
            }
          ],
          "recommendations": []
        }
      ]
    },
    "governance": {
      "category_id": "governance",
      "category_name": "AI Governance",
      "score": 0.0,
      "max_score": 100.0,
      "percentage": 0.0,
      "detected_count": 0,
      "total_count": 5,
      "controls": [
        {
          "control_id": "GV-01",
          "control_name": "Model Explainability",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use SHAP or LIME for model explanations",
            "Provide decision explanations in outputs",
            "Implement feature attribution tracking"
          ]
        },
        {
          "control_id": "GV-02",
          "control_name": "Bias Detection",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Fairlearn or AIF360 for bias detection",
            "Implement fairness metrics tracking",
            "Test for demographic parity and equalized odds"
          ]
        },
        {
          "control_id": "GV-03",
          "control_name": "Model Documentation",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "GV-04",
          "control_name": "Compliance Tracking",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "GV-05",
          "control_name": "Human Oversight",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        }
      ]
    },
    "supply_chain": {
      "category_id": "supply_chain",
      "category_name": "Supply Chain Security",
      "score": 25.0,
      "max_score": 100.0,
      "percentage": 25.0,
      "detected_count": 1,
      "total_count": 3,
      "controls": [
        {
          "control_id": "SC-01",
          "control_name": "Dependency Scanning",
          "category": "supply_chain",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "SC-02",
          "control_name": "Model Provenance Tracking",
          "category": "supply_chain",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use MLflow, DVC, or Weights & Biases for model tracking",
            "Implement model versioning with metadata",
            "Maintain model registry with provenance information"
          ]
        },
        {
          "control_id": "SC-03",
          "control_name": "Model Integrity Verification",
          "category": "supply_chain",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8500000000000001,
          "score": 75,
          "evidence": [
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docstrings_checksum.py",
              "line_number": 2,
              "snippet": "hashlib",
              "description": "Hash library: hashlib",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/document.py",
              "line_number": 5,
              "snippet": "hashlib",
              "description": "Hash library: hashlib",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/check_imports.py",
              "line_number": 64,
              "snippet": "imported, failed = validate_module_imports(root_dir=\"haystack\", exclude_subdirs=exclude_subdirs)",
              "description": "Verification: validate_module_imports",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
              "line_number": 195,
              "snippet": "resolved_param_value = type_adapter.validate_python(param_value)",
              "description": "Verification: type_adapter.validate_python",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    },
    "hallucination": {
      "category_id": "hallucination",
      "category_name": "Hallucination Mitigation",
      "score": 35.0,
      "max_score": 100.0,
      "percentage": 35.0,
      "detected_count": 3,
      "total_count": 5,
      "controls": [
        {
          "control_id": "HM-01",
          "control_name": "RAG Implementation",
          "category": "hallucination",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_rag_pipelines_e2e.py",
              "line_number": 36,
              "snippet": "rag_pipeline.add_component(instance=InMemoryBM25Retriever(document_store=InMemoryDocumentStore()), name=\"retriever\")",
              "description": "Retrieval pattern: InMemoryBM25Retriever",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_hybrid_doc_search_pipeline.py",
              "line_number": 21,
              "snippet": "hybrid_pipeline.add_component(instance=InMemoryBM25Retriever(document_store=document_store), name=\"bm25_retriever\")",
              "description": "Retrieval pattern: InMemoryBM25Retriever",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "HM-02",
          "control_name": "Confidence Scoring",
          "category": "hallucination",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "HM-03",
          "control_name": "Source Attribution",
          "category": "hallucination",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8000000000000002,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/datadog.py",
              "line_number": 76,
              "snippet": "resource_name = self._get_span_resource_name(operation_name, tags)",
              "description": "Citation pattern: self._get_span_resource_name",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/file_type_router.py",
              "line_number": 187,
              "snippet": "source.meta.update(meta_dict)",
              "description": "Citation pattern: source.meta.update",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/e2e/pipelines/test_evaluation_pipeline.py",
              "line_number": 130,
              "snippet": "retrieved_docs.append(response[\"answer_builder\"][\"answers\"][0].documents)",
              "description": "Source documents: retrieved_docs.append",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "HM-04",
          "control_name": "Temperature Control",
          "category": "hallucination",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "HM-05",
          "control_name": "Fact Checking",
          "category": "hallucination",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/check_imports.py",
              "line_number": 64,
              "snippet": "imported, failed = validate_module_imports(root_dir=\"haystack\", exclude_subdirs=exclude_subdirs)",
              "description": "Fact checking: validate_module_imports",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
              "line_number": 195,
              "snippet": "resolved_param_value = type_adapter.validate_python(param_value)",
              "description": "Fact checking: type_adapter.validate_python",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    },
    "ethical_ai": {
      "category_id": "ethical_ai",
      "category_name": "Ethical AI & Bias",
      "score": 12.5,
      "max_score": 100.0,
      "percentage": 12.5,
      "detected_count": 1,
      "total_count": 4,
      "controls": [
        {
          "control_id": "EA-01",
          "control_name": "Fairness Metrics",
          "category": "ethical_ai",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Fairlearn or AIF360 for fairness metrics",
            "Implement demographic parity testing",
            "Monitor fairness metrics in production"
          ]
        },
        {
          "control_id": "EA-02",
          "control_name": "Model Explainability",
          "category": "ethical_ai",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
              "line_number": 360,
              "snippet": "query_similarities = query_similarities_as_tensor.reshape(-1)",
              "description": "Explainability: query_similarities_as_tensor.reshape",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "EA-03",
          "control_name": "Bias Testing",
          "category": "ethical_ai",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement adversarial testing for bias",
            "Test across demographic groups",
            "Use TextAttack or CheckList for NLP bias testing"
          ]
        },
        {
          "control_id": "EA-04",
          "control_name": "Model Cards",
          "category": "ethical_ai",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'"
          ]
        }
      ]
    },
    "incident_response": {
      "category_id": "incident_response",
      "category_name": "Incident Response",
      "score": 0.0,
      "max_score": 100.0,
      "percentage": 0.0,
      "detected_count": 0,
      "total_count": 3,
      "controls": [
        {
          "control_id": "IR-01",
          "control_name": "Monitoring Integration",
          "category": "incident_response",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "IR-02",
          "control_name": "Audit Logging",
          "category": "incident_response",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        },
        {
          "control_id": "IR-03",
          "control_name": "Rollback Capability",
          "category": "incident_response",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'bool' object has no attribute 'lower'"
          ]
        }
      ]
    }
  },
  "recommendations": [
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-02",
      "title": "Rate Limiting",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-05",
      "title": "Context Window Protection",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-06",
      "title": "Red Team Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-08",
      "title": "System Prompt Protection",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-01",
      "title": "Access Control",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-02",
      "title": "Model Versioning",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-03",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-04",
      "title": "API Security",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-06",
      "title": "Differential Privacy",
      "description": "Control not detected or below threshold",
      "remediation": "Use Opacus or TensorFlow Privacy for differential privacy",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-06",
      "title": "Differential Privacy",
      "description": "Control not detected or below threshold",
      "remediation": "Implement privacy budgets for model queries",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-06",
      "title": "Differential Privacy",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor epsilon values for privacy guarantees",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-07",
      "title": "Model Watermarking",
      "description": "Control not detected or below threshold",
      "remediation": "Implement watermarking for model outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-07",
      "title": "Model Watermarking",
      "description": "Control not detected or below threshold",
      "remediation": "Use cryptographic watermarks for model weights",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-07",
      "title": "Model Watermarking",
      "description": "Control not detected or below threshold",
      "remediation": "Track watermark verification for model theft detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-05",
      "title": "Consent Management",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-06",
      "title": "NER PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use Presidio or SpaCy for NER-based PII detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-06",
      "title": "NER PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement custom NER models for domain-specific PII",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-06",
      "title": "NER PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Run PII detection on all inputs and outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-07",
      "title": "Data Retention Policy",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-08",
      "title": "GDPR Compliance",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-03",
      "title": "LLM03: Training Data Poisoning",
      "description": "Control not detected or below threshold",
      "remediation": "Implement data validation pipelines",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-03",
      "title": "LLM03: Training Data Poisoning",
      "description": "Control not detected or below threshold",
      "remediation": "Verify data source integrity",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-03",
      "title": "LLM03: Training Data Poisoning",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor for anomalies in training data",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-04",
      "title": "LLM04: Model Denial of Service",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-08",
      "title": "LLM08: Excessive Agency",
      "description": "Control not detected or below threshold",
      "remediation": "Implement human-in-the-loop for critical actions",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-08",
      "title": "LLM08: Excessive Agency",
      "description": "Control not detected or below threshold",
      "remediation": "Use principle of least privilege for LLM access",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-08",
      "title": "LLM08: Excessive Agency",
      "description": "Control not detected or below threshold",
      "remediation": "Add approval workflows for sensitive operations",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-10",
      "title": "LLM10: Model Theft",
      "description": "Control not detected or below threshold",
      "remediation": "Implement rate limiting on API endpoints",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-10",
      "title": "LLM10: Model Theft",
      "description": "Control not detected or below threshold",
      "remediation": "Add query logging and anomaly detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-10",
      "title": "LLM10: Model Theft",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor for extraction patterns",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-02",
      "title": "Drift Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement drift detection with evidently or alibi-detect",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-02",
      "title": "Drift Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor input data distribution changes",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-02",
      "title": "Drift Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Set up automated alerts for drift events",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-03",
      "title": "Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement anomaly detection on model inputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-03",
      "title": "Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor for unusual query patterns",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-03",
      "title": "Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use statistical methods or ML-based detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-04",
      "title": "Adversarial Attack Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement adversarial input detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-04",
      "title": "Adversarial Attack Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use adversarial robustness toolkits",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-04",
      "title": "Adversarial Attack Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Add input perturbation analysis",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-05",
      "title": "AI Incident Response",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-06",
      "title": "Model Drift Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Use Evidently or alibi-detect for drift monitoring",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-06",
      "title": "Model Drift Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Set up automated alerts for significant drift",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-06",
      "title": "Model Drift Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Implement automatic retraining pipelines",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-01",
      "title": "Model Explainability",
      "description": "Control not detected or below threshold",
      "remediation": "Use SHAP or LIME for model explanations",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-01",
      "title": "Model Explainability",
      "description": "Control not detected or below threshold",
      "remediation": "Provide decision explanations in outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-01",
      "title": "Model Explainability",
      "description": "Control not detected or below threshold",
      "remediation": "Implement feature attribution tracking",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-02",
      "title": "Bias Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use Fairlearn or AIF360 for bias detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-02",
      "title": "Bias Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement fairness metrics tracking",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-02",
      "title": "Bias Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Test for demographic parity and equalized odds",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-03",
      "title": "Model Documentation",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-04",
      "title": "Compliance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-05",
      "title": "Human Oversight",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-01",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-02",
      "title": "Model Provenance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Use MLflow, DVC, or Weights & Biases for model tracking",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-02",
      "title": "Model Provenance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Implement model versioning with metadata",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-02",
      "title": "Model Provenance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Maintain model registry with provenance information",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "hallucination",
      "control_id": "HM-02",
      "title": "Confidence Scoring",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "hallucination",
      "control_id": "HM-04",
      "title": "Temperature Control",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-01",
      "title": "Fairness Metrics",
      "description": "Control not detected or below threshold",
      "remediation": "Use Fairlearn or AIF360 for fairness metrics",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-01",
      "title": "Fairness Metrics",
      "description": "Control not detected or below threshold",
      "remediation": "Implement demographic parity testing",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-01",
      "title": "Fairness Metrics",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor fairness metrics in production",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-03",
      "title": "Bias Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Implement adversarial testing for bias",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-03",
      "title": "Bias Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Test across demographic groups",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-03",
      "title": "Bias Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Use TextAttack or CheckList for NLP bias testing",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-04",
      "title": "Model Cards",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "incident_response",
      "control_id": "IR-01",
      "title": "Monitoring Integration",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "incident_response",
      "control_id": "IR-02",
      "title": "Audit Logging",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "incident_response",
      "control_id": "IR-03",
      "title": "Rollback Capability",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'bool' object has no attribute 'lower'",
      "docs_url": null
    }
  ]
}