{
  "report_type": "static_scan",
  "generated_at": "2026-01-08T18:12:51.112602Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack",
    "files_scanned": 305,
    "overall_score": 1.84,
    "confidence": 0.66,
    "duration_seconds": 1.65,
    "findings_count": 322,
    "severity_breakdown": {
      "CRITICAL": 11,
      "HIGH": 291,
      "MEDIUM": 11,
      "LOW": 9,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 6,
      "confidence": 0.49,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "2 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.47,
      "subscores": {
        "model_protection": 55,
        "extraction_defense": 37,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption at rest",
        "Encryption in transit",
        "OAuth",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": [
        "Extraction defense is weak",
        "Implement rate limiting and output protections"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.52,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII encryption",
        "PII masking",
        "Explicit consent",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 1,
      "confidence": 0.38,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 50,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.88,
      "subscores": {
        "LLM01": 72,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 100,
        "LLM07": 66,
        "LLM08": 23,
        "LLM09": 0,
        "LLM10": 100
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 1 critical",
        "Insecure Output Handling: 2 critical, 3 high",
        "Model Denial of Service: 7 critical, 7 high",
        "Supply Chain Vulnerabilities: 274 high, 2 medium",
        "Insecure Plugin Design: 1 critical",
        "Excessive Agency: 4 high, 3 medium",
        "Overreliance: 3 high, 6 medium, 9 low"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/logging.py_327_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 327. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/logging.py",
      "line_number": 327,
      "code_snippet": "\n    # We roughly follow the structlog documentation here:\n    # https://www.structlog.org/en/stable/standard-library.html#rendering-using-structlog-based-formatters-within-logging\n    # This means that we use structlog to format the log entries for entries emitted via `logging` and `structlog`.\n\n    if use_json is None:  # explicit parameter takes precedence over everything else",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py_41_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 41. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py",
      "line_number": 41,
      "code_snippet": "    \"\"\"\n    if version == \"main\":\n        url = \"https://raw.githubusercontent.com/deepset-ai/haystack/refs/heads/main/pyproject.toml\"\n    elif version == \"develop\":\n        url = \"https://raw.githubusercontent.com/deepset-ai/haystack/refs/heads/develop/pyproject.toml\"\n    else:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py",
      "line_number": 43,
      "code_snippet": "        url = \"https://raw.githubusercontent.com/deepset-ai/haystack/refs/heads/main/pyproject.toml\"\n    elif version == \"develop\":\n        url = \"https://raw.githubusercontent.com/deepset-ai/haystack/refs/heads/develop/pyproject.toml\"\n    else:\n        # Format version tag properly (add 'v' prefix if not present)\n        if not version.startswith(\"v\"):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py_48_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 48. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/docs-website/scripts/generate_requirements.py",
      "line_number": 48,
      "code_snippet": "        if not version.startswith(\"v\"):\n            version = f\"v{version}\"\n        url = f\"https://raw.githubusercontent.com/deepset-ai/haystack/refs/tags/{version}/pyproject.toml\"\n\n    try:\n        response = requests.get(url, timeout=30)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docstrings_checksum.py_28_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 28. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docstrings_checksum.py",
      "line_number": 28,
      "code_snippet": "    # Sort them to be safe, since ast.walk() returns\n    # nodes in no specified order.\n    # See https://docs.python.org/3/library/ast.html#ast.walk\n    docstrings.sort()\n\n    return hashlib.md5(str(docstrings).encode(\"utf-8\")).hexdigest()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docs_search_sync.py_64_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 64. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/.github/utils/docs_search_sync.py",
      "line_number": 64,
      "code_snippet": "    Delete files from the deepset workspace.\n    \"\"\"\n    url = f\"https://api.cloud.deepset.ai/api/v1/workspaces/{DEEPSET_WORKSPACE_DOCS_SEARCH}/files\"\n    payload = {\"names\": file_names}\n    headers = {\"Accept\": \"application/json\", \"Authorization\": f\"Bearer {DEEPSET_API_KEY_DOCS_SEARCH}\"}\n    response = requests.delete(url, json=payload, headers=headers, timeout=300)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/datadog.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/datadog.py",
      "line_number": 35,
      "code_snippet": "        coerced_value = tracing_utils.coerce_tag_value(value)\n        # Although set_tag declares value: Optional[str], its implementation accepts other types.\n        # https://github.com/DataDog/dd-trace-py/blob/200b33c5221db1af975f6f7017738cd99a2da4a4/ddtrace/_trace/span.py\n        self._span.set_tag(key, coerced_value)  # type: ignore[arg-type]\n\n    def raw_span(self) -> Any:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/datadog.py_49_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 49. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/datadog.py",
      "line_number": 49,
      "code_snippet": "        \"\"\"Return a dictionary with correlation data for logs.\"\"\"\n\n        # https://docs.datadoghq.com/tracing/other_telemetry/connect_logs_and_traces/python/#no-standard-library-logging\n        return ddtrace.tracer.get_log_correlation_context()\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/logging_tracer.py_45_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 45. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tracing/logging_tracer.py",
      "line_number": 45,
      "code_snippet": "            A dictionary that maps tag names to color strings that should be used when logging the tags.\n            The color strings should be in the format of\n            [ANSI escape codes](https://en.wikipedia.org/wiki/ANSI_escape_code#Colors).\n            For example, to color the tag \"haystack.component.input\" in red, you would pass\n            `tags_color_strings={\"haystack.component.input\": \"\\x1b[1;31m\"}`.\n        \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py_14",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'create_tool_from_function' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py",
      "line_number": 14,
      "code_snippet": "def create_tool_from_function(\n    function: Callable,\n    name: str | None = None,\n    description: str | None = None,\n    inputs_from_state: dict[str, str] | None = None,\n    outputs_to_state: dict[str, dict[str, Any]] | None = None,\n) -> \"Tool\":\n    \"\"\"\n    Create a Tool instance from a function.\n\n    Allows customizing the Tool name and description.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py_122_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 122. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py",
      "line_number": 122,
      "code_snippet": "    # we don't want to include title keywords in the schema, as they contain redundant information\n    # there is no programmatic way to prevent Pydantic from adding them, so we remove them later\n    # see https://github.com/pydantic/pydantic/discussions/8504\n    _remove_title_from_schema(schema)\n\n    # add parameters descriptions to the schema",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py_14_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/network operation without confirmation in 'create_tool_from_function'",
      "description": "Function 'create_tool_from_function' on line 14 performs high-risk delete/write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py",
      "line_number": 14,
      "code_snippet": "\n\ndef create_tool_from_function(\n    function: Callable,\n    name: str | None = None,\n    description: str | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py_14_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_tool_from_function'",
      "description": "Function 'create_tool_from_function' on line 14 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/from_function.py",
      "line_number": 14,
      "code_snippet": "\n\ndef create_tool_from_function(\n    function: Callable,\n    name: str | None = None,\n    description: str | None = None,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py_312",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_create_tool_parameters_schema' on line 312 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
      "line_number": 312,
      "code_snippet": "    def _create_tool_parameters_schema(self, component: Component, inputs_from_state: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"\n        Creates an OpenAI tools schema from a component's run method parameters.\n\n        :param component: The component to create the schema from.\n        :raises SchemaGenerationError: If schema generation fails\n        :returns: OpenAI tools schema for the component's run method parameters.\n        \"\"\"\n        component_run_description, param_descriptions = _get_component_param_descriptions(component)\n\n        # collect fields (types and defaults) and descriptions from function parameters",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py_348_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 348. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
      "line_number": 348,
      "code_snippet": "        # we don't want to include title keywords in the schema, as they contain redundant information\n        # there is no programmatic way to prevent Pydantic from adding them, so we remove them later\n        # see https://github.com/pydantic/pydantic/discussions/8504\n        _remove_title_from_schema(parameters_schema)\n\n        return parameters_schema",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py_312_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in '_create_tool_parameters_schema'",
      "description": "Function '_create_tool_parameters_schema' on line 312 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
      "line_number": 312,
      "code_snippet": "        )\n\n    def _create_tool_parameters_schema(self, component: Component, inputs_from_state: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"\n        Creates an OpenAI tools schema from a component's run method parameters.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py_312_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_create_tool_parameters_schema'",
      "description": "Function '_create_tool_parameters_schema' on line 312 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/tools/component_tool.py",
      "line_number": 312,
      "code_snippet": "        )\n\n    def _create_tool_parameters_schema(self, component: Component, inputs_from_state: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"\n        Creates an OpenAI tools schema from a component's run method parameters.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/misc.py_14_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 14. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/misc.py",
      "line_number": 14,
      "code_snippet": "CUSTOM_MIMETYPES = {\n    # we add markdown because it is not added by the mimetypes module\n    # see https://github.com/python/cpython/pull/17995\n    \".md\": \"text/markdown\",\n    \".markdown\": \"text/markdown\",\n    # we add msg because it is not added by the mimetypes module",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/jinja2_chat_extension.py_38_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 38. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/jinja2_chat_extension.py",
      "line_number": 38,
      "code_snippet": "    with different attributes (role, name, meta) and mixed content types (text, images, etc.).\n\n    Inspired by [Banks](https://github.com/masci/banks).\n\n    Example:\n    ```",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/azure.py_13_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 13. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/azure.py",
      "line_number": 13,
      "code_snippet": "def default_azure_ad_token_provider() -> str:\n    \"\"\"\n    Get a Azure AD token using the DefaultAzureCredential and the \"https://cognitiveservices.azure.com/.default\" scope.\n    \"\"\"\n    azure_import.check()\n    return get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/azure.py_16_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 16. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/azure.py",
      "line_number": 16,
      "code_snippet": "    \"\"\"\n    azure_import.check()\n    return get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_26_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 26. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 26,
      "code_snippet": "\n    # Sending an HTTP request with default retry configs\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\")\n\n    # Sending an HTTP request with custom number of attempts\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", attempts=10)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_29_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 29. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 29,
      "code_snippet": "\n    # Sending an HTTP request with custom number of attempts\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", attempts=10)\n\n    # Sending an HTTP request with custom HTTP codes to retry\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", status_codes_to_retry=[408, 503])",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 32,
      "code_snippet": "\n    # Sending an HTTP request with custom HTTP codes to retry\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", status_codes_to_retry=[408, 503])\n\n    # Sending an HTTP request with custom timeout in seconds\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", timeout=5)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 35,
      "code_snippet": "\n    # Sending an HTTP request with custom timeout in seconds\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", timeout=5)\n\n    # Sending an HTTP request with custom authorization handling\n    class CustomAuth(requests.auth.AuthBase):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 43,
      "code_snippet": "            return r\n\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", auth=CustomAuth())\n\n    # All of the above combined\n    res = request_with_retry(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_48_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 48. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 48,
      "code_snippet": "    res = request_with_retry(\n        method=\"GET\",\n        url=\"https://example.com\",\n        auth=CustomAuth(),\n        attempts=10,\n        status_codes_to_retry=[408, 503],",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_56_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 56. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 56,
      "code_snippet": "\n    # Sending a POST request\n    res = request_with_retry(method=\"POST\", url=\"https://example.com\", data={\"key\": \"value\"}, attempts=10)\n\n    # Retry all 5xx status codes\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", status_codes_to_retry=list(range(500, 600)))",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_59_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 59. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 59,
      "code_snippet": "\n    # Retry all 5xx status codes\n    res = request_with_retry(method=\"GET\", url=\"https://example.com\", status_codes_to_retry=list(range(500, 600)))\n    ```\n\n    :param attempts:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_114_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 114. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 114,
      "code_snippet": "    # Sending an async HTTP request with default retry configs\n    async def example():\n        res = await async_request_with_retry(method=\"GET\", url=\"https://example.com\")\n        return res\n\n    # Sending an async HTTP request with custom number of attempts",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_119_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 119. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 119,
      "code_snippet": "    # Sending an async HTTP request with custom number of attempts\n    async def example_with_attempts():\n        res = await async_request_with_retry(method=\"GET\", url=\"https://example.com\", attempts=10)\n        return res\n\n    # Sending an async HTTP request with custom HTTP codes to retry",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_124_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 124. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 124,
      "code_snippet": "    # Sending an async HTTP request with custom HTTP codes to retry\n    async def example_with_status_codes():\n        res = await async_request_with_retry(method=\"GET\", url=\"https://example.com\", status_codes_to_retry=[408, 503])\n        return res\n\n    # Sending an async HTTP request with custom timeout in seconds",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_129_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 129. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 129,
      "code_snippet": "    # Sending an async HTTP request with custom timeout in seconds\n    async def example_with_timeout():\n        res = await async_request_with_retry(method=\"GET\", url=\"https://example.com\", timeout=5)\n        return res\n\n    # Sending an async HTTP request with custom headers",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_135_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 135. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 135,
      "code_snippet": "    async def example_with_headers():\n        headers = {\"Authorization\": \"Bearer <my_token_here>\"}\n        res = await async_request_with_retry(method=\"GET\", url=\"https://example.com\", headers=headers)\n        return res\n\n    # All of the above combined",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_143_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 143. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 143,
      "code_snippet": "        res = await async_request_with_retry(\n            method=\"GET\",\n            url=\"https://example.com\",\n            headers=headers,\n            attempts=10,\n            status_codes_to_retry=[408, 503],",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_155_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 155. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 155,
      "code_snippet": "        res = await async_request_with_retry(\n            method=\"POST\",\n            url=\"https://example.com\",\n            json={\"key\": \"value\"},\n            attempts=10\n        )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py_165_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 165. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/requests_utils.py",
      "line_number": 165,
      "code_snippet": "        res = await async_request_with_retry(\n            method=\"GET\",\n            url=\"https://example.com\",\n            status_codes_to_retry=list(range(500, 600))\n        )\n        return res",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/filters.py_20_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 20. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/filters.py",
      "line_number": 20,
      "code_snippet": "    \"\"\"\n    if filters and (\"operator\" not in filters or \"conditions\" not in filters):\n        msg = \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n        raise FilterError(msg)\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py_40_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 40. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py",
      "line_number": 40,
      "code_snippet": "    \"\"\"\n\n    # HF [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference).\n    TEXT_GENERATION_INFERENCE = \"text_generation_inference\"\n\n    # HF [Inference Endpoints](https://huggingface.co/inference-endpoints).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py_74_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 74. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/hf.py",
      "line_number": 74,
      "code_snippet": "    \"\"\"\n\n    # HF [Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference).\n    TEXT_EMBEDDINGS_INFERENCE = \"text_embeddings_inference\"\n\n    # HF [Inference Endpoints](https://huggingface.co/inference-endpoints).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/jupyter.py_11_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 11. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/utils/jupyter.py",
      "line_number": 11,
      "code_snippet": "    \"\"\"\n    # Inspired by:\n    # https://github.com/explosion/spaCy/blob/e1249d3722765aaca56f538e830add7014d20e2a/spacy/util.py#L1079\n    try:\n        # We don't need to import `get_ipython` as it's always present in Jupyter notebooks\n        if get_ipython().__class__.__name__ == \"ZMQInteractiveShell\":  # type: ignore[name-defined]",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py_40_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 40. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py",
      "line_number": 40,
      "code_snippet": "    You can opt-out of sharing usage statistics by manually setting the environment\n    variable `HAYSTACK_TELEMETRY_ENABLED` as described for different operating systems on the\n    [documentation page](https://docs.haystack.deepset.ai/docs/telemetry#how-can-i-opt-out).\n\n    Check out the documentation for more details: [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry).\n    \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py_42_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 42. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py",
      "line_number": 42,
      "code_snippet": "    [documentation page](https://docs.haystack.deepset.ai/docs/telemetry#how-can-i-opt-out).\n\n    Check out the documentation for more details: [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry).\n    \"\"\"\n\n    def __init__(self):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py_55_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 55. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py",
      "line_number": 55,
      "code_snippet": "        \"\"\"\n        posthog.api_key = \"phc_C44vUK9R1J6HYVdfJarTEPqVAoRPJzMXzFcj8PIrJgP\"\n        posthog.host = \"https://eu.posthog.com\"\n\n        # disable posthog logging\n        for module_name in [\"posthog\", \"backoff\"]:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py_84_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 84. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py",
      "line_number": 84,
      "code_snippet": "                \"setting the environment variable HAYSTACK_TELEMETRY_ENABLED as described for different \"\n                \"operating systems in the \"\n                \"[documentation page](https://docs.haystack.deepset.ai/docs/telemetry#how-can-i-opt-out). \"\n                \"More information at [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry).\"\n            )\n            CONFIG_PATH.parents[0].mkdir(parents=True, exist_ok=True)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py_85_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 85. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_telemetry.py",
      "line_number": 85,
      "code_snippet": "                \"operating systems in the \"\n                \"[documentation page](https://docs.haystack.deepset.ai/docs/telemetry#how-can-i-opt-out). \"\n                \"More information at [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry).\"\n            )\n            CONFIG_PATH.parents[0].mkdir(parents=True, exist_ok=True)\n            self.user_id = str(uuid.uuid4())",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_environment.py_26_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 26. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_environment.py",
      "line_number": 26,
      "code_snippet": "\n    Podman run would create the file /run/.containernv, see:\n    https://github.com/containers/podman/blob/main/docs/source/markdown/podman-run.1.md.in#L31\n    \"\"\"\n    return os.path.exists(\"/run/.containerenv\")\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_environment.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_environment.py",
      "line_number": 36,
      "code_snippet": "\n    This might not work anymore at some point (even if it's been a while now), see:\n    https://github.com/moby/moby/issues/18355#issuecomment-220484748\n    \"\"\"\n    return os.path.exists(\"/.dockerenv\")\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_environment.py_53_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 53. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/telemetry/_environment.py",
      "line_number": 53,
      "code_snippet": "    Check if the code is running in a Docker container using the cgroups v2 version.\n\n    inspired from: https://github.com/jenkinsci/docker-workflow-plugin/blob/master/src/main/java/org/jenkinsci/plugins/docker/workflow/client/DockerClient.java\n    \"\"\"\n    path = \"/proc/self/mountinfo\"  # 'self' should be always symlinked to the actual PID\n    return os.path.isfile(path) and _str_in_any_line_of_file(\"/docker/containers/\", path)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/chat_message.py_271_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 271. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/chat_message.py",
      "line_number": 271,
      "code_snippet": "            \"Use the `from_assistant`, `from_user`, `from_system`, and `from_tool` class methods to create a \"\n            \"ChatMessage. For more information about the new API and how to migrate, see the documentation:\"\n            \" https://docs.haystack.deepset.ai/docs/chatmessage\"\n        )\n\n        if any(param in kwargs for param in LEGACY_INIT_PARAMETERS):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/chat_message.py_292_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 292. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/dataclasses/chat_message.py",
      "line_number": 292,
      "code_snippet": "                \"Use the `text` property to access the textual value. \"\n                \"For more information about the new API and how to migrate, see the documentation: \"\n                \"https://docs.haystack.deepset.ai/docs/chatmessage\"\n            )\n            raise AttributeError(msg)\n        return object.__getattribute__(self, name)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/text_language_router.py_53_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 53. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/text_language_router.py",
      "line_number": 53,
      "code_snippet": "\n        :param languages: A list of ISO language codes.\n            See the supported languages in [`langdetect` documentation](https://github.com/Mimino666/langdetect#languages).\n            If not specified, defaults to [\"en\"].\n        \"\"\"\n        langdetect_import.check()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/metadata_router.py_65_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 65. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/metadata_router.py",
      "line_number": 65,
      "code_snippet": "        :param rules: A dictionary defining how to route documents or byte streams to output connections based on their\n            metadata. Keys are output connection names, and values are dictionaries of\n            [filtering expressions](https://docs.haystack.deepset.ai/docs/metadata-filtering) in Haystack.\n            For example:\n            ```python\n            {",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/metadata_router.py_106_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 106. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/metadata_router.py",
      "line_number": 106,
      "code_snippet": "            if \"operator\" not in rule:\n                raise ValueError(\n                    \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n                )\n        component.set_output_types(self, unmatched=self.output_type, **dict.fromkeys(rules, self.output_type))\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/document_type_router.py_58_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in '__init__'",
      "description": "Function '__init__' on line 58 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/routers/document_type_router.py",
      "line_number": 58,
      "code_snippet": "    }\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mime_types: list[str],\n        mime_type_meta_field: str | None = None,\n        file_path_meta_field: str | None = None,",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi.py",
      "line_number": 32,
      "code_snippet": "\n    connector = OpenAPIConnector(\n        openapi_spec=\"https://bit.ly/serperdev_openapi\",\n        credentials=Secret.from_env_var(\"SERPERDEV_API_KEY\"),\n        service_kwargs={\"config_factory\": my_custom_config_factory}\n    )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py_21_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 21. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py",
      "line_number": 21,
      "code_snippet": "\n    # Patch the request method to add support for the proper raw_response handling\n    # If you see that https://github.com/Dorthu/openapi3/pull/124/\n    # is merged, we can remove this patch - notify authors of this code\n    def patch_request(\n        self,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py_125_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 125. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py",
      "line_number": 125,
      "code_snippet": "            # type should always be chosen, but if we do not have a match here\n            # a generic range should be accepted if one if provided\n            # https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.1.md#response-object\n\n            generic_type = content_type.split(\"/\")[0] + \"/*\"\n            expected_media = expected_response.content.get(generic_type, None)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py_163_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 163. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py",
      "line_number": 163,
      "code_snippet": "    `OpenAPIServiceToFunctions` component.\n\n    The example below demonstrates how to use the `OpenAPIServiceConnector` to invoke a method on a https://serper.dev/\n    service specified via OpenAPI specification.\n\n    Note, however, that `OpenAPIServiceConnector` is usually not meant to be used directly, but rather as part of a",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py_185_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 185. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/connectors/openapi_service.py",
      "line_number": 185,
      "code_snippet": "\n    serper_token = <your_serper_dev_token>\n    serperdev_openapi_spec = json.loads(requests.get(\"https://bit.ly/serper_dev_spec\").text)\n    service_connector = OpenAPIServiceConnector()\n    result = service_connector.run(messages=[ChatMessage.from_assistant(json.dumps(fc_payload))],\n                                   service_openapi_spec=serperdev_openapi_spec, service_credentials=serper_token)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/sentence_tokenizer.py_84_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 84. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/sentence_tokenizer.py",
      "line_number": 84,
      "code_snippet": "    class CustomPunktLanguageVars(nltk.tokenize.punkt.PunktLanguageVars):\n        # The following adjustment of PunktSentenceTokenizer is inspired by:\n        # https://stackoverflow.com/questions/33139531/preserve-empty-lines-with-nltks-punkt-tokenizer\n        # It is needed for preserving whitespace while splitting text into sentences.\n        _period_context_fmt = r\"\"\"\n                %(SentEndChars)s             # a potential sentence ending",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 30,
      "code_snippet": "\n    The DocumentSplitter is compatible with the following DocumentStores:\n    - [Astra](https://docs.haystack.deepset.ai/docs/astradocumentstore)\n    - [Chroma](https://docs.haystack.deepset.ai/docs/chromadocumentstore) limited support, overlapping information is\n      not stored\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 31,
      "code_snippet": "    The DocumentSplitter is compatible with the following DocumentStores:\n    - [Astra](https://docs.haystack.deepset.ai/docs/astradocumentstore)\n    - [Chroma](https://docs.haystack.deepset.ai/docs/chromadocumentstore) limited support, overlapping information is\n      not stored\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 33,
      "code_snippet": "    - [Chroma](https://docs.haystack.deepset.ai/docs/chromadocumentstore) limited support, overlapping information is\n      not stored\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store) limited support, overlapping",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_34_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 34. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 34,
      "code_snippet": "      not stored\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store) limited support, overlapping\n       information is not stored",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 35,
      "code_snippet": "    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store) limited support, overlapping\n       information is not stored\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 36,
      "code_snippet": "    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store) limited support, overlapping\n       information is not stored\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)\n    - [Weaviate](https://docs.haystack.deepset.ai/docs/weaviatedocumentstore)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_38_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 38. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 38,
      "code_snippet": "    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store) limited support, overlapping\n       information is not stored\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)\n    - [Weaviate](https://docs.haystack.deepset.ai/docs/weaviatedocumentstore)\n\n    ### Usage example",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py_39_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 39. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/document_splitter.py",
      "line_number": 39,
      "code_snippet": "       information is not stored\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)\n    - [Weaviate](https://docs.haystack.deepset.ai/docs/weaviatedocumentstore)\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/csv_document_splitter.py_48_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 48. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/csv_document_splitter.py",
      "line_number": 48,
      "code_snippet": "            - `skip_blank_lines=False` to preserve blank lines\n            - `dtype=object` to prevent type inference (e.g., converting numbers to floats).\n            See https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html for more information.\n        :param split_mode:\n            If `threshold`, the component will split the document based on the number of\n            consecutive empty rows or columns that exceed the `row_split_threshold` or `column_split_threshold`.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/embedding_based_document_splitter.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/preprocessors/embedding_based_document_splitter.py",
      "line_number": 30,
      "code_snippet": "\n    This component is inspired by [5 Levels of Text Splitting](\n        https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n    ) by Greg Kamradt.\n\n    ### Usage example",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py",
      "line_number": 32,
      "code_snippet": "\n    The SentenceWindowRetriever is compatible with the following DocumentStores:\n    - [Astra](https://docs.haystack.deepset.ai/docs/astradocumentstore)\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py",
      "line_number": 33,
      "code_snippet": "    The SentenceWindowRetriever is compatible with the following DocumentStores:\n    - [Astra](https://docs.haystack.deepset.ai/docs/astradocumentstore)\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py_34_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 34. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py",
      "line_number": 34,
      "code_snippet": "    - [Astra](https://docs.haystack.deepset.ai/docs/astradocumentstore)\n    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store)\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py",
      "line_number": 35,
      "code_snippet": "    - [Elasticsearch](https://docs.haystack.deepset.ai/docs/elasticsearch-document-store)\n    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store)\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py",
      "line_number": 36,
      "code_snippet": "    - [OpenSearch](https://docs.haystack.deepset.ai/docs/opensearch-document-store)\n    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store)\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)\n\n    ### Usage example",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/sentence_window_retriever.py",
      "line_number": 37,
      "code_snippet": "    - [Pgvector](https://docs.haystack.deepset.ai/docs/pgvectordocumentstore)\n    - [Pinecone](https://docs.haystack.deepset.ai/docs/pinecone-document-store)\n    - [Qdrant](https://docs.haystack.deepset.ai/docs/qdrant-document-store)\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py",
      "line_number": 30,
      "code_snippet": "\n    Currently the AutoMergingRetriever can only be used by the following DocumentStores:\n    - [AstraDB](https://haystack.deepset.ai/integrations/astradb)\n    - [ElasticSearch](https://haystack.deepset.ai/docs/latest/documentstore/elasticsearch)\n    - [OpenSearch](https://haystack.deepset.ai/docs/latest/documentstore/opensearch)\n    - [PGVector](https://haystack.deepset.ai/docs/latest/documentstore/pgvector)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py",
      "line_number": 31,
      "code_snippet": "    Currently the AutoMergingRetriever can only be used by the following DocumentStores:\n    - [AstraDB](https://haystack.deepset.ai/integrations/astradb)\n    - [ElasticSearch](https://haystack.deepset.ai/docs/latest/documentstore/elasticsearch)\n    - [OpenSearch](https://haystack.deepset.ai/docs/latest/documentstore/opensearch)\n    - [PGVector](https://haystack.deepset.ai/docs/latest/documentstore/pgvector)\n    - [Qdrant](https://haystack.deepset.ai/docs/latest/documentstore/qdrant)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py",
      "line_number": 32,
      "code_snippet": "    - [AstraDB](https://haystack.deepset.ai/integrations/astradb)\n    - [ElasticSearch](https://haystack.deepset.ai/docs/latest/documentstore/elasticsearch)\n    - [OpenSearch](https://haystack.deepset.ai/docs/latest/documentstore/opensearch)\n    - [PGVector](https://haystack.deepset.ai/docs/latest/documentstore/pgvector)\n    - [Qdrant](https://haystack.deepset.ai/docs/latest/documentstore/qdrant)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py",
      "line_number": 33,
      "code_snippet": "    - [ElasticSearch](https://haystack.deepset.ai/docs/latest/documentstore/elasticsearch)\n    - [OpenSearch](https://haystack.deepset.ai/docs/latest/documentstore/opensearch)\n    - [PGVector](https://haystack.deepset.ai/docs/latest/documentstore/pgvector)\n    - [Qdrant](https://haystack.deepset.ai/docs/latest/documentstore/qdrant)\n\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py_34_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 34. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/auto_merging_retriever.py",
      "line_number": 34,
      "code_snippet": "    - [OpenSearch](https://haystack.deepset.ai/docs/latest/documentstore/opensearch)\n    - [PGVector](https://haystack.deepset.ai/docs/latest/documentstore/pgvector)\n    - [Qdrant](https://haystack.deepset.ai/docs/latest/documentstore/qdrant)\n\n    ```python\n    from haystack import Document",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py",
      "line_number": 31,
      "code_snippet": "    \"User-Agent\": DEFAULT_USER_AGENT,\n    \"Accept-Language\": \"en-US,en;q=0.9,it;q=0.8,es;q=0.7\",\n    \"referer\": \"https://www.google.com/\",\n}\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py_91_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 91. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py",
      "line_number": 91,
      "code_snippet": "\n    fetcher = LinkContentFetcher()\n    streams = fetcher.run(urls=[\"https://www.google.com\"])[\"streams\"]\n\n    assert len(streams) == 1\n    assert streams[0].meta == {'content_type': 'text/html', 'url': 'https://www.google.com'}",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py_94_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 94. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py",
      "line_number": 94,
      "code_snippet": "\n    assert len(streams) == 1\n    assert streams[0].meta == {'content_type': 'text/html', 'url': 'https://www.google.com'}\n    assert streams[0].data\n    ```\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py_106_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 106. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py",
      "line_number": 106,
      "code_snippet": "    async def fetch_async():\n        fetcher = LinkContentFetcher()\n        result = await fetcher.run_async(urls=[\"https://www.google.com\"])\n        return result[\"streams\"]\n\n    streams = asyncio.run(fetch_async())",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py_128_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 128. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/fetchers/link_content.py",
      "line_number": 128,
      "code_snippet": "        :param raise_on_failure: If `True`, raises an exception if it fails to fetch a single URL.\n            For multiple URLs, it logs errors and returns the content it successfully fetched.\n        :param user_agents: [User agents](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent)\n            for fetching content. If `None`, a default user agent is used.\n        :param retry_attempts: The number of times to retry to fetch the URL's content.\n        :param timeout: Timeout in seconds for the request.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/joiners/document_joiner.py_237_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 237. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/joiners/document_joiner.py",
      "line_number": 237,
      "code_snippet": "        Merge multiple lists of Documents and assign scores based on Distribution-Based Score Fusion.\n\n        (https://medium.com/plain-simple-software/distribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18)\n        If a Document is in more than one retriever, the one with the highest score is used.\n        \"\"\"\n        for documents in document_lists:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py_15_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 15. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py",
      "line_number": 15,
      "code_snippet": "\n\nSEARCHAPI_BASE_URL = \"https://www.searchapi.io/api/v1/search\"\n\n\nclass SearchApiError(ComponentError): ...",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py_24_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 24. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py",
      "line_number": 24,
      "code_snippet": "class SearchApiWebSearch:\n    \"\"\"\n    Uses [SearchApi](https://www.searchapi.io/) to search the web for relevant documents.\n\n    Usage example:\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py_54_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 54. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py",
      "line_number": 54,
      "code_snippet": "        :param search_params: Additional parameters passed to the SearchApi API.\n            For example, you can set 'num' to 100 to increase the number of search results.\n            See the [SearchApi website](https://www.searchapi.io/) for more details.\n\n            The default search engine is Google, however, users can change it by setting the `engine`\n            parameter in the `search_params`.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py_101_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 101. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/searchapi.py",
      "line_number": 101,
      "code_snippet": "    def run(self, query: str) -> dict[str, list[Document] | list[str]]:\n        \"\"\"\n        Uses [SearchApi](https://www.searchapi.io/) to search the web.\n\n        :param query: Search query.\n        :returns: A dictionary with the following keys:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py_17_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 17. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py",
      "line_number": 17,
      "code_snippet": "\n\nSERPERDEV_BASE_URL = \"https://google.serper.dev/search\"\n\n\nclass SerperDevError(ComponentError): ...",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py_26_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 26. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py",
      "line_number": 26,
      "code_snippet": "class SerperDevWebSearch:\n    \"\"\"\n    Uses [Serper](https://serper.dev/) to search the web for relevant documents.\n\n    See the [Serper Dev website](https://serper.dev/) for more details.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py_28_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 28. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py",
      "line_number": 28,
      "code_snippet": "    Uses [Serper](https://serper.dev/) to search the web for relevant documents.\n\n    See the [Serper Dev website](https://serper.dev/) for more details.\n\n    Usage example:\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py_72_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 72. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py",
      "line_number": 72,
      "code_snippet": "        :param search_params: Additional parameters passed to the Serper API.\n            For example, you can set 'num' to 20 to increase the number of search results.\n            See the [Serper website](https://serper.dev/) for more details.\n        \"\"\"\n        self.api_key = api_key\n        self.top_k = top_k",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py_143_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 143. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/websearch/serper_dev.py",
      "line_number": 143,
      "code_snippet": "    def run(self, query: str) -> dict[str, list[Document] | list[str]]:\n        \"\"\"\n        Use [Serper](https://serper.dev/) to search the web.\n\n        :param query: Search query.\n        :returns: A dictionary with the following keys:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/docx.py_310_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 310. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/docx.py",
      "line_number": 310,
      "code_snippet": "        text = \"\"\n        # Iterate over all hyperlinks and other content in the paragraph\n        # https://python-docx.readthedocs.io/en/latest/api/text.html#docx.text.paragraph.Paragraph.iter_inner_content\n        for content in paragraph.iter_inner_content():\n            if isinstance(content, Run):\n                text += content.text",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/tika.py_57_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 57. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/tika.py",
      "line_number": 57,
      "code_snippet": "    Converts files of different types to Documents.\n\n    This component uses [Apache Tika](https://tika.apache.org/) for parsing the files and, therefore,\n    requires a running Tika server.\n    For more options on running Tika,\n    see the [official documentation](https://github.com/apache/tika-docker/blob/main/README.md#usage).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/tika.py_60_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 60. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/tika.py",
      "line_number": 60,
      "code_snippet": "    requires a running Tika server.\n    For more options on running Tika,\n    see the [official documentation](https://github.com/apache/tika-docker/blob/main/README.md#usage).\n\n    Usage example:\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/tika.py_77_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 77. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/tika.py",
      "line_number": 77,
      "code_snippet": "    \"\"\"\n\n    def __init__(self, tika_url: str = \"http://localhost:9998/tika\", store_full_path: bool = False):\n        \"\"\"\n        Create a TikaDocumentConverter component.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py_38_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 38. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py",
      "line_number": 38,
      "code_snippet": "    To use this component, you need an active Azure account\n    and a Document Intelligence or Cognitive Services resource. For help with setting up your resource, see\n    [Azure documentation](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py_83_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 83. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py",
      "line_number": 83,
      "code_snippet": "        :param model_id:\n            The ID of the model you want to use. For a list of available models, see [Azure documentation]\n            (https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/choose-model-feature).\n        :param preceding_context_len: Number of lines before a table to include as preceding context\n            (this will be added to the metadata).\n        :param following_context_len: Number of lines after a table to include as subsequent context (",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py_202_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 202. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py",
      "line_number": 202,
      "code_snippet": "\n        :param result: The AnalyzeResult object returned by the `begin_analyze_document` method. Docs on Analyze result\n            can be found [here](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-ai-formrecognizer/3.3.0/azure.ai.formrecognizer.html?highlight=read#azure.ai.formrecognizer.AnalyzeResult).\n        :param meta: Optional dictionary with metadata that shall be attached to all resulting documents.\n            Can be any custom keys and values.\n        :returns: List of Documents containing the tables and text extracted from the AnalyzeResult object.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py_331_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 331. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py",
      "line_number": 331,
      "code_snippet": "\n        :param result: The AnalyzeResult object returned by the `begin_analyze_document` method. Docs on Analyze result\n            can be found [here](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-ai-formrecognizer/3.3.0/azure.ai.formrecognizer.html?highlight=read#azure.ai.formrecognizer.AnalyzeResult).\n        :param meta: Optional dictionary with metadata that shall be attached to all resulting documents.\n            Can be any custom keys and values.\n        :returns: A single Document containing all the text extracted from the AnalyzeResult object.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py_377_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 377. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/azure.py",
      "line_number": 377,
      "code_snippet": "\n        :param result: The AnalyzeResult object returned by the `begin_analyze_document` method. Docs on Analyze result\n            can be found [here](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-ai-formrecognizer/3.3.0/azure.ai.formrecognizer.html?highlight=read#azure.ai.formrecognizer.AnalyzeResult).\n        :param meta: Optional dictionary with metadata that shall be attached to all resulting documents.\n            Can be any custom keys and values.\n        :param threshold_y: height threshold in inches for PDF and pixels for images",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/pdfminer.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/pdfminer.py",
      "line_number": 30,
      "code_snippet": "    Converts PDF files to Documents.\n\n    Uses `pdfminer` compatible converters to convert PDF files to Documents. https://pdfminersix.readthedocs.io/en/latest/\n\n    Usage example:\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/pdfminer.py_145_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 145. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/pdfminer.py",
      "line_number": 145,
      "code_snippet": "        as is.\n\n        see: https://pdfminersix.readthedocs.io/en/latest/faq.html#why-are-there-cid-x-values-in-the-textual-output\n\n        :param: text: The text to check for undecoded CID characters\n        :returns:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/html.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/html.py",
      "line_number": 43,
      "code_snippet": "        :param extraction_kwargs: A dictionary containing keyword arguments to customize the extraction process. These\n            are passed to the underlying Trafilatura `extract` function. For the full list of available arguments, see\n            the [Trafilatura documentation](https://trafilatura.readthedocs.io/en/latest/corefunctions.html#extract).\n        :param store_full_path:\n        If True, the full path of the file is stored in the metadata of the document.\n        If False, only the file name is stored.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/openapi_functions.py_34_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 34. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/openapi_functions.py",
      "line_number": 34,
      "code_snippet": "        - requestBody and/or parameters\n        - schema for the requestBody and/or parameters\n    For more details on OpenAPI specification see the [official documentation](https://github.com/OAI/OpenAPI-Specification).\n    For more details on OpenAI function calling see the [official documentation](https://platform.openai.com/docs/guides/function-calling).\n\n    Usage example:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/openapi_functions.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/openapi_functions.py",
      "line_number": 35,
      "code_snippet": "        - schema for the requestBody and/or parameters\n    For more details on OpenAPI specification see the [official documentation](https://github.com/OAI/OpenAPI-Specification).\n    For more details on OpenAI function calling see the [official documentation](https://platform.openai.com/docs/guides/function-calling).\n\n    Usage example:\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/xlsx.py_63_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 63. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/xlsx.py",
      "line_number": 63,
      "code_snippet": "        :param sheet_name: The name of the sheet to read. If None, all sheets are read.\n        :param read_excel_kwargs: Additional arguments to pass to `pandas.read_excel`.\n            See https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html#pandas-read-excel\n        :param table_format_kwargs: Additional keyword arguments to pass to the table format function.\n            - If `table_format` is \"csv\", these arguments are passed to `pandas.DataFrame.to_csv`.\n              See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/xlsx.py_66_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 66. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/xlsx.py",
      "line_number": 66,
      "code_snippet": "        :param table_format_kwargs: Additional keyword arguments to pass to the table format function.\n            - If `table_format` is \"csv\", these arguments are passed to `pandas.DataFrame.to_csv`.\n              See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv\n            - If `table_format` is \"markdown\", these arguments are passed to `pandas.DataFrame.to_markdown`.\n              See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_markdown.html#pandas-dataframe-to-markdown\n        :param store_full_path:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/xlsx.py_68_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 68. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/xlsx.py",
      "line_number": 68,
      "code_snippet": "              See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html#pandas-dataframe-to-csv\n            - If `table_format` is \"markdown\", these arguments are passed to `pandas.DataFrame.to_markdown`.\n              See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_markdown.html#pandas-dataframe-to-markdown\n        :param store_full_path:\n            If True, the full path of the file is stored in the metadata of the document.\n            If False, only the file name is stored.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/json.py_103_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 103. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/json.py",
      "line_number": 103,
      "code_snippet": "\n        An optional `jq_schema` can be provided to extract nested data in the JSON source files.\n        See the [official jq documentation](https://jqlang.github.io/jq/) for more info on the filters syntax.\n        If `jq_schema` is not set, whole JSON source files will be used to extract content.\n\n        Optionally, you can provide a `content_key` to specify which key in the extracted object must",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/agents/agent.py_246",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'warm_up' on line 246 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/agents/agent.py",
      "line_number": 246,
      "code_snippet": "    def warm_up(self) -> None:\n        \"\"\"\n        Warm up the Agent.\n        \"\"\"\n        if not self._is_warmed_up:\n            if hasattr(self.chat_generator, \"warm_up\"):\n                self.chat_generator.warm_up()\n            if hasattr(self._tool_invoker, \"warm_up\") and self._tool_invoker is not None:\n                self._tool_invoker.warm_up()\n            self._is_warmed_up = True\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/document_ndcg.py_114_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 114. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/document_ndcg.py",
      "line_number": 114,
      "code_snippet": "        relevant_id_to_score = {doc.id: doc.score if doc.score is not None else 1 for doc in gt_docs}\n        for i, doc in enumerate(ret_docs):\n            if doc.id in relevant_id_to_score:  # TODO Related to https://github.com/deepset-ai/haystack/issues/8412\n                dcg += relevant_id_to_score[doc.id] / log2(i + 2)  # i + 2 because i is 0-indexed\n        return dcg\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/document_mrr.py_44_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 44. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/document_mrr.py",
      "line_number": 44,
      "code_snippet": "    \"\"\"\n\n    # Refer to https://www.pinecone.io/learn/offline-evaluation/ for the algorithm.\n    @component.output_types(score=float, individual_scores=list[float])\n    def run(\n        self, ground_truth_documents: list[list[Document]], retrieved_documents: list[list[Document]]",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py_141",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 141 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py",
      "line_number": 141,
      "code_snippet": "    def run(self, ground_truth_answers: list[str], predicted_answers: list[str]) -> dict[str, Any]:\n        \"\"\"\n        SASEvaluator component run method.\n\n        Run the SASEvaluator to compute the Semantic Answer Similarity (SAS) between a list of predicted answers\n        and a list of ground truth answers. Both must be list of strings of same length.\n\n        :param ground_truth_answers:\n            A list of expected answers for each question.\n        :param predicted_answers:\n            A list of generated answers for each question.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py_141_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'run'",
      "description": "Function 'run' on line 141 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py",
      "line_number": 141,
      "code_snippet": "\n    @component.output_types(score=float, individual_scores=list[float])\n    def run(self, ground_truth_answers: list[str], predicted_answers: list[str]) -> dict[str, Any]:\n        \"\"\"\n        SASEvaluator component run method.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py_141_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run'",
      "description": "Function 'run' on line 141 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py",
      "line_number": 141,
      "code_snippet": "\n    @component.output_types(score=float, individual_scores=list[float])\n    def run(self, ground_truth_answers: list[str], predicted_answers: list[str]) -> dict[str, Any]:\n        \"\"\"\n        SASEvaluator component run method.\n",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py_141_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'run'",
      "description": "Function 'run' on line 141 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/sas_evaluator.py",
      "line_number": 141,
      "code_snippet": "\n    @component.output_types(score=float, individual_scores=list[float])\n    def run(self, ground_truth_answers: list[str], predicted_answers: list[str]) -> dict[str, Any]:\n        \"\"\"\n        SASEvaluator component run method.\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/document_map.py_46_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 46. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/evaluators/document_map.py",
      "line_number": 46,
      "code_snippet": "    \"\"\"\n\n    # Refer to https://www.pinecone.io/learn/offline-evaluation/ for the algorithm.\n    @component.output_types(score=float, individual_scores=list[float])\n    def run(\n        self, ground_truth_documents: list[list[Document]], retrieved_documents: list[list[Document]]",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_text_embedder.py_75_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 75. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_text_embedder.py",
      "line_number": 75,
      "code_snippet": "        :param azure_ad_token:\n            Microsoft Entra ID token, see Microsoft's\n            [Entra ID](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id)\n            documentation for more information. You can set it with an environment variable\n            `AZURE_OPENAI_AD_TOKEN`, or pass with this parameter during initialization.\n            Previously called Azure Active Directory.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_text_embedder.py_81_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 81. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_text_embedder.py",
      "line_number": 81,
      "code_snippet": "        :param organization:\n            Your organization ID. See OpenAI's\n            [Setting Up Your Organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization)\n            for more information.\n        :param timeout: The timeout for `AzureOpenAI` client calls, in seconds.\n            If not set, defaults to either the",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_text_embedder.py_97_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 97. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_text_embedder.py",
      "line_number": 97,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n\n        \"\"\"\n        # We intentionally do not call super().__init__ here because we only need to instantiate the client to interact",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_document_embedder.py_83_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 83. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_document_embedder.py",
      "line_number": 83,
      "code_snippet": "        :param organization:\n            Your OpenAI organization ID. See OpenAI's\n            [Setting Up Your Organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization)\n            for more information.\n        :param prefix:\n            A string to add at the beginning of each text.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_document_embedder.py_105_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 105. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_document_embedder.py",
      "line_number": 105,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        :param raise_on_failure:\n            Whether to raise an exception if the embedding request fails. If `False`, the component will log the error\n            and continue processing the remaining documents. If `True`, it will raise an exception on failure.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_document_embedder.py_82_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 82. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_document_embedder.py",
      "line_number": 82,
      "code_snippet": "        :param azure_ad_token:\n            Microsoft Entra ID token, see Microsoft's\n            [Entra ID](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id)\n            documentation for more information. You can set it with an environment variable\n            `AZURE_OPENAI_AD_TOKEN`, or pass with this parameter during initialization.\n            Previously called Azure Active Directory.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_document_embedder.py_88_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 88. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_document_embedder.py",
      "line_number": 88,
      "code_snippet": "        :param organization:\n            Your organization ID. See OpenAI's\n            [Setting Up Your Organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization)\n            for more information.\n        :param prefix:\n            A string to add at the beginning of each text.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_document_embedder.py_112_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 112. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/azure_document_embedder.py",
      "line_number": 112,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        :param raise_on_failure:\n            Whether to raise an exception if the embedding request fails. If `False`, the component will log the error\n            and continue processing the remaining documents. If `True`, it will raise an exception on failure.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_text_embedder.py_74_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 74. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_text_embedder.py",
      "line_number": 74,
      "code_snippet": "        :param organization:\n            Your organization ID. See OpenAI's\n            [production best practices](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization)\n            for more information.\n        :param prefix:\n            A string to add at the beginning of each text to embed.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_text_embedder.py_88_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 88. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/openai_text_embedder.py",
      "line_number": 88,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        \"\"\"\n        self.model = model\n        self.dimensions = dimensions",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_text_embedder.py_110_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 110. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_text_embedder.py",
      "line_number": 110,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        :param revision:\n            The specific model version to use. It can be a branch name, a tag name, or a commit id,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_text_embedder.py_27_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 27. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_text_embedder.py",
      "line_number": 27,
      "code_snippet": "    - [Free Serverless Inference API](https://huggingface.co/inference-api)\n    - [Paid Inference Endpoints](https://huggingface.co/inference-endpoints)\n    - [Self-hosted Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference)\n\n    ### Usage examples\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_text_embedder.py_67_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 67. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_text_embedder.py",
      "line_number": 67,
      "code_snippet": "\n    text_embedder = HuggingFaceAPITextEmbedder(api_type=\"text_embeddings_inference\",\n                                               api_params={\"url\": \"http://localhost:8080\"})\n\n    print(text_embedder.run(\"I love pizza!\"))\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_document_embedder.py_121_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 121. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_document_embedder.py",
      "line_number": 121,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        :param revision:\n            The specific model version to use. It can be a branch name, a tag name, or a commit id,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_sparse_text_embedder.py_85_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 85. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_sparse_text_embedder.py",
      "line_number": 85,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        :param revision:\n            The specific model version to use. It can be a branch name, a tag name, or a commit id,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_document_embedder.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_document_embedder.py",
      "line_number": 32,
      "code_snippet": "    - [Free Serverless Inference API](https://huggingface.co/inference-api)\n    - [Paid Inference Endpoints](https://huggingface.co/inference-endpoints)\n    - [Self-hosted Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference)\n\n\n    ### Usage examples",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_document_embedder.py_84_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 84. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/hugging_face_api_document_embedder.py",
      "line_number": 84,
      "code_snippet": "\n    doc_embedder = HuggingFaceAPIDocumentEmbedder(api_type=\"text_embeddings_inference\",\n                                                  api_params={\"url\": \"http://localhost:8080\"})\n\n    result = document_embedder.run([doc])\n    print(result[\"documents\"][0].embedding)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_sparse_document_embedder.py_102_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 102. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/sentence_transformers_sparse_document_embedder.py",
      "line_number": 102,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        :param revision:\n            The specific model version to use. It can be a branch name, a tag name, or a commit id,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py_25_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 25. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py",
      "line_number": 25,
      "code_snippet": "\n    The component requires an OpenAI API key, see the\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/authentication) for more details.\n    For the supported audio formats, languages, and other parameters, see the\n    [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text).\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py_27_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 27. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py",
      "line_number": 27,
      "code_snippet": "    [OpenAI documentation](https://platform.openai.com/docs/api-reference/authentication) for more details.\n    For the supported audio formats, languages, and other parameters, see the\n    [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py_59_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 59. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py",
      "line_number": 59,
      "code_snippet": "        :param organization:\n            Your OpenAI organization ID. See OpenAI's documentation on\n            [Setting Up Your Organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param api_base:\n            An optional URL to use as the API base. For details, see the\n            OpenAI [documentation](https://platform.openai.com/docs/api-reference/audio).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py_62_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 62. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py",
      "line_number": 62,
      "code_snippet": "        :param api_base:\n            An optional URL to use as the API base. For details, see the\n            OpenAI [documentation](https://platform.openai.com/docs/api-reference/audio).\n        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py_65_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 65. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py",
      "line_number": 65,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        :param kwargs:\n            Other optional parameters for the model. These are sent directly to the OpenAI\n            endpoint. See OpenAI [documentation](https://platform.openai.com/docs/api-reference/audio) for more details.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py_68_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 68. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_remote.py",
      "line_number": 68,
      "code_snippet": "        :param kwargs:\n            Other optional parameters for the model. These are sent directly to the OpenAI\n            endpoint. See OpenAI [documentation](https://platform.openai.com/docs/api-reference/audio) for more details.\n            Some of the supported parameters are:\n            - `language`: The language of the input audio.\n              Provide the input language in ISO-639-1 format",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_172",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_raw_transcribe' on line 172 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 172,
      "code_snippet": "    def _raw_transcribe(self, sources: list[str | Path | ByteStream], **kwargs) -> dict[Path, Any]:\n        \"\"\"\n        Transcribes the given audio files. Returns the output of the model, a dictionary, for each input file.\n\n        For the supported audio formats, languages, and other parameters, see the\n        [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n        [github repo](https://github.com/openai/whisper).\n\n        :param sources:\n            A list of paths or binary streams to transcribe.\n        :returns:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_39_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 39. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 39,
      "code_snippet": "\n    For the supported audio formats, languages, and other parameters, see the\n    [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n    [GitHub repository](https://github.com/openai/whisper).\n\n    ### Usage example",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_40_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 40. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 40,
      "code_snippet": "    For the supported audio formats, languages, and other parameters, see the\n    [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n    [GitHub repository](https://github.com/openai/whisper).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_66_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 66. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 66,
      "code_snippet": "            \"tiny\", \"base\", \"small\", \"medium\", \"large\" (default).\n            For details on the models and their modifications, see the\n            [Whisper documentation](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages).\n        :param device:\n            The device for loading the model. If `None`, automatically selects the default device.\n        \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_120_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 120. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 120,
      "code_snippet": "        :param whisper_params:\n            For the supported audio formats, languages, and other parameters, see the\n            [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n            [GitHup repo](https://github.com/openai/whisper).\n\n        :returns: A dictionary with the following keys:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_121_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 121. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 121,
      "code_snippet": "            For the supported audio formats, languages, and other parameters, see the\n            [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n            [GitHup repo](https://github.com/openai/whisper).\n\n        :returns: A dictionary with the following keys:\n            - `documents`: A list of documents where each document is a transcribed audio file. The content of",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_143_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 143. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 143,
      "code_snippet": "\n        For the supported audio formats, languages, and other parameters, see the\n        [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n        [github repo](https://github.com/openai/whisper).\n\n        :param sources:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_144_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 144. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 144,
      "code_snippet": "        For the supported audio formats, languages, and other parameters, see the\n        [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n        [github repo](https://github.com/openai/whisper).\n\n        :param sources:\n            A list of paths or binary streams to transcribe.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_177_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 177. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 177,
      "code_snippet": "\n        For the supported audio formats, languages, and other parameters, see the\n        [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n        [github repo](https://github.com/openai/whisper).\n\n        :param sources:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_178_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 178. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 178,
      "code_snippet": "        For the supported audio formats, languages, and other parameters, see the\n        [Whisper API documentation](https://platform.openai.com/docs/guides/speech-to-text) and the official Whisper\n        [github repo](https://github.com/openai/whisper).\n\n        :param sources:\n            A list of paths or binary streams to transcribe.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_172_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_raw_transcribe'",
      "description": "Function '_raw_transcribe' on line 172 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 172,
      "code_snippet": "            return path\n\n    def _raw_transcribe(self, sources: list[str | Path | ByteStream], **kwargs) -> dict[Path, Any]:\n        \"\"\"\n        Transcribes the given audio files. Returns the output of the model, a dictionary, for each input file.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py_172_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_raw_transcribe'",
      "description": "Function '_raw_transcribe' on line 172 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/audio/whisper_local.py",
      "line_number": 172,
      "code_snippet": "            return path\n\n    def _raw_transcribe(self, sources: list[str | Path | ByteStream], **kwargs) -> dict[Path, Any]:\n        \"\"\"\n        Transcribes the given audio files. Returns the output of the model, a dictionary, for each input file.\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/validators/json_schema.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/validators/json_schema.py",
      "line_number": 31,
      "code_snippet": "class JsonSchemaValidator:\n    \"\"\"\n    Validates JSON content of `ChatMessage` against a specified [JSON Schema](https://json-schema.org/).\n\n    If JSON content of a message conforms to the provided schema, the message is passed along the \"validated\" output.\n    If the JSON content does not conform to the schema, the message is passed along the \"validation_error\" output.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/validators/json_schema.py_106_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 106. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/validators/json_schema.py",
      "line_number": 106,
      "code_snippet": "        Initialize the JsonSchemaValidator component.\n\n        :param json_schema: A dictionary representing the [JSON schema](https://json-schema.org/) against which\n            the messages' content is validated.\n        :param error_template: A custom template string for formatting the error message in case of validation failure.\n        \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/validators/json_schema.py_125_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 125. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/validators/json_schema.py",
      "line_number": 125,
      "code_snippet": "        :param messages: A list of ChatMessage instances to be validated. The last message in this list is the one\n            that is validated.\n        :param json_schema: A dictionary representing the [JSON schema](https://json-schema.org/)\n            against which the messages' content is validated. If not provided, the schema from the component init\n            is used.\n        :param error_template: A custom template string for formatting the error message in case of validation. If not",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/hugging_face_api.py_42_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 42. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/hugging_face_api.py",
      "line_number": 42,
      "code_snippet": "    Use it with the following Hugging Face APIs:\n    - [Paid Inference Endpoints](https://huggingface.co/inference-endpoints)\n    - [Self-hosted Text Generation Inference](https://github.com/huggingface/text-generation-inference)\n\n    **Note:** As of July 2025, the Hugging Face Inference API no longer offers generative models through the\n    `text_generation` endpoint. Generative models are now only available through providers supporting the",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/hugging_face_api.py_70_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 70. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/hugging_face_api.py",
      "line_number": 70,
      "code_snippet": "\n    generator = HuggingFaceAPIGenerator(api_type=\"text_generation_inference\",\n                                        api_params={\"url\": \"http://localhost:8080\"})\n\n    result = generator.run(prompt=\"What's Natural Language Processing?\")\n    print(result)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/hugging_face_api.py_109_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 109. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/hugging_face_api.py",
      "line_number": 109,
      "code_snippet": "        :param api_type:\n            The type of Hugging Face API to use. Available types:\n            - `text_generation_inference`: See [TGI](https://github.com/huggingface/text-generation-inference).\n            - `inference_endpoints`: See [Inference Endpoints](https://huggingface.co/inference-endpoints).\n            - `serverless_inference_api`: See [Serverless Inference API](https://huggingface.co/inference-api).\n              This might no longer work due to changes in the models offered in the Hugging Face Inference API.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 32,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat).\n\n\n    ### Usage example",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_41_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 41. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 41,
      "code_snippet": "    from haystack.utils import Secret\n    client = AzureOpenAIGenerator(\n        azure_endpoint=\"<Your Azure endpoint e.g. `https://your-company.azure.openai.com/>\",\n        api_key=Secret.from_token(\"<your-api-key>\"),\n        azure_deployment=\"<this a model name, e.g.  gpt-4.1-mini>\")\n    response = client.run(\"What's Natural Language Processing? Be brief.\")",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_79_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 79. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 79,
      "code_snippet": "        Initialize the Azure OpenAI Generator.\n\n        :param azure_endpoint: The endpoint of the deployed model, for example `https://example-resource.azure.openai.com/`.\n        :param api_version: The version of the API to use. Defaults to 2024-12-01-preview.\n        :param azure_deployment: The deployment of the model, usually the model name.\n        :param api_key: The API key to use for authentication.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_83_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 83. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 83,
      "code_snippet": "        :param azure_deployment: The deployment of the model, usually the model name.\n        :param api_key: The API key to use for authentication.\n        :param azure_ad_token: [Azure Active Directory token](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id).\n        :param organization: Your organization ID, defaults to `None`. For help, see\n        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_85_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 85. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 85,
      "code_snippet": "        :param azure_ad_token: [Azure Active Directory token](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id).\n        :param organization: Your organization ID, defaults to `None`. For help, see\n        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.\n            It accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_87_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 87. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 87,
      "code_snippet": "        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.\n            It accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.\n        :param system_prompt: The system prompt to use for text generation. If not provided, the Generator\n        omits the system prompt and uses the default system prompt.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_97_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 97. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 97,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        :param generation_kwargs: Other parameters to use for the model, sent directly to\n            the OpenAI endpoint. See [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat) for\n            more details.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py_99_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 99. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/azure.py",
      "line_number": 99,
      "code_snippet": "            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        :param generation_kwargs: Other parameters to use for the model, sent directly to\n            the OpenAI endpoint. See [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat) for\n            more details.\n            Some of the supported parameters:\n            - `max_completion_tokens`: An upper bound for the number of tokens that can be generated for a completion,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_125",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.client.images.generate' is used in 'Response' on line 125 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 125,
      "code_snippet": "        response_format = response_format or self.response_format\n        response = self.client.images.generate(  # type: ignore[union-attr]\n            model=self.model, prompt=prompt, size=size, quality=quality, response_format=response_format, n=1\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_98",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 98 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 98,
      "code_snippet": "    def run(\n        self,\n        prompt: str,\n        size: Literal[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"] | None = None,\n        quality: Literal[\"standard\", \"hd\"] | None = None,\n        response_format: Literal[\"url\", \"b64_json\"] | None = None,\n    ):\n        \"\"\"\n        Invokes the image generation inference based on the provided prompt and generation parameters.\n\n        :param prompt: The prompt to generate the image.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_125_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.model' is used without version pinning on line 125. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 125,
      "code_snippet": "        quality = quality or self.quality\n        response_format = response_format or self.response_format\n        response = self.client.images.generate(  # type: ignore[union-attr]\n            model=self.model, prompt=prompt, size=size, quality=quality, response_format=response_format, n=1\n        )\n        if response.data is not None:",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_22_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 22. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 22,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/images/create).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_67_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 67. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 67,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        \"\"\"\n        self.model = model\n        self.quality = quality",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_98_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run'",
      "description": "Function 'run' on line 98 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 98,
      "code_snippet": "\n    @component.output_types(images=list[str], revised_prompt=str)\n    def run(\n        self,\n        prompt: str,\n        size: Literal[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"] | None = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py_98_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'run'",
      "description": "Function 'run' on line 98 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai_dalle.py",
      "line_number": 98,
      "code_snippet": "\n    @component.output_types(images=list[str], revised_prompt=str)\n    def run(\n        self,\n        prompt: str,\n        size: Literal[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"] | None = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_233",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.client.chat.completions.create' is used in 'Response' on line 233 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 233,
      "code_snippet": "\n        completion: Stream[ChatCompletionChunk] | ChatCompletion = self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_formatted_messages,  # type: ignore",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_189",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 189 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 189,
      "code_snippet": "    def run(\n        self,\n        prompt: str,\n        system_prompt: str | None = None,\n        streaming_callback: StreamingCallbackT | None = None,\n        generation_kwargs: dict[str, Any] | None = None,\n    ) -> dict[str, list[str] | list[dict[str, Any]]]:\n        \"\"\"\n        Invoke the text generation inference based on the provided messages and generation parameters.\n\n        :param prompt:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_233_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.model' is used without version pinning on line 233. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 233,
      "code_snippet": "        openai_formatted_messages = [message.to_openai_dict_format() for message in messages]\n\n        completion: Stream[ChatCompletionChunk] | ChatCompletion = self.client.chat.completions.create(\n            model=self.model,\n            messages=openai_formatted_messages,  # type: ignore\n            stream=streaming_callback is not None,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_46_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 46. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 46,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_92_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 92. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 92,
      "code_snippet": "        omitted, and the default system prompt of the model is used.\n        :param generation_kwargs: Other parameters to use for the model. These parameters are all sent directly to\n            the OpenAI endpoint. See OpenAI [documentation](https://platform.openai.com/docs/api-reference/chat) for\n            more details.\n            Some of the supported parameters:\n            - `max_completion_tokens`: An upper bound for the number of tokens that can be generated for a completion,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_119_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 119. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 119,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        \"\"\"\n        self.api_key = api_key\n        self.model = model",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_209_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 209. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 209,
      "code_snippet": "            Additional keyword arguments for text generation. These parameters will potentially override the parameters\n            passed in the `__init__` method. For more details on the parameters supported by the OpenAI API, refer to\n            the OpenAI [documentation](https://platform.openai.com/docs/api-reference/chat/create).\n        :returns:\n            A list of strings containing the generated responses and a list of dictionaries containing the metadata\n        for each response.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py_189_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run'",
      "description": "Function 'run' on line 189 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/openai.py",
      "line_number": 189,
      "code_snippet": "\n    @component.output_types(replies=list[str], meta=list[dict[str, Any]])\n    def run(\n        self,\n        prompt: str,\n        system_prompt: str | None = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/readers/extractive.py_418_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 418. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/readers/extractive.py",
      "line_number": 418,
      "code_snippet": "\n        This Stack overflow\n        [post](https://stackoverflow.com/questions/325933/determine-whether-two-date-ranges-overlap/325964#325964)\n        explains how to calculate the overlap between two ranges.\n        \"\"\"\n        # Check for overlap: (StartA <= EndB) and (StartB <= EndA)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_similarity.py_109_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 109. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_similarity.py",
      "line_number": 109,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        :param batch_size:\n            The batch size to use for inference. The higher the batch size, the more memory is required.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/hugging_face_tei.py_34_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 34. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/hugging_face_tei.py",
      "line_number": 34,
      "code_snippet": "\n    It can be used with a Text Embeddings Inference (TEI) API endpoint:\n    - [Self-hosted Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference)\n    - [Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints)\n\n    Usage example:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/hugging_face_tei.py_44_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 44. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/hugging_face_tei.py",
      "line_number": 44,
      "code_snippet": "\n    reranker = HuggingFaceTEIRanker(\n        url=\"http://localhost:8080\",\n        top_k=5,\n        timeout=30,\n        token=Secret.from_token(\"my_api_token\")",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/hugging_face_tei.py_75_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 75. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/hugging_face_tei.py",
      "line_number": 75,
      "code_snippet": "        Initializes the TEI reranker component.\n\n        :param url: Base URL of the TEI reranking service (for example, \"https://api.example.com\").\n        :param top_k: Maximum number of top documents to return.\n        :param raw_scores: If True, include raw relevance scores in the API payload.\n        :param timeout: Request timeout in seconds.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py_327",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'query' directly embedded in LLM prompt",
      "description": "The function '_embed_and_normalize' embeds user input ('query') directly into an LLM prompt using concatenation. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
      "line_number": 327,
      "code_snippet": "        doc_embeddings = self.model.encode(texts_to_embed, convert_to_tensor=True)\n        query_embedding = self.model.encode([self.query_prefix + query + self.query_suffix], convert_to_tensor=True)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py_322",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_embed_and_normalize' on line 322 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
      "line_number": 322,
      "code_snippet": "    def _embed_and_normalize(self, query, texts_to_embed):\n        assert self.model is not None  # verified in run but mypy doesn't see it\n\n        # Calculate embeddings\n        doc_embeddings = self.model.encode(texts_to_embed, convert_to_tensor=True)\n        query_embedding = self.model.encode([self.query_prefix + query + self.query_suffix], convert_to_tensor=True)\n\n        # Normalize embeddings to unit length for computing cosine similarity\n        if self.similarity == DiversityRankingSimilarity.COSINE:\n            doc_embeddings = doc_embeddings / torch.norm(doc_embeddings, p=2, dim=-1).unsqueeze(-1)\n            query_embedding = query_embedding / torch.norm(query_embedding, p=2, dim=-1).unsqueeze(-1)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py_167_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 167. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
      "line_number": 167,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        \"\"\"\n        torch_and_sentence_transformers_import.check()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py_351_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 351. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
      "line_number": 351,
      "code_snippet": "\n        See : \"The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries\"\n               https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf\n        \"\"\"\n\n        texts_to_embed = self._prepare_texts_to_embed(documents)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py_322_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_embed_and_normalize'",
      "description": "Function '_embed_and_normalize' on line 322 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
      "line_number": 322,
      "code_snippet": "        return ranked_docs\n\n    def _embed_and_normalize(self, query, texts_to_embed):\n        assert self.model is not None  # verified in run but mypy doesn't see it\n\n        # Calculate embeddings",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py_322_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_embed_and_normalize'",
      "description": "Function '_embed_and_normalize' on line 322 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/sentence_transformers_diversity.py",
      "line_number": 322,
      "code_snippet": "        return ranked_docs\n\n    def _embed_and_normalize(self, query, texts_to_embed):\n        assert self.model is not None  # verified in run but mypy doesn't see it\n\n        # Calculate embeddings",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/lost_in_the_middle.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/lost_in_the_middle.py",
      "line_number": 23,
      "code_snippet": "    Lost in the Middle ranking lays out document contents into LLM context so that the most relevant contents are at\n    the beginning or end of the input context, while the least relevant is in the middle of the context. See the\n    paper [\"Lost in the Middle: How Language Models Use Long Contexts\"](https://arxiv.org/abs/2307.03172) for more\n    details.\n\n    Usage example:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/meta_field.py_410_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 410. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/rankers/meta_field.py",
      "line_number": 410,
      "code_snippet": "\n        The constant K is set to 61 (60 was suggested by the original paper, plus 1 as python lists are 0-based and\n        the [paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) used 1-based ranking).\n        \"\"\"\n        return 1 / (k + rank)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_213",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.chat_generator.run' is used in 'run(' on line 213 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 213,
      "code_snippet": "            prompt_result = self._prompt_builder.run(query=query.strip(), n_expansions=expansion_count)\n            generator_result = self.chat_generator.run(messages=[ChatMessage.from_user(prompt_result[\"prompt\"])])\n\n            if not generator_result.get(\"replies\") or len(generator_result[\"replies\"]) == 0:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_213",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.chat_generator.run' is used in 'run(' on line 213 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 213,
      "code_snippet": "            prompt_result = self._prompt_builder.run(query=query.strip(), n_expansions=expansion_count)\n            generator_result = self.chat_generator.run(messages=[ChatMessage.from_user(prompt_result[\"prompt\"])])\n\n            if not generator_result.get(\"replies\") or len(generator_result[\"replies\"]) == 0:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_252",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat_generator.warm_up' is used in 'Response' on line 252 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 252,
      "code_snippet": "            if hasattr(self.chat_generator, \"warm_up\"):\n                self.chat_generator.warm_up()\n            self._is_warmed_up = True\n",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_183",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'run' on line 183 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 183,
      "code_snippet": "    def run(self, query: str, n_expansions: int | None = None) -> dict[str, list[str]]:\n        \"\"\"\n        Expand the input query into multiple semantically similar queries.\n\n        The language of the original query is preserved in the expanded queries.\n\n        :param query: The original query to expand.\n        :param n_expansions: Number of additional queries to generate (not including the original).\n            If None, uses the value from initialization. Can be 0 to generate no additional queries.\n        :return: Dictionary with \"queries\" key containing the list of expanded queries.\n            If include_original_query=True, the original query will be included in addition",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_246",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'warm_up' on line 246 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 246,
      "code_snippet": "    def warm_up(self):\n        \"\"\"\n        Warm up the LLM provider component.\n        \"\"\"\n        if not self._is_warmed_up:\n            if hasattr(self.chat_generator, \"warm_up\"):\n                self.chat_generator.warm_up()\n            self._is_warmed_up = True\n\n    @staticmethod\n    def _parse_expanded_queries(generator_response: str) -> list[str]:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_100_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 100. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 100,
      "code_snippet": "        :param chat_generator: The chat generator component to use for query expansion.\n            If None, a default OpenAIChatGenerator with gpt-4.1-mini model is used.\n        :param prompt_template: Custom [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder)\n            template for query expansion. The template should instruct the LLM to return a JSON response with the\n            structure: `{\"queries\": [\"query1\", \"query2\", \"query3\"]}`. The template should include 'query' and\n            'n_expansions' variables.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_183_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run'",
      "description": "Function 'run' on line 183 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 183,
      "code_snippet": "\n    @component.output_types(queries=list[str])\n    def run(self, query: str, n_expansions: int | None = None) -> dict[str, list[str]]:\n        \"\"\"\n        Expand the input query into multiple semantically similar queries.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_183_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run'",
      "description": "Function 'run' on line 183 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 183,
      "code_snippet": "\n    @component.output_types(queries=list[str])\n    def run(self, query: str, n_expansions: int | None = None) -> dict[str, list[str]]:\n        \"\"\"\n        Expand the input query into multiple semantically similar queries.\n",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py_183_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'run'",
      "description": "Function 'run' on line 183 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/query/query_expander.py",
      "line_number": 183,
      "code_snippet": "\n    @component.output_types(queries=list[str])\n    def run(self, query: str, n_expansions: int | None = None) -> dict[str, list[str]]:\n        \"\"\"\n        Expand the input query into multiple semantically similar queries.\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_29_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 29. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 29,
      "code_snippet": "    docstore = InMemoryDocumentStore()\n    documents = [\n        Document(content=\"doc1\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc2\", meta={\"url\": \"https://example.com/2\"}),\n        Document(content=\"doc3\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc4\", meta={\"url\": \"https://example.com/2\"}),",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 30,
      "code_snippet": "    documents = [\n        Document(content=\"doc1\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc2\", meta={\"url\": \"https://example.com/2\"}),\n        Document(content=\"doc3\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc4\", meta={\"url\": \"https://example.com/2\"}),\n    ]",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 31,
      "code_snippet": "        Document(content=\"doc1\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc2\", meta={\"url\": \"https://example.com/2\"}),\n        Document(content=\"doc3\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc4\", meta={\"url\": \"https://example.com/2\"}),\n    ]\n    docstore.write_documents(documents)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_32_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 32. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 32,
      "code_snippet": "        Document(content=\"doc2\", meta={\"url\": \"https://example.com/2\"}),\n        Document(content=\"doc3\", meta={\"url\": \"https://example.com/1\"}),\n        Document(content=\"doc4\", meta={\"url\": \"https://example.com/2\"}),\n    ]\n    docstore.write_documents(documents)\n    checker = CacheChecker(docstore, cache_field=\"url\")",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 36,
      "code_snippet": "    docstore.write_documents(documents)\n    checker = CacheChecker(docstore, cache_field=\"url\")\n    results = checker.run(items=[\"https://example.com/1\", \"https://example.com/5\"])\n    assert results == {\"hits\": [documents[0], documents[2]], \"misses\": [\"https://example.com/5\"]}\n    ```\n    \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 36,
      "code_snippet": "    docstore.write_documents(documents)\n    checker = CacheChecker(docstore, cache_field=\"url\")\n    results = checker.run(items=[\"https://example.com/1\", \"https://example.com/5\"])\n    assert results == {\"hits\": [documents[0], documents[2]], \"misses\": [\"https://example.com/5\"]}\n    ```\n    \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/caching/cache_checker.py",
      "line_number": 37,
      "code_snippet": "    checker = CacheChecker(docstore, cache_field=\"url\")\n    results = checker.run(items=[\"https://example.com/1\", \"https://example.com/5\"])\n    assert results == {\"hits\": [documents[0], documents[2]], \"misses\": [\"https://example.com/5\"]}\n    ```\n    \"\"\"\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/extractors/named_entity_extractor.py_85_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 85. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/extractors/named_entity_extractor.py",
      "line_number": 85,
      "code_snippet": "    former can be used with any sequence classification model from the\n    [Hugging Face model hub](https://huggingface.co/models), while the\n    latter can be used with any [spaCy model](https://spacy.io/models)\n    that contains an NER component. Annotations are stored as metadata\n    in the documents.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/classifiers/document_language_classifier.py_68_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 68. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/classifiers/document_language_classifier.py",
      "line_number": 68,
      "code_snippet": "\n        :param languages: A list of ISO language codes.\n            See the supported languages in [`langdetect` documentation](https://github.com/Mimino666/langdetect#languages).\n            If not specified, defaults to [\"en\"].\n        \"\"\"\n        langdetect_import.check()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_local.py_612_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 612. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_local.py",
      "line_number": 612,
      "code_snippet": "            enable_thinking=self.enable_thinking,\n        )\n        # prepared_prompt is a string since we set tokenize=False https://hf.co/docs/transformers/main/chat_templating\n        assert isinstance(prepared_prompt, str)\n\n        # Avoid some unnecessary warnings in the generation pipeline call",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_51_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 51. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 51,
      "code_snippet": "\n    It works with the gpt-4 and o-series models and supports streaming responses\n    from OpenAI API. It uses [ChatMessage](https://docs.haystack.deepset.ai/docs/chatmessage)\n    format in input and output.\n\n    You can customize how the text is generated by passing parameters to the",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_60_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 60. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 60,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/responses).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_103_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 103. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 103,
      "code_snippet": "        :param model: The name of the model to use.\n        :param streaming_callback: A callback function that is called when a new token is received from the stream.\n            The callback function accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.\n        :param api_base_url: An optional base URL.\n        :param organization: Your organization ID, defaults to `None`. See",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_107_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 107. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 107,
      "code_snippet": "        :param api_base_url: An optional base URL.\n        :param organization: Your organization ID, defaults to `None`. See\n        [production best practices](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param generation_kwargs: Other parameters to use for the model. These parameters are sent\n           directly to the OpenAI endpoint.\n           See OpenAI [documentation](https://platform.openai.com/docs/api-reference/responses) for",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_110_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 110. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 110,
      "code_snippet": "        :param generation_kwargs: Other parameters to use for the model. These parameters are sent\n           directly to the OpenAI endpoint.\n           See OpenAI [documentation](https://platform.openai.com/docs/api-reference/responses) for\n            more details.\n            Some of the supported parameters:\n            - `temperature`: What sampling temperature to use. Higher values like 0.8 will make the output more random,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_123_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 123. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 123,
      "code_snippet": "                If provided, the output will always be validated against this\n                format (unless the model returns a tool call).\n                For details, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs).\n            - `text`: A JSON schema that enforces the structure of the model's response.\n                If provided, the output will always be validated against this\n                format (unless the model returns a tool call).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_132_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 132. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 132,
      "code_snippet": "                - Currently, this component doesn't support streaming for structured outputs.\n                - Older models only support basic version of structured outputs through `{\"type\": \"json_object\"}`.\n                    For detailed information on JSON mode, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs#json-mode).\n            - `reasoning`: A dictionary of parameters for reasoning. For example:\n                - `summary`: The summary of the reasoning.\n                - `effort`: The level of effort to put into the reasoning. Can be `low`, `medium` or `high`.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_138_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 138. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 138,
      "code_snippet": "                - `generate_summary`: Whether to generate a summary of the reasoning.\n                Note: OpenAI does not return the reasoning tokens, but we can view summary if its enabled.\n                For details, see the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning).\n        :param timeout:\n            Timeout for OpenAI client calls. If not set, it defaults to either the\n            `OPENAI_TIMEOUT` environment variable, or 30 seconds.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_150_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 150. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 150,
      "code_snippet": "            OpenAI/MCP tool definitions.\n            Note: You cannot pass OpenAI/MCP tools and Haystack tools together.\n            For details on tool support, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/responses/create#responses-create-tools).\n        :param tools_strict:\n            Whether to enable strict schema adherence for tool calls. If set to `False`, the model may not exactly\n            follow the schema provided in the `parameters` field of the tool definition. In Response API, tool calls",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_157_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 157. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 157,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n\n        \"\"\"\n        self.api_key = api_key",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_311_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 311. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 311,
      "code_snippet": "            Additional keyword arguments for text generation. These parameters will\n            override the parameters passed during component initialization.\n            For details on OpenAI API parameters, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/responses/create).\n        :param tools:\n            The tools that the model can use to prepare calls. If set, it will override the\n            `tools` parameter set during component initialization. This parameter can accept either a",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_318_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 318. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 318,
      "code_snippet": "            OpenAI/MCP tool definitions.\n            Note: You cannot pass OpenAI/MCP tools and Haystack tools together.\n            For details on tool support, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/responses/create#responses-create-tools).\n        :param tools_strict:\n            Whether to enable strict schema adherence for tool calls. If set to `False`, the model may not exactly\n            follow the schema provided in the `parameters` field of the tool definition. In Response API, tool calls",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_385_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 385. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 385,
      "code_snippet": "            Additional keyword arguments for text generation. These parameters will\n            override the parameters passed during component initialization.\n            For details on OpenAI API parameters, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/responses/create).\n        :param tools:\n            A list of tools or a Toolset for which the model can prepare calls. If set, it will override the\n            `tools` parameter set during component initialization. This parameter can accept either a list of",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py_539_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 539. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai_responses.py",
      "line_number": 539,
      "code_snippet": "        if output.type == \"reasoning\":\n            # openai doesn't return the reasoning tokens, but we can view summary if its enabled\n            # https://platform.openai.com/docs/guides/reasoning#reasoning-summaries\n            summaries = output.summary\n            extra = output.to_dict()\n            # we dont need the summary in the extra",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_553",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_run_streaming' on line 553 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 553,
      "code_snippet": "    def _run_streaming(\n        self,\n        messages: list[dict[str, str]],\n        generation_kwargs: dict[str, Any],\n        streaming_callback: SyncStreamingCallbackT,\n    ):\n        api_output: Iterable[ChatCompletionStreamOutput] = self._client.chat_completion(\n            messages,\n            stream=True,\n            stream_options=ChatCompletionInputStreamOptions(include_usage=True),\n            **generation_kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_581",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_run_non_streaming' on line 581 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 581,
      "code_snippet": "    def _run_non_streaming(\n        self,\n        messages: list[dict[str, str]],\n        generation_kwargs: dict[str, Any],\n        tools: list[\"ChatCompletionInputTool\"] | None = None,\n    ) -> dict[str, list[ChatMessage]]:\n        api_chat_output: ChatCompletionOutput = self._client.chat_completion(\n            messages=messages, tools=tools, **generation_kwargs\n        )\n\n        if api_chat_output.choices is None or len(api_chat_output.choices) == 0:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_136_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 136. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 136,
      "code_snippet": "    Uses the full choice object to detect tool calls and provide accurate mapping.\n\n    HuggingFace finish reasons (can be found here https://huggingface.github.io/text-generation-inference/ under\n    FinishReason):\n    - \"length\": number of generated tokens == `max_new_tokens`\n    - \"eos_token\": the model generated its end of sequence token",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_217_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 217. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 217,
      "code_snippet": "    Completes chats using Hugging Face APIs.\n\n    HuggingFaceAPIChatGenerator uses the [ChatMessage](https://docs.haystack.deepset.ai/docs/chatmessage)\n    format for input and output. Use it to generate text with Hugging Face APIs:\n    - [Serverless Inference API (Inference Providers)](https://huggingface.co/docs/inference-providers)\n    - [Paid Inference Endpoints](https://huggingface.co/inference-endpoints)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_221_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 221. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 221,
      "code_snippet": "    - [Serverless Inference API (Inference Providers)](https://huggingface.co/docs/inference-providers)\n    - [Paid Inference Endpoints](https://huggingface.co/inference-endpoints)\n    - [Self-hosted Text Generation Inference](https://github.com/huggingface/text-generation-inference)\n\n    ### Usage examples\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_303_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 303. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 303,
      "code_snippet": "\n    generator = HuggingFaceAPIChatGenerator(api_type=\"text_generation_inference\",\n                                            api_params={\"url\": \"http://localhost:8080\"})\n\n    result = generator.run(messages)\n    print(result)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_325_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 325. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 325,
      "code_snippet": "        :param api_type:\n            The type of Hugging Face API to use. Available types:\n            - `text_generation_inference`: See [TGI](https://github.com/huggingface/text-generation-inference).\n            - `inference_endpoints`: See [Inference Endpoints](https://huggingface.co/inference-endpoints).\n            - `serverless_inference_api`: See\n            [Serverless Inference API - Inference Providers](https://huggingface.co/docs/inference-providers).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_587_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_run_non_streaming'",
      "description": "Function '_run_non_streaming' on line 581 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 587,
      "code_snippet": "        tools: list[\"ChatCompletionInputTool\"] | None = None,\n    ) -> dict[str, list[ChatMessage]]:\n        api_chat_output: ChatCompletionOutput = self._client.chat_completion(\n            messages=messages, tools=tools, **generation_kwargs\n        )\n",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_553_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run_streaming'",
      "description": "Function '_run_streaming' on line 553 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 553,
      "code_snippet": "        return await self._run_non_streaming_async(formatted_messages, generation_kwargs, hf_tools)\n\n    def _run_streaming(\n        self,\n        messages: list[dict[str, str]],\n        generation_kwargs: dict[str, Any],",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_581_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run_non_streaming'",
      "description": "Function '_run_non_streaming' on line 581 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 581,
      "code_snippet": "        return {\"replies\": [message]}\n\n    def _run_non_streaming(\n        self,\n        messages: list[dict[str, str]],\n        generation_kwargs: dict[str, Any],",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_553_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run_streaming'",
      "description": "Function '_run_streaming' on line 553 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 553,
      "code_snippet": "        return await self._run_non_streaming_async(formatted_messages, generation_kwargs, hf_tools)\n\n    def _run_streaming(\n        self,\n        messages: list[dict[str, str]],\n        generation_kwargs: dict[str, Any],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py_581_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run_non_streaming'",
      "description": "Function '_run_non_streaming' on line 581 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/hugging_face_api.py",
      "line_number": 581,
      "code_snippet": "        return {\"replies\": [message]}\n\n    def _run_non_streaming(\n        self,\n        messages: list[dict[str, str]],\n        generation_kwargs: dict[str, Any],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 33,
      "code_snippet": "\n    It works with the gpt-4 - type models and supports streaming responses\n    from OpenAI API. It uses [ChatMessage](https://docs.haystack.deepset.ai/docs/chatmessage)\n    format in input and output.\n\n    You can customize how the text is generated by passing parameters to the",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_42_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 42. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 42,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_54_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 54. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 54,
      "code_snippet": "\n    client = AzureOpenAIChatGenerator(\n        azure_endpoint=\"<Your Azure endpoint e.g. `https://your-company.azure.openai.com/>\",\n        api_key=Secret.from_token(\"<your-api-key>\"),\n        azure_deployment=\"<this a model name, e.g. gpt-4.1-mini>\")\n    response = client.run(messages)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_97_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 97. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 97,
      "code_snippet": "        Initialize the Azure OpenAI Chat Generator component.\n\n        :param azure_endpoint: The endpoint of the deployed model, for example `\"https://example-resource.azure.openai.com/\"`.\n        :param api_version: The version of the API to use. Defaults to 2024-12-01-preview.\n        :param azure_deployment: The deployment of the model, usually the model name.\n        :param api_key: The API key to use for authentication.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_101_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 101. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 101,
      "code_snippet": "        :param azure_deployment: The deployment of the model, usually the model name.\n        :param api_key: The API key to use for authentication.\n        :param azure_ad_token: [Azure Active Directory token](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id).\n        :param organization: Your organization ID, defaults to `None`. For help, see\n        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_103_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 103. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 103,
      "code_snippet": "        :param azure_ad_token: [Azure Active Directory token](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id).\n        :param organization: Your organization ID, defaults to `None`. For help, see\n        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.\n            It accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_105_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 105. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 105,
      "code_snippet": "        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.\n            It accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.\n        :param timeout: Timeout for OpenAI client calls. If not set, it defaults to either the\n            `OPENAI_TIMEOUT` environment variable, or 30 seconds.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_112_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 112. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 112,
      "code_snippet": "            If not set, it defaults to either the `OPENAI_MAX_RETRIES` environment variable, or set to 5.\n        :param generation_kwargs: Other parameters to use for the model. These parameters are sent directly to\n            the OpenAI endpoint. For details, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat).\n            Some of the supported parameters:\n            - `max_completion_tokens`: An upper bound for the number of tokens that can be generated for a completion,\n                including visible output tokens and reasoning tokens.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_133_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 133. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 133,
      "code_snippet": "                If provided, the output will always be validated against this\n                format (unless the model returns a tool call).\n                For details, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs).\n                Notes:\n                - This parameter accepts Pydantic models and JSON schemas for latest models starting from GPT-4o.\n                  Older models only support basic version of structured outputs through `{\"type\": \"json_object\"}`.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_137_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 137. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 137,
      "code_snippet": "                - This parameter accepts Pydantic models and JSON schemas for latest models starting from GPT-4o.\n                  Older models only support basic version of structured outputs through `{\"type\": \"json_object\"}`.\n                  For detailed information on JSON mode, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs#json-mode).\n                - For structured outputs with streaming,\n                  the `response_format` must be a JSON schema and not a Pydantic model.\n        :param default_headers: Default headers to use for the AzureOpenAI client.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py_150_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 150. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure.py",
      "line_number": 150,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        \"\"\"\n        # We intentionally do not call super().__init__ here because we only need to instantiate the client to interact\n        # with the API.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_24_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 24. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 24,
      "code_snippet": "\n    It works with the gpt-5 and o-series models and supports streaming responses\n    from OpenAI API. It uses [ChatMessage](https://docs.haystack.deepset.ai/docs/chatmessage)\n    format in input and output.\n\n    You can customize how the text is generated by passing parameters to the",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 33,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/responses).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_44_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 44. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 44,
      "code_snippet": "\n    client = AzureOpenAIResponsesChatGenerator(\n        azure_endpoint=\"https://example-resource.azure.openai.com/\",\n        generation_kwargs={\"reasoning\": {\"effort\": \"low\", \"summary\": \"auto\"}}\n    )\n    response = client.run(messages)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_75_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 75. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 75,
      "code_snippet": "        :param api_key: The API key to use for authentication. Can be:\n            - A `Secret` object containing the API key.\n            - A `Secret` object containing the [Azure Active Directory token](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id).\n            - A function that returns an Azure Active Directory token.\n        :param azure_endpoint: The endpoint of the deployed model, for example `\"https://example-resource.azure.openai.com/\"`.\n        :param azure_deployment: The deployment of the model, usually the model name.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_77_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 77. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 77,
      "code_snippet": "            - A `Secret` object containing the [Azure Active Directory token](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id).\n            - A function that returns an Azure Active Directory token.\n        :param azure_endpoint: The endpoint of the deployed model, for example `\"https://example-resource.azure.openai.com/\"`.\n        :param azure_deployment: The deployment of the model, usually the model name.\n        :param organization: Your organization ID, defaults to `None`. For help, see\n        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_80_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 80. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 80,
      "code_snippet": "        :param azure_deployment: The deployment of the model, usually the model name.\n        :param organization: Your organization ID, defaults to `None`. For help, see\n        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.\n            It accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_82_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 82. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 82,
      "code_snippet": "        [Setting up your organization](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param streaming_callback: A callback function called when a new token is received from the stream.\n            It accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.\n        :param timeout: Timeout for OpenAI client calls. If not set, it defaults to either the\n            `OPENAI_TIMEOUT` environment variable, or 30 seconds.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_90_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 90. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 90,
      "code_snippet": "        :param generation_kwargs: Other parameters to use for the model. These parameters are sent\n           directly to the OpenAI endpoint.\n           See OpenAI [documentation](https://platform.openai.com/docs/api-reference/responses) for\n            more details.\n            Some of the supported parameters:\n            - `temperature`: What sampling temperature to use. Higher values like 0.8 will make the output more random,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_103_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 103. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 103,
      "code_snippet": "                If provided, the output will always be validated against this\n                format (unless the model returns a tool call).\n                For details, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs).\n            - `text`: A JSON schema that enforces the structure of the model's response.\n                If provided, the output will always be validated against this\n                format (unless the model returns a tool call).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_112_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 112. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 112,
      "code_snippet": "                - Currently, this component doesn't support streaming for structured outputs.\n                - Older models only support basic version of structured outputs through `{\"type\": \"json_object\"}`.\n                    For detailed information on JSON mode, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs#json-mode).\n            - `reasoning`: A dictionary of parameters for reasoning. For example:\n                - `summary`: The summary of the reasoning.\n                - `effort`: The level of effort to put into the reasoning. Can be `low`, `medium` or `high`.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_118_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 118. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 118,
      "code_snippet": "                - `generate_summary`: Whether to generate a summary of the reasoning.\n                Note: OpenAI does not return the reasoning tokens, but we can view summary if its enabled.\n                For details, see the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning).\n        :param tools:\n            A list of Tool and/or Toolset objects, or a single Toolset for which the model can prepare calls.\n        :param tools_strict:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py_126_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 126. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/azure_responses.py",
      "line_number": 126,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n        \"\"\"\n        azure_endpoint = azure_endpoint or os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n        if azure_endpoint is None:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_59_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 59. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 59,
      "code_snippet": "\n    It works with the gpt-4 and gpt-5 series models and supports streaming responses\n    from OpenAI API. It uses [ChatMessage](https://docs.haystack.deepset.ai/docs/chatmessage)\n    format in input and output.\n\n    You can customize how the text is generated by passing parameters to the",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_68_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 68. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 68,
      "code_snippet": "\n    For details on OpenAI API parameters, see\n    [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat).\n\n    ### Usage example\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_123_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 123. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 123,
      "code_snippet": "        :param model: The name of the model to use.\n        :param streaming_callback: A callback function that is called when a new token is received from the stream.\n            The callback function accepts [StreamingChunk](https://docs.haystack.deepset.ai/docs/data-classes#streamingchunk)\n            as an argument.\n        :param api_base_url: An optional base URL.\n        :param organization: Your organization ID, defaults to `None`. See",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_127_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 127. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 127,
      "code_snippet": "        :param api_base_url: An optional base URL.\n        :param organization: Your organization ID, defaults to `None`. See\n        [production best practices](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param generation_kwargs: Other parameters to use for the model. These parameters are sent directly to\n            the OpenAI endpoint. See OpenAI [documentation](https://platform.openai.com/docs/api-reference/chat) for\n            more details.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_129_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 129. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 129,
      "code_snippet": "        [production best practices](https://platform.openai.com/docs/guides/production-best-practices/setting-up-your-organization).\n        :param generation_kwargs: Other parameters to use for the model. These parameters are sent directly to\n            the OpenAI endpoint. See OpenAI [documentation](https://platform.openai.com/docs/api-reference/chat) for\n            more details.\n            Some of the supported parameters:\n            - `max_completion_tokens`: An upper bound for the number of tokens that can be generated for a completion,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_151_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 151. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 151,
      "code_snippet": "                If provided, the output will always be validated against this\n                format (unless the model returns a tool call).\n                For details, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs).\n                Notes:\n                - This parameter accepts Pydantic models and JSON schemas for latest models starting from GPT-4o.\n                  Older models only support basic version of structured outputs through `{\"type\": \"json_object\"}`.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_155_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 155. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 155,
      "code_snippet": "                - This parameter accepts Pydantic models and JSON schemas for latest models starting from GPT-4o.\n                  Older models only support basic version of structured outputs through `{\"type\": \"json_object\"}`.\n                  For detailed information on JSON mode, see the [OpenAI Structured Outputs documentation](https://platform.openai.com/docs/guides/structured-outputs#json-mode).\n                - For structured outputs with streaming,\n                  the `response_format` must be a JSON schema and not a Pydantic model.\n        :param timeout:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_171_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 171. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 171,
      "code_snippet": "        :param http_client_kwargs:\n            A dictionary of keyword arguments to configure a custom `httpx.Client`or `httpx.AsyncClient`.\n            For more information, see the [HTTPX documentation](https://www.python-httpx.org/api/#client).\n\n        \"\"\"\n        self.api_key = api_key",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_301_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 301. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 301,
      "code_snippet": "            Additional keyword arguments for text generation. These parameters will\n            override the parameters passed during component initialization.\n            For details on OpenAI API parameters, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat/create).\n        :param tools:\n            A list of Tool and/or Toolset objects, or a single Toolset for which the model can prepare calls.\n            If set, it will override the `tools` parameter provided during initialization.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_339_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 339. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 339,
      "code_snippet": "            completions = self._handle_stream_response(\n                # we cannot check isinstance(chat_completion, Stream) because some observability tools wrap Stream\n                # and return a different type. See https://github.com/deepset-ai/haystack/issues/9014.\n                chat_completion,  # type: ignore\n                streaming_callback,\n            )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_380_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 380. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 380,
      "code_snippet": "            Additional keyword arguments for text generation. These parameters will\n            override the parameters passed during component initialization.\n            For details on OpenAI API parameters, see [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat/create).\n        :param tools: A list of Tool and/or Toolset objects, or a single Toolset for which the model can prepare calls.\n            If set, it will override the `tools` parameter provided during initialization.\n        :param tools_strict:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_419_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 419. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 419,
      "code_snippet": "            completions = await self._handle_async_stream_response(\n                # we cannot check isinstance(chat_completion, AsyncStream) because some observability tools wrap\n                # AsyncStream and return a different type. See https://github.com/deepset-ai/haystack/issues/9014.\n                chat_completion,  # type: ignore\n                streaming_callback,\n            )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_528_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 528. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 528,
      "code_snippet": "            # close the stream when task is cancelled\n            # asyncio.shield ensures the close operation completes\n            # https://docs.python.org/3/library/asyncio-task.html#shielding-from-cancellation\n            raise  # Re-raise to propagate cancellation\n\n        return [_convert_streaming_chunks_to_chat_message(chunks=chunks)]",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py_565_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 565. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/openai.py",
      "line_number": 565,
      "code_snippet": "    if message.tool_calls:\n        # we currently only support function tools (not custom tools)\n        # https://platform.openai.com/docs/guides/function-calling#custom-tools\n        openai_tool_calls = [tc for tc in message.tool_calls if not isinstance(tc, ChatCompletionMessageCustomToolCall)]\n        for openai_tc in openai_tool_calls:\n            arguments_str = openai_tc.function.arguments",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/types/protocol.py_9_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 9. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/generators/chat/types/protocol.py",
      "line_number": 9,
      "code_snippet": "from haystack.dataclasses import ChatMessage\n\n# Ellipsis are needed to define the Protocol but pylint complains. See https://github.com/pylint-dev/pylint/issues/9319.\n# pylint: disable=unnecessary-ellipsis\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/types/protocol.py_9_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 9. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/types/protocol.py",
      "line_number": 9,
      "code_snippet": "from haystack import Document\n\n# See https://github.com/pylint-dev/pylint/issues/9319.\n# pylint: disable=unnecessary-ellipsis\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_backend.py_113",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'embed' on line 113 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_backend.py",
      "line_number": 113,
      "code_snippet": "    def embed(self, data: list[str] | list[\"Image\"], **kwargs: Any) -> list[list[float]]:\n        # Sentence Transformers encode can work with Images, but the type hint does not reflect that\n        # https://sbert.net/examples/sentence_transformer/applications/image-search\n        embeddings = self.model.encode(data, **kwargs).tolist()  # type: ignore[arg-type]\n        return embeddings",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_backend.py_115_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 115. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_backend.py",
      "line_number": 115,
      "code_snippet": "    def embed(self, data: list[str] | list[\"Image\"], **kwargs: Any) -> list[list[float]]:\n        # Sentence Transformers encode can work with Images, but the type hint does not reflect that\n        # https://sbert.net/examples/sentence_transformer/applications/image-search\n        embeddings = self.model.encode(data, **kwargs).tolist()  # type: ignore[arg-type]\n        return embeddings",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_backend.py_113_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'embed'",
      "description": "Function 'embed' on line 113 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_backend.py",
      "line_number": 113,
      "code_snippet": "        )\n\n    def embed(self, data: list[str] | list[\"Image\"], **kwargs: Any) -> list[list[float]]:\n        # Sentence Transformers encode can work with Images, but the type hint does not reflect that\n        # https://sbert.net/examples/sentence_transformer/applications/image-search\n        embeddings = self.model.encode(data, **kwargs).tolist()  # type: ignore[arg-type]",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_sparse_backend.py_106",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'embed' on line 106 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_sparse_backend.py",
      "line_number": 106,
      "code_snippet": "    def embed(self, *, data: list[str], **kwargs) -> list[SparseEmbedding]:\n        embeddings_list = self.model.encode(\n            data,\n            convert_to_tensor=False,  # output is a list of individual tensors\n            convert_to_sparse_tensor=True,\n            **kwargs,\n        )\n\n        sparse_embeddings: list[SparseEmbedding] = []\n        for embedding_tensor in embeddings_list:\n            # encode returns a list of tensors with the parameters above, but the type hint is too broad",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_sparse_backend.py_106_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'embed'",
      "description": "Function 'embed' on line 106 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_sparse_backend.py",
      "line_number": 106,
      "code_snippet": "        )\n\n    def embed(self, *, data: list[str], **kwargs) -> list[SparseEmbedding]:\n        embeddings_list = self.model.encode(\n            data,\n            convert_to_tensor=False,  # output is a list of individual tensors",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_sparse_backend.py_106_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'embed'",
      "description": "Function 'embed' on line 106 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/backends/sentence_transformers_sparse_backend.py",
      "line_number": 106,
      "code_snippet": "        )\n\n    def embed(self, *, data: list[str], **kwargs) -> list[SparseEmbedding]:\n        embeddings_list = self.model.encode(\n            data,\n            convert_to_tensor=False,  # output is a list of individual tensors",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/image/sentence_transformers_doc_image_embedder.py_132_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 132. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/embedders/image/sentence_transformers_doc_image_embedder.py",
      "line_number": 132,
      "code_snippet": "        :param backend:\n            The backend to use for the Sentence Transformers model. Choose from \"torch\", \"onnx\", or \"openvino\".\n            Refer to the [Sentence Transformers documentation](https://sbert.net/docs/sentence_transformer/usage/efficiency.html)\n            for more information on acceleration and quantization options.\n        \"\"\"\n        pillow_import.check()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/image/image_utils.py_73_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 73. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/image/image_utils.py",
      "line_number": 73,
      "code_snippet": "    if size is not None:\n        # Set reducing_gap=None to disable multi-step shrink; better quality.\n        # https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.thumbnail\n        image.thumbnail(size=size, reducing_gap=None)\n\n    # Convert the image to base64 string",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/image/image_utils.py_179_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 179. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/image/image_utils.py",
      "line_number": 179,
      "code_snippet": "        # the resolution of the image. To convert a DPI value to a scale factor, multiply it by the size of 1 canvas\n        # unit in inches (usually 1/72in).\n        # https://pypdfium2.readthedocs.io/en/stable/python_api.html#pypdfium2._helpers.page.PdfPage.render\n        target_scale = target_resolution_dpi / 72.0\n\n        # Calculate potential pixels for target_dpi",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/image/image_utils.py_203_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 203. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/converters/image/image_utils.py",
      "line_number": 203,
      "code_snippet": "        if size is not None:\n            # Set reducing_gap=None to disable multi-step shrink; better quality.\n            # https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.thumbnail\n            image.thumbnail(size=size, reducing_gap=None)\n\n        all_pdf_images.append((page_number, image))",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/types/protocol.py_7_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 7. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/components/retrievers/types/protocol.py",
      "line_number": 7,
      "code_snippet": "from typing import Any, Protocol\n\n# Ellipsis are needed to define the Protocol but pylint complains. See https://github.com/pylint-dev/pylint/issues/9319.\n# pylint: disable=unnecessary-ellipsis\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/utils.py_176_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 176. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/utils.py",
      "line_number": 176,
      "code_snippet": "    Decorator to warn about the use of positional arguments in a function.\n\n    Adapted from https://stackoverflow.com/questions/68432070/\n    :param func:\n    \"\"\"\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/draw.py_174_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 174. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/draw.py",
      "line_number": 174,
      "code_snippet": "def _to_mermaid_image(\n    graph: networkx.MultiDiGraph,\n    server_url: str = \"https://mermaid.ink\",\n    params: dict | None = None,\n    timeout: int = 30,\n    super_component_mapping: dict[str, str] | None = None,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/draw.py_185_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 185. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/draw.py",
      "line_number": 185,
      "code_snippet": "        The graph to render as a Mermaid pipeline.\n    :param server_url:\n        Base URL of the Mermaid server (default: 'https://mermaid.ink').\n    :param params:\n        Dictionary of customization parameters. See `validate_mermaid_params` for valid keys.\n    :param timeout:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_234_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 234. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 234,
      "code_snippet": "                        \"malformed serialized data, mismatch between the serialized component and the \"\n                        \"loaded one (due to a breaking change, see \"\n                        \"https://github.com/deepset-ai/haystack/releases), etc.\"\n                    )\n                    raise DeserializationError(msg) from e\n            pipe.add_component(name=name, instance=instance)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_689_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 689. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 689,
      "code_snippet": "        self,\n        *,\n        server_url: str = \"https://mermaid.ink\",\n        params: dict | None = None,\n        timeout: int = 30,\n        super_component_expansion: bool = False,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_701_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 701. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 701,
      "code_snippet": "\n        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_702_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 702. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 702,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_702_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 702. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 702,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_756_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 756. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 756,
      "code_snippet": "        *,\n        path: Path,\n        server_url: str = \"https://mermaid.ink\",\n        params: dict | None = None,\n        timeout: int = 30,\n        super_component_expansion: bool = False,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_770_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 770. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 770,
      "code_snippet": "\n        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_771_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 771. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 771,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py_771_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 771. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/pipeline/base.py",
      "line_number": 771,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/component/component.py_163_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 163. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/component/component.py",
      "line_number": 163,
      "code_snippet": "    # The following expression defines a run method compatible with any input signature.\n    # Its type is equivalent to Callable[..., dict[str, Any]].\n    # See https://typing.python.org/en/latest/spec/callables.html#meaning-of-in-callable.\n    #\n    # Using `run: Callable[..., dict[str, Any]]` directly leads to type errors: the protocol would expect a settable\n    # attribute `run`, while the actual implementation is a read-only method.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_490_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 490. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 490,
      "code_snippet": "        return default_from_dict(cls, data)\n\n    def show(self, server_url: str = \"https://mermaid.ink\", params: dict | None = None, timeout: int = 30) -> None:\n        \"\"\"\n        Display an image representing this SuperComponent's underlying pipeline in a Jupyter notebook.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_498_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 498. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 498,
      "code_snippet": "\n        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_499_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 499. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 499,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_499_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 499. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 499,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_525_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 525. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 525,
      "code_snippet": "\n    def draw(\n        self, path: Path, server_url: str = \"https://mermaid.ink\", params: dict | None = None, timeout: int = 30\n    ) -> None:\n        \"\"\"\n        Save an image representing this SuperComponent's underlying pipeline to the specified file path.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_535_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 535. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 535,
      "code_snippet": "            The file path where the generated image will be saved.\n        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n        :param params:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_536_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 536. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 536,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n        :param params:\n            Dictionary of customization parameters to modify the output. Refer to Mermaid documentation for more details",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py_536_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 536. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/core/super_component/super_component.py",
      "line_number": 536,
      "code_snippet": "        :param server_url:\n            The base URL of the Mermaid server used for rendering (default: 'https://mermaid.ink').\n            See https://github.com/jihchi/mermaid.ink and https://github.com/mermaid-js/mermaid-live-editor for more\n            info on how to set up your own Mermaid server.\n        :param params:\n            Dictionary of customization parameters to modify the output. Refer to Mermaid documentation for more details",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py_80_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 80. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
      "line_number": 80,
      "code_snippet": "        :param bm25_parameters: Parameters for BM25 implementation in a dictionary format.\n            For example: `{'k1':1.5, 'b':0.75, 'epsilon':0.25}`\n            You can learn more about these parameters by visiting https://github.com/dorianbrown/rank_bm25.\n        :param embedding_similarity_function: The similarity function used to compare Documents embeddings.\n            One of \"dot_product\" (default) or \"cosine\". To choose the most appropriate function, look for information\n            about your embedding model.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py_431_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 431. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
      "line_number": 431,
      "code_snippet": "            if \"operator\" not in filters and \"conditions\" not in filters:\n                raise ValueError(\n                    \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n                )\n            docs = [doc for doc in self.storage.values() if document_matches_filter(filters=filters, document=doc)]\n        else:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py_527_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 527. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
      "line_number": 527,
      "code_snippet": "            if \"operator\" not in filters:\n                raise ValueError(\n                    \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n                )\n            filters = {\"operator\": \"AND\", \"conditions\": [content_type_filter, filters]}\n        else:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py_542_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 542. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
      "line_number": 542,
      "code_snippet": "        # BM25Okapi can return meaningful negative values, so they should not be filtered out when scale_score is False.\n        # It's the only algorithm supported by rank_bm25 at the time of writing (2024) that can return negative scores.\n        # see https://github.com/deepset-ai/haystack/pull/6889 for more context.\n        negatives_are_valid = self.bm25_algorithm == \"BM25Okapi\" and not scale_score\n\n        # Create documents with the BM25 score to return them",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py_592_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 592. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/haystack/haystack/document_stores/in_memory/document_store.py",
      "line_number": 592,
      "code_snippet": "            if \"operator\" not in filters and \"conditions\" not in filters:\n                raise ValueError(\n                    \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n                )\n            all_documents = [\n                doc for doc in self.storage.values() if document_matches_filter(filters=filters, document=doc)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    }
  ],
  "metadata": {}
}