{
  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "ai-security-cli",
          "version": "1.0.0",
          "informationUri": "https://github.com/ai-security-cli/ai-security-cli",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker's intentions."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts, and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate and sanitize all user inputs\n- Use parameterized prompts\n- Implement output filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed to other components."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding, validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM outputs may contain malicious content like XSS payloads or SQL injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply context-appropriate output encoding\n- Validate outputs against expected schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is manipulated to introduce vulnerabilities, backdoors, or biases that compromise model security and effectiveness."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify and validate training data sources\n- Implement data sanitization pipelines\n- Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact with LLMs in ways that consume excessive resources, leading to service degradation or high costs."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement rate limiting per user/API key\n- Set maximum token/context limits\n- Use request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can arise from compromised pre-trained models, poisoned training data from third parties, or vulnerable third-party plugins."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:** Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models from trusted sources only\n- Verify model signatures and checksums\n- Scan third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs inadvertently reveal confidential data through their responses, potentially exposing PII, credentials, or proprietary information."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:** Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n- Sanitize training data to remove sensitive information\n- Implement output filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack proper input validation, have excessive permissions, or fail to implement adequate security controls."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may execute malicious actions or expose sensitive functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n- Apply principle of least privilege\n- Use sandboxing for plugin execution\n- Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are granted too much autonomy to take actions, potentially leading to unintended consequences from hallucinations or malicious prompts."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM permissions to minimum necessary\n- Require human-in-the-loop for critical actions\n- Implement comprehensive logging\n- Use rate limiting and action budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust LLM outputs without adequate verification, potentially leading to misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs, provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs without verification can lead to errors.\n\n**Mitigation:**\n- Implement human review for critical decisions\n- Validate LLM outputs against trusted sources\n- Display confidence scores and limitations\n- Educate users about LLM limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or extraction of proprietary LLM models, leading to economic loss, competitive disadvantage, and potential security risks."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting, monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n- Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_input_limit' on line 16 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_no_input_limit' on line 16 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )\n    return {\"response\": response.choices[0].message.content}\n\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )\n    return {\"response\": response.choices[0].message.content}\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_16-16"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_timeout' on line 52 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_no_timeout' on line 52 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_no_timeout(prompt: str) -> str:\n    \"\"\"Vulnerable: No timeout on LLM API call.\"\"\"\n    # VULNERABLE - no timeout, can hang indefinitely\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n\n# VULN:LLM04:MEDIUM:LINE=64 - Recursive prompt generation\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 52,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_timeout(prompt: str) -> str:\n    \"\"\"Vulnerable: No timeout on LLM API call.\"\"\"\n    # VULNERABLE - no timeout, can hang indefinitely\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n\n# VULN:LLM04:MEDIUM:LINE=64 - Recursive prompt generation"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_52-52"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_recursive_generation' on line 63 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_recursive_generation' on line 63 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_recursive_generation(prompt: str, depth: int = 0) -> str:\n    \"\"\"Vulnerable: Allows recursive/chained prompt generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    result = response.choices[0].message.content\n\n    # VULNERABLE - can recurse indefinitely\n    if \"CONTINUE\" in result and depth < 100:\n        return vulnerable_recursive_generation(result, depth + 1)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_recursive_generation(prompt: str, depth: int = 0) -> str:\n    \"\"\"Vulnerable: Allows recursive/chained prompt generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    result = response.choices[0].message.content\n\n    # VULNERABLE - can recurse indefinitely\n    if \"CONTINUE\" in result and depth < 100:\n        return vulnerable_recursive_generation(result, depth + 1)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_63-63"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'chat' on line 84 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'chat' on line 84 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def chat(self, user_input: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=self.messages\n        )\n        assistant_msg = response.choices[0].message.content\n        self.messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n        return assistant_msg\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 84,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def chat(self, user_input: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=self.messages\n        )\n        assistant_msg = response.choices[0].message.content\n        self.messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n        return assistant_msg\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_84-84"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 20. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 20. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 20,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_20_unpinned-20"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 30. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 30. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\ndef vulnerable_no_output_limit(prompt: str) -> str:\n    \"\"\"Vulnerable: No max_tokens set, allows maximum generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n        # VULNERABLE - no max_tokens limit\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 30,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_output_limit(prompt: str) -> str:\n    \"\"\"Vulnerable: No max_tokens set, allows maximum generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n        # VULNERABLE - no max_tokens limit"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_30_unpinned-30"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 44. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 44. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_44_unpinned-44"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 55. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 55. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    \"\"\"Vulnerable: No timeout on LLM API call.\"\"\"\n    # VULNERABLE - no timeout, can hang indefinitely\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 55,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: No timeout on LLM API call.\"\"\"\n    # VULNERABLE - no timeout, can hang indefinitely\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_55_unpinned-55"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\ndef vulnerable_recursive_generation(prompt: str, depth: int = 0) -> str:\n    \"\"\"Vulnerable: Allows recursive/chained prompt generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 65,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_recursive_generation(prompt: str, depth: int = 0) -> str:\n    \"\"\"Vulnerable: Allows recursive/chained prompt generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_65_unpinned-65"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 103. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 103. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n        raise ValueError(\"Input too long\")\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=500,  # Limit output\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 103,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        raise ValueError(\"Input too long\")\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=500,  # Limit output"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_103_unpinned-103"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 87. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 87. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=self.messages\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 87,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=self.messages\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_87_unpinned-87"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "warning",
          "message": {
            "text": "Function 'vulnerable_no_input_limit' on line 16 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
            "markdown": "**High-risk write/network operation without confirmation in 'vulnerable_no_input_limit'**\n\nFunction 'vulnerable_no_input_limit' on line 16 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.\n\n**Code:**\n```python\n# VULN:LLM04:HIGH:LINE=17 - No input length limits\n@app.route(\"/chat\", methods=[\"POST\"])\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n```\n\n**Remediation:**\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM04:HIGH:LINE=17 - No input length limits\n@app.route(\"/chat\", methods=[\"POST\"])\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_16_risk-16"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.7,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "warning",
          "message": {
            "text": "Function 'vulnerable_no_rate_limit' on line 40 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
            "markdown": "**High-risk write/network operation without confirmation in 'vulnerable_no_rate_limit'**\n\nFunction 'vulnerable_no_rate_limit' on line 40 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.\n\n**Code:**\n```python\n# VULN:LLM04:MEDIUM:LINE=40 - No rate limiting\n@app.route(\"/unlimited\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting on API endpoint.\"\"\"\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")\n```\n\n**Remediation:**\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 40,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM04:MEDIUM:LINE=40 - No rate limiting\n@app.route(\"/unlimited\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting on API endpoint.\"\"\"\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_40_risk-40"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.7,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "warning",
          "message": {
            "text": "Function 'chat' on line 84 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
            "markdown": "**High-risk write/network operation without confirmation in 'chat'**\n\nFunction 'chat' on line 84 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.\n\n**Code:**\n```python\n        self.messages = []\n\n    def chat(self, user_input: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create(\n```\n\n**Remediation:**\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 84,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self.messages = []\n\n    def chat(self, user_input: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_84_risk-84"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.7,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_input_limit' on line 16 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_input_limit'**\n\nFunction 'vulnerable_no_input_limit' on line 16 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n# VULN:LLM04:HIGH:LINE=17 - No input length limits\n@app.route(\"/chat\", methods=[\"POST\"])\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM04:HIGH:LINE=17 - No input length limits\n@app.route(\"/chat\", methods=[\"POST\"])\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_16_critical_decision-16"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_output_limit' on line 28 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_output_limit'**\n\nFunction 'vulnerable_no_output_limit' on line 28 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM04:HIGH:LINE=29 - No output token limits\ndef vulnerable_no_output_limit(prompt: str) -> str:\n    \"\"\"Vulnerable: No max_tokens set, allows maximum generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 28,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM04:HIGH:LINE=29 - No output token limits\ndef vulnerable_no_output_limit(prompt: str) -> str:\n    \"\"\"Vulnerable: No max_tokens set, allows maximum generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_28_critical_decision-28"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_rate_limit' on line 40 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_rate_limit'**\n\nFunction 'vulnerable_no_rate_limit' on line 40 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n# VULN:LLM04:MEDIUM:LINE=40 - No rate limiting\n@app.route(\"/unlimited\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting on API endpoint.\"\"\"\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 40,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM04:MEDIUM:LINE=40 - No rate limiting\n@app.route(\"/unlimited\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting on API endpoint.\"\"\"\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_40_critical_decision-40"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_timeout' on line 52 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_timeout'**\n\nFunction 'vulnerable_no_timeout' on line 52 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM04:HIGH:LINE=53 - No timeout handling\ndef vulnerable_no_timeout(prompt: str) -> str:\n    \"\"\"Vulnerable: No timeout on LLM API call.\"\"\"\n    # VULNERABLE - no timeout, can hang indefinitely\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 52,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM04:HIGH:LINE=53 - No timeout handling\ndef vulnerable_no_timeout(prompt: str) -> str:\n    \"\"\"Vulnerable: No timeout on LLM API call.\"\"\"\n    # VULNERABLE - no timeout, can hang indefinitely\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_52_critical_decision-52"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_recursive_generation' on line 63 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_recursive_generation'**\n\nFunction 'vulnerable_recursive_generation' on line 63 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM04:MEDIUM:LINE=64 - Recursive prompt generation\ndef vulnerable_recursive_generation(prompt: str, depth: int = 0) -> str:\n    \"\"\"Vulnerable: Allows recursive/chained prompt generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM04:MEDIUM:LINE=64 - Recursive prompt generation\ndef vulnerable_recursive_generation(prompt: str, depth: int = 0) -> str:\n    \"\"\"Vulnerable: Allows recursive/chained prompt generation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_63_critical_decision-63"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'safe_chat_with_limits' on line 97 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'safe_chat_with_limits'**\n\nFunction 'safe_chat_with_limits' on line 97 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# SAFE: Proper resource limits (for comparison)\ndef safe_chat_with_limits(prompt: str) -> str:\n    \"\"\"Safe: Proper limits on input/output and timeout.\"\"\"\n    # Input validation\n    if len(prompt) > 4000:\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 97,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# SAFE: Proper resource limits (for comparison)\ndef safe_chat_with_limits(prompt: str) -> str:\n    \"\"\"Safe: Proper limits on input/output and timeout.\"\"\"\n    # Input validation\n    if len(prompt) > 4000:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_97_critical_decision-97"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'chat' on line 84 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'chat'**\n\nFunction 'chat' on line 84 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n        self.messages = []\n\n    def chat(self, user_input: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create(\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 84,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self.messages = []\n\n    def chat(self, user_input: str) -> str:\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        # VULNERABLE - context grows indefinitely\n        response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_84_critical_decision-84"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "error",
          "message": {
            "text": "API endpoint 'vulnerable_no_input_limit' on line 16 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
            "markdown": "**Model API without rate limiting in 'vulnerable_no_input_limit'**\n\nAPI endpoint 'vulnerable_no_input_limit' on line 16 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.\n\n**Code:**\n```python\n# VULN:LLM04:HIGH:LINE=17 - No input length limits\n@app.route(\"/chat\", methods=[\"POST\"])\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input\n```\n\n**Remediation:**\nImplement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM04:HIGH:LINE=17 - No input length limits\n@app.route(\"/chat\", methods=[\"POST\"])\ndef vulnerable_no_input_limit():\n    \"\"\"Vulnerable: No limit on input size.\"\"\"\n    user_input = request.json.get(\"message\", \"\")\n    # VULNERABLE - accepts arbitrarily long input"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_16_unrestricted_api-16"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "note",
          "message": {
            "text": "API endpoint 'vulnerable_no_rate_limit' on line 40 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.",
            "markdown": "**Model output without protection in 'vulnerable_no_rate_limit'**\n\nAPI endpoint 'vulnerable_no_rate_limit' on line 40 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.\n\n**Code:**\n```python\n# VULN:LLM04:MEDIUM:LINE=40 - No rate limiting\n@app.route(\"/unlimited\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting on API endpoint.\"\"\"\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")\n```\n\n**Remediation:**\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 40,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM04:MEDIUM:LINE=40 - No rate limiting\n@app.route(\"/unlimited\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting on API endpoint.\"\"\"\n    # VULNERABLE - no rate limiting\n    user_input = request.json.get(\"message\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos/app.py_40_missing_protection-40"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Add output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-10T15:46:23.037126+00:00",
          "workingDirectory": {
            "uri": "file:///Users/deo/secscan-cli/llm-sec-eval/testbed/llm04_model_dos"
          }
        }
      ],
      "automationDetails": {
        "id": "ai-security-cli/static-scan/20260110154623"
      }
    }
  ]
}