{
  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "ai-security-cli",
          "version": "1.0.0",
          "informationUri": "https://github.com/ai-security-cli/ai-security-cli",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker's intentions."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts, and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate and sanitize all user inputs\n- Use parameterized prompts\n- Implement output filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed to other components."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding, validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM outputs may contain malicious content like XSS payloads or SQL injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply context-appropriate output encoding\n- Validate outputs against expected schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is manipulated to introduce vulnerabilities, backdoors, or biases that compromise model security and effectiveness."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify and validate training data sources\n- Implement data sanitization pipelines\n- Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact with LLMs in ways that consume excessive resources, leading to service degradation or high costs."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement rate limiting per user/API key\n- Set maximum token/context limits\n- Use request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can arise from compromised pre-trained models, poisoned training data from third parties, or vulnerable third-party plugins."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:** Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models from trusted sources only\n- Verify model signatures and checksums\n- Scan third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs inadvertently reveal confidential data through their responses, potentially exposing PII, credentials, or proprietary information."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:** Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n- Sanitize training data to remove sensitive information\n- Implement output filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack proper input validation, have excessive permissions, or fail to implement adequate security controls."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may execute malicious actions or expose sensitive functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n- Apply principle of least privilege\n- Use sandboxing for plugin execution\n- Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are granted too much autonomy to take actions, potentially leading to unintended consequences from hallucinations or malicious prompts."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM permissions to minimum necessary\n- Require human-in-the-loop for critical actions\n- Implement comprehensive logging\n- Use rate limiting and action budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust LLM outputs without adequate verification, potentially leading to misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs, provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs without verification can lead to errors.\n\n**Mitigation:**\n- Implement human review for critical decisions\n- Validate LLM outputs against trusted sources\n- Display confidence scores and limitations\n- Educate users about LLM limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or extraction of proprietary LLM models, leading to economic loss, competitive disadvantage, and potential security risks."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting, monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n- Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 20 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 20 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    # VULNERABLE - unlimited queries for extraction\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 20,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    # VULNERABLE - unlimited queries for extraction\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_20-20"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 48 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 48 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 48,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_48-48"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 74 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 74 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    # VULNERABLE - logprobs can be used for distillation\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 74,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    # VULNERABLE - logprobs can be used for distillation\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_74-74"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 131 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 131 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 131,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_131-131"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 107 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 107 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 107,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_107-107"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_rate_limit' on line 16 has 3 DoS risk(s): LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_no_rate_limit' on line 16 has 3 DoS risk(s): LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return jsonify({\"response\": response.choices[0].message.content})\n\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return jsonify({\"response\": response.choices[0].message.content})\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_16-16"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_auth' on line 44 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_no_auth' on line 44 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return jsonify({\"response\": response.choices[0].message.content})\n\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return jsonify({\"response\": response.choices[0].message.content})\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_44-44"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_logprobs' on line 70 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_logprobs' on line 70 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        logprobs=True,\n        top_logprobs=5\n    )\n    return jsonify({\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        logprobs=True,\n        top_logprobs=5\n    )\n    return jsonify({"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_70-70"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'safe_protected_api' on line 116 has 3 DoS risk(s): LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: LLM calls in loops, No timeout configuration, No token/context limits**\n\nFunction 'safe_protected_api' on line 116 has 3 DoS risk(s): LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef safe_protected_api():\n    \"\"\"Safe: Properly protected model API.\"\"\"\n    # Authentication\n    api_key = request.headers.get(\"X-API-Key\")\n    if not api_key or not validate_api_key(api_key):\n        return jsonify({\"error\": \"Unauthorized\"}), 401\n\n    # Rate limiting\n    if is_rate_limited(api_key):\n        return jsonify({\"error\": \"Rate limited\"}), 429\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 116,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def safe_protected_api():\n    \"\"\"Safe: Properly protected model API.\"\"\"\n    # Authentication\n    api_key = request.headers.get(\"X-API-Key\")\n    if not api_key or not validate_api_key(api_key):\n        return jsonify({\"error\": \"Unauthorized\"}), 401\n\n    # Rate limiting\n    if is_rate_limited(api_key):\n        return jsonify({\"error\": \"Rate limited\"}), 429\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_116-116"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'query' on line 105 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'query' on line 105 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n\n\n# SAFE: Protected model API (for comparison)\n@app.route(\"/api/protected/chat\", methods=[\"POST\"])\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 105,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n\n\n# SAFE: Protected model API (for comparison)\n@app.route(\"/api/protected/chat\", methods=[\"POST\"])"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_105-105"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 20. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 20. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 20,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_20_unpinned-20"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 48. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 48. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 48,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_48_unpinned-48"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 74. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 74. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        logprobs=True,\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 74,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        logprobs=True,"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_74_unpinned-74"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 131. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 131. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 131,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    prompt = request.json.get(\"prompt\", \"\")\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_131_unpinned-131"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 107. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 107. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 107,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_107_unpinned-107"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Local file path '\"/models/custom_model.pt\"' found on line 91. Loading models from local paths without verification can introduce compromised models if the filesystem is not properly secured.",
            "markdown": "**Untrusted model source: local_path**\n\nLocal file path '\"/models/custom_model.pt\"' found on line 91. Loading models from local paths without verification can introduce compromised models if the filesystem is not properly secured.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Endpoint that could expose model weights.\"\"\"\n    # VULNERABLE - model weights accessible\n    model_path = \"/models/custom_model.pt\"\n    return {\"path\": model_path, \"accessible\": True}\n\n\n```\n\n**Remediation:**\nSecure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 91,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Endpoint that could expose model weights.\"\"\"\n    # VULNERABLE - model weights accessible\n    model_path = \"/models/custom_model.pt\"\n    return {\"path\": model_path, \"accessible\": True}\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_91_local_path-91"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.7,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_rate_limit' on line 16 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_rate_limit'**\n\nFunction 'vulnerable_no_rate_limit' on line 16 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=16 - No rate limiting on model API\n@app.route(\"/api/generate\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=16 - No rate limiting on model API\n@app.route(\"/api/generate\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_16_critical_decision-16"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_auth' on line 44 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_auth'**\n\nFunction 'vulnerable_no_auth' on line 44 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_44_critical_decision-44"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_logprobs' on line 70 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_logprobs'**\n\nFunction 'vulnerable_logprobs' on line 70 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_70_critical_decision-70"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'query' on line 105 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'query'**\n\nFunction 'query' on line 105 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n        return None  # Placeholder\n\n    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 105,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return None  # Placeholder\n\n    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_105_critical_decision-105"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_rate_limit' on line 16 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_no_rate_limit'**\n\nFunction 'vulnerable_no_rate_limit' on line 16 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=16 - No rate limiting on model API\n@app.route(\"/api/generate\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=16 - No rate limiting on model API\n@app.route(\"/api/generate\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_16_missing_verification-16"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_auth' on line 44 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_no_auth'**\n\nFunction 'vulnerable_no_auth' on line 44 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_44_missing_verification-44"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_logprobs' on line 70 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_logprobs'**\n\nFunction 'vulnerable_logprobs' on line 70 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_70_missing_verification-70"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'query' on line 105 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'query'**\n\nFunction 'query' on line 105 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n        return None  # Placeholder\n\n    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 105,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        return None  # Placeholder\n\n    def query(self, prompt: str) -> str:\n        # VULNERABLE - no logging of queries\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_105_missing_verification-105"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "error",
          "message": {
            "text": "API endpoint 'vulnerable_no_auth' on line 44 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
            "markdown": "**Model API without rate limiting in 'vulnerable_no_auth'**\n\nAPI endpoint 'vulnerable_no_auth' on line 44 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.\n\n**Code:**\n```python\n# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n```\n\n**Remediation:**\nImplement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_44_unrestricted_api-44"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "error",
          "message": {
            "text": "API endpoint 'vulnerable_logprobs' on line 70 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
            "markdown": "**Model API without rate limiting in 'vulnerable_logprobs'**\n\nAPI endpoint 'vulnerable_logprobs' on line 70 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n```\n\n**Remediation:**\nImplement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_70_unrestricted_api-70"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.85,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "note",
          "message": {
            "text": "API endpoint 'vulnerable_no_rate_limit' on line 16 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.",
            "markdown": "**Model output without protection in 'vulnerable_no_rate_limit'**\n\nAPI endpoint 'vulnerable_no_rate_limit' on line 16 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=16 - No rate limiting on model API\n@app.route(\"/api/generate\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction\n```\n\n**Remediation:**\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=16 - No rate limiting on model API\n@app.route(\"/api/generate\", methods=[\"POST\"])\ndef vulnerable_no_rate_limit():\n    \"\"\"Vulnerable: No rate limiting allows model extraction queries.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - unlimited queries for extraction"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_16_missing_protection-16"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Add output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "note",
          "message": {
            "text": "API endpoint 'vulnerable_no_auth' on line 44 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.",
            "markdown": "**Model output without protection in 'vulnerable_no_auth'**\n\nAPI endpoint 'vulnerable_no_auth' on line 44 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.\n\n**Code:**\n```python\n# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")\n```\n\n**Remediation:**\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:MEDIUM:LINE=44 - No authentication on API\n@app.route(\"/api/chat\", methods=[\"POST\"])\ndef vulnerable_no_auth():\n    \"\"\"Vulnerable: API endpoint without authentication.\"\"\"\n    # VULNERABLE - no API key or auth required\n    prompt = request.json.get(\"prompt\", \"\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_44_missing_protection-44"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Add output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "note",
          "message": {
            "text": "API endpoint 'vulnerable_logprobs' on line 70 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.",
            "markdown": "**Model output without protection in 'vulnerable_logprobs'**\n\nAPI endpoint 'vulnerable_logprobs' on line 70 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.\n\n**Code:**\n```python\n# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation\n```\n\n**Remediation:**\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM10:HIGH:LINE=72 - Exposing logprobs for extraction\n@app.route(\"/api/logprobs\", methods=[\"POST\"])\ndef vulnerable_logprobs():\n    \"\"\"Vulnerable: Returns logprobs enabling model distillation.\"\"\"\n    prompt = request.json.get(\"prompt\", \"\")\n    # VULNERABLE - logprobs can be used for distillation"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_70_missing_protection-70"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Add output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
              }
            }
          ]
        },
        {
          "ruleId": "LLM10",
          "ruleIndex": 9,
          "level": "note",
          "message": {
            "text": "API endpoint 'safe_protected_api' on line 116 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.",
            "markdown": "**Model output without protection in 'safe_protected_api'**\n\nAPI endpoint 'safe_protected_api' on line 116 returns model output without watermarking or obfuscation. Consider adding output protection to make model extraction more difficult and traceable.\n\n**Code:**\n```python\n# SAFE: Protected model API (for comparison)\n@app.route(\"/api/protected/chat\", methods=[\"POST\"])\ndef safe_protected_api():\n    \"\"\"Safe: Properly protected model API.\"\"\"\n    # Authentication\n    api_key = request.headers.get(\"X-API-Key\")\n```\n\n**Remediation:**\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 116,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# SAFE: Protected model API (for comparison)\n@app.route(\"/api/protected/chat\", methods=[\"POST\"])\ndef safe_protected_api():\n    \"\"\"Safe: Properly protected model API.\"\"\"\n    # Authentication\n    api_key = request.headers.get(\"X-API-Key\")"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft/app.py_116_missing_protection-116"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM10: Model Theft"
          },
          "fixes": [
            {
              "description": {
                "text": "Add output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-08T17:49:22.981515+00:00",
          "workingDirectory": {
            "uri": "file:///Users/deo/secscan-cli/llm-sec-eval/testbed/llm10_model_theft"
          }
        }
      ],
      "automationDetails": {
        "id": "ai-security-cli/static-scan/20260108174922"
      }
    }
  ]
}