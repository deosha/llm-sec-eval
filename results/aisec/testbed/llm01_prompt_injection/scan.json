{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T14:52:03.701328Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection",
    "files_scanned": 1,
    "overall_score": 2.94,
    "confidence": 0.59,
    "duration_seconds": 0.009,
    "findings_count": 22,
    "severity_breakdown": {
      "CRITICAL": 9,
      "HIGH": 3,
      "MEDIUM": 0,
      "LOW": 5,
      "INFO": 5
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 21,
      "confidence": 0.43,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "2 jailbreak prevention mechanisms active"
      ],
      "gaps": [
        "No input validation detected",
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "model_protection": 0,
        "extraction_defense": 25,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.81,
      "subscores": {
        "LLM01": 0,
        "LLM02": 100,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 82,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 72,
        "LLM09": 96,
        "LLM10": 100
      },
      "detected_controls": [
        "Insecure Output Handling (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 5 critical",
        "Model Denial of Service: 4 critical, 1 high",
        "Supply Chain Vulnerabilities: 5 low",
        "Excessive Agency: 2 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_14",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'user_input' embedded in LLM prompt",
      "description": "User input parameter 'user_input' is directly passed to LLM API call 'client.chat.completions.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 14,
      "code_snippet": "    \"\"\"Vulnerable: Direct f-string interpolation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_28",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'user_input' embedded in LLM prompt",
      "description": "User input 'user_input' flows to LLM call via format_call in variable 'prompt'. Function 'vulnerable_chat_v2' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 28,
      "code_snippet": "    prompt_template = \"You are an assistant helping with: {topic}. User says: {input}\"\n    prompt = prompt_template.format(topic=\"general\", input=user_input)  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_40",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'user_input' embedded in LLM prompt",
      "description": "User input 'user_input' flows to LLM call via concatenation in variable 'system_prompt'. Function 'vulnerable_chat_v3' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 40,
      "code_snippet": "    system_prompt = \"You are a helpful assistant. Instructions: \"\n    system_prompt = system_prompt + user_input  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_55",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'url' embedded in LLM prompt",
      "description": "User input 'url' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_indirect_injection' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 55,
      "code_snippet": "    import requests\n    external_content = requests.get(url).text  # Fetches potentially malicious content\n    prompt = f\"Summarize this document: {external_content}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_69",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'user_query' embedded in LLM prompt",
      "description": "User input 'user_query' flows to LLM call via template_substitute in variable 'prompt'. Function 'vulnerable_template' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 69,
      "code_snippet": "    template = Template(\"Answer the question: $query based on context: $ctx\")\n    prompt = template.substitute(query=user_query, ctx=str(context))  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_12",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_chat_v1' on line 12 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 12,
      "code_snippet": "def vulnerable_chat_v1(user_input: str) -> str:\n    \"\"\"Vulnerable: Direct f-string interpolation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant. User context: {user_input}\"},  # VULNERABLE\n            {\"role\": \"user\", \"content\": \"Help me with my request\"}\n        ]\n    )\n    return response.choices[0].message.content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_25",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_chat_v2' on line 25 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 25,
      "code_snippet": "def vulnerable_chat_v2(user_input: str, template: str) -> str:\n    \"\"\"Vulnerable: .format() with user input.\"\"\"\n    prompt_template = \"You are an assistant helping with: {topic}. User says: {input}\"\n    prompt = prompt_template.format(topic=\"general\", input=user_input)  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_37",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_chat_v3' on line 37 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 37,
      "code_snippet": "def vulnerable_chat_v3(user_input: str) -> str:\n    \"\"\"Vulnerable: String concatenation.\"\"\"\n    system_prompt = \"You are a helpful assistant. Instructions: \"\n    system_prompt = system_prompt + user_input  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": \"Continue\"}\n        ]\n    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_52",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_indirect_injection' on line 52 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 52,
      "code_snippet": "def vulnerable_indirect_injection(url: str) -> str:\n    \"\"\"Vulnerable: External content injected into prompt.\"\"\"\n    import requests\n    external_content = requests.get(url).text  # Fetches potentially malicious content\n    prompt = f\"Summarize this document: {external_content}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_65",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_template' on line 65 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 65,
      "code_snippet": "def vulnerable_template(user_query: str, context: dict) -> str:\n    \"\"\"Vulnerable: Jinja-style template with user input.\"\"\"\n    from string import Template\n    template = Template(\"Answer the question: $query based on context: $ctx\")\n    prompt = template.substitute(query=user_query, ctx=str(context))  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_14_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 14. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 14,
      "code_snippet": "def vulnerable_chat_v1(user_input: str) -> str:\n    \"\"\"Vulnerable: Direct f-string interpolation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant. User context: {user_input}\"},  # VULNERABLE",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_29_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 29. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 29,
      "code_snippet": "    prompt_template = \"You are an assistant helping with: {topic}. User says: {input}\"\n    prompt = prompt_template.format(topic=\"general\", input=user_input)  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_41_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 41. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 41,
      "code_snippet": "    system_prompt = \"You are a helpful assistant. Instructions: \"\n    system_prompt = system_prompt + user_input  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_57_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 57. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 57,
      "code_snippet": "    external_content = requests.get(url).text  # Fetches potentially malicious content\n    prompt = f\"Summarize this document: {external_content}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_70_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 70. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 70,
      "code_snippet": "    template = Template(\"Answer the question: $query based on context: $ctx\")\n    prompt = template.substitute(query=user_query, ctx=str(context))  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_12_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'vulnerable_chat_v1'",
      "description": "Function 'vulnerable_chat_v1' on line 12 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 12,
      "code_snippet": "\n# VULN:LLM01:HIGH:LINE=16 - Direct string interpolation in system prompt\ndef vulnerable_chat_v1(user_input: str) -> str:\n    \"\"\"Vulnerable: Direct f-string interpolation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_52_api",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unrestricted API access from LLM in 'vulnerable_indirect_injection'",
      "description": "Function 'vulnerable_indirect_injection' on line 52 makes HTTP/API requests based on LLM outputs without URL validation or allowlisting. This allows the LLM to make requests to arbitrary endpoints, potentially exfiltrating data, performing SSRF attacks, or accessing unauthorized resources.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 52,
      "code_snippet": "\n# VULN:LLM01:HIGH:LINE=55 - Indirect injection via external data\ndef vulnerable_indirect_injection(url: str) -> str:\n    \"\"\"Vulnerable: External content injected into prompt.\"\"\"\n    import requests\n    external_content = requests.get(url).text  # Fetches potentially malicious content",
      "recommendation": "API Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_12_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_chat_v1'",
      "description": "Function 'vulnerable_chat_v1' on line 12 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 12,
      "code_snippet": "\n# VULN:LLM01:HIGH:LINE=16 - Direct string interpolation in system prompt\ndef vulnerable_chat_v1(user_input: str) -> str:\n    \"\"\"Vulnerable: Direct f-string interpolation.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_25_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_chat_v2'",
      "description": "Function 'vulnerable_chat_v2' on line 25 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 25,
      "code_snippet": "\n# VULN:LLM01:HIGH:LINE=28 - Format string injection\ndef vulnerable_chat_v2(user_input: str, template: str) -> str:\n    \"\"\"Vulnerable: .format() with user input.\"\"\"\n    prompt_template = \"You are an assistant helping with: {topic}. User says: {input}\"\n    prompt = prompt_template.format(topic=\"general\", input=user_input)  # VULNERABLE",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_37_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_chat_v3'",
      "description": "Function 'vulnerable_chat_v3' on line 37 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 37,
      "code_snippet": "\n# VULN:LLM01:MEDIUM:LINE=40 - Concatenation without sanitization\ndef vulnerable_chat_v3(user_input: str) -> str:\n    \"\"\"Vulnerable: String concatenation.\"\"\"\n    system_prompt = \"You are a helpful assistant. Instructions: \"\n    system_prompt = system_prompt + user_input  # VULNERABLE",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_52_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_indirect_injection'",
      "description": "Function 'vulnerable_indirect_injection' on line 52 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 52,
      "code_snippet": "\n# VULN:LLM01:HIGH:LINE=55 - Indirect injection via external data\ndef vulnerable_indirect_injection(url: str) -> str:\n    \"\"\"Vulnerable: External content injected into prompt.\"\"\"\n    import requests\n    external_content = requests.get(url).text  # Fetches potentially malicious content",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py_65_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_template'",
      "description": "Function 'vulnerable_template' on line 65 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm01_prompt_injection/app.py",
      "line_number": 65,
      "code_snippet": "\n# VULN:LLM01:MEDIUM:LINE=68 - Template without proper escaping\ndef vulnerable_template(user_query: str, context: dict) -> str:\n    \"\"\"Vulnerable: Jinja-style template with user input.\"\"\"\n    from string import Template\n    template = Template(\"Answer the question: $query based on context: $ctx\")",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    }
  ],
  "metadata": {}
}