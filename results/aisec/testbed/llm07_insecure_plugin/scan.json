{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T14:49:56.470084Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin",
    "files_scanned": 1,
    "overall_score": 0.0,
    "confidence": 0.56,
    "duration_seconds": 0.009,
    "findings_count": 5,
    "severity_breakdown": {
      "CRITICAL": 4,
      "HIGH": 1,
      "MEDIUM": 0,
      "LOW": 0,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 0,
      "confidence": 0.42,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "No input validation detected",
        "No jailbreak prevention mechanisms detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.37,
      "subscores": {
        "model_protection": 16,
        "extraction_defense": 25,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit"
      ],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.74,
      "subscores": {
        "LLM01": 100,
        "LLM02": 100,
        "LLM03": 100,
        "LLM04": 100,
        "LLM05": 16,
        "LLM06": 100,
        "LLM07": 51,
        "LLM08": 100,
        "LLM09": 100,
        "LLM10": 100
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Insecure Output Handling (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Model Denial of Service (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Excessive Agency (no vulnerabilities found)",
        "Overreliance (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Supply Chain Vulnerabilities: 3 critical",
        "Insecure Plugin Design: 1 critical, 1 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py_13_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [62, 65]) and executes code (lines [13, 15, 17]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py",
      "line_number": 13,
      "code_snippet": "\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n\n# VULN:LLM07:CRITICAL:LINE=16 - Shell command execution from LLM\ndef vulnerable_shell_tool(llm_output: dict) -> str:\n    \"\"\"Vulnerable: Executes shell commands based on LLM output.\"\"\"\n    command = llm_output.get(\"command\", \"\")\n    # VULNERABLE - arbitrary command execution\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py_111_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 111. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py",
      "line_number": 111,
      "code_snippet": "    expression = llm_output.get(\"expression\", \"\")\n    # VULNERABLE - eval on user/LLM input\n    result = eval(expression)\n    return result\n\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py_98_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 98. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py",
      "line_number": 98,
      "code_snippet": "        \"\"\"Load and execute plugin code.\"\"\"\n        # VULNERABLE - executing arbitrary code\n        exec(code, {\"__builtins__\": __builtins__}, self.plugins)\n\n    def run_plugin(self, name: str, *args):\n        \"\"\"Run a loaded plugin.\"\"\"",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py_95_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'load_plugin'",
      "description": "Function 'load_plugin' on line 95 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py",
      "line_number": 95,
      "code_snippet": "\n    def __init__(self):\n        self.plugins = {}\n\n    def load_plugin(self, name: str, code: str):\n        \"\"\"Load and execute plugin code.\"\"\"\n        # VULNERABLE - executing arbitrary code\n        exec(code, {\"__builtins__\": __builtins__}, self.plugins)\n\n    def run_plugin(self, name: str, *args):",
      "recommendation": "Plugin Input Validation Best Practices:\n1. Validate all plugin parameters against a strict schema\n2. Use allowlists for permitted plugin names/sources\n3. Verify plugin signatures before loading\n4. Check plugin metadata and version compatibility\n5. Sanitize all user-provided plugin configuration\n6. Implement plugin capability restrictions\n7. Use type validation (Pydantic, marshmallow, JSON Schema)\n8. Reject plugins with suspicious characteristics\n9. Log all plugin registration attempts\n10. Implement rate limiting on plugin operations\n\nSafe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies\n\nPlugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    },
    {
      "id": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py_100_auth",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Plugin management function without authentication in 'run_plugin'",
      "description": "Function 'run_plugin' on line 100 manages plugins (install/enable/execute) without requiring authentication. This allows unauthorized users to manipulate the plugin system, potentially installing malicious plugins or executing arbitrary code.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm07_insecure_plugin/app.py",
      "line_number": 100,
      "code_snippet": "        exec(code, {\"__builtins__\": __builtins__}, self.plugins)\n\n    def run_plugin(self, name: str, *args):\n        \"\"\"Run a loaded plugin.\"\"\"\n        if name in self.plugins:\n            return self.plugins[name](*args)",
      "recommendation": "Plugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    }
  ],
  "metadata": {}
}