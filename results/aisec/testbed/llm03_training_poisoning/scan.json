{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T15:46:22.722044Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning",
    "files_scanned": 1,
    "overall_score": 0.11,
    "confidence": 0.6,
    "duration_seconds": 0.006,
    "findings_count": 5,
    "severity_breakdown": {
      "CRITICAL": 2,
      "HIGH": 2,
      "MEDIUM": 1,
      "LOW": 0,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [],
      "gaps": [
        "No input validation detected",
        "No jailbreak prevention mechanisms detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "model_protection": 0,
        "extraction_defense": 0,
        "supply_chain_security": 37,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.37,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 1,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 50,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.85,
      "subscores": {
        "LLM01": 100,
        "LLM02": 100,
        "LLM03": 49,
        "LLM04": 100,
        "LLM05": 76,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 100,
        "LLM09": 100,
        "LLM10": 64
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Insecure Output Handling (no vulnerabilities found)",
        "Model Denial of Service (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Excessive Agency (no vulnerabilities found)",
        "Overreliance (no vulnerabilities found)"
      ],
      "gaps": [
        "Training Data Poisoning: 1 critical, 1 high",
        "Supply Chain Vulnerabilities: 1 high, 1 medium",
        "Model Theft: 1 critical"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM03_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_24_unsafe_load",
      "category": "LLM03: Training Data Poisoning",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe data loading with pickle.load in training context",
      "description": "Function 'vulnerable_load_dataset' uses pickle.load on line 24. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 24,
      "code_snippet": "    with open(path, \"rb\") as f:\n        dataset = pickle.load(f)  # VULNERABLE - arbitrary code execution\n    return dataset\n",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify checksums/signatures before loading\n4. Only load data from trusted, verified sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM03_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_54_unsafe_load",
      "category": "LLM03: Training Data Poisoning",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Unsafe data loading with torch.load in training context",
      "description": "Function 'vulnerable_load_torch_model' uses torch.load on line 54. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 54,
      "code_snippet": "    model_path = download_file(url)\n    model = torch.load(model_path)  # VULNERABLE - pickle under the hood\n    return model\n",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify checksums/signatures before loading\n4. Only load data from trusted, verified sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_7_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 7. This library can execute arbitrary code during deserialization. File fetches external data - HIGH RISK if deserializing remote content.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 7,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_54_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "torch.load without weights_only=True can execute arbitrary code on line 54.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 54,
      "code_snippet": "    # Download and load without verification\n    model_path = download_file(url)\n    model = torch.load(model_path)  # VULNERABLE - pickle under the hood\n    return model\n\n",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_79_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in 'download_file'",
      "description": "Function 'download_file' on line 79 exposes pytorch model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 79,
      "code_snippet": "    pass\n\ndef download_file(url: str) -> str:\n    \"\"\"Download file from URL.\"\"\"\n    return \"/tmp/model.pt\"\n",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets with signed URLs\n   - Never store in /static or /public directories\n   - Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model encryption\n   - Implement model quantization\n   - Remove unnecessary metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor access:\n   - Track who downloads models\n   - Alert on unauthorized access\n   - Maintain audit logs"
    }
  ],
  "metadata": {}
}