{
  "report_type": "static_scan",
  "generated_at": "2026-01-08T17:49:21.447800Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning",
    "files_scanned": 1,
    "overall_score": 11.06,
    "confidence": 0.59,
    "duration_seconds": 0.005,
    "findings_count": 3,
    "severity_breakdown": {
      "CRITICAL": 1,
      "HIGH": 1,
      "MEDIUM": 1,
      "LOW": 0,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [],
      "gaps": [
        "No input validation detected",
        "No jailbreak prevention mechanisms detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "model_protection": 0,
        "extraction_defense": 0,
        "supply_chain_security": 37,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.37,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 1,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 50,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 40,
      "confidence": 0.86,
      "subscores": {
        "LLM01": 100,
        "LLM02": 100,
        "LLM03": 100,
        "LLM04": 100,
        "LLM05": 76,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 100,
        "LLM09": 100,
        "LLM10": 64
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Insecure Output Handling (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Model Denial of Service (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Excessive Agency (no vulnerabilities found)",
        "Overreliance (no vulnerabilities found)"
      ],
      "gaps": [
        "Supply Chain Vulnerabilities: 1 high, 1 medium",
        "Model Theft: 1 critical"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_81_local_path",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Untrusted model source: local_path",
      "description": "Local file path '\"/tmp/model.pt\"' found on line 81. Loading models from local paths without verification can introduce compromised models if the filesystem is not properly secured.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 81,
      "code_snippet": "def download_file(url: str) -> str:\n    \"\"\"Download file from URL.\"\"\"\n    return \"/tmp/model.pt\"\n\n\n# SAFE: Validated data loading (for comparison)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_7_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for model serialization",
      "description": "Import of 'pickle' on line 7 for model serialization. This library can execute arbitrary code during deserialization, making it vulnerable to supply chain attacks if loading untrusted models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 7,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors for PyTorch/TensorFlow\n2. Never deserialize models from untrusted sources\n3. Scan serialized models before loading\n4. Use sandboxed environments for model loading\n5. Implement allowlists for model formats\n6. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM10_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py_79_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in 'download_file'",
      "description": "Function 'download_file' on line 79 exposes pytorch model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm03_training_poisoning/app.py",
      "line_number": 79,
      "code_snippet": "    pass\n\ndef download_file(url: str) -> str:\n    \"\"\"Download file from URL.\"\"\"\n    return \"/tmp/model.pt\"\n",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets with signed URLs\n   - Never store in /static or /public directories\n   - Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model encryption\n   - Implement model quantization\n   - Remove unnecessary metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor access:\n   - Track who downloads models\n   - Alert on unauthorized access\n   - Maintain audit logs"
    }
  ],
  "metadata": {}
}