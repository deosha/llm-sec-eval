{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T15:46:23.151262Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain",
    "files_scanned": 1,
    "overall_score": 0.0,
    "confidence": 0.57,
    "duration_seconds": 0.014,
    "findings_count": 13,
    "severity_breakdown": {
      "CRITICAL": 6,
      "HIGH": 5,
      "MEDIUM": 2,
      "LOW": 0,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 0,
      "confidence": 0.44,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Pattern Detection input validation detected"
      ],
      "gaps": [
        "No jailbreak prevention mechanisms detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.43,
      "subscores": {
        "model_protection": 25,
        "extraction_defense": 0,
        "supply_chain_security": 25,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [
        "NER models for PII"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.38,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.73,
      "subscores": {
        "LLM01": 100,
        "LLM02": 100,
        "LLM03": 49,
        "LLM04": 100,
        "LLM05": 0,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 100,
        "LLM09": 100,
        "LLM10": 100
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Insecure Output Handling (no vulnerabilities found)",
        "Model Denial of Service (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Excessive Agency (no vulnerabilities found)",
        "Overreliance (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Training Data Poisoning: 1 critical, 1 high",
        "Supply Chain Vulnerabilities: 5 critical, 4 high, 2 medium"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM03_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_24_unsafe_load",
      "category": "LLM03: Training Data Poisoning",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe data loading with pickle.load in training context",
      "description": "Function 'vulnerable_load_pickle_model' uses pickle.load on line 24. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 24,
      "code_snippet": "    with open(path, \"rb\") as f:\n        model = pickle.load(f)  # VULNERABLE - arbitrary code execution\n    return model\n",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify checksums/signatures before loading\n4. Only load data from trusted, verified sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM03_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_33_unsafe_load",
      "category": "LLM03: Training Data Poisoning",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Unsafe data loading with torch.load in training context",
      "description": "Function 'vulnerable_torch_load' uses torch.load on line 33. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 33,
      "code_snippet": "    # VULNERABLE - pickle under the hood\n    model = torch.load(path)\n    return model\n",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify checksums/signatures before loading\n4. Only load data from trusted, verified sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_22_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 22. This library can execute arbitrary code during deserialization. File fetches external data - HIGH RISK if deserializing remote content.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 22,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_24_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [43, 87, 97]) and executes code (lines [24, 51, 60]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 24,
      "code_snippet": "def vulnerable_load_pickle_model(path: str):\n    \"\"\"Vulnerable: Loading model with pickle.\"\"\"\n    import pickle\n    with open(path, \"rb\") as f:\n        model = pickle.load(f)  # VULNERABLE - arbitrary code execution\n    return model\n\n\n# VULN:LLM05:HIGH:LINE=32 - torch.load without weights_only\ndef vulnerable_torch_load(path: str):",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_14_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "trust_remote_code=True enables arbitrary code execution on line 14.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 14,
      "code_snippet": "    from transformers import AutoModelForCausalLM, AutoTokenizer\n    # VULNERABLE - no checksum or signature verification\n    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    return model, tokenizer\n",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_15_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "trust_remote_code=True enables arbitrary code execution on line 15.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 15,
      "code_snippet": "    # VULNERABLE - no checksum or signature verification\n    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    return model, tokenizer\n\n",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_33_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "torch.load without weights_only=True can execute arbitrary code on line 33.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 33,
      "code_snippet": "    import torch\n    # VULNERABLE - pickle under the hood\n    model = torch.load(path)\n    return model\n\n",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_175_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "torch.load without weights_only=True can execute arbitrary code on line 175.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 175,
      "code_snippet": "\n    # weights_only=True prevents pickle code execution\n    model = torch.load(model_path, weights_only=True)\n    return model\n\n",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_65_dynamic_tool",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Dynamic tool/plugin loading",
      "description": "Loading dynamic LangChain tools without verification on line 65.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 65,
      "code_snippet": "    from langchain_community.tools import load_tools\n    # VULNERABLE - loading unverified community tools\n    tools = load_tools([tool_name])\n    return tools\n\n",
      "recommendation": "Secure Tool Loading:\n1. Validate tool names against an allowlist\n2. Only load tools from trusted sources\n3. Review tool permissions before loading\n4. Sandbox tool execution"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_75_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 75. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 75,
      "code_snippet": "        config_str = f.read()\n    # VULNERABLE - eval on config file\n    config = eval(config_str)\n    return config\n\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_80_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 80. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 80,
      "code_snippet": "\n# =============================================================================\n# CONFIRMER PATTERNS - Dynamic fetch + exec (should be HIGH/CRITICAL)\n# =============================================================================\n\n# VULN:LLM05:CRITICAL:LINE=83 - Download + exec pattern (strong confirmer)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_84_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 84. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 84,
      "code_snippet": "\n# VULN:LLM05:CRITICAL:LINE=83 - Download + exec pattern (strong confirmer)\ndef vulnerable_fetch_and_exec(script_url: str):\n    \"\"\"Vulnerable: Fetching and executing remote code.\"\"\"\n    import requests\n    response = requests.get(script_url)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py_90_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 90. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm05_supply_chain/app.py",
      "line_number": 90,
      "code_snippet": "    code = response.text\n    # VULNERABLE - fetching and executing remote code\n    exec(code)\n\n\n# VULN:LLM05:CRITICAL:LINE=93 - Download + subprocess pattern",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    }
  ],
  "metadata": {}
}