{
  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "ai-security-cli",
          "version": "1.0.0",
          "informationUri": "https://github.com/ai-security-cli/ai-security-cli",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker's intentions."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts, and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate and sanitize all user inputs\n- Use parameterized prompts\n- Implement output filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed to other components."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding, validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM outputs may contain malicious content like XSS payloads or SQL injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply context-appropriate output encoding\n- Validate outputs against expected schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is manipulated to introduce vulnerabilities, backdoors, or biases that compromise model security and effectiveness."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify and validate training data sources\n- Implement data sanitization pipelines\n- Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact with LLMs in ways that consume excessive resources, leading to service degradation or high costs."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement rate limiting per user/API key\n- Set maximum token/context limits\n- Use request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can arise from compromised pre-trained models, poisoned training data from third parties, or vulnerable third-party plugins."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:** Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models from trusted sources only\n- Verify model signatures and checksums\n- Scan third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs inadvertently reveal confidential data through their responses, potentially exposing PII, credentials, or proprietary information."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:** Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n- Sanitize training data to remove sensitive information\n- Implement output filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack proper input validation, have excessive permissions, or fail to implement adequate security controls."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may execute malicious actions or expose sensitive functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n- Apply principle of least privilege\n- Use sandboxing for plugin execution\n- Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are granted too much autonomy to take actions, potentially leading to unintended consequences from hallucinations or malicious prompts."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM permissions to minimum necessary\n- Require human-in-the-loop for critical actions\n- Implement comprehensive logging\n- Use rate limiting and action budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust LLM outputs without adequate verification, potentially leading to misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs, provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs without verification can lead to errors.\n\n**Mitigation:**\n- Implement human review for critical decisions\n- Validate LLM outputs against trusted sources\n- Display confidence scores and limitations\n- Educate users about LLM limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or extraction of proprietary LLM models, leading to economic loss, competitive disadvantage, and potential security risks."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting, monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n- Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input 'user_request' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_auto_execute' may be vulnerable to prompt injection attacks.",
            "markdown": "**User input 'user_request' embedded in LLM prompt**\n\nUser input 'user_request' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_auto_execute' may be vulnerable to prompt injection attacks.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_16-16"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input 'request' flows to LLM call via f-string in variable 'prompt'. Function 'handle_request' may be vulnerable to prompt injection attacks.",
            "markdown": "**User input 'request' embedded in LLM prompt**\n\nUser input 'request' flows to LLM call via f-string in variable 'prompt'. Function 'handle_request' may be vulnerable to prompt injection attacks.\n\n**Code:**\n```python\n    def handle_request(self, request: str) -> str:\n        prompt = f\"Draft and send an email for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 99,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def handle_request(self, request: str) -> str:\n        prompt = f\"Draft and send an email for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_99-99"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'execute(' on line 17 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous sql_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'execute(' on line 17 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 17,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_17-17"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'exec(' on line 24 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous code_execution sink**\n\nLLM output from 'client.chat.completions.create' is used in 'exec(' on line 24 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    # VULNERABLE - auto-execution without human review\n    exec(code)\n    return \"Code executed\"\n\n```\n\n**Remediation:**\nMitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists for permitted operations\n5. Consider structured output formats (JSON) instead"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 24,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    # VULNERABLE - auto-execution without human review\n    exec(code)\n    return \"Code executed\"\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_24-24"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists for permitted operations\n5. Consider structured output formats (JSON) instead"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'execute(' on line 37 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous sql_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'execute(' on line 37 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 37,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_37-37"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'execute(' on line 44 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous sql_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'execute(' on line 44 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        # VULNERABLE - unrestricted write access\n        self.db.execute(sql)\n        self.db.commit()\n        return \"Query executed\"\n```\n\n**Remediation:**\nMitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 44,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        # VULNERABLE - unrestricted write access\n        self.db.execute(sql)\n        self.db.commit()\n        return \"Query executed\""
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_44-44"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.7999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'commands.' on line 55 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'commands.' on line 55 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 55,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_55-55"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'shell=True' on line 62 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous command_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'shell=True' on line 62 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        # VULNERABLE - system-level access without approval\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        return result.stdout\n\n```\n\n**Remediation:**\nMitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 62,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        # VULNERABLE - system-level access without approval\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        return result.stdout\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_62-62"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8999999999999999,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'execute(' on line 117 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous sql_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'execute(' on line 117 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 117,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_117-117"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'execute(' on line 123 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous sql_injection sink**\n\nLLM output from 'client.chat.completions.create' is used in 'execute(' on line 123 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n        # VULNERABLE - no audit trail\n        result = self._execute(response.choices[0].message.content)\n        return result\n\n```\n\n**Remediation:**\nMitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 123,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        # VULNERABLE - no audit trail\n        result = self._execute(response.choices[0].message.content)\n        return result\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_123-123"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_auto_execute' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_auto_execute' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_auto_execute(user_request: str) -> str:\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    code = response.choices[0].message.content\n\n    # VULNERABLE - auto-execution without human review\n    exec(code)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 14,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_auto_execute(user_request: str) -> str:\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    code = response.choices[0].message.content\n\n    # VULNERABLE - auto-execution without human review\n    exec(code)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_14-14"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'process_request' on line 35 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'process_request' on line 35 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        sql = response.choices[0].message.content\n\n        # VULNERABLE - unrestricted write access\n        self.db.execute(sql)\n        self.db.commit()\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 35,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def process_request(self, user_request: str) -> str:\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        sql = response.choices[0].message.content\n\n        # VULNERABLE - unrestricted write access\n        self.db.execute(sql)\n        self.db.commit()"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_35-35"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'execute_task' on line 53 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'execute_task' on line 53 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        command = response.choices[0].message.content\n\n        # VULNERABLE - system-level access without approval\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        return result.stdout\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 53,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        command = response.choices[0].message.content\n\n        # VULNERABLE - system-level access without approval\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        return result.stdout"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_53-53"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'process_request' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'process_request' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Should I process this payment request: {user_request}? If yes, respond with the amount.\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        decision = response.choices[0].message.content\n\n        # VULNERABLE - no human approval for financial transactions\n        if \"yes\" in decision.lower():\n            amount = self._extract_amount(decision)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 73,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def process_request(self, user_request: str) -> str:\n        prompt = f\"Should I process this payment request: {user_request}? If yes, respond with the amount.\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        decision = response.choices[0].message.content\n\n        # VULNERABLE - no human approval for financial transactions\n        if \"yes\" in decision.lower():\n            amount = self._extract_amount(decision)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_73-73"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'handle_request' on line 98 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'handle_request' on line 98 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def handle_request(self, request: str) -> str:\n        prompt = f\"Draft and send an email for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        email_content = response.choices[0].message.content\n\n        # VULNERABLE - sending emails without human review\n        self.email.send(email_content)\n        return \"Email sent\"\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 98,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def handle_request(self, request: str) -> str:\n        prompt = f\"Draft and send an email for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        email_content = response.choices[0].message.content\n\n        # VULNERABLE - sending emails without human review\n        self.email.send(email_content)\n        return \"Email sent\""
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_98-98"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'perform_action' on line 115 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'perform_action' on line 115 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def perform_action(self, action: str) -> str:\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        # VULNERABLE - no audit trail\n        result = self._execute(response.choices[0].message.content)\n        return result\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 115,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def perform_action(self, action: str) -> str:\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        # VULNERABLE - no audit trail\n        result = self._execute(response.choices[0].message.content)\n        return result\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_115-115"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'process_request' on line 138 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'process_request' on line 138 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\n    def process_request(self, request: str) -> str:\n        prompt = f\"Suggest an action for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        suggested_action = response.choices[0].message.content\n\n        # Require human approval\n        if self.approve(suggested_action):\n            return self._execute_safe(suggested_action)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 138,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def process_request(self, request: str) -> str:\n        prompt = f\"Suggest an action for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        suggested_action = response.choices[0].message.content\n\n        # Require human approval\n        if self.approve(suggested_action):\n            return self._execute_safe(suggested_action)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_138-138"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 17. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 17. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 17,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_17_unpinned-17"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 37. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 37. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 37,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def process_request(self, user_request: str) -> str:\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_37_unpinned-37"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 55. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 55. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 55,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_55_unpinned-55"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 75. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 75. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Should I process this payment request: {user_request}? If yes, respond with the amount.\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 75,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def process_request(self, user_request: str) -> str:\n        prompt = f\"Should I process this payment request: {user_request}? If yes, respond with the amount.\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_75_unpinned-75"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 100. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 100. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    def handle_request(self, request: str) -> str:\n        prompt = f\"Draft and send an email for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 100,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def handle_request(self, request: str) -> str:\n        prompt = f\"Draft and send an email for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_100_unpinned-100"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 117. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 117. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    def perform_action(self, action: str) -> str:\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 117,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def perform_action(self, action: str) -> str:\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_117_unpinned-117"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 140. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 140. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.\n\n**Code:**\n```python\n    def process_request(self, request: str) -> str:\n        prompt = f\"Suggest an action for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 140,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    def process_request(self, request: str) -> str:\n        prompt = f\"Suggest an action for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_140_unpinned-140"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "error",
          "message": {
            "text": "exec() on non-literal content on line 24. ",
            "markdown": "**Code execution on external content**\n\nexec() on non-literal content on line 24. \n\n**Code:**\n```python\n\n    # VULNERABLE - auto-execution without human review\n    exec(code)\n    return \"Code executed\"\n\n\n```\n\n**Remediation:**\nSecure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 24,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    # VULNERABLE - auto-execution without human review\n    exec(code)\n    return \"Code executed\"\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_24_code_exec-24"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
              }
            }
          ]
        },
        {
          "ruleId": "LLM07",
          "ruleIndex": 6,
          "level": "error",
          "message": {
            "text": "Tool function 'execute_task' on line 53 takes LLM output as a parameter and performs dangerous operations (shell_exec) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
            "markdown": "**Insecure tool function 'execute_task' executes dangerous operations**\n\nTool function 'execute_task' on line 53 takes LLM output as a parameter and performs dangerous operations (shell_exec) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.\n\n**Code:**\n```python\n# VULN:LLM08:CRITICAL:LINE=51 - Agent with system-level access\nclass VulnerableSystemAgent:\n    \"\"\"Vulnerable: Agent can execute system commands.\"\"\"\n\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nSecure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 53,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM08:CRITICAL:LINE=51 - Agent with system-level access\nclass VulnerableSystemAgent:\n    \"\"\"Vulnerable: Agent can execute system commands.\"\"\"\n\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_53_tool-53"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM07: Insecure Plugin Design"
          },
          "fixes": [
            {
              "description": {
                "text": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_auto_execute' on line 14 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in 'vulnerable_auto_execute'**\n\nFunction 'vulnerable_auto_execute' on line 14 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.\n\n**Code:**\n```python\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n\n# VULN:LLM08:CRITICAL:LINE=16 - Auto-execution of LLM-generated code\ndef vulnerable_auto_execute(user_request: str) -> str:\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nCode Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 14,
                  "startColumn": 1,
                  "snippet": {
                    "text": "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n\n# VULN:LLM08:CRITICAL:LINE=16 - Auto-execution of LLM-generated code\ndef vulnerable_auto_execute(user_request: str) -> str:\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_14_exec-14"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "error",
          "message": {
            "text": "Function 'execute_task' on line 53 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
            "markdown": "**Direct execution of LLM-generated code in 'execute_task'**\n\nFunction 'execute_task' on line 53 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.\n\n**Code:**\n```python\n# VULN:LLM08:CRITICAL:LINE=51 - Agent with system-level access\nclass VulnerableSystemAgent:\n    \"\"\"Vulnerable: Agent can execute system commands.\"\"\"\n\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n```\n\n**Remediation:**\nCode Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 53,
                  "startColumn": 1,
                  "snippet": {
                    "text": "# VULN:LLM08:CRITICAL:LINE=51 - Agent with system-level access\nclass VulnerableSystemAgent:\n    \"\"\"Vulnerable: Agent can execute system commands.\"\"\"\n\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_53_exec-53"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_auto_execute' on line 14 directly executes LLM-generated code using exec(. This is extremely dangerous and allows arbitrary code execution.",
            "markdown": "**Direct execution of LLM output in 'vulnerable_auto_execute'**\n\nFunction 'vulnerable_auto_execute' on line 14 directly executes LLM-generated code using exec(. This is extremely dangerous and allows arbitrary code execution.\n\n**Code:**\n```python\n\n# VULN:LLM08:CRITICAL:LINE=16 - Auto-execution of LLM-generated code\ndef vulnerable_auto_execute(user_request: str) -> str:\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nNEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 14,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM08:CRITICAL:LINE=16 - Auto-execution of LLM-generated code\ndef vulnerable_auto_execute(user_request: str) -> str:\n    \"\"\"Vulnerable: Automatically executes code generated by LLM.\"\"\"\n    prompt = f\"Write Python code to: {user_request}. Output only code, no explanation.\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_14_direct_execution-14"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.85,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'process_request' on line 35 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
            "markdown": "**Critical decision without oversight in 'process_request'**\n\nFunction 'process_request' on line 35 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.\n\n**Code:**\n```python\n        self.db = db_connection\n\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nNEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 35,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self.db = db_connection\n\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Generate SQL to: {user_request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_35_critical_decision-35"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'execute_task' on line 53 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
            "markdown": "**Critical decision without oversight in 'execute_task'**\n\nFunction 'execute_task' on line 53 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Agent can execute system commands.\"\"\"\n\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nNEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 53,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Agent can execute system commands.\"\"\"\n\n    def execute_task(self, task: str) -> str:\n        prompt = f\"What shell command should I run to: {task}?\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_53_critical_decision-53"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'process_request' on line 73 makes critical financial, security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'process_request'**\n\nFunction 'process_request' on line 73 makes critical financial, security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n        self.payment = payment_api\n\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Should I process this payment request: {user_request}? If yes, respond with the amount.\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical financial, security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 73,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self.payment = payment_api\n\n    def process_request(self, user_request: str) -> str:\n        prompt = f\"Should I process this payment request: {user_request}? If yes, respond with the amount.\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_73_critical_decision-73"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical financial, security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'perform_action' on line 115 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'perform_action'**\n\nFunction 'perform_action' on line 115 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Agent actions are not logged.\"\"\"\n\n    def perform_action(self, action: str) -> str:\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 115,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Agent actions are not logged.\"\"\"\n\n    def perform_action(self, action: str) -> str:\n        prompt = f\"Perform this action: {action}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_115_critical_decision-115"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'process_request' on line 138 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'process_request'**\n\nFunction 'process_request' on line 138 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n        self.approve = approval_callback\n\n    def process_request(self, request: str) -> str:\n        prompt = f\"Suggest an action for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 138,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        self.approve = approval_callback\n\n    def process_request(self, request: str) -> str:\n        prompt = f\"Suggest an action for: {request}\"\n        response = client.chat.completions.create(\n            model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency/app.py_138_critical_decision-138"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-10T15:46:23.899309+00:00",
          "workingDirectory": {
            "uri": "file:///Users/deo/secscan-cli/llm-sec-eval/testbed/llm08_excessive_agency"
          }
        }
      ],
      "automationDetails": {
        "id": "ai-security-cli/static-scan/20260110154623"
      }
    }
  ]
}