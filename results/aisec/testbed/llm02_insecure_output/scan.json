{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T14:52:03.810108Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output",
    "files_scanned": 1,
    "overall_score": 0.0,
    "confidence": 0.56,
    "duration_seconds": 0.008,
    "findings_count": 12,
    "severity_breakdown": {
      "CRITICAL": 8,
      "HIGH": 2,
      "MEDIUM": 1,
      "LOW": 0,
      "INFO": 1
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [],
      "gaps": [
        "No input validation detected",
        "No jailbreak prevention mechanisms detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.37,
      "subscores": {
        "model_protection": 16,
        "extraction_defense": 25,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit"
      ],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.73,
      "subscores": {
        "LLM01": 100,
        "LLM02": 64,
        "LLM03": 100,
        "LLM04": 86,
        "LLM05": 0,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 100,
        "LLM09": 100,
        "LLM10": 100
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Excessive Agency (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Insecure Output Handling: 1 critical",
        "Model Denial of Service: 1 high",
        "Supply Chain Vulnerabilities: 7 critical, 1 high, 1 medium"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_18",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous code_execution sink",
      "description": "LLM output from 'client.chat.completions.create' is used in 'eval(' on line 18 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 18,
      "code_snippet": "    \"\"\"Get response from LLM.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists for permitted operations\n5. Consider structured output formats (JSON) instead"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_16",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_llm_response' on line 16 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 16,
      "code_snippet": "def get_llm_response(prompt: str) -> str:\n    \"\"\"Get response from LLM.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n\n# VULN:LLM02:CRITICAL:LINE=29 - Eval with LLM output\ndef vulnerable_eval(user_request: str) -> str:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_18_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 18. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 18,
      "code_snippet": "def get_llm_response(prompt: str) -> str:\n    \"\"\"Get response from LLM.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_87_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 87. This library can execute arbitrary code during deserialization. File fetches external data - HIGH RISK if deserializing remote content.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 87,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_25_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [80]) and executes code (lines [25, 26, 27]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 25,
      "code_snippet": "    )\n    return response.choices[0].message.content\n\n\n# VULN:LLM02:CRITICAL:LINE=29 - Eval with LLM output\ndef vulnerable_eval(user_request: str) -> str:\n    \"\"\"Vulnerable: Using eval() on LLM output.\"\"\"\n    prompt = f\"Generate a Python expression to calculate: {user_request}\"\n    llm_code = get_llm_response(prompt)\n    result = eval(llm_code)  # VULNERABLE - arbitrary code execution",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_26_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 26. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 26,
      "code_snippet": "\n# VULN:LLM02:CRITICAL:LINE=29 - Eval with LLM output\ndef vulnerable_eval(user_request: str) -> str:\n    \"\"\"Vulnerable: Using eval() on LLM output.\"\"\"\n    prompt = f\"Generate a Python expression to calculate: {user_request}\"\n    llm_code = get_llm_response(prompt)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_27_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 27. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 27,
      "code_snippet": "# VULN:LLM02:CRITICAL:LINE=29 - Eval with LLM output\ndef vulnerable_eval(user_request: str) -> str:\n    \"\"\"Vulnerable: Using eval() on LLM output.\"\"\"\n    prompt = f\"Generate a Python expression to calculate: {user_request}\"\n    llm_code = get_llm_response(prompt)\n    result = eval(llm_code)  # VULNERABLE - arbitrary code execution",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_30_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 30. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 30,
      "code_snippet": "    prompt = f\"Generate a Python expression to calculate: {user_request}\"\n    llm_code = get_llm_response(prompt)\n    result = eval(llm_code)  # VULNERABLE - arbitrary code execution\n    return str(result)\n\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_35_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 35. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 35,
      "code_snippet": "\n# VULN:LLM02:CRITICAL:LINE=39 - Exec with LLM output\ndef vulnerable_exec(user_request: str) -> None:\n    \"\"\"Vulnerable: Using exec() on LLM-generated code.\"\"\"\n    prompt = f\"Write Python code to: {user_request}\"\n    llm_code = get_llm_response(prompt)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_36_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 36. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 36,
      "code_snippet": "# VULN:LLM02:CRITICAL:LINE=39 - Exec with LLM output\ndef vulnerable_exec(user_request: str) -> None:\n    \"\"\"Vulnerable: Using exec() on LLM-generated code.\"\"\"\n    prompt = f\"Write Python code to: {user_request}\"\n    llm_code = get_llm_response(prompt)\n    exec(llm_code)  # VULNERABLE - arbitrary code execution",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_39_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 39. File fetches external content - HIGH RISK.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 39,
      "code_snippet": "    prompt = f\"Write Python code to: {user_request}\"\n    llm_code = get_llm_response(prompt)\n    exec(llm_code)  # VULNERABLE - arbitrary code execution\n\n\n# VULN:LLM02:HIGH:LINE=49 - SQL Injection via LLM output",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py_16_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_llm_response'",
      "description": "Function 'get_llm_response' on line 16 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm02_insecure_output/app.py",
      "line_number": 16,
      "code_snippet": "\n\ndef get_llm_response(prompt: str) -> str:\n    \"\"\"Get response from LLM.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    }
  ],
  "metadata": {}
}