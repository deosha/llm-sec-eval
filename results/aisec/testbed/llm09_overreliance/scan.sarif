{
  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "ai-security-cli",
          "version": "1.0.0",
          "informationUri": "https://github.com/ai-security-cli/ai-security-cli",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker's intentions."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts, and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate and sanitize all user inputs\n- Use parameterized prompts\n- Implement output filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed to other components."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding, validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM outputs may contain malicious content like XSS payloads or SQL injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply context-appropriate output encoding\n- Validate outputs against expected schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is manipulated to introduce vulnerabilities, backdoors, or biases that compromise model security and effectiveness."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify and validate training data sources\n- Implement data sanitization pipelines\n- Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact with LLMs in ways that consume excessive resources, leading to service degradation or high costs."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement rate limiting per user/API key\n- Set maximum token/context limits\n- Use request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can arise from compromised pre-trained models, poisoned training data from third parties, or vulnerable third-party plugins."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:** Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models from trusted sources only\n- Verify model signatures and checksums\n- Scan third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs inadvertently reveal confidential data through their responses, potentially exposing PII, credentials, or proprietary information."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:** Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n- Sanitize training data to remove sensitive information\n- Implement output filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack proper input validation, have excessive permissions, or fail to implement adequate security controls."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may execute malicious actions or expose sensitive functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n- Apply principle of least privilege\n- Use sandboxing for plugin execution\n- Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are granted too much autonomy to take actions, potentially leading to unintended consequences from hallucinations or malicious prompts."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM permissions to minimum necessary\n- Require human-in-the-loop for critical actions\n- Implement comprehensive logging\n- Use rate limiting and action budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust LLM outputs without adequate verification, potentially leading to misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs, provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs without verification can lead to errors.\n\n**Mitigation:**\n- Implement human review for critical decisions\n- Validate LLM outputs against trusted sources\n- Display confidence scores and limitations\n- Educate users about LLM limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or extraction of proprietary LLM models, leading to economic loss, competitive disadvantage, and potential security risks."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting, monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n- Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "The function 'vulnerable_security_decision' embeds user input ('request') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
            "markdown": "**User input 'request' directly embedded in LLM prompt**\n\nThe function 'vulnerable_security_decision' embeds user input ('request') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 49,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_49-49"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.75,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 16 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 16 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_16-16"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 38 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 38 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 38,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_38-38"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 50 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 50 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 50,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_50-50"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 65 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 65 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 65,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_65-65"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 80 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 80 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 80,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_80-80"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 92 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 92 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 92,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_92-92"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 107 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 107 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 107,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_107-107"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM02",
          "ruleIndex": 1,
          "level": "error",
          "message": {
            "text": "LLM output from 'client.chat.completions.create' is used in 'Response' on line 131 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
            "markdown": "**LLM output used in dangerous xss sink**\n\nLLM output from 'client.chat.completions.create' is used in 'Response' on line 131 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.\n\n**Code:**\n```python\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n```\n\n**Remediation:**\nMitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 131,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_131-131"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM02: Insecure Output Handling"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_auto_apply_code' on line 13 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_auto_apply_code' on line 13 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_auto_apply_code(description: str) -> str:\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    code = response.choices[0].message.content\n\n    # VULNERABLE - applying code without human review\n    with open(\"generated_code.py\", \"w\") as f:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 13,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_auto_apply_code(description: str) -> str:\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    code = response.choices[0].message.content\n\n    # VULNERABLE - applying code without human review\n    with open(\"generated_code.py\", \"w\") as f:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_13-13"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_medical_advice' on line 35 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_medical_advice' on line 35 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    # VULNERABLE - no disclaimer, presented as authoritative\n    return response.choices[0].message.content\n\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 35,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    # VULNERABLE - no disclaimer, presented as authoritative\n    return response.choices[0].message.content\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_35-35"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_security_decision' on line 47 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_security_decision' on line 47 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    decision = response.choices[0].message.content.lower()\n\n    # VULNERABLE - security decision based on LLM output\n    if \"yes\" in decision:\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 47,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    decision = response.choices[0].message.content.lower()\n\n    # VULNERABLE - security decision based on LLM output\n    if \"yes\" in decision:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_47-47"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_confidence' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_no_confidence' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n    )\n    # VULNERABLE - no confidence score, presented as certain\n    return {\n        \"answer\": response.choices[0].message.content,\n        # Missing: confidence, sources, uncertainty\n    }\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n    )\n    # VULNERABLE - no confidence score, presented as certain\n    return {\n        \"answer\": response.choices[0].message.content,\n        # Missing: confidence, sources, uncertainty\n    }"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63-63"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_citations' on line 77 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_no_citations' on line 77 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    # VULNERABLE - no sources provided\n    return response.choices[0].message.content\n\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 77,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    # VULNERABLE - no sources provided\n    return response.choices[0].message.content\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_77-77"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_auto_publish' on line 89 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_auto_publish' on line 89 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_auto_publish(topic: str) -> str:\n    \"\"\"Vulnerable: Auto-publishes LLM-generated content without review.\"\"\"\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    content = response.choices[0].message.content\n\n    # VULNERABLE - auto-publish without fact-checking\n    publish_to_website(content)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 89,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_auto_publish(topic: str) -> str:\n    \"\"\"Vulnerable: Auto-publishes LLM-generated content without review.\"\"\"\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    content = response.choices[0].message.content\n\n    # VULNERABLE - auto-publish without fact-checking\n    publish_to_website(content)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_89-89"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_critical_decision' on line 104 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_critical_decision' on line 104 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    recommendation = response.choices[0].message.content\n\n    # VULNERABLE - acting on LLM recommendation without verification\n    execute_trade(recommendation)\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 104,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    recommendation = response.choices[0].message.content\n\n    # VULNERABLE - acting on LLM recommendation without verification\n    execute_trade(recommendation)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_104-104"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'safe_verified_response' on line 129 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'safe_verified_response' on line 129 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef safe_verified_response(query: str) -> dict:\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n        # Using logprobs for confidence would be ideal\n    )\n\n    return {\n        \"answer\": response.choices[0].message.content,\n        \"confidence\": \"medium\",  # Assess confidence\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 129,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def safe_verified_response(query: str) -> dict:\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n        # Using logprobs for confidence would be ideal\n    )\n\n    return {\n        \"answer\": response.choices[0].message.content,\n        \"confidence\": \"medium\",  # Assess confidence"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_129-129"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 16. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 16. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 16,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_16_unpinned-16"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 38. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 38. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 38,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_38_unpinned-38"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 50. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 50. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 50,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_50_unpinned-50"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 65,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_65_unpinned-65"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 80. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 80. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 80,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_80_unpinned-80"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 92. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 92. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Auto-publishes LLM-generated content without review.\"\"\"\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 92,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Auto-publishes LLM-generated content without review.\"\"\"\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_92_unpinned-92"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 107. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 107. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 107,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_107_unpinned-107"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "warning",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 131. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 131. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').\n\n**Code:**\n```python\ndef safe_verified_response(query: str) -> dict:\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n        # Using logprobs for confidence would be ideal\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 131,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def safe_verified_response(query: str) -> dict:\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n        # Using logprobs for confidence would be ideal"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_131_unpinned-131"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
              }
            }
          ]
        },
        {
          "ruleId": "LLM08",
          "ruleIndex": 7,
          "level": "warning",
          "message": {
            "text": "Function 'vulnerable_no_confidence' on line 63 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
            "markdown": "**High-risk write operation without confirmation in 'vulnerable_no_confidence'**\n\nFunction 'vulnerable_no_confidence' on line 63 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63_risk-63"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.7,
            "category": "LLM08: Excessive Agency"
          },
          "fixes": [
            {
              "description": {
                "text": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_medical_advice' on line 35 makes critical medical, security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_medical_advice'**\n\nFunction 'vulnerable_medical_advice' on line 35 makes critical medical, security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n\n# VULN:LLM09:HIGH:LINE=35 - Medical/legal advice without disclaimer\ndef vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nCritical medical, security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 35,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:HIGH:LINE=35 - Medical/legal advice without disclaimer\ndef vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_35_critical_decision-35"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical medical, security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_security_decision' on line 47 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_security_decision'**\n\nFunction 'vulnerable_security_decision' on line 47 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n\n# VULN:LLM09:HIGH:LINE=48 - Security decisions based on LLM output\ndef vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 47,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:HIGH:LINE=48 - Security decisions based on LLM output\ndef vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_47_critical_decision-47"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_confidence' on line 63 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_confidence'**\n\nFunction 'vulnerable_no_confidence' on line 63 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63_critical_decision-63"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_no_citations' on line 77 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_no_citations'**\n\nFunction 'vulnerable_no_citations' on line 77 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=78 - No source citations\ndef vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 77,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=78 - No source citations\ndef vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_77_critical_decision-77"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_critical_decision' on line 104 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
            "markdown": "**Critical decision without oversight in 'vulnerable_critical_decision'**\n\nFunction 'vulnerable_critical_decision' on line 104 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=107 - Critical decisions without verification\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 104,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=107 - Critical decisions without verification\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_104_critical_decision-104"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "warning",
          "message": {
            "text": "Function 'vulnerable_auto_apply_code' on line 13 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
            "markdown": "**Automated action without confidence threshold in 'vulnerable_auto_apply_code'**\n\nFunction 'vulnerable_auto_apply_code' on line 13 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.\n\n**Code:**\n```python\n\n# VULN:LLM09:HIGH:LINE=14 - Auto-applying LLM-generated code\ndef vulnerable_auto_apply_code(description: str) -> str:\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 13,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:HIGH:LINE=14 - Auto-applying LLM-generated code\ndef vulnerable_auto_apply_code(description: str) -> str:\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_13_automated_action-13"
          },
          "properties": {
            "security-severity": "5.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_medical_advice' on line 35 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_medical_advice'**\n\nFunction 'vulnerable_medical_advice' on line 35 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n\n# VULN:LLM09:HIGH:LINE=35 - Medical/legal advice without disclaimer\ndef vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 35,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:HIGH:LINE=35 - Medical/legal advice without disclaimer\ndef vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_35_missing_verification-35"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_security_decision' on line 47 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_security_decision'**\n\nFunction 'vulnerable_security_decision' on line 47 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n\n# VULN:LLM09:HIGH:LINE=48 - Security decisions based on LLM output\ndef vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 47,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:HIGH:LINE=48 - Security decisions based on LLM output\ndef vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_47_missing_verification-47"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_confidence' on line 63 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_no_confidence'**\n\nFunction 'vulnerable_no_confidence' on line 63 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63_missing_verification-63"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_no_citations' on line 77 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_no_citations'**\n\nFunction 'vulnerable_no_citations' on line 77 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=78 - No source citations\ndef vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 77,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=78 - No source citations\ndef vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_77_missing_verification-77"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_critical_decision' on line 104 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
            "markdown": "**LLM output returned without verification in 'vulnerable_critical_decision'**\n\nFunction 'vulnerable_critical_decision' on line 104 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.\n\n**Code:**\n```python\n\n# VULN:LLM09:MEDIUM:LINE=107 - Critical decisions without verification\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nAdd verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 104,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM09:MEDIUM:LINE=107 - Critical decisions without verification\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_104_missing_verification-104"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.7,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-08T17:49:22.773581+00:00",
          "workingDirectory": {
            "uri": "file:///Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance"
          }
        }
      ],
      "automationDetails": {
        "id": "ai-security-cli/static-scan/20260108174922"
      }
    }
  ]
}