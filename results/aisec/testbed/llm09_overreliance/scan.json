{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T15:46:24.008949Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance",
    "files_scanned": 1,
    "overall_score": 0.79,
    "confidence": 0.57,
    "duration_seconds": 0.011,
    "findings_count": 28,
    "severity_breakdown": {
      "CRITICAL": 5,
      "HIGH": 10,
      "MEDIUM": 9,
      "LOW": 0,
      "INFO": 4
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 6,
      "confidence": 0.4,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 0,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [],
      "gaps": [
        "No input validation detected",
        "No jailbreak prevention mechanisms detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.37,
      "subscores": {
        "model_protection": 16,
        "extraction_defense": 25,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit"
      ],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 0
      },
      "detected_controls": [],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.78,
      "subscores": {
        "LLM01": 0,
        "LLM02": 100,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 40,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 65,
        "LLM09": 65,
        "LLM10": 100
      },
      "detected_controls": [
        "Insecure Output Handling (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 3 critical",
        "Model Denial of Service: 2 critical, 6 high",
        "Supply Chain Vulnerabilities: 8 medium",
        "Excessive Agency: 2 high, 1 medium",
        "Overreliance: 2 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_49",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "User input 'request' embedded in LLM prompt",
      "description": "User input 'request' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_security_decision' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 49,
      "code_snippet": "    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_79",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input 'question' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_no_citations' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 79,
      "code_snippet": "    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_108",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "User input 'data' embedded in LLM prompt",
      "description": "User input 'data' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_critical_decision' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 108,
      "code_snippet": "    import requests\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_13",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_auto_apply_code' on line 13 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 13,
      "code_snippet": "def vulnerable_auto_apply_code(description: str) -> str:\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    code = response.choices[0].message.content\n\n    # VULNERABLE - applying code without human review\n    with open(\"generated_code.py\", \"w\") as f:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_35",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_medical_advice' on line 35 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 35,
      "code_snippet": "def vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    # VULNERABLE - no disclaimer, presented as authoritative\n    return response.choices[0].message.content\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_47",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_security_decision' on line 47 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 47,
      "code_snippet": "def vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    decision = response.choices[0].message.content.lower()\n\n    # VULNERABLE - security decision based on LLM output\n    if \"yes\" in decision:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_no_confidence' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 63,
      "code_snippet": "def vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n    )\n    # VULNERABLE - no confidence score, presented as certain\n    return {\n        \"answer\": response.choices[0].message.content,\n        # Missing: confidence, sources, uncertainty\n    }",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_77",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_no_citations' on line 77 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 77,
      "code_snippet": "def vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    # VULNERABLE - no sources provided\n    return response.choices[0].message.content\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_89",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_auto_publish' on line 89 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 89,
      "code_snippet": "def vulnerable_auto_publish(topic: str) -> str:\n    \"\"\"Vulnerable: Auto-publishes LLM-generated content without review.\"\"\"\n    import requests\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    content = response.choices[0].message.content\n\n    # VULNERABLE - auto-publish without fact-checking (detectable HTTP POST)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_105",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_critical_decision' on line 105 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 105,
      "code_snippet": "def vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    import requests\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    recommendation = response.choices[0].message.content\n\n    # VULNERABLE - acting on LLM recommendation without verification (detectable HTTP POST)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_121",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'safe_verified_response' on line 121 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 121,
      "code_snippet": "def safe_verified_response(query: str) -> dict:\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n        # Using logprobs for confidence would be ideal\n    )\n\n    return {\n        \"answer\": response.choices[0].message.content,\n        \"confidence\": \"medium\",  # Assess confidence",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_16_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 16. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 16,
      "code_snippet": "    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_38_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 38. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 38,
      "code_snippet": "    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_50_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 50. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 50,
      "code_snippet": "    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_65_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 65. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 65,
      "code_snippet": "def vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_80_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 80. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 80,
      "code_snippet": "    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_93_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 93. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 93,
      "code_snippet": "    import requests\n    prompt = f\"Write a blog post about: {topic}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_109_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 109. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 109,
      "code_snippet": "    import requests\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_123_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 123. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 123,
      "code_snippet": "def safe_verified_response(query: str) -> dict:\n    \"\"\"Safe: Includes confidence, sources, and disclaimers.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": query}],\n        # Using logprobs for confidence would be ideal",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'vulnerable_no_confidence'",
      "description": "Function 'vulnerable_no_confidence' on line 63 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 63,
      "code_snippet": "\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_89_api",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unrestricted API access from LLM in 'vulnerable_auto_publish'",
      "description": "Function 'vulnerable_auto_publish' on line 89 makes HTTP/API requests based on LLM outputs without URL validation or allowlisting. This allows the LLM to make requests to arbitrary endpoints, potentially exfiltrating data, performing SSRF attacks, or accessing unauthorized resources.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 89,
      "code_snippet": "\n# VULN:LLM09:HIGH:LINE=91 - Auto-publishing LLM content\ndef vulnerable_auto_publish(topic: str) -> str:\n    \"\"\"Vulnerable: Auto-publishes LLM-generated content without review.\"\"\"\n    import requests\n    prompt = f\"Write a blog post about: {topic}\"",
      "recommendation": "API Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_105_api",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unrestricted API access from LLM in 'vulnerable_critical_decision'",
      "description": "Function 'vulnerable_critical_decision' on line 105 makes HTTP/API requests based on LLM outputs without URL validation or allowlisting. This allows the LLM to make requests to arbitrary endpoints, potentially exfiltrating data, performing SSRF attacks, or accessing unauthorized resources.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 105,
      "code_snippet": "\n# VULN:LLM09:HIGH:LINE=107 - Critical decisions without verification\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    import requests\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"",
      "recommendation": "API Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_35_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_medical_advice'",
      "description": "Function 'vulnerable_medical_advice' on line 35 makes critical medical, security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 35,
      "code_snippet": "\n# VULN:LLM09:HIGH:LINE=35 - Medical/legal advice without disclaimer\ndef vulnerable_medical_advice(symptoms: str) -> str:\n    \"\"\"Vulnerable: Provides medical advice without proper disclaimers.\"\"\"\n    prompt = f\"Based on these symptoms: {symptoms}, what is the diagnosis and treatment?\"\n    response = client.chat.completions.create(",
      "recommendation": "Critical medical, security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_47_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_security_decision'",
      "description": "Function 'vulnerable_security_decision' on line 47 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 47,
      "code_snippet": "\n# VULN:LLM09:HIGH:LINE=48 - Security decisions based on LLM output\ndef vulnerable_security_decision(request: str) -> bool:\n    \"\"\"Vulnerable: Makes security decisions based on LLM recommendation.\"\"\"\n    prompt = f\"Should I allow this request? {request}. Answer yes or no.\"\n    response = client.chat.completions.create(",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_63_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_no_confidence'",
      "description": "Function 'vulnerable_no_confidence' on line 63 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 63,
      "code_snippet": "\n# VULN:LLM09:MEDIUM:LINE=64 - No confidence indicators\ndef vulnerable_no_confidence(query: str) -> dict:\n    \"\"\"Vulnerable: Returns answers without confidence or uncertainty indicators.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_77_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_no_citations'",
      "description": "Function 'vulnerable_no_citations' on line 77 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 77,
      "code_snippet": "\n# VULN:LLM09:MEDIUM:LINE=78 - No source citations\ndef vulnerable_no_citations(question: str) -> str:\n    \"\"\"Vulnerable: Factual claims without source citations.\"\"\"\n    prompt = f\"Answer this factual question: {question}\"\n    response = client.chat.completions.create(",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_105_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_critical_decision'",
      "description": "Function 'vulnerable_critical_decision' on line 105 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 105,
      "code_snippet": "\n# VULN:LLM09:HIGH:LINE=107 - Critical decisions without verification\ndef vulnerable_critical_decision(data: dict) -> str:\n    \"\"\"Vulnerable: Makes critical business decisions based on LLM analysis.\"\"\"\n    import requests\n    prompt = f\"Analyze this data and recommend: buy, sell, or hold. Data: {data}\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py_13_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'vulnerable_auto_apply_code'",
      "description": "Function 'vulnerable_auto_apply_code' on line 13 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm09_overreliance/app.py",
      "line_number": 13,
      "code_snippet": "\n# VULN:LLM09:HIGH:LINE=14 - Auto-applying LLM-generated code\ndef vulnerable_auto_apply_code(description: str) -> str:\n    \"\"\"Vulnerable: Automatically applies LLM-generated code without review.\"\"\"\n    prompt = f\"Write a Python function to: {description}\"\n    response = client.chat.completions.create(",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    }
  ],
  "metadata": {}
}