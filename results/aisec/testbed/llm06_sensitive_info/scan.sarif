{
  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
  "version": "2.1.0",
  "runs": [
    {
      "tool": {
        "driver": {
          "name": "aisentry",
          "version": "1.0.0",
          "informationUri": "https://github.com/aisentry/aisentry",
          "rules": [
            {
              "id": "LLM01",
              "name": "PromptInjection",
              "shortDescription": {
                "text": "Prompt Injection"
              },
              "fullDescription": {
                "text": "Prompt Injection vulnerability occurs when an attacker manipulates a large language model (LLM) through crafted inputs, causing the LLM to unknowingly execute the attacker's intentions."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement input validation, use parameterized prompts, and apply output filtering to prevent prompt injection attacks.",
                "markdown": "## Prompt Injection\n\n**Risk:** Attackers can manipulate LLM behavior through malicious inputs.\n\n**Mitigation:**\n- Validate and sanitize all user inputs\n- Use parameterized prompts\n- Implement output filtering\n- Use context isolation between system and user prompts"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "injection"
                ],
                "security-severity": "8.0"
              }
            },
            {
              "id": "LLM02",
              "name": "InsecureOutputHandling",
              "shortDescription": {
                "text": "Insecure Output Handling"
              },
              "fullDescription": {
                "text": "Insecure Output Handling refers to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed to other components."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Treat LLM output as untrusted, apply output encoding, validate against schemas, and use content security policies.",
                "markdown": "## Insecure Output Handling\n\n**Risk:** LLM outputs may contain malicious content like XSS payloads or SQL injection.\n\n**Mitigation:**\n- Treat all LLM output as untrusted\n- Apply context-appropriate output encoding\n- Validate outputs against expected schemas\n- Implement Content Security Policy (CSP)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "xss",
                  "injection"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM03",
              "name": "TrainingDataPoisoning",
              "shortDescription": {
                "text": "Training Data Poisoning"
              },
              "fullDescription": {
                "text": "Training Data Poisoning occurs when training data is manipulated to introduce vulnerabilities, backdoors, or biases that compromise model security and effectiveness."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify training data sources, implement data validation pipelines, and monitor for anomalous model behavior.",
                "markdown": "## Training Data Poisoning\n\n**Risk:** Compromised training data can introduce backdoors or biases.\n\n**Mitigation:**\n- Verify and validate training data sources\n- Implement data sanitization pipelines\n- Use anomaly detection during training\n- Maintain data provenance records"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-integrity"
                ],
                "security-severity": "6.5"
              }
            },
            {
              "id": "LLM04",
              "name": "ModelDenialOfService",
              "shortDescription": {
                "text": "Model Denial of Service"
              },
              "fullDescription": {
                "text": "Model Denial of Service occurs when attackers interact with LLMs in ways that consume excessive resources, leading to service degradation or high costs."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement rate limiting, set token limits, use timeouts, and monitor resource consumption.",
                "markdown": "## Model Denial of Service\n\n**Risk:** Resource exhaustion through expensive LLM operations.\n\n**Mitigation:**\n- Implement rate limiting per user/API key\n- Set maximum token/context limits\n- Use request timeouts\n- Monitor and alert on resource consumption"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "availability",
                  "dos"
                ],
                "security-severity": "5.5"
              }
            },
            {
              "id": "LLM05",
              "name": "SupplyChainVulnerabilities",
              "shortDescription": {
                "text": "Supply Chain Vulnerabilities"
              },
              "fullDescription": {
                "text": "Supply Chain Vulnerabilities in LLM applications can arise from compromised pre-trained models, poisoned training data from third parties, or vulnerable third-party plugins."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Verify model sources, use signed models, scan dependencies, and maintain software bill of materials.",
                "markdown": "## Supply Chain Vulnerabilities\n\n**Risk:** Compromised models, datasets, or dependencies.\n\n**Mitigation:**\n- Use models from trusted sources only\n- Verify model signatures and checksums\n- Scan third-party dependencies\n- Maintain Software Bill of Materials (SBOM)"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "supply-chain"
                ],
                "security-severity": "7.0"
              }
            },
            {
              "id": "LLM06",
              "name": "SensitiveInformationDisclosure",
              "shortDescription": {
                "text": "Sensitive Information Disclosure"
              },
              "fullDescription": {
                "text": "Sensitive Information Disclosure occurs when LLMs inadvertently reveal confidential data through their responses, potentially exposing PII, credentials, or proprietary information."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement data sanitization, use output filters, apply access controls, and avoid training on sensitive data.",
                "markdown": "## Sensitive Information Disclosure\n\n**Risk:** Exposure of PII, credentials, or proprietary data.\n\n**Mitigation:**\n- Sanitize training data to remove sensitive information\n- Implement output filtering for PII/secrets\n- Apply principle of least privilege\n- Use data loss prevention (DLP) tools"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "data-exposure",
                  "pii"
                ],
                "security-severity": "8.5"
              }
            },
            {
              "id": "LLM07",
              "name": "InsecurePluginDesign",
              "shortDescription": {
                "text": "Insecure Plugin Design"
              },
              "fullDescription": {
                "text": "Insecure Plugin Design occurs when LLM plugins lack proper input validation, have excessive permissions, or fail to implement adequate security controls."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Validate all plugin inputs, apply least privilege, use sandboxing, and require user confirmation for sensitive actions.",
                "markdown": "## Insecure Plugin Design\n\n**Risk:** Plugins may execute malicious actions or expose sensitive functionality.\n\n**Mitigation:**\n- Validate and sanitize all plugin inputs\n- Apply principle of least privilege\n- Use sandboxing for plugin execution\n- Require user confirmation for sensitive actions"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "plugins"
                ],
                "security-severity": "7.5"
              }
            },
            {
              "id": "LLM08",
              "name": "ExcessiveAgency",
              "shortDescription": {
                "text": "Excessive Agency"
              },
              "fullDescription": {
                "text": "Excessive Agency occurs when LLM-based systems are granted too much autonomy to take actions, potentially leading to unintended consequences from hallucinations or malicious prompts."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Limit LLM permissions, require human approval for critical actions, implement logging, and use rate limiting.",
                "markdown": "## Excessive Agency\n\n**Risk:** LLMs may take unintended actions with real-world consequences.\n\n**Mitigation:**\n- Limit LLM permissions to minimum necessary\n- Require human-in-the-loop for critical actions\n- Implement comprehensive logging\n- Use rate limiting and action budgets"
              },
              "defaultConfiguration": {
                "level": "warning"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "authorization"
                ],
                "security-severity": "6.0"
              }
            },
            {
              "id": "LLM09",
              "name": "Overreliance",
              "shortDescription": {
                "text": "Overreliance"
              },
              "fullDescription": {
                "text": "Overreliance on LLMs occurs when systems or users trust LLM outputs without adequate verification, potentially leading to misinformation, security vulnerabilities, or faulty code."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement human review processes, validate LLM outputs, provide confidence scores, and educate users about limitations.",
                "markdown": "## Overreliance\n\n**Risk:** Trusting LLM outputs without verification can lead to errors.\n\n**Mitigation:**\n- Implement human review for critical decisions\n- Validate LLM outputs against trusted sources\n- Display confidence scores and limitations\n- Educate users about LLM limitations"
              },
              "defaultConfiguration": {
                "level": "note"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "trust"
                ],
                "security-severity": "4.0"
              }
            },
            {
              "id": "LLM10",
              "name": "ModelTheft",
              "shortDescription": {
                "text": "Model Theft"
              },
              "fullDescription": {
                "text": "Model Theft refers to unauthorized access, copying, or extraction of proprietary LLM models, leading to economic loss, competitive disadvantage, and potential security risks."
              },
              "helpUri": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
              "help": {
                "text": "Implement access controls, use API rate limiting, monitor for extraction attempts, and apply watermarking.",
                "markdown": "## Model Theft\n\n**Risk:** Unauthorized access or extraction of proprietary models.\n\n**Mitigation:**\n- Implement strong access controls\n- Use API rate limiting\n- Monitor for model extraction attempts\n- Apply model watermarking techniques"
              },
              "defaultConfiguration": {
                "level": "error"
              },
              "properties": {
                "tags": [
                  "security",
                  "owasp-llm-top-10",
                  "intellectual-property"
                ],
                "security-severity": "7.0"
              }
            }
          ],
          "properties": {
            "tags": [
              "security",
              "llm",
              "ai",
              "owasp"
            ]
          }
        }
      },
      "results": [
        {
          "ruleId": "LLM01",
          "ruleIndex": 0,
          "level": "error",
          "message": {
            "text": "User input 'user_query' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_secret_in_prompt' may be vulnerable to prompt injection attacks.",
            "markdown": "**User input 'user_query' embedded in LLM prompt**\n\nUser input 'user_query' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_secret_in_prompt' may be vulnerable to prompt injection attacks.\n\n**Code:**\n```python\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nMitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 18,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_18-18"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM01: Prompt Injection"
          },
          "fixes": [
            {
              "description": {
                "text": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_secret_in_prompt' on line 15 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_secret_in_prompt' on line 15 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 15,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_15-15"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_pii_exposure' on line 27 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_pii_exposure' on line 27 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_pii_exposure(user_id: int) -> str:\n    \"\"\"Vulnerable: PII data included in prompt context.\"\"\"\n    # Simulating fetching user PII\n    user_data = {\n        \"name\": \"John Doe\",\n        \"ssn\": \"123-45-6789\",  # VULNERABLE - SSN exposed\n        \"email\": \"john.doe@example.com\",\n        \"credit_card\": \"4111-1111-1111-1111\"  # VULNERABLE - CC exposed\n    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 27,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_pii_exposure(user_id: int) -> str:\n    \"\"\"Vulnerable: PII data included in prompt context.\"\"\"\n    # Simulating fetching user PII\n    user_data = {\n        \"name\": \"John Doe\",\n        \"ssn\": \"123-45-6789\",  # VULNERABLE - SSN exposed\n        \"email\": \"john.doe@example.com\",\n        \"credit_card\": \"4111-1111-1111-1111\"  # VULNERABLE - CC exposed\n    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_27-27"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_system_prompt' on line 45 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_system_prompt' on line 45 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_system_prompt() -> str:\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 45,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_system_prompt() -> str:\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_45-45"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'vulnerable_logging' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits**\n\nFunction 'vulnerable_logging' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef vulnerable_logging(user_input: str, user_data: dict) -> str:\n    \"\"\"Vulnerable: Logging user PII and prompts.\"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n    logger = logging.getLogger(__name__)\n\n    # VULNERABLE - logging PII\n    logger.debug(f\"User data: {user_data}\")\n    logger.debug(f\"User input: {user_input}\")\n\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_logging(user_input: str, user_data: dict) -> str:\n    \"\"\"Vulnerable: Logging user PII and prompts.\"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n    logger = logging.getLogger(__name__)\n\n    # VULNERABLE - logging PII\n    logger.debug(f\"User data: {user_data}\")\n    logger.debug(f\"User input: {user_input}\")\n\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_63-63"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.8,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM04",
          "ruleIndex": 3,
          "level": "error",
          "message": {
            "text": "Function 'safe_secret_handling' on line 102 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
            "markdown": "**Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits**\n\nFunction 'safe_secret_handling' on line 102 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.\n\n**Code:**\n```python\ndef safe_secret_handling() -> str:\n    \"\"\"Safe: Secrets from environment, not in code or prompts.\"\"\"\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"API key not configured\")\n\n    client = OpenAI(api_key=api_key)\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\n```\n\n**Remediation:**\nModel DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 102,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def safe_secret_handling() -> str:\n    \"\"\"Safe: Secrets from environment, not in code or prompts.\"\"\"\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"API key not configured\")\n\n    client = OpenAI(api_key=api_key)\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_102-102"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.7,
            "category": "LLM04: Model Denial of Service"
          },
          "fixes": [
            {
              "description": {
                "text": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 19. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 19. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 19,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_19_unpinned-19"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 37. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 37. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 37,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_37_unpinned-37"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 52. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 52. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 52,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_52_unpinned-52"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 73. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 73. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n    logger.debug(f\"User input: {user_input}\")\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 73,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    logger.debug(f\"User input: {user_input}\")\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_73_unpinned-73"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM05",
          "ruleIndex": 4,
          "level": "note",
          "message": {
            "text": "Model ''gpt-4'' is used without version pinning on line 109. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
            "markdown": "**Unpinned model version in API call**\n\nModel ''gpt-4'' is used without version pinning on line 109. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)\n\n**Code:**\n```python\n\n    client = OpenAI(api_key=api_key)\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\n```\n\n**Remediation:**\nSupply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 109,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    client = OpenAI(api_key=api_key)\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_109_unpinned-109"
          },
          "properties": {
            "security-severity": "3.0",
            "confidence": 0.75,
            "category": "LLM05: Supply Chain Vulnerabilities"
          },
          "fixes": [
            {
              "description": {
                "text": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "Hardcoded OpenAI API Key found in assignment on line 10. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version control history, or compiled binaries.",
            "markdown": "**Hardcoded OpenAI API Key detected in assignment**\n\nHardcoded OpenAI API Key found in assignment on line 10. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version control history, or compiled binaries.\n\n**Code:**\n```python\n\n# VULN:LLM06:CRITICAL:LINE=11 - Hardcoded API key\nAPI_KEY = \"sk-1234567890abcdef1234567890abcdef1234567890abcdef\"  # VULNERABLE\nclient = OpenAI(api_key=API_KEY)\n\n```\n\n**Remediation:**\nRemove hardcoded secrets immediately:\n1. Use environment variables: os.getenv('API_KEY')\n2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault\n3. Use configuration files (never commit to git): config.ini, .env\n4. Rotate the exposed secret immediately\n5. Scan git history for leaked secrets: git-secrets, truffleHog\n6. Add secret scanning to CI/CD pipeline"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 10,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM06:CRITICAL:LINE=11 - Hardcoded API key\nAPI_KEY = \"sk-1234567890abcdef1234567890abcdef1234567890abcdef\"  # VULNERABLE\nclient = OpenAI(api_key=API_KEY)\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_10_assignment-10"
          },
          "properties": {
            "security-severity": "9.0",
            "confidence": 0.9,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "Remove hardcoded secrets immediately:\n1. Use environment variables: os.getenv('API_KEY')\n2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault\n3. Use configuration files (never commit to git): config.ini, .env\n4. Rotate the exposed secret immediately\n5. Scan git history for leaked secrets: git-secrets, truffleHog\n6. Add secret scanning to CI/CD pipeline"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "Variable 'secret_key' containing sensitive data is included in a prompt string on line 18. This can lead to data leakage through model outputs, logs, or training data.",
            "markdown": "**Sensitive data passed into LLM prompt**\n\nVariable 'secret_key' containing sensitive data is included in a prompt string on line 18. This can lead to data leakage through model outputs, logs, or training data.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nSensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 18,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_18_secret_in_prompt-18"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.8,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "Variable 'user_data' containing sensitive data is included in a prompt string on line 36. This can lead to data leakage through model outputs, logs, or training data.",
            "markdown": "**Sensitive data passed into LLM prompt**\n\nVariable 'user_data' containing sensitive data is included in a prompt string on line 36. This can lead to data leakage through model outputs, logs, or training data.\n\n**Code:**\n```python\n        \"credit_card\": \"4111-1111-1111-1111\"  # VULNERABLE - CC exposed\n    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n```\n\n**Remediation:**\nSensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 36,
                  "startColumn": 1,
                  "snippet": {
                    "text": "        \"credit_card\": \"4111-1111-1111-1111\"  # VULNERABLE - CC exposed\n    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\","
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_36_secret_in_prompt-36"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.8,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "Variable 'user_data' containing sensitive data is included in a prompt string on line 70. This can lead to data leakage through model outputs, logs, or training data.",
            "markdown": "**Sensitive data passed into LLM prompt**\n\nVariable 'user_data' containing sensitive data is included in a prompt string on line 70. This can lead to data leakage through model outputs, logs, or training data.\n\n**Code:**\n```python\n\n    # VULNERABLE - logging PII\n    logger.debug(f\"User data: {user_data}\")\n    logger.debug(f\"User input: {user_input}\")\n\n```\n\n**Remediation:**\nSensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields\n\nSecure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 70,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n    # VULNERABLE - logging PII\n    logger.debug(f\"User data: {user_data}\")\n    logger.debug(f\"User input: {user_input}\")\n"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_70_secret_in_prompt-70"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.8,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields\n\nSecure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "System prompt contains sensitive keyword 'secret' on line 14. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
            "markdown": "**Sensitive information in system prompt**\n\nSystem prompt contains sensitive keyword 'secret' on line 14. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.\n\n**Code:**\n```python\n\n\n# VULN:LLM06:CRITICAL:LINE=17 - Secret in prompt\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n```\n\n**Remediation:**\nSystem Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 14,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n\n# VULN:LLM06:CRITICAL:LINE=17 - Secret in prompt\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\""
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_14_sensitive_system_prompt-14"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.75,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "System Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "System prompt contains sensitive keyword 'password' on line 17. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
            "markdown": "**Sensitive information in system prompt**\n\nSystem prompt contains sensitive keyword 'password' on line 17. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.\n\n**Code:**\n```python\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nSystem Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 17,
                  "startColumn": 1,
                  "snippet": {
                    "text": "def vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_17_sensitive_system_prompt-17"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.75,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "System Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "System prompt contains sensitive keyword 'confidential' on line 48. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
            "markdown": "**Sensitive information in system prompt**\n\nSystem prompt contains sensitive keyword 'confidential' on line 48. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.\n\n**Code:**\n```python\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n```\n\n**Remediation:**\nSystem Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 48,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_48_sensitive_system_prompt-48"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.75,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "System Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "System prompt contains sensitive keyword 'internal' on line 49. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
            "markdown": "**Sensitive information in system prompt**\n\nSystem prompt contains sensitive keyword 'internal' on line 49. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.\n\n**Code:**\n```python\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n```\n\n**Remediation:**\nSystem Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 49,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_49_sensitive_system_prompt-49"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.75,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "System Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
              }
            }
          ]
        },
        {
          "ruleId": "LLM06",
          "ruleIndex": 5,
          "level": "error",
          "message": {
            "text": "System prompt contains sensitive keyword 'secret' on line 50. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.",
            "markdown": "**Sensitive information in system prompt**\n\nSystem prompt contains sensitive keyword 'secret' on line 50. Confidential business logic, pricing, or internal policies in system prompts can be extracted through prompt injection attacks.\n\n**Code:**\n```python\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n```\n\n**Remediation:**\nSystem Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 50,
                  "startColumn": 1,
                  "snippet": {
                    "text": "    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create("
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_50_sensitive_system_prompt-50"
          },
          "properties": {
            "security-severity": "7.5",
            "confidence": 0.75,
            "category": "LLM06: Sensitive Information Disclosure"
          },
          "fixes": [
            {
              "description": {
                "text": "System Prompt Security:\n1. Never embed secrets or confidential data in prompts\n2. Move business logic to backend, not LLM instructions\n3. Use output filtering to prevent system prompt leakage\n4. Implement prompt injection defenses\n5. Consider prompt obfuscation for sensitive instructions"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_secret_in_prompt' on line 15 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_secret_in_prompt'**\n\nFunction 'vulnerable_secret_in_prompt' on line 15 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM06:CRITICAL:LINE=17 - Secret in prompt\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 15,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM06:CRITICAL:LINE=17 - Secret in prompt\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_15_critical_decision-15"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_pii_exposure' on line 27 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_pii_exposure'**\n\nFunction 'vulnerable_pii_exposure' on line 27 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM06:HIGH:LINE=29 - PII in training/context\ndef vulnerable_pii_exposure(user_id: int) -> str:\n    \"\"\"Vulnerable: PII data included in prompt context.\"\"\"\n    # Simulating fetching user PII\n    user_data = {\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 27,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM06:HIGH:LINE=29 - PII in training/context\ndef vulnerable_pii_exposure(user_id: int) -> str:\n    \"\"\"Vulnerable: PII data included in prompt context.\"\"\"\n    # Simulating fetching user PII\n    user_data = {"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_27_critical_decision-27"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_system_prompt' on line 45 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_system_prompt'**\n\nFunction 'vulnerable_system_prompt' on line 45 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM06:HIGH:LINE=47 - System prompt leakage risk\ndef vulnerable_system_prompt() -> str:\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 45,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM06:HIGH:LINE=47 - System prompt leakage risk\ndef vulnerable_system_prompt() -> str:\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades."
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_45_critical_decision-45"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'vulnerable_logging' on line 63 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'vulnerable_logging'**\n\nFunction 'vulnerable_logging' on line 63 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# VULN:LLM06:MEDIUM:LINE=65 - Logging sensitive data\ndef vulnerable_logging(user_input: str, user_data: dict) -> str:\n    \"\"\"Vulnerable: Logging user PII and prompts.\"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 63,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# VULN:LLM06:MEDIUM:LINE=65 - Logging sensitive data\ndef vulnerable_logging(user_input: str, user_data: dict) -> str:\n    \"\"\"Vulnerable: Logging user PII and prompts.\"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG)"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_63_critical_decision-63"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        },
        {
          "ruleId": "LLM09",
          "ruleIndex": 8,
          "level": "note",
          "message": {
            "text": "Function 'safe_secret_handling' on line 102 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
            "markdown": "**Critical decision without oversight in 'safe_secret_handling'**\n\nFunction 'safe_secret_handling' on line 102 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.\n\n**Code:**\n```python\n\n# SAFE: Proper secrets handling (for comparison)\ndef safe_secret_handling() -> str:\n    \"\"\"Safe: Secrets from environment, not in code or prompts.\"\"\"\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n```\n\n**Remediation:**\nCritical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
          },
          "locations": [
            {
              "physicalLocation": {
                "artifactLocation": {
                  "uri": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
                  "uriBaseId": "%SRCROOT%"
                },
                "region": {
                  "startLine": 102,
                  "startColumn": 1,
                  "snippet": {
                    "text": "\n# SAFE: Proper secrets handling (for comparison)\ndef safe_secret_handling() -> str:\n    \"\"\"Safe: Secrets from environment, not in code or prompts.\"\"\"\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:"
                  }
                }
              }
            }
          ],
          "fingerprints": {
            "primaryLocationLineHash": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_102_critical_decision-102"
          },
          "properties": {
            "security-severity": "1.0",
            "confidence": 0.9,
            "category": "LLM09: Overreliance"
          },
          "fixes": [
            {
              "description": {
                "text": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
              }
            }
          ]
        }
      ],
      "invocations": [
        {
          "executionSuccessful": true,
          "endTimeUtc": "2026-01-10T21:37:27.836871+00:00",
          "workingDirectory": {
            "uri": "file:///Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info"
          }
        }
      ],
      "automationDetails": {
        "id": "aisentry/static-scan/20260110213727"
      }
    }
  ]
}