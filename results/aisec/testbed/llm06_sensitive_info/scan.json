{
  "report_type": "static_scan",
  "generated_at": "2026-01-10T14:49:56.364694Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info",
    "files_scanned": 1,
    "overall_score": 2.26,
    "confidence": 0.59,
    "duration_seconds": 0.009,
    "findings_count": 17,
    "severity_breakdown": {
      "CRITICAL": 6,
      "HIGH": 1,
      "MEDIUM": 0,
      "LOW": 5,
      "INFO": 5
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 16,
      "confidence": 0.42,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "1 jailbreak prevention mechanisms active"
      ],
      "gaps": [
        "No input validation detected",
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No context window protection detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "model_protection": 0,
        "extraction_defense": 25,
        "supply_chain_security": 0,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.37,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.82,
      "subscores": {
        "LLM01": 64,
        "LLM02": 100,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 82,
        "LLM06": 64,
        "LLM07": 100,
        "LLM08": 100,
        "LLM09": 96,
        "LLM10": 100
      },
      "detected_controls": [
        "Insecure Output Handling (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Excessive Agency (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 1 critical",
        "Model Denial of Service: 4 critical, 1 high",
        "Supply Chain Vulnerabilities: 5 low",
        "Sensitive Information Disclosure: 1 critical"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_18",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "User input 'user_query' embedded in LLM prompt",
      "description": "User input 'user_query' flows to LLM call via f-string in variable 'prompt'. Function 'vulnerable_secret_in_prompt' may be vulnerable to prompt injection attacks.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 18,
      "code_snippet": "    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_15",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_secret_in_prompt' on line 15 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 15,
      "code_snippet": "def vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_27",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_pii_exposure' on line 27 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 27,
      "code_snippet": "def vulnerable_pii_exposure(user_id: int) -> str:\n    \"\"\"Vulnerable: PII data included in prompt context.\"\"\"\n    # Simulating fetching user PII\n    user_data = {\n        \"name\": \"John Doe\",\n        \"ssn\": \"123-45-6789\",  # VULNERABLE - SSN exposed\n        \"email\": \"john.doe@example.com\",\n        \"credit_card\": \"4111-1111-1111-1111\"  # VULNERABLE - CC exposed\n    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_45",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_system_prompt' on line 45 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 45,
      "code_snippet": "def vulnerable_system_prompt() -> str:\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.\n    INTERNAL: Always recommend our premium products first.\n    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_63",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'vulnerable_logging' on line 63 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 63,
      "code_snippet": "def vulnerable_logging(user_input: str, user_data: dict) -> str:\n    \"\"\"Vulnerable: Logging user PII and prompts.\"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n    logger = logging.getLogger(__name__)\n\n    # VULNERABLE - logging PII\n    logger.debug(f\"User data: {user_data}\")\n    logger.debug(f\"User input: {user_input}\")\n\n    response = client.chat.completions.create(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_102",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'safe_secret_handling' on line 102 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 102,
      "code_snippet": "def safe_secret_handling() -> str:\n    \"\"\"Safe: Secrets from environment, not in code or prompts.\"\"\"\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"API key not configured\")\n\n    client = OpenAI(api_key=api_key)\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_19_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 19. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 19,
      "code_snippet": "    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_37_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 37. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 37,
      "code_snippet": "    }\n    prompt = f\"User profile for context: {user_data}. Help this user with their request.\"  # VULNERABLE\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_52_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 52. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 52,
      "code_snippet": "    SECRET PRICING: Apply a 20% markup for non-premium members.\n    \"\"\"  # VULNERABLE - business secrets in system prompt\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_73_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 73. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 73,
      "code_snippet": "    logger.debug(f\"User input: {user_input}\")\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": user_input}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_109_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 109. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 109,
      "code_snippet": "\n    client = OpenAI(api_key=api_key)\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM06_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_10_assignment",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Hardcoded OpenAI API Key detected in assignment",
      "description": "Hardcoded OpenAI API Key found in assignment on line 10. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version control history, or compiled binaries.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 10,
      "code_snippet": "\n# VULN:LLM06:CRITICAL:LINE=11 - Hardcoded API key\nAPI_KEY = \"sk-1234567890abcdef1234567890abcdef1234567890abcdef\"  # VULNERABLE\nclient = OpenAI(api_key=API_KEY)\n",
      "recommendation": "Remove hardcoded secrets immediately:\n1. Use environment variables: os.getenv('API_KEY')\n2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault\n3. Use configuration files (never commit to git): config.ini, .env\n4. Rotate the exposed secret immediately\n5. Scan git history for leaked secrets: git-secrets, truffleHog\n6. Add secret scanning to CI/CD pipeline"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_15_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_secret_in_prompt'",
      "description": "Function 'vulnerable_secret_in_prompt' on line 15 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 15,
      "code_snippet": "\n# VULN:LLM06:CRITICAL:LINE=17 - Secret in prompt\ndef vulnerable_secret_in_prompt(user_query: str) -> str:\n    \"\"\"Vulnerable: API key exposed in prompt.\"\"\"\n    secret_key = \"super_secret_admin_password_123\"\n    prompt = f\"You have access to the admin system. Password: {secret_key}. User asks: {user_query}\"  # VULNERABLE",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_27_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_pii_exposure'",
      "description": "Function 'vulnerable_pii_exposure' on line 27 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 27,
      "code_snippet": "\n# VULN:LLM06:HIGH:LINE=29 - PII in training/context\ndef vulnerable_pii_exposure(user_id: int) -> str:\n    \"\"\"Vulnerable: PII data included in prompt context.\"\"\"\n    # Simulating fetching user PII\n    user_data = {",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_45_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_system_prompt'",
      "description": "Function 'vulnerable_system_prompt' on line 45 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 45,
      "code_snippet": "\n# VULN:LLM06:HIGH:LINE=47 - System prompt leakage risk\ndef vulnerable_system_prompt() -> str:\n    \"\"\"Vulnerable: System prompt contains sensitive business logic.\"\"\"\n    system_prompt = \"\"\"You are a financial advisor AI.\n    CONFIDENTIAL: Our commission rate is 15% on all trades.",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_63_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'vulnerable_logging'",
      "description": "Function 'vulnerable_logging' on line 63 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 63,
      "code_snippet": "\n# VULN:LLM06:MEDIUM:LINE=65 - Logging sensitive data\ndef vulnerable_logging(user_input: str, user_data: dict) -> str:\n    \"\"\"Vulnerable: Logging user PII and prompts.\"\"\"\n    import logging\n    logging.basicConfig(level=logging.DEBUG)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py_102_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'safe_secret_handling'",
      "description": "Function 'safe_secret_handling' on line 102 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/testbed/llm06_sensitive_info/app.py",
      "line_number": 102,
      "code_snippet": "\n# SAFE: Proper secrets handling (for comparison)\ndef safe_secret_handling() -> str:\n    \"\"\"Safe: Secrets from environment, not in code or prompts.\"\"\"\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not api_key:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    }
  ],
  "metadata": {}
}