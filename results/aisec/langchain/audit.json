{
  "audit_id": "93034075",
  "project_path": "repos/langchain/libs/langchain/langchain_classic",
  "timestamp": "2026-01-08T23:30:02.748312",
  "overall_score": 33.1,
  "maturity_level": "Developing",
  "files_scanned": 1321,
  "scan_duration_seconds": 0.37,
  "detected_controls": 26,
  "total_controls": 61,
  "categories": {
    "prompt_security": {
      "category_id": "prompt_security",
      "category_name": "Prompt Security",
      "score": 34.4,
      "max_score": 100.0,
      "percentage": 34.4,
      "detected_count": 4,
      "total_count": 8,
      "controls": [
        {
          "control_id": "PS-01",
          "control_name": "Prompt Sanitization",
          "category": "prompt_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/flare/prompts.py",
              "line_number": 16,
              "snippet": "return cleaned.replace(self.finished_value, \"\"), finished",
              "description": "Sanitization function call: cleaned.replace",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 29,
              "snippet": "return list(filter(None, lines))  # Remove empty lines",
              "description": "Sanitization function call: filter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/indexes/_sql_record_manager.py",
              "line_number": 421,
              "snippet": "filtered_query: Query = session.query(UpsertionRecord.key).filter(",
              "description": "Sanitization function call: filter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/indexes/_sql_record_manager.py",
              "line_number": 462,
              "snippet": "query: Query = session.query(UpsertionRecord).filter(",
              "description": "Sanitization function call: filter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/output_parsers/regex_dict.py",
              "line_number": 27,
              "snippet": "specific_regex = self.regex_pattern.format(re.escape(expected_format))",
              "description": "Sanitization function call: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/output_parsers/xml.py",
              "line_number": 96,
              "snippet": "_tool = _unescape(_tool)",
              "description": "Sanitization function call: _unescape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/output_parsers/xml.py",
              "line_number": 97,
              "snippet": "_tool_input = _unescape(_tool_input)",
              "description": "Sanitization function call: _unescape",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-02",
          "control_name": "Rate Limiting",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement rate limiting using fastapi-limiter, flask-limiter, or slowapi",
            "Configure per-user and per-endpoint rate limits",
            "Add exponential backoff for repeated violations"
          ]
        },
        {
          "control_id": "PS-03",
          "control_name": "Input Validation",
          "category": "prompt_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8500000000000001,
          "score": 75,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/base_memory.py",
              "line_number": 16,
              "snippet": "pydantic",
              "description": "Pydantic validation library imported",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/model_laboratory.py",
              "line_number": 15,
              "snippet": "class ModelLaboratory:",
              "description": "Validation model class: ModelLaboratory",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
              "line_number": 645,
              "snippet": "class _ConfigurableModel(Runnable[LanguageModelInput, Any]):",
              "description": "Validation model class: _ConfigurableModel",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
              "line_number": 297,
              "snippet": "class ChatModelInput(TypedDict):",
              "description": "Validation model class: ChatModelInput",
              "confidence": 0.8
            },
            {
              "type": "decorator",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/ensemble.py",
              "line_number": 80,
              "snippet": "@model_validator",
              "description": "Pydantic validator decorator",
              "confidence": 0.9
            },
            {
              "type": "decorator",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_vector.py",
              "line_number": 50,
              "snippet": "@model_validator",
              "description": "Pydantic validator decorator",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-04",
          "control_name": "Output Filtering",
          "category": "prompt_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8000000000000002,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/base.py",
              "line_number": 489,
              "snippet": "self._validate_outputs(outputs)",
              "description": "Output filtering function: self._validate_outputs",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/base.py",
              "line_number": 514,
              "snippet": "self._validate_outputs(outputs)",
              "description": "Output filtering function: self._validate_outputs",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/router/llm_router.py",
              "line_number": 125,
              "snippet": "super()._validate_outputs(outputs)",
              "description": "Output filtering function: _validate_outputs",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/moderation.py",
              "line_number": 128,
              "snippet": "output = self._moderate(text, results.results[0])",
              "description": "Output filtering function: self._moderate",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/moderation.py",
              "line_number": 113,
              "snippet": "output = self._moderate(text, results[\"results\"][0])",
              "description": "Output filtering function: self._moderate",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/moderation.py",
              "line_number": 116,
              "snippet": "output = self._moderate(text, results.results[0])",
              "description": "Output filtering function: self._moderate",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-05",
          "control_name": "Context Window Protection",
          "category": "prompt_security",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8333333333333334,
          "score": 50,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/evaluation/embedding_distance/base.py",
              "line_number": 141,
              "snippet": "tiktoken",
              "description": "tiktoken imported for token counting",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py",
              "line_number": 115,
              "snippet": "curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)",
              "description": "Token/length handling: self.llm.get_num_tokens_from_messages",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py",
              "line_number": 129,
              "snippet": "curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)",
              "description": "Token/length handling: self.llm.get_num_tokens_from_messages",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "PS-06",
          "control_name": "Red Team Testing",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'"
          ]
        },
        {
          "control_id": "PS-07",
          "control_name": "Prompt Anomaly Detection",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement statistical analysis on prompt patterns",
            "Use ML-based anomaly detection for unusual inputs",
            "Set up alerts for prompt anomaly detection"
          ]
        },
        {
          "control_id": "PS-08",
          "control_name": "System Prompt Protection",
          "category": "prompt_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'ASTAnalyzer' object has no attribute 'find_string_literals'"
          ]
        }
      ]
    },
    "model_security": {
      "category_id": "model_security",
      "category_name": "Model Security",
      "score": 31.2,
      "max_score": 100.0,
      "percentage": 31.2,
      "detected_count": 4,
      "total_count": 8,
      "controls": [
        {
          "control_id": "MS-01",
          "control_name": "Access Control",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement authentication on all model endpoints",
            "Use OAuth 2.0 or API keys for access control",
            "Implement role-based access control (RBAC)"
          ]
        },
        {
          "control_id": "MS-02",
          "control_name": "Model Versioning",
          "category": "model_security",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8666666666666667,
          "score": 50,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/embeddings/mlflow.py",
              "line_number": 6,
              "snippet": "mlflow",
              "description": "MLflow imported for model tracking",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/embeddings/__init__.py",
              "line_number": 17,
              "snippet": "mlflow",
              "description": "MLflow imported for model tracking",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
              "line_number": 449,
              "snippet": "return ChatHuggingFace.from_model_id(model_id=model, **kwargs)",
              "description": "Model versioning function: ChatHuggingFace.from_model_id",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "MS-03",
          "control_name": "Dependency Scanning",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use safety or pip-audit for dependency scanning",
            "Integrate vulnerability scanning in CI/CD",
            "Set up automated dependency updates with Dependabot"
          ]
        },
        {
          "control_id": "MS-04",
          "control_name": "API Security",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Enforce HTTPS for all API endpoints",
            "Implement proper CORS configuration",
            "Add security headers (CSP, HSTS, etc.)"
          ]
        },
        {
          "control_id": "MS-05",
          "control_name": "Model Source Verification",
          "category": "model_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.9,
          "score": 75,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/embeddings/cache.py",
              "line_number": 12,
              "snippet": "hashlib",
              "description": "Hash verification library: hashlib",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/cache.py",
              "line_number": 6,
              "snippet": "langchain_community.cache.FullMd5LLMCache",
              "description": "Hash verification library: langchain_community.cache.FullMd5LLMCache",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/cache.py",
              "line_number": 6,
              "snippet": "langchain_community.cache.FullMd5LLMCache",
              "description": "Hash verification library: langchain_community.cache.FullMd5LLMCache",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "MS-06",
          "control_name": "Differential Privacy",
          "category": "model_security",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py",
              "line_number": 90,
              "snippet": "self.client = self.page.context.new_cdp_session(self.page)",
              "description": "Differential privacy: self.page.context.new_cdp_session",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "MS-07",
          "control_name": "Model Watermarking",
          "category": "model_security",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement watermarking for model outputs",
            "Use cryptographic watermarks for model weights",
            "Track watermark verification for model theft detection"
          ]
        },
        {
          "control_id": "MS-08",
          "control_name": "Secure Model Loading",
          "category": "model_security",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/loading.py",
              "line_number": 143,
              "snippet": "config = yaml.safe_load(f)",
              "description": "Safe loading: yaml.safe_load",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/output_parsers/yaml.py",
              "line_number": 38,
              "snippet": "json_object = yaml.safe_load(yaml_str)",
              "description": "Safe loading: yaml.safe_load",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    },
    "data_privacy": {
      "category_id": "data_privacy",
      "category_name": "Data Privacy",
      "score": 12.5,
      "max_score": 100.0,
      "percentage": 12.5,
      "detected_count": 2,
      "total_count": 8,
      "controls": [
        {
          "control_id": "DP-01",
          "control_name": "PII Detection",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Presidio or similar for PII detection",
            "Implement NER-based PII detection with spaCy",
            "Add custom regex patterns for domain-specific PII"
          ]
        },
        {
          "control_id": "DP-02",
          "control_name": "Data Redaction",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement data masking for sensitive fields",
            "Use tokenization for reversible anonymization",
            "Apply redaction before logging or storage"
          ]
        },
        {
          "control_id": "DP-03",
          "control_name": "Data Encryption",
          "category": "data_privacy",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.9,
          "score": 50,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/__init__.py",
              "line_number": 67,
              "snippet": "langchain_classic.chains.ConversationChain",
              "description": "Encryption module imported: langchain_classic.chains.ConversationChain",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/router/multi_prompt.py",
              "line_number": 12,
              "snippet": "langchain_classic.chains.ConversationChain",
              "description": "Encryption module imported: langchain_classic.chains.ConversationChain",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "DP-04",
          "control_name": "Audit Logging",
          "category": "data_privacy",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.9,
          "score": 50,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/re_phraser.py",
              "line_number": 1,
              "snippet": "logging",
              "description": "Logging module imported: logging",
              "confidence": 0.9
            },
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 2,
              "snippet": "logging",
              "description": "Logging module imported: logging",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "DP-05",
          "control_name": "Consent Management",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement consent tracking for data collection",
            "Provide opt-out mechanisms for users",
            "Store consent records with timestamps"
          ]
        },
        {
          "control_id": "DP-06",
          "control_name": "NER PII Detection",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Presidio or SpaCy for NER-based PII detection",
            "Implement custom NER models for domain-specific PII",
            "Run PII detection on all inputs and outputs"
          ]
        },
        {
          "control_id": "DP-07",
          "control_name": "Data Retention Policy",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Define data retention policies for AI training data",
            "Implement automated data deletion after retention period",
            "Maintain data inventory with retention metadata"
          ]
        },
        {
          "control_id": "DP-08",
          "control_name": "GDPR Compliance",
          "category": "data_privacy",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'ASTAnalyzer' object has no attribute 'find_string_literals'"
          ]
        }
      ]
    },
    "owasp_llm": {
      "category_id": "owasp_llm",
      "category_name": "OWASP LLM Top 10",
      "score": 27.5,
      "max_score": 100.0,
      "percentage": 27.5,
      "detected_count": 5,
      "total_count": 10,
      "controls": [
        {
          "control_id": "OWASP-01",
          "control_name": "LLM01: Prompt Injection Defense",
          "category": "owasp_llm",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/output_parsers/regex_dict.py",
              "line_number": 27,
              "snippet": "specific_regex = self.regex_pattern.format(re.escape(expected_format))",
              "description": "Input sanitization: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/output_parsers/xml.py",
              "line_number": 96,
              "snippet": "_tool = _unescape(_tool)",
              "description": "Input sanitization: _unescape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 29,
              "snippet": "return list(filter(None, lines))  # Remove empty lines",
              "description": "Input sanitization: filter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/indexes/_sql_record_manager.py",
              "line_number": 421,
              "snippet": "filtered_query: Query = session.query(UpsertionRecord.key).filter(",
              "description": "Input sanitization: filter",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/flare/prompts.py",
              "line_number": 16,
              "snippet": "return cleaned.replace(self.finished_value, \"\"), finished",
              "description": "Input sanitization: cleaned.replace",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/model_laboratory.py",
              "line_number": 78,
              "snippet": "prompt = PromptTemplate(input_variables=[\"_input\"], template=\"{_input}\")",
              "description": "Prompt template usage: PromptTemplate",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 33,
              "snippet": "DEFAULT_QUERY_PROMPT = PromptTemplate(",
              "description": "Prompt template usage: PromptTemplate",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py",
              "line_number": 70,
              "snippet": "prompt = ChatPromptTemplate.from_messages(",
              "description": "Prompt template usage: ChatPromptTemplate.from_messages",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/question_answering/map_reduce_prompt.py",
              "line_number": 29,
              "snippet": "CHAT_QUESTION_PROMPT = ChatPromptTemplate.from_messages(messages)",
              "description": "Prompt template usage: ChatPromptTemplate.from_messages",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/model_laboratory.py",
              "line_number": 78,
              "snippet": "prompt = PromptTemplate(input_variables=[\"_input\"], template=\"{_input}\")",
              "description": "Prompt template usage: PromptTemplate",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 33,
              "snippet": "DEFAULT_QUERY_PROMPT = PromptTemplate(",
              "description": "Prompt template usage: PromptTemplate",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-02",
          "control_name": "LLM02: Insecure Output Handling",
          "category": "owasp_llm",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/output_parsers/regex_dict.py",
              "line_number": 27,
              "snippet": "specific_regex = self.regex_pattern.format(re.escape(expected_format))",
              "description": "Output escaping: re.escape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/output_parsers/xml.py",
              "line_number": 96,
              "snippet": "_tool = _unescape(_tool)",
              "description": "Output escaping: _unescape",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-03",
          "control_name": "LLM03: Training Data Poisoning",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement data validation pipelines",
            "Verify data source integrity",
            "Monitor for anomalies in training data"
          ]
        },
        {
          "control_id": "OWASP-04",
          "control_name": "LLM04: Model Denial of Service",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement rate limiting on API endpoints",
            "Set maximum token limits for inputs and outputs",
            "Add timeout mechanisms for LLM calls"
          ]
        },
        {
          "control_id": "OWASP-05",
          "control_name": "LLM05: Supply Chain Vulnerabilities",
          "category": "owasp_llm",
          "detected": true,
          "level": "basic",
          "confidence": 0.9,
          "score": 25,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/embeddings/cache.py",
              "line_number": 12,
              "snippet": "hashlib",
              "description": "Integrity verification: hashlib",
              "confidence": 0.9
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-06",
          "control_name": "LLM06: Sensitive Information Disclosure",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement PII detection and filtering",
            "Never include secrets in prompts",
            "Add output filtering for sensitive patterns"
          ]
        },
        {
          "control_id": "OWASP-07",
          "control_name": "LLM07: Insecure Plugin Design",
          "category": "owasp_llm",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/agent.py",
              "line_number": 905,
              "snippet": "cls._validate_tools(tools)",
              "description": "Plugin validation: cls._validate_tools",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/chat/base.py",
              "line_number": 153,
              "snippet": "cls._validate_tools(tools)",
              "description": "Plugin validation: cls._validate_tools",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-08",
          "control_name": "LLM08: Excessive Agency",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement human-in-the-loop for critical actions",
            "Use principle of least privilege for LLM access",
            "Add approval workflows for sensitive operations"
          ]
        },
        {
          "control_id": "OWASP-09",
          "control_name": "LLM09: Overreliance",
          "category": "owasp_llm",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
              "line_number": 222,
              "snippet": "low_confidence_spans = _low_confidence_spans(",
              "description": "Confidence scoring: _low_confidence_spans",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/time_weighted_retriever.py",
              "line_number": 86,
              "snippet": "docs_and_scores = self.vectorstore.similarity_search_with_relevance_scores(",
              "description": "Confidence scoring: self.vectorstore.similarity_search_with_relevance_scores",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_vector.py",
              "line_number": 85,
              "snippet": "self.vectorstore.similarity_search_with_relevance_scores(",
              "description": "Confidence scoring: self.vectorstore.similarity_search_with_relevance_scores",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/base.py",
              "line_number": 489,
              "snippet": "self._validate_outputs(outputs)",
              "description": "Output verification: self._validate_outputs",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/base.py",
              "line_number": 514,
              "snippet": "self._validate_outputs(outputs)",
              "description": "Output verification: self._validate_outputs",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "OWASP-10",
          "control_name": "LLM10: Model Theft",
          "category": "owasp_llm",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement rate limiting on API endpoints",
            "Add query logging and anomaly detection",
            "Monitor for extraction patterns"
          ]
        }
      ]
    },
    "blue_team": {
      "category_id": "blue_team",
      "category_name": "Blue Team Operations",
      "score": 7.1,
      "max_score": 100.0,
      "percentage": 7.1,
      "detected_count": 1,
      "total_count": 7,
      "controls": [
        {
          "control_id": "BT-01",
          "control_name": "Model Monitoring",
          "category": "blue_team",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/evaluation/string_distance/base.py",
              "line_number": 147,
              "snippet": "return _RapidFuzzChainMixin._get_metric(",
              "description": "Metrics tracking: _RapidFuzzChainMixin._get_metric",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/evaluation/string_distance/base.py",
              "line_number": 162,
              "snippet": "return self.metric(a, b)",
              "description": "Metrics tracking: self.metric",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "BT-02",
          "control_name": "Drift Detection",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement drift detection with evidently or alibi-detect",
            "Monitor input data distribution changes",
            "Set up automated alerts for drift events"
          ]
        },
        {
          "control_id": "BT-03",
          "control_name": "Anomaly Detection",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement anomaly detection on model inputs",
            "Monitor for unusual query patterns",
            "Use statistical methods or ML-based detection"
          ]
        },
        {
          "control_id": "BT-04",
          "control_name": "Adversarial Attack Detection",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement adversarial input detection",
            "Use adversarial robustness toolkits",
            "Add input perturbation analysis"
          ]
        },
        {
          "control_id": "BT-05",
          "control_name": "AI Incident Response",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Create AI-specific incident response playbooks",
            "Implement model rollback capabilities",
            "Set up automated incident alerting"
          ]
        },
        {
          "control_id": "BT-06",
          "control_name": "Model Drift Monitoring",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Evidently or alibi-detect for drift monitoring",
            "Set up automated alerts for significant drift",
            "Implement automatic retraining pipelines"
          ]
        },
        {
          "control_id": "BT-07",
          "control_name": "Data Quality Monitoring",
          "category": "blue_team",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Great Expectations for data validation",
            "Implement data quality checks in pipeline",
            "Set up alerts for data quality issues"
          ]
        }
      ]
    },
    "governance": {
      "category_id": "governance",
      "category_name": "AI Governance",
      "score": 10.0,
      "max_score": 100.0,
      "percentage": 10.0,
      "detected_count": 1,
      "total_count": 5,
      "controls": [
        {
          "control_id": "GV-01",
          "control_name": "Model Explainability",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use SHAP or LIME for model explanations",
            "Provide decision explanations in outputs",
            "Implement feature attribution tracking"
          ]
        },
        {
          "control_id": "GV-02",
          "control_name": "Bias Detection",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Fairlearn or AIF360 for bias detection",
            "Implement fairness metrics tracking",
            "Test for demographic parity and equalized odds"
          ]
        },
        {
          "control_id": "GV-03",
          "control_name": "Model Documentation",
          "category": "governance",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/model_laboratory.py",
              "line_number": 15,
              "snippet": "class ModelLaboratory:",
              "description": "Model class with documentation: ModelLaboratory",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
              "line_number": 645,
              "snippet": "class _ConfigurableModel(Runnable[LanguageModelInput, Any]):",
              "description": "Model class with documentation: _ConfigurableModel",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "GV-04",
          "control_name": "Compliance Tracking",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement compliance checklists for AI systems",
            "Track EU AI Act and other regulatory requirements",
            "Maintain audit logs for compliance evidence"
          ]
        },
        {
          "control_id": "GV-05",
          "control_name": "Human Oversight",
          "category": "governance",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement human-in-the-loop for critical decisions",
            "Add manual review workflows for high-risk outputs",
            "Create escalation procedures for edge cases"
          ]
        }
      ]
    },
    "supply_chain": {
      "category_id": "supply_chain",
      "category_name": "Supply Chain Security",
      "score": 25.0,
      "max_score": 100.0,
      "percentage": 25.0,
      "detected_count": 1,
      "total_count": 3,
      "controls": [
        {
          "control_id": "SC-01",
          "control_name": "Dependency Scanning",
          "category": "supply_chain",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Add safety or pip-audit to your dependencies",
            "Configure CI/CD to run security scans on every commit",
            "Set up Dependabot or Renovate for automatic updates"
          ]
        },
        {
          "control_id": "SC-02",
          "control_name": "Model Provenance Tracking",
          "category": "supply_chain",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use MLflow, DVC, or Weights & Biases for model tracking",
            "Implement model versioning with metadata",
            "Maintain model registry with provenance information"
          ]
        },
        {
          "control_id": "SC-03",
          "control_name": "Model Integrity Verification",
          "category": "supply_chain",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8333333333333334,
          "score": 75,
          "evidence": [
            {
              "type": "import",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/embeddings/cache.py",
              "line_number": 12,
              "snippet": "hashlib",
              "description": "Hash library: hashlib",
              "confidence": 0.9
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/agent.py",
              "line_number": 905,
              "snippet": "cls._validate_tools(tools)",
              "description": "Verification: cls._validate_tools",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/chat/base.py",
              "line_number": 153,
              "snippet": "cls._validate_tools(tools)",
              "description": "Verification: cls._validate_tools",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    },
    "hallucination": {
      "category_id": "hallucination",
      "category_name": "Hallucination Mitigation",
      "score": 45.0,
      "max_score": 100.0,
      "percentage": 45.0,
      "detected_count": 4,
      "total_count": 5,
      "controls": [
        {
          "control_id": "HM-01",
          "control_name": "RAG Implementation",
          "category": "hallucination",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8000000000000002,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/contextual_compression.py",
              "line_number": 34,
              "snippet": "docs = self.base_retriever.invoke(",
              "description": "Retrieval pattern: self.base_retriever.invoke",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/contextual_compression.py",
              "line_number": 56,
              "snippet": "docs = await self.base_retriever.ainvoke(",
              "description": "Retrieval pattern: self.base_retriever.ainvoke",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/time_weighted_retriever.py",
              "line_number": 86,
              "snippet": "docs_and_scores = self.vectorstore.similarity_search_with_relevance_scores(",
              "description": "Retrieval pattern: self.vectorstore.similarity_search_with_relevance_scores",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_vector.py",
              "line_number": 85,
              "snippet": "self.vectorstore.similarity_search_with_relevance_scores(",
              "description": "Retrieval pattern: self.vectorstore.similarity_search_with_relevance_scores",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
              "line_number": 485,
              "snippet": "return RetrievalQA(",
              "description": "Retrieval pattern: RetrievalQA",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
              "line_number": 518,
              "snippet": "return RetrievalQAWithSourcesChain(",
              "description": "Retrieval pattern: RetrievalQAWithSourcesChain",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "HM-02",
          "control_name": "Confidence Scoring",
          "category": "hallucination",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
              "line_number": 37,
              "snippet": "log_probs.append(token[\"logprob\"])",
              "description": "Logprobs usage: log_probs.append",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
              "line_number": 216,
              "snippet": "tokens, log_probs = _extract_tokens_and_log_probs(",
              "description": "Logprobs usage: _extract_tokens_and_log_probs",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "HM-03",
          "control_name": "Source Attribution",
          "category": "hallucination",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
              "line_number": 413,
              "snippet": "return QAWithSourcesChain(combine_documents_chain=combine_documents_chain, **config)",
              "description": "Citation pattern: QAWithSourcesChain",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
              "line_number": 455,
              "snippet": "return VectorDBQAWithSourcesChain(",
              "description": "Citation pattern: VectorDBQAWithSourcesChain",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "HM-04",
          "control_name": "Temperature Control",
          "category": "hallucination",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use lower temperature (0-0.3) for factual tasks",
            "Make temperature configurable per use case",
            "Document temperature settings for different tasks"
          ]
        },
        {
          "control_id": "HM-05",
          "control_name": "Fact Checking",
          "category": "hallucination",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/agent.py",
              "line_number": 905,
              "snippet": "cls._validate_tools(tools)",
              "description": "Fact checking: cls._validate_tools",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/agents/chat/base.py",
              "line_number": 153,
              "snippet": "cls._validate_tools(tools)",
              "description": "Fact checking: cls._validate_tools",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    },
    "ethical_ai": {
      "category_id": "ethical_ai",
      "category_name": "Ethical AI & Bias",
      "score": 12.5,
      "max_score": 100.0,
      "percentage": 12.5,
      "detected_count": 1,
      "total_count": 4,
      "controls": [
        {
          "control_id": "EA-01",
          "control_name": "Fairness Metrics",
          "category": "ethical_ai",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Use Fairlearn or AIF360 for fairness metrics",
            "Implement demographic parity testing",
            "Monitor fairness metrics in production"
          ]
        },
        {
          "control_id": "EA-02",
          "control_name": "Model Explainability",
          "category": "ethical_ai",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/evaluation/embedding_distance/base.py",
              "line_number": 340,
              "snippet": "score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()",
              "description": "Explainability: reshape",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/evaluation/embedding_distance/base.py",
              "line_number": 340,
              "snippet": "score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()",
              "description": "Explainability: reshape",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "EA-03",
          "control_name": "Bias Testing",
          "category": "ethical_ai",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Implement adversarial testing for bias",
            "Test across demographic groups",
            "Use TextAttack or CheckList for NLP bias testing"
          ]
        },
        {
          "control_id": "EA-04",
          "control_name": "Model Cards",
          "category": "ethical_ai",
          "detected": false,
          "level": "none",
          "confidence": 0.0,
          "score": 0,
          "evidence": [],
          "recommendations": [
            "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'"
          ]
        }
      ]
    },
    "incident_response": {
      "category_id": "incident_response",
      "category_name": "Incident Response",
      "score": 58.3,
      "max_score": 100.0,
      "percentage": 58.3,
      "detected_count": 3,
      "total_count": 3,
      "controls": [
        {
          "control_id": "IR-01",
          "control_name": "Monitoring Integration",
          "category": "incident_response",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
              "line_number": 1186,
              "snippet": "all_eval_results, all_runs = self._collect_metrics()",
              "description": "Monitoring pattern: self._collect_metrics",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "IR-02",
          "control_name": "Audit Logging",
          "category": "incident_response",
          "detected": true,
          "level": "advanced",
          "confidence": 0.8,
          "score": 75,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/re_phraser.py",
              "line_number": 15,
              "snippet": "logger = logging.getLogger(__name__)",
              "description": "Logging pattern: logging.getLogger",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 20,
              "snippet": "logger = logging.getLogger(__name__)",
              "description": "Logging pattern: logging.getLogger",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/re_phraser.py",
              "line_number": 15,
              "snippet": "logger = logging.getLogger(__name__)",
              "description": "Logging pattern: logging.getLogger",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
              "line_number": 20,
              "snippet": "logger = logging.getLogger(__name__)",
              "description": "Logging pattern: logging.getLogger",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        },
        {
          "control_id": "IR-03",
          "control_name": "Rollback Capability",
          "category": "incident_response",
          "detected": true,
          "level": "intermediate",
          "confidence": 0.8,
          "score": 50,
          "evidence": [
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/env.py",
              "line_number": 16,
              "snippet": "\"runtime_version\": platform.python_version(),",
              "description": "Rollback pattern: platform.python_version",
              "confidence": 0.8
            },
            {
              "type": "ast",
              "file_path": "repos/langchain/libs/langchain/langchain_classic/__init__.py",
              "line_number": 10,
              "snippet": "__version__ = metadata.version(__package__)",
              "description": "Rollback pattern: metadata.version",
              "confidence": 0.8
            }
          ],
          "recommendations": []
        }
      ]
    }
  },
  "recommendations": [
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-02",
      "title": "Rate Limiting",
      "description": "Control not detected or below threshold",
      "remediation": "Implement rate limiting using fastapi-limiter, flask-limiter, or slowapi",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-02",
      "title": "Rate Limiting",
      "description": "Control not detected or below threshold",
      "remediation": "Configure per-user and per-endpoint rate limits",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-02",
      "title": "Rate Limiting",
      "description": "Control not detected or below threshold",
      "remediation": "Add exponential backoff for repeated violations",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-06",
      "title": "Red Team Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-07",
      "title": "Prompt Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement statistical analysis on prompt patterns",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-07",
      "title": "Prompt Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use ML-based anomaly detection for unusual inputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-07",
      "title": "Prompt Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Set up alerts for prompt anomaly detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "prompt_security",
      "control_id": "PS-08",
      "title": "System Prompt Protection",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'ASTAnalyzer' object has no attribute 'find_string_literals'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-01",
      "title": "Access Control",
      "description": "Control not detected or below threshold",
      "remediation": "Implement authentication on all model endpoints",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-01",
      "title": "Access Control",
      "description": "Control not detected or below threshold",
      "remediation": "Use OAuth 2.0 or API keys for access control",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-01",
      "title": "Access Control",
      "description": "Control not detected or below threshold",
      "remediation": "Implement role-based access control (RBAC)",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-03",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Use safety or pip-audit for dependency scanning",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-03",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Integrate vulnerability scanning in CI/CD",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-03",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Set up automated dependency updates with Dependabot",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-04",
      "title": "API Security",
      "description": "Control not detected or below threshold",
      "remediation": "Enforce HTTPS for all API endpoints",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-04",
      "title": "API Security",
      "description": "Control not detected or below threshold",
      "remediation": "Implement proper CORS configuration",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-04",
      "title": "API Security",
      "description": "Control not detected or below threshold",
      "remediation": "Add security headers (CSP, HSTS, etc.)",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-07",
      "title": "Model Watermarking",
      "description": "Control not detected or below threshold",
      "remediation": "Implement watermarking for model outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-07",
      "title": "Model Watermarking",
      "description": "Control not detected or below threshold",
      "remediation": "Use cryptographic watermarks for model weights",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "model_security",
      "control_id": "MS-07",
      "title": "Model Watermarking",
      "description": "Control not detected or below threshold",
      "remediation": "Track watermark verification for model theft detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-01",
      "title": "PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use Presidio or similar for PII detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-01",
      "title": "PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement NER-based PII detection with spaCy",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-01",
      "title": "PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Add custom regex patterns for domain-specific PII",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-02",
      "title": "Data Redaction",
      "description": "Control not detected or below threshold",
      "remediation": "Implement data masking for sensitive fields",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-02",
      "title": "Data Redaction",
      "description": "Control not detected or below threshold",
      "remediation": "Use tokenization for reversible anonymization",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-02",
      "title": "Data Redaction",
      "description": "Control not detected or below threshold",
      "remediation": "Apply redaction before logging or storage",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-05",
      "title": "Consent Management",
      "description": "Control not detected or below threshold",
      "remediation": "Implement consent tracking for data collection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-05",
      "title": "Consent Management",
      "description": "Control not detected or below threshold",
      "remediation": "Provide opt-out mechanisms for users",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-05",
      "title": "Consent Management",
      "description": "Control not detected or below threshold",
      "remediation": "Store consent records with timestamps",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-06",
      "title": "NER PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use Presidio or SpaCy for NER-based PII detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-06",
      "title": "NER PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement custom NER models for domain-specific PII",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-06",
      "title": "NER PII Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Run PII detection on all inputs and outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-07",
      "title": "Data Retention Policy",
      "description": "Control not detected or below threshold",
      "remediation": "Define data retention policies for AI training data",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-07",
      "title": "Data Retention Policy",
      "description": "Control not detected or below threshold",
      "remediation": "Implement automated data deletion after retention period",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-07",
      "title": "Data Retention Policy",
      "description": "Control not detected or below threshold",
      "remediation": "Maintain data inventory with retention metadata",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "data_privacy",
      "control_id": "DP-08",
      "title": "GDPR Compliance",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'ASTAnalyzer' object has no attribute 'find_string_literals'",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-03",
      "title": "LLM03: Training Data Poisoning",
      "description": "Control not detected or below threshold",
      "remediation": "Implement data validation pipelines",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-03",
      "title": "LLM03: Training Data Poisoning",
      "description": "Control not detected or below threshold",
      "remediation": "Verify data source integrity",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-03",
      "title": "LLM03: Training Data Poisoning",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor for anomalies in training data",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-04",
      "title": "LLM04: Model Denial of Service",
      "description": "Control not detected or below threshold",
      "remediation": "Implement rate limiting on API endpoints",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-04",
      "title": "LLM04: Model Denial of Service",
      "description": "Control not detected or below threshold",
      "remediation": "Set maximum token limits for inputs and outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-04",
      "title": "LLM04: Model Denial of Service",
      "description": "Control not detected or below threshold",
      "remediation": "Add timeout mechanisms for LLM calls",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-06",
      "title": "LLM06: Sensitive Information Disclosure",
      "description": "Control not detected or below threshold",
      "remediation": "Implement PII detection and filtering",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-06",
      "title": "LLM06: Sensitive Information Disclosure",
      "description": "Control not detected or below threshold",
      "remediation": "Never include secrets in prompts",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-06",
      "title": "LLM06: Sensitive Information Disclosure",
      "description": "Control not detected or below threshold",
      "remediation": "Add output filtering for sensitive patterns",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-08",
      "title": "LLM08: Excessive Agency",
      "description": "Control not detected or below threshold",
      "remediation": "Implement human-in-the-loop for critical actions",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-08",
      "title": "LLM08: Excessive Agency",
      "description": "Control not detected or below threshold",
      "remediation": "Use principle of least privilege for LLM access",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-08",
      "title": "LLM08: Excessive Agency",
      "description": "Control not detected or below threshold",
      "remediation": "Add approval workflows for sensitive operations",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-10",
      "title": "LLM10: Model Theft",
      "description": "Control not detected or below threshold",
      "remediation": "Implement rate limiting on API endpoints",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-10",
      "title": "LLM10: Model Theft",
      "description": "Control not detected or below threshold",
      "remediation": "Add query logging and anomaly detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "owasp_llm",
      "control_id": "OWASP-10",
      "title": "LLM10: Model Theft",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor for extraction patterns",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-02",
      "title": "Drift Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement drift detection with evidently or alibi-detect",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-02",
      "title": "Drift Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor input data distribution changes",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-02",
      "title": "Drift Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Set up automated alerts for drift events",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-03",
      "title": "Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement anomaly detection on model inputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-03",
      "title": "Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor for unusual query patterns",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-03",
      "title": "Anomaly Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use statistical methods or ML-based detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-04",
      "title": "Adversarial Attack Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement adversarial input detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-04",
      "title": "Adversarial Attack Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use adversarial robustness toolkits",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-04",
      "title": "Adversarial Attack Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Add input perturbation analysis",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-05",
      "title": "AI Incident Response",
      "description": "Control not detected or below threshold",
      "remediation": "Create AI-specific incident response playbooks",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-05",
      "title": "AI Incident Response",
      "description": "Control not detected or below threshold",
      "remediation": "Implement model rollback capabilities",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-05",
      "title": "AI Incident Response",
      "description": "Control not detected or below threshold",
      "remediation": "Set up automated incident alerting",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-06",
      "title": "Model Drift Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Use Evidently or alibi-detect for drift monitoring",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-06",
      "title": "Model Drift Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Set up automated alerts for significant drift",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-06",
      "title": "Model Drift Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Implement automatic retraining pipelines",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-07",
      "title": "Data Quality Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Use Great Expectations for data validation",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-07",
      "title": "Data Quality Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Implement data quality checks in pipeline",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "blue_team",
      "control_id": "BT-07",
      "title": "Data Quality Monitoring",
      "description": "Control not detected or below threshold",
      "remediation": "Set up alerts for data quality issues",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-01",
      "title": "Model Explainability",
      "description": "Control not detected or below threshold",
      "remediation": "Use SHAP or LIME for model explanations",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-01",
      "title": "Model Explainability",
      "description": "Control not detected or below threshold",
      "remediation": "Provide decision explanations in outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-01",
      "title": "Model Explainability",
      "description": "Control not detected or below threshold",
      "remediation": "Implement feature attribution tracking",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-02",
      "title": "Bias Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Use Fairlearn or AIF360 for bias detection",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-02",
      "title": "Bias Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Implement fairness metrics tracking",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-02",
      "title": "Bias Detection",
      "description": "Control not detected or below threshold",
      "remediation": "Test for demographic parity and equalized odds",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-04",
      "title": "Compliance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Implement compliance checklists for AI systems",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-04",
      "title": "Compliance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Track EU AI Act and other regulatory requirements",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-04",
      "title": "Compliance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Maintain audit logs for compliance evidence",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-05",
      "title": "Human Oversight",
      "description": "Control not detected or below threshold",
      "remediation": "Implement human-in-the-loop for critical decisions",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-05",
      "title": "Human Oversight",
      "description": "Control not detected or below threshold",
      "remediation": "Add manual review workflows for high-risk outputs",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "governance",
      "control_id": "GV-05",
      "title": "Human Oversight",
      "description": "Control not detected or below threshold",
      "remediation": "Create escalation procedures for edge cases",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-01",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Add safety or pip-audit to your dependencies",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-01",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Configure CI/CD to run security scans on every commit",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-01",
      "title": "Dependency Scanning",
      "description": "Control not detected or below threshold",
      "remediation": "Set up Dependabot or Renovate for automatic updates",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-02",
      "title": "Model Provenance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Use MLflow, DVC, or Weights & Biases for model tracking",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-02",
      "title": "Model Provenance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Implement model versioning with metadata",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "supply_chain",
      "control_id": "SC-02",
      "title": "Model Provenance Tracking",
      "description": "Control not detected or below threshold",
      "remediation": "Maintain model registry with provenance information",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "hallucination",
      "control_id": "HM-04",
      "title": "Temperature Control",
      "description": "Control not detected or below threshold",
      "remediation": "Use lower temperature (0-0.3) for factual tasks",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "hallucination",
      "control_id": "HM-04",
      "title": "Temperature Control",
      "description": "Control not detected or below threshold",
      "remediation": "Make temperature configurable per use case",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "hallucination",
      "control_id": "HM-04",
      "title": "Temperature Control",
      "description": "Control not detected or below threshold",
      "remediation": "Document temperature settings for different tasks",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-01",
      "title": "Fairness Metrics",
      "description": "Control not detected or below threshold",
      "remediation": "Use Fairlearn or AIF360 for fairness metrics",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-01",
      "title": "Fairness Metrics",
      "description": "Control not detected or below threshold",
      "remediation": "Implement demographic parity testing",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-01",
      "title": "Fairness Metrics",
      "description": "Control not detected or below threshold",
      "remediation": "Monitor fairness metrics in production",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-03",
      "title": "Bias Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Implement adversarial testing for bias",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-03",
      "title": "Bias Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Test across demographic groups",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-03",
      "title": "Bias Testing",
      "description": "Control not detected or below threshold",
      "remediation": "Use TextAttack or CheckList for NLP bias testing",
      "docs_url": null
    },
    {
      "priority": "critical",
      "category": "ethical_ai",
      "control_id": "EA-04",
      "title": "Model Cards",
      "description": "Control not detected or below threshold",
      "remediation": "Detection failed: 'ConfigAnalyzer' object has no attribute 'file_exists'",
      "docs_url": null
    }
  ]
}