{
  "report_type": "static_scan",
  "generated_at": "2026-01-08T17:59:53.599966Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic",
    "files_scanned": 1321,
    "overall_score": 1.86,
    "confidence": 0.65,
    "duration_seconds": 1.798,
    "findings_count": 313,
    "severity_breakdown": {
      "CRITICAL": 20,
      "HIGH": 238,
      "MEDIUM": 26,
      "LOW": 29,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 6,
      "confidence": 0.51,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "3 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.5,
      "subscores": {
        "model_protection": 75,
        "extraction_defense": 35,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "Model registry",
        "Rate limiting",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": [
        "Extraction defense is weak",
        "Implement rate limiting and output protections"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.53,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII masking",
        "Explicit consent",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 1,
      "confidence": 0.4,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 50,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.84,
      "subscores": {
        "LLM01": 70,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 100
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 1 critical",
        "Insecure Output Handling: 2 critical, 4 high",
        "Model Denial of Service: 17 critical, 28 high",
        "Supply Chain Vulnerabilities: 184 high, 5 medium",
        "Excessive Agency: 11 high, 7 medium",
        "Overreliance: 11 high, 14 medium, 29 low"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/base_memory.py_24_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 24. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/base_memory.py",
      "line_number": 24,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass BaseMemory(Serializable, ABC):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/__init__.py_76_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 76. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/__init__.py",
      "line_number": 76,
      "code_snippet": "            \"This module has been moved to langchain-experimental. \"\n            \"For more details: \"\n            \"https://github.com/langchain-ai/langchain/discussions/11352.\"\n            \"To access this code, install it with `pip install langchain-experimental`.\"\n            \"`from langchain_experimental.llm_bash.base \"\n            \"import LLMBashChain`\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/hub.py_1_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/hub.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Interface with the [LangChain Hub](https://smith.langchain.com/hub).\"\"\"\n\nfrom __future__ import annotations\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/ensemble.py_295_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 295. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/ensemble.py",
      "line_number": 295,
      "code_snippet": "\n        You can find more details about RRF here:\n        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf.\n\n        Args:\n            doc_lists: A list of rank lists, where each rank list contains unique items.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py_164",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_relevant_documents' on line 164 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
      "line_number": 164,
      "code_snippet": "    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n    ) -> list[Document]:\n        \"\"\"Get relevant documents given a user query.\n\n        Args:\n            query: user query\n            run_manager: the callback handler to use.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py_164_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_get_relevant_documents'",
      "description": "Function '_get_relevant_documents' on line 164 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
      "line_number": 164,
      "code_snippet": "        return [doc for docs in document_lists for doc in docs]\n\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py_164_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_relevant_documents'",
      "description": "Function '_get_relevant_documents' on line 164 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/multi_query.py",
      "line_number": 164,
      "code_snippet": "        return [doc for docs in document_lists for doc in docs]\n\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/tools/__init__.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/tools/__init__.py",
      "line_number": 33,
      "code_snippet": "        \"This tool has access to a python REPL. \"\n        \"For best practices make sure to sandbox this tool. \"\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n        \"To keep using this code as is, install langchain_experimental and \"\n        \"update relevant imports replacing 'langchain' with 'langchain_experimental'\"\n    )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/tools/__init__.py_45_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 45. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/tools/__init__.py",
      "line_number": 45,
      "code_snippet": "        \"This tool has access to a python REPL. \"\n        \"For best practices make sure to sandbox this tool. \"\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n        \"To keep using this code as is, install langchain_experimental and \"\n        \"update relevant imports replacing 'langchain' with 'langchain_experimental'\"\n    )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_28",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_parse_model_string' on line 28 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 28,
      "code_snippet": "def _parse_model_string(model_name: str) -> tuple[str, str]:\n    \"\"\"Parse a model string into provider and model name components.\n\n    The model string should be in the format 'provider:model-name', where provider\n    is one of the supported providers.\n\n    Args:\n        model_name: A model string in the format 'provider:model-name'\n\n    Returns:\n        A tuple of (provider, model_name)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_83",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_infer_model_and_provider' on line 83 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 83,
      "code_snippet": "def _infer_model_and_provider(\n    model: str,\n    *,\n    provider: str | None = None,\n) -> tuple[str, str]:\n    if not model.strip():\n        msg = \"Model name cannot be empty\"\n        raise ValueError(msg)\n    if provider is None and \":\" in model:\n        provider, model_name = _parse_model_string(model)\n    else:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_155_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 155. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 155,
      "code_snippet": "            Supported providers:\n\n            - `openai`                  -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_156_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 156. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 156,
      "code_snippet": "\n            - `openai`                  -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_157_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 157. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 157,
      "code_snippet": "            - `openai`                  -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_158_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 158. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 158,
      "code_snippet": "            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_159_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 159. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 159,
      "code_snippet": "            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_160_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 160. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 160,
      "code_snippet": "            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_161_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 161. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 161,
      "code_snippet": "            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_162_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 162. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 162,
      "code_snippet": "            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n\n        **kwargs: Additional model-specific parameters passed to the embedding model.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_163_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 163. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 163,
      "code_snippet": "            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n\n        **kwargs: Additional model-specific parameters passed to the embedding model.\n            These vary by provider, see the provider-specific documentation for details.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_28_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_parse_model_string'",
      "description": "Function '_parse_model_string' on line 28 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 28,
      "code_snippet": "\n\ndef _parse_model_string(model_name: str) -> tuple[str, str]:\n    \"\"\"Parse a model string into provider and model name components.\n\n    The model string should be in the format 'provider:model-name', where provider",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py_83_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_infer_model_and_provider'",
      "description": "Function '_infer_model_and_provider' on line 83 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/embeddings/base.py",
      "line_number": 83,
      "code_snippet": "\n\ndef _infer_model_and_provider(\n    model: str,\n    *,\n    provider: str | None = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py_74",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'save_context' on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py",
      "line_number": 74,
      "code_snippet": "    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:\n        \"\"\"Save context from this conversation to buffer.\"\"\"\n        input_str, output_str = self._get_input_output(inputs, outputs)\n        self.chat_memory.add_messages(\n            [\n                HumanMessage(content=input_str),\n                AIMessage(content=output_str),\n            ],\n        )\n\n    async def asave_context(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py_98",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'clear' on line 98 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py",
      "line_number": 98,
      "code_snippet": "    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        self.chat_memory.clear()\n\n    async def aclear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        await self.chat_memory.aclear()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py_22_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 22. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py",
      "line_number": 22,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass BaseChatMemory(BaseMemory, ABC):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py_74_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'save_context'",
      "description": "Function 'save_context' on line 74 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/chat_memory.py",
      "line_number": 74,
      "code_snippet": "        return inputs[prompt_input_key], outputs[output_key]\n\n    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:\n        \"\"\"Save context from this conversation to buffer.\"\"\"\n        input_str, output_str = self._get_input_output(inputs, outputs)\n        self.chat_memory.add_messages(",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/buffer_window.py_15_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 15. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/buffer_window.py",
      "line_number": 15,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationBufferWindowMemory(BaseChatMemory):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py_112",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration",
      "description": "Function 'prune' on line 112 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py",
      "line_number": 112,
      "code_snippet": "    def prune(self) -> None:\n        \"\"\"Prune buffer if it exceeds max token limit.\"\"\"\n        buffer = self.chat_memory.messages\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\n        if curr_buffer_length > self.max_token_limit:\n            pruned_memory = []\n            while curr_buffer_length > self.max_token_limit:\n                pruned_memory.append(buffer.pop(0))\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\n            self.moving_summary_buffer = self.predict_new_summary(\n                pruned_memory,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py_17_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 17. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py",
      "line_number": 17,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationSummaryBufferMemory(BaseChatMemory, SummarizerMixin):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py_112_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'prune'",
      "description": "Function 'prune' on line 112 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py",
      "line_number": 112,
      "code_snippet": "        await self.aprune()\n\n    def prune(self) -> None:\n        \"\"\"Prune buffer if it exceeds max token limit.\"\"\"\n        buffer = self.chat_memory.messages\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py_112_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'prune'",
      "description": "Function 'prune' on line 112 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary_buffer.py",
      "line_number": 112,
      "code_snippet": "        await self.aprune()\n\n    def prune(self) -> None:\n        \"\"\"Prune buffer if it exceeds max token limit.\"\"\"\n        buffer = self.chat_memory.messages\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py_145",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration",
      "description": "Function 'save_context' on line 145 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py",
      "line_number": 145,
      "code_snippet": "    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\n        BaseChatMemory.save_context(self, inputs, outputs)\n        self._timestamps.append(datetime.now().astimezone())\n        # Prune buffer if it exceeds max token limit\n        buffer = self.chat_memory.messages\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\n        if curr_buffer_length > self.max_token_limit:\n            while curr_buffer_length > self.max_token_limit:\n                self._pop_and_store_interaction(buffer)\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py_145_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'save_context'",
      "description": "Function 'save_context' on line 145 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py",
      "line_number": 145,
      "code_snippet": "        return {self.memory_key: messages}\n\n    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\n        BaseChatMemory.save_context(self, inputs, outputs)\n        self._timestamps.append(datetime.now().astimezone())",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/vectorstore.py_20_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 20. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/vectorstore.py",
      "line_number": 20,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass VectorStoreRetrieverMemory(BaseMemory):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary.py_24_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 24. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary.py",
      "line_number": 24,
      "code_snippet": "    message=(\n        \"Refer here for how to incorporate summaries of conversation history: \"\n        \"https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/\"\n    ),\n)\nclass SummarizerMixin(BaseModel):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary.py_88_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 88. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/summary.py",
      "line_number": 88,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationSummaryMemory(BaseChatMemory, SummarizerMixin):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/buffer.py_18_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 18. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/buffer.py",
      "line_number": 18,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationBufferMemory(BaseChatMemory):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/buffer.py_96_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 96. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/buffer.py",
      "line_number": 96,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationStringBufferMemory(BaseMemory):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_607",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'clear' on line 607 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 607,
      "code_snippet": "    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        self.chat_memory.clear()\n        self.entity_cache.clear()\n        self.entity_store.clear()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 35,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass BaseEntityStore(BaseModel, ABC):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_67_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 67. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 67,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass InMemoryEntityStore(BaseEntityStore):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_101_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 101. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 101,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass UpstashRedisEntityStore(BaseEntityStore):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_216_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 216. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 216,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass RedisEntityStore(BaseEntityStore):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_340_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 340. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 340,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass SQLiteEntityStore(BaseEntityStore):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py_462_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 462. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/entity.py",
      "line_number": 462,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationEntityMemory(BaseChatMemory):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/token_buffer.py_61",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration",
      "description": "Function 'save_context' on line 61 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/token_buffer.py",
      "line_number": 61,
      "code_snippet": "    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\n        super().save_context(inputs, outputs)\n        # Prune buffer if it exceeds max token limit\n        buffer = self.chat_memory.messages\n        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)\n        if curr_buffer_length > self.max_token_limit:\n            pruned_memory = []\n            while curr_buffer_length > self.max_token_limit:\n                pruned_memory.append(buffer.pop(0))\n                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/token_buffer.py_16_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 16. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/token_buffer.py",
      "line_number": 16,
      "code_snippet": "    message=(\n        \"Please see the migration guide at: \"\n        \"https://python.langchain.com/docs/versions/migrating_memory/\"\n    ),\n)\nclass ConversationTokenBufferMemory(BaseChatMemory):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/token_buffer.py_61_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'save_context'",
      "description": "Function 'save_context' on line 61 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/memory/token_buffer.py",
      "line_number": 61,
      "code_snippet": "        return {self.memory_key: self.buffer}\n\n    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:\n        \"\"\"Save context from this conversation to buffer. Pruned.\"\"\"\n        super().save_context(inputs, outputs)\n        # Prune buffer if it exceeds max token limit",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/__init__.py_4_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 4. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/__init__.py",
      "line_number": 4,
      "code_snippet": "\nThis module provides utilities for connecting to\n[LangSmith](https://docs.langchain.com/langsmith/home).\n\n**Evaluation**\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_372",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_init_chat_model_helper' on line 372 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 372,
      "code_snippet": "def _init_chat_model_helper(\n    model: str,\n    *,\n    model_provider: str | None = None,\n    **kwargs: Any,\n) -> BaseChatModel:\n    model, model_provider = _parse_model(model, model_provider)\n    if model_provider == \"openai\":\n        _check_pkg(\"langchain_openai\")\n        from langchain_openai import ChatOpenAI\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_603",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_parse_model' on line 603 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 603,
      "code_snippet": "def _parse_model(model: str, model_provider: str | None) -> tuple[str, str]:\n    \"\"\"Parse model name and provider, inferring provider if necessary.\"\"\"\n    if not model_provider and \":\" in model:\n        prefix, suffix = model.split(\":\", 1)\n        if prefix in _SUPPORTED_PROVIDERS:\n            model_provider = prefix\n            model = suffix\n        else:\n            inferred = _attempt_infer_model_provider(prefix)\n            if inferred:\n                model_provider = inferred",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_383_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 383. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 383,
      "code_snippet": "        from langchain_openai import ChatOpenAI\n\n        return ChatOpenAI(model=model, **kwargs)\n    if model_provider == \"anthropic\":\n        _check_pkg(\"langchain_anthropic\")\n        from langchain_anthropic import ChatAnthropic",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_388_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 388. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 388,
      "code_snippet": "        from langchain_anthropic import ChatAnthropic\n\n        return ChatAnthropic(model=model, **kwargs)  # type: ignore[call-arg,unused-ignore]\n    if model_provider == \"azure_openai\":\n        _check_pkg(\"langchain_openai\")\n        from langchain_openai import AzureChatOpenAI",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_393_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 393. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 393,
      "code_snippet": "        from langchain_openai import AzureChatOpenAI\n\n        return AzureChatOpenAI(model=model, **kwargs)\n    if model_provider == \"azure_ai\":\n        _check_pkg(\"langchain_azure_ai\")\n        from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_471_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 471. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 471,
      "code_snippet": "        from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n\n        return ChatAnthropicVertex(model=model, **kwargs)\n    if model_provider == \"deepseek\":\n        _check_pkg(\"langchain_deepseek\", pkg_kebab=\"langchain-deepseek\")\n        from langchain_deepseek import ChatDeepSeek",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_95_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 95. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 95,
      "code_snippet": "        (e.g., `pip install langchain-openai`).\n\n        Refer to the [provider integration's API reference](https://docs.langchain.com/oss/python/integrations/providers)\n        for supported model parameters to use as `**kwargs`.\n\n    Args:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_124_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 124. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 124,
      "code_snippet": "            are:\n\n            - `openai`                  -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `anthropic`               -> [`langchain-anthropic`](https://docs.langchain.com/oss/python/integrations/providers/anthropic)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_ai`                -> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_125_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 125. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 125,
      "code_snippet": "\n            - `openai`                  -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `anthropic`               -> [`langchain-anthropic`](https://docs.langchain.com/oss/python/integrations/providers/anthropic)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_ai`                -> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_126_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 126. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 126,
      "code_snippet": "            - `openai`                  -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `anthropic`               -> [`langchain-anthropic`](https://docs.langchain.com/oss/python/integrations/providers/anthropic)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_ai`                -> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_127_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 127. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 127,
      "code_snippet": "            - `anthropic`               -> [`langchain-anthropic`](https://docs.langchain.com/oss/python/integrations/providers/anthropic)\n            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_ai`                -> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_128_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 128. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 128,
      "code_snippet": "            - `azure_openai`            -> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)\n            - `azure_ai`                -> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `bedrock_converse`        -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_129_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 129. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 129,
      "code_snippet": "            - `azure_ai`                -> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)\n            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `bedrock_converse`        -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_130_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 130. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 130,
      "code_snippet": "            - `google_vertexai`         -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `bedrock_converse`        -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `fireworks`               -> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_131_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 131. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 131,
      "code_snippet": "            - `google_genai`            -> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `bedrock_converse`        -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `fireworks`               -> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)\n            - `together`                -> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_132_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 132. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 132,
      "code_snippet": "            - `bedrock`                 -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `bedrock_converse`        -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `fireworks`               -> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)\n            - `together`                -> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_133_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 133. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 133,
      "code_snippet": "            - `bedrock_converse`        -> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)\n            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `fireworks`               -> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)\n            - `together`                -> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_134_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 134. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 134,
      "code_snippet": "            - `cohere`                  -> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)\n            - `fireworks`               -> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)\n            - `together`                -> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `groq`                    -> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_135_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 135. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 135,
      "code_snippet": "            - `fireworks`               -> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)\n            - `together`                -> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `groq`                    -> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_136_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 136. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 136,
      "code_snippet": "            - `together`                -> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)\n            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `groq`                    -> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n            - `google_anthropic_vertex` -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_137_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 137. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 137,
      "code_snippet": "            - `mistralai`               -> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)\n            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `groq`                    -> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n            - `google_anthropic_vertex` -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `deepseek`                -> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_138_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 138. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 138,
      "code_snippet": "            - `huggingface`             -> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)\n            - `groq`                    -> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n            - `google_anthropic_vertex` -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `deepseek`                -> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)\n            - `ibm`                     -> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_139_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 139. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 139,
      "code_snippet": "            - `groq`                    -> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)\n            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n            - `google_anthropic_vertex` -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `deepseek`                -> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)\n            - `ibm`                     -> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)\n            - `nvidia`                  -> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_140_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 140. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 140,
      "code_snippet": "            - `ollama`                  -> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n            - `google_anthropic_vertex` -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `deepseek`                -> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)\n            - `ibm`                     -> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)\n            - `nvidia`                  -> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)\n            - `xai`                     -> [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_141_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 141. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 141,
      "code_snippet": "            - `google_anthropic_vertex` -> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)\n            - `deepseek`                -> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)\n            - `ibm`                     -> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)\n            - `nvidia`                  -> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)\n            - `xai`                     -> [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai)\n            - `perplexity`              -> [`langchain-perplexity`](https://docs.langchain.com/oss/python/integrations/providers/perplexity)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_142_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 142. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 142,
      "code_snippet": "            - `deepseek`                -> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)\n            - `ibm`                     -> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)\n            - `nvidia`                  -> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)\n            - `xai`                     -> [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai)\n            - `perplexity`              -> [`langchain-perplexity`](https://docs.langchain.com/oss/python/integrations/providers/perplexity)\n        configurable_fields: Which model parameters are configurable at runtime:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_143_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 143. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 143,
      "code_snippet": "            - `ibm`                     -> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)\n            - `nvidia`                  -> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)\n            - `xai`                     -> [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai)\n            - `perplexity`              -> [`langchain-perplexity`](https://docs.langchain.com/oss/python/integrations/providers/perplexity)\n        configurable_fields: Which model parameters are configurable at runtime:\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_144_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 144. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 144,
      "code_snippet": "            - `nvidia`                  -> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)\n            - `xai`                     -> [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai)\n            - `perplexity`              -> [`langchain-perplexity`](https://docs.langchain.com/oss/python/integrations/providers/perplexity)\n        configurable_fields: Which model parameters are configurable at runtime:\n\n            - `None`: No configurable fields (i.e., a fixed model).",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_190_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 190. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 190,
      "code_snippet": "\n            Refer to the specific model provider's\n            [integration reference](https://reference.langchain.com/python/integrations/)\n            for all available parameters.\n\n    Returns:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_624_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 624. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 624,
      "code_snippet": "            f\"Supported providers: {supported_list}\\n\\n\"\n            f\"For help with specific providers, see: \"\n            f\"https://docs.langchain.com/oss/python/integrations/providers\"\n        )\n        raise ValueError(msg)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_372_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_init_chat_model_helper'",
      "description": "Function '_init_chat_model_helper' on line 372 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 372,
      "code_snippet": "\n\ndef _init_chat_model_helper(\n    model: str,\n    *,\n    model_provider: str | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py_603_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_parse_model'",
      "description": "Function '_parse_model' on line 603 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chat_models/base.py",
      "line_number": 603,
      "code_snippet": "\n\ndef _parse_model(model: str, model_provider: str | None) -> tuple[str, str]:\n    \"\"\"Parse model name and provider, inferring provider if necessary.\"\"\"\n    if not model_provider and \":\" in model:\n        prefix, suffix = model.split(\":\", 1)",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/deprecation.py_16_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 16. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/deprecation.py",
      "line_number": 16,
      "code_snippet": "    \"tool-calling, persistence of state, and human-in-the-loop workflows. For \"\n    \"details, refer to the \"\n    \"[LangGraph documentation](https://langchain-ai.github.io/langgraph/)\"\n    \" as well as guides for \"\n    \"[Migrating from AgentExecutor](https://python.langchain.com/docs/how_to/migrate_agent/)\"\n    \" and LangGraph's \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/deprecation.py_18_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 18. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/deprecation.py",
      "line_number": 18,
      "code_snippet": "    \"[LangGraph documentation](https://langchain-ai.github.io/langgraph/)\"\n    \" as well as guides for \"\n    \"[Migrating from AgentExecutor](https://python.langchain.com/docs/how_to/migrate_agent/)\"\n    \" and LangGraph's \"\n    \"[Pre-built ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/).\"\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/deprecation.py_20_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 20. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/deprecation.py",
      "line_number": 20,
      "code_snippet": "    \"[Migrating from AgentExecutor](https://python.langchain.com/docs/how_to/migrate_agent/)\"\n    \" and LangGraph's \"\n    \"[Pre-built ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/).\"\n)\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/module_import.py_109_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 109. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/module_import.py",
      "line_number": 109,
      "code_snippet": "                            \"You can use the langchain cli to **automatically** \"\n                            \"upgrade many imports. Please see documentation here \"\n                            \"<https://python.langchain.com/docs/versions/v0_2/>\"\n                        ),\n                    )\n            except Exception as e:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/module_import.py_143_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 143. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/_api/module_import.py",
      "line_number": 143,
      "code_snippet": "                            \"You can use the langchain cli to **automatically** \"\n                            \"upgrade many imports. Please see documentation here \"\n                            \"<https://python.langchain.com/docs/versions/v0_2/>\"\n                        ),\n                    )\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/loading.py_19_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 19. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/loading.py",
      "line_number": 19,
      "code_snippet": "logger = logging.getLogger(__name__)\n\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/agents/\"\n\n\ndef _load_agent_from_tools(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/loading.py_122_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 122. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/loading.py",
      "line_number": 122,
      "code_snippet": "        msg = (\n            \"Loading from the deprecated github-based Hub is no longer supported. \"\n            \"Please use the new LangChain Hub at https://smith.langchain.com/hub \"\n            \"instead.\"\n        )\n        raise RuntimeError(msg)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/__init__.py_113_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 113. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/__init__.py",
      "line_number": 113,
      "code_snippet": "        msg = (\n            f\"{name} has been moved to langchain_experimental. \"\n            \"See https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"for more information.\\n\"\n            f\"Please update your import statement from: `{old_path}` to `{new_path}`.\"\n        )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/loading.py_115",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'load_evaluator' on line 115 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/loading.py",
      "line_number": 115,
      "code_snippet": "def load_evaluator(\n    evaluator: EvaluatorType,\n    *,\n    llm: BaseLanguageModel | None = None,\n    **kwargs: Any,\n) -> Chain | StringEvaluator:\n    \"\"\"Load the requested evaluation chain specified by a string.\n\n    Parameters\n    ----------\n    evaluator : EvaluatorType",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/loading.py_168_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4'' is used without version pinning on line 168. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. Always pin to specific versions (e.g., 'gpt-4-0613' instead of 'gpt-4').",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/loading.py",
      "line_number": 168,
      "code_snippet": "                    raise ImportError(msg) from e\n\n            llm = llm or ChatOpenAI(model=\"gpt-4\", seed=42, temperature=0)\n        except Exception as e:\n            msg = (\n                f\"Evaluation with the {evaluator_cls} requires a \"",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines\n5. Test thoroughly before upgrading model versions\n6. Monitor for model deprecation notices"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/loading.py_115_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'load_evaluator'",
      "description": "Function 'load_evaluator' on line 115 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/loading.py",
      "line_number": 115,
      "code_snippet": "\n\ndef load_evaluator(\n    evaluator: EvaluatorType,\n    *,\n    llm: BaseLanguageModel | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/loading.py_69_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 69. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
      "line_number": 69,
      "code_snippet": "\n\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/chains/\"\n\n\ndef _load_llm_chain(config: dict, **kwargs: Any) -> LLMChain:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/loading.py_424_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 424. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
      "line_number": 424,
      "code_snippet": "        \"concerns. Please refer to langchain-experimental repository for more details, \"\n        \"or refer to this tutorial for best practices: \"\n        \"https://python.langchain.com/docs/tutorials/sql_qa/\"\n    )\n    raise NotImplementedError(msg)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/loading.py_713_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 713. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/loading.py",
      "line_number": 713,
      "code_snippet": "        msg = (\n            \"Loading from the deprecated github-based Hub is no longer supported. \"\n            \"Please use the new LangChain Hub at https://smith.langchain.com/hub \"\n            \"instead.\"\n        )\n        raise RuntimeError(msg)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/moderation.py_47",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'validate_environment' on line 47 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/moderation.py",
      "line_number": 47,
      "code_snippet": "    def validate_environment(cls, values: dict) -> Any:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        openai_api_key = get_from_dict_or_env(\n            values,\n            \"openai_api_key\",\n            \"OPENAI_API_KEY\",\n        )\n        openai_organization = get_from_dict_or_env(\n            values,\n            \"openai_organization\",\n            \"OPENAI_ORGANIZATION\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_117",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.generate' is used in 'Response' on line 117 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 117,
      "code_snippet": "    ) -> dict[str, str]:\n        response = self.generate([inputs], run_manager=run_manager)\n        return self.create_outputs(response)[0]\n",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_241",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.generate' is used in 'Response' on line 241 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 241,
      "code_snippet": "        try:\n            response = self.generate(input_list, run_manager=run_manager)\n        except BaseException as e:\n            run_manager.on_chain_error(e)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_112",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 112 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 112,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict[str, Any],\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict[str, str]:\n        response = self.generate([inputs], run_manager=run_manager)\n        return self.create_outputs(response)[0]\n\n    def generate(\n        self,\n        input_list: list[dict[str, Any]],",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_120",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate' on line 120 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 120,
      "code_snippet": "    def generate(\n        self,\n        input_list: list[dict[str, Any]],\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) -> LLMResult:\n        \"\"\"Generate LLM result from inputs.\"\"\"\n        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n        callbacks = run_manager.get_child() if run_manager else None\n        if isinstance(self.llm, BaseLanguageModel):\n            return self.llm.generate_prompt(\n                prompts,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_224",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'apply' on line 224 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 224,
      "code_snippet": "    def apply(\n        self,\n        input_list: list[dict[str, Any]],\n        callbacks: Callbacks = None,\n    ) -> list[dict[str, str]]:\n        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_112_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in '_call'",
      "description": "Function '_call' on line 112 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 112,
      "code_snippet": "        return [self.output_key, \"full_generation\"]\n\n    def _call(\n        self,\n        inputs: dict[str, Any],\n        run_manager: CallbackManagerForChainRun | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_224_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'apply'",
      "description": "Function 'apply' on line 224 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 224,
      "code_snippet": "        return prompts, stop\n\n    def apply(\n        self,\n        input_list: list[dict[str, Any]],\n        callbacks: Callbacks = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_224_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'apply'",
      "description": "Function 'apply' on line 224 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 224,
      "code_snippet": "        return prompts, stop\n\n    def apply(\n        self,\n        input_list: list[dict[str, Any]],\n        callbacks: Callbacks = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_112_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_call'",
      "description": "Function '_call' on line 112 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 112,
      "code_snippet": "        return [self.output_key, \"full_generation\"]\n\n    def _call(\n        self,\n        inputs: dict[str, Any],\n        run_manager: CallbackManagerForChainRun | None = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_120_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'generate'",
      "description": "Function 'generate' on line 120 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 120,
      "code_snippet": "        return self.create_outputs(response)[0]\n\n    def generate(\n        self,\n        input_list: list[dict[str, Any]],\n        run_manager: CallbackManagerForChainRun | None = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py_224_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'apply'",
      "description": "Function 'apply' on line 224 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm.py",
      "line_number": 224,
      "code_snippet": "        return prompts, stop\n\n    def apply(\n        self,\n        input_list: list[dict[str, Any]],\n        callbacks: Callbacks = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/mapreduce.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/mapreduce.py",
      "line_number": 35,
      "code_snippet": "    message=(\n        \"Refer to migration guide here for a recommended implementation using \"\n        \"LangGraph: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\"\n        \". See also LangGraph guides for map-reduce: \"\n        \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/.\"\n    ),",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/mapreduce.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/mapreduce.py",
      "line_number": 37,
      "code_snippet": "        \"LangGraph: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\"\n        \". See also LangGraph guides for map-reduce: \"\n        \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/.\"\n    ),\n)\nclass MapReduceChain(Chain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py_53",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'create_extraction_chain_pydantic' on line 53 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py",
      "line_number": 53,
      "code_snippet": "def create_extraction_chain_pydantic(\n    pydantic_schemas: list[type[BaseModel]] | type[BaseModel],\n    llm: BaseLanguageModel,\n    system_message: str = _EXTRACTION_TEMPLATE,\n) -> Runnable:\n    \"\"\"Creates a chain that extracts information from a passage.\n\n    Args:\n        pydantic_schemas: The schema of the entities to extract.\n        llm: The language model to use.\n        system_message: The system message to use for extraction.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py",
      "line_number": 23,
      "code_snippet": "        \"is available on ChatModels capable of tool calling.\"\n        \"You can read more about the method here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"Please follow our extraction use case documentation for more guidelines\"\n        \"on how to do information extraction with LLMs.\"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py_26_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 26. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py",
      "line_number": 26,
      "code_snippet": "        \"Please follow our extraction use case documentation for more guidelines\"\n        \"on how to do information extraction with LLMs.\"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"\n        \"with_structured_output does not currently support a list of pydantic schemas. \"\n        \"If this is a blocker or if you notice other issues, please provide \"\n        \"feedback here:\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py",
      "line_number": 30,
      "code_snippet": "        \"If this is a blocker or if you notice other issues, please provide \"\n        \"feedback here:\"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"\n    ),\n    removal=\"1.0\",\n    alternative=(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py_78_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'create_extraction_chain_pydantic'",
      "description": "Function 'create_extraction_chain_pydantic' on line 53 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_tools/extraction.py",
      "line_number": 78,
      "code_snippet": "    functions = [convert_pydantic_to_openai_function(p) for p in pydantic_schemas]\n    tools = [{\"type\": \"function\", \"function\": d} for d in functions]\n    model = llm.bind(tools=tools)\n    return prompt | model | PydanticToolsParser(tools=pydantic_schemas)",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/hyde/__init__.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/hyde/__init__.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Hypothetical Document Embeddings.\n\nhttps://arxiv.org/abs/2212.10496\n\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/hyde/base.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/hyde/base.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Hypothetical Document Embeddings.\n\nhttps://arxiv.org/abs/2212.10496\n\"\"\"\n\nfrom __future__ import annotations",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/hyde/base.py_29_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 29. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/hyde/base.py",
      "line_number": 29,
      "code_snippet": "    \"\"\"Generate hypothetical document for query, and then embed that.\n\n    Based on https://arxiv.org/abs/2212.10496\n    \"\"\"\n\n    base_embeddings: Embeddings",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py_238_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 238. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py",
      "line_number": 238,
      "code_snippet": "        \"This class is deprecated. See the following migration guides for replacements \"\n        \"based on `chain_type`:\\n\"\n        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"\n        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py_239_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 239. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py",
      "line_number": 239,
      "code_snippet": "        \"based on `chain_type`:\\n\"\n        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"\n        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n        \"\\nSee also guides on retrieval and question-answering here: \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py_240_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 240. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py",
      "line_number": 240,
      "code_snippet": "        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"\n        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n        \"\\nSee also guides on retrieval and question-answering here: \"\n        \"https://python.langchain.com/docs/how_to/#qa-with-rag\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py_241_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 241. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py",
      "line_number": 241,
      "code_snippet": "        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n        \"\\nSee also guides on retrieval and question-answering here: \"\n        \"https://python.langchain.com/docs/how_to/#qa-with-rag\"\n    ),",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py_243_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 243. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/question_answering/chain.py",
      "line_number": 243,
      "code_snippet": "        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n        \"\\nSee also guides on retrieval and question-answering here: \"\n        \"https://python.langchain.com/docs/how_to/#qa-with-rag\"\n    ),\n)\ndef load_qa_chain(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py",
      "line_number": 37,
      "code_snippet": "        from elasticsearch import Elasticsearch\n\n        database = Elasticsearch(\"http://localhost:9200\")\n        db_chain = ElasticsearchDatabaseChain.from_llm(OpenAI(), database)\n        ```\n    \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/qa_with_structure.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/qa_with_structure.py",
      "line_number": 36,
      "code_snippet": "        \"This function is deprecated. Refer to this guide on retrieval and question \"\n        \"answering with structured responses: \"\n        \"https://python.langchain.com/docs/how_to/qa_sources/#structure-sources-in-model-response\"\n    ),\n)\ndef create_qa_with_structure_chain(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/qa_with_structure.py_119_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 119. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/qa_with_structure.py",
      "line_number": 119,
      "code_snippet": "        \"This function is deprecated. Refer to this guide on retrieval and question \"\n        \"answering with sources: \"\n        \"https://python.langchain.com/docs/how_to/qa_sources/#structure-sources-in-model-response\"\n    ),\n)\ndef create_qa_with_sources_chain(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/openapi.py_257_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 257. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/openapi.py",
      "line_number": 257,
      "code_snippet": "        \"This function is deprecated and will be removed in langchain 1.0. \"\n        \"See API reference for replacement: \"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.openapi.get_openapi_chain.html\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/openapi.py_298_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 298. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/openapi.py",
      "line_number": 298,
      "code_snippet": "        \"servers\": [\n            {\n            \"url\": \"https://jsonplaceholder.typicode.com\"\n            }\n        ],\n        \"paths\": {",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py_53_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 53. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py",
      "line_number": 53,
      "code_snippet": "        \"is available on ChatModels capable of tool calling.\"\n        \"You can read more about the method here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>.\"\n    ),\n    removal=\"1.0\",\n    alternative=(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py_116_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 116. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py",
      "line_number": 116,
      "code_snippet": "        \"is available on ChatModels capable of tool calling.\"\n        \"You can read more about the method here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"Please follow our extraction use case documentation for more guidelines\"\n        \"on how to do information extraction with LLMs.\"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py_119_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 119. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py",
      "line_number": 119,
      "code_snippet": "        \"Please follow our extraction use case documentation for more guidelines\"\n        \"on how to do information extraction with LLMs.\"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"\n        \"If you notice other issues, please provide \"\n        \"feedback here:\"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py_122_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 122. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/extraction.py",
      "line_number": 122,
      "code_snippet": "        \"If you notice other issues, please provide \"\n        \"feedback here:\"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"\n    ),\n    removal=\"1.0\",\n    alternative=(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py_77",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_citation_fuzzy_match_runnable' on line 77 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py",
      "line_number": 77,
      "code_snippet": "def create_citation_fuzzy_match_runnable(llm: BaseChatModel) -> Runnable:\n    \"\"\"Create a citation fuzzy match Runnable.\n\n    Example usage:\n\n        ```python\n        from langchain_classic.chains import create_citation_fuzzy_match_runnable\n        from langchain_openai import ChatOpenAI\n\n        model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py_77_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_citation_fuzzy_match_runnable'",
      "description": "Function 'create_citation_fuzzy_match_runnable' on line 77 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py",
      "line_number": 77,
      "code_snippet": "\n\ndef create_citation_fuzzy_match_runnable(llm: BaseChatModel) -> Runnable:\n    \"\"\"Create a citation fuzzy match Runnable.\n\n    Example usage:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py_77_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_citation_fuzzy_match_runnable'",
      "description": "Function 'create_citation_fuzzy_match_runnable' on line 77 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py",
      "line_number": 77,
      "code_snippet": "\n\ndef create_citation_fuzzy_match_runnable(llm: BaseChatModel) -> Runnable:\n    \"\"\"Create a citation fuzzy match Runnable.\n\n    Example usage:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_42_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 42. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 42,
      "code_snippet": "        \"is available on ChatModels capable of tool calling. \"\n        \"See API reference for this function for replacement: <\"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain.html\"\n        \"> You can read more about `with_structured_output` here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"If you notice other issues, please provide \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_44_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 44. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 44,
      "code_snippet": "        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain.html\"\n        \"> You can read more about `with_structured_output` here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_47_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 47. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 47,
      "code_snippet": "        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_87_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 87. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 87,
      "code_snippet": "    ```\n\n    Read more here: https://docs.langchain.com/oss/python/langchain/models#structured-outputs\n\n    Args:\n        schema: The schema of the entities to extract.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_118_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 118. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 118,
      "code_snippet": "        \"is available on ChatModels capable of tool calling. \"\n        \"See API reference for this function for replacement: <\"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html\"\n        \"> You can read more about `with_structured_output` here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"If you notice other issues, please provide \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_120_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 120. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 120,
      "code_snippet": "        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html\"\n        \"> You can read more about `with_structured_output` here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_123_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 123. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 123,
      "code_snippet": "        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py_163_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 163. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/openai_functions/tagging.py",
      "line_number": 163,
      "code_snippet": "    ```\n\n    Read more here: https://docs.langchain.com/oss/python/langchain/models#structured-outputs\n\n    Args:\n        pydantic_schema: The Pydantic schema of the entities to extract.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py_33",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'create_sql_query_chain' on line 33 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py",
      "line_number": 33,
      "code_snippet": "def create_sql_query_chain(\n    llm: BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None = None,\n    k: int = 5,\n    *,\n    get_col_comments: bool | None = None,\n) -> Runnable[SQLInput | SQLInputWithTables | dict[str, Any], str]:\n    r\"\"\"Create a chain that generates SQL queries.\n\n    *Security Note*: This chain generates SQL queries for the given database.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py_56_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 56. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py",
      "line_number": 56,
      "code_snippet": "        Control access to who can submit requests to this chain.\n\n        See https://docs.langchain.com/oss/python/security-policy for more information.\n\n    Args:\n        llm: The language model to use.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py_33_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_sql_query_chain'",
      "description": "Function 'create_sql_query_chain' on line 33 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py",
      "line_number": 33,
      "code_snippet": "\n\ndef create_sql_query_chain(\n    llm: BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None = None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py_33_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_sql_query_chain'",
      "description": "Function 'create_sql_query_chain' on line 33 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py",
      "line_number": 33,
      "code_snippet": "\n\ndef create_sql_query_chain(\n    llm: BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py_33_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_sql_query_chain'",
      "description": "Function 'create_sql_query_chain' on line 33 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/sql_database/query.py",
      "line_number": 33,
      "code_snippet": "\n\ndef create_sql_query_chain(\n    llm: BaseLanguageModel,\n    db: SQLDatabase,\n    prompt: BasePromptTemplate | None = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_540",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'llm.bind' is used in 'Response' on line 540 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 540,
      "code_snippet": "\n    llm = llm.bind(response_format={\"type\": \"json_object\"})\n    if prompt:\n        if \"output_schema\" in prompt.input_variables:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_66",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'create_openai_fn_runnable' on line 66 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 66,
      "code_snippet": "def create_openai_fn_runnable(\n    functions: Sequence[dict[str, Any] | type[BaseModel] | Callable],\n    llm: Runnable,\n    prompt: BasePromptTemplate | None = None,\n    *,\n    enforce_single_function_usage: bool = True,\n    output_parser: BaseOutputParser | BaseGenerationOutputParser | None = None,\n    **llm_kwargs: Any,\n) -> Runnable:\n    \"\"\"Create a runnable sequence that uses OpenAI functions.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_450",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_create_openai_tools_runnable' on line 450 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 450,
      "code_snippet": "def _create_openai_tools_runnable(\n    tool: dict[str, Any] | type[BaseModel] | Callable,\n    llm: Runnable,\n    *,\n    prompt: BasePromptTemplate | None,\n    output_parser: BaseOutputParser | BaseGenerationOutputParser | None,\n    enforce_tool_usage: bool,\n    first_tool_only: bool,\n) -> Runnable:\n    oai_tool = convert_to_openai_tool(tool)\n    llm_kwargs: dict[str, Any] = {\"tools\": [oai_tool]}",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_524",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_create_openai_json_runnable' on line 524 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 524,
      "code_snippet": "def _create_openai_json_runnable(\n    output_schema: dict[str, Any] | type[BaseModel],\n    llm: Runnable,\n    prompt: BasePromptTemplate | None = None,\n    *,\n    output_parser: BaseOutputParser | BaseGenerationOutputParser | None = None,\n) -> Runnable:\n    if isinstance(output_schema, type) and is_basemodel_subclass(output_schema):\n        output_parser = output_parser or PydanticOutputParser(\n            pydantic_object=output_schema,\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 37,
      "code_snippet": "        \"is available on ChatModels capable of tool calling. \"\n        \"You can read more about the method here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>. \"\n        \"Please follow our extraction use case documentation for more guidelines \"\n        \"on how to do information extraction with LLMs. \"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_40_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 40. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 40,
      "code_snippet": "        \"Please follow our extraction use case documentation for more guidelines \"\n        \"on how to do information extraction with LLMs. \"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"\n        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 43,
      "code_snippet": "        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"\n    ),\n    removal=\"1.0\",\n    alternative=(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_156_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 156. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 156,
      "code_snippet": "        \"is available on ChatModels capable of tool calling. \"\n        \"You can read more about the method here: \"\n        \"<https://docs.langchain.com/oss/python/langchain/models#structured-outputs>.\"\n        \"Please follow our extraction use case documentation for more guidelines \"\n        \"on how to do information extraction with LLMs. \"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_159_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 159. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 159,
      "code_snippet": "        \"Please follow our extraction use case documentation for more guidelines \"\n        \"on how to do information extraction with LLMs. \"\n        \"<https://python.langchain.com/docs/use_cases/extraction/>. \"\n        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_162_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 162. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 162,
      "code_snippet": "        \"If you notice other issues, please provide \"\n        \"feedback here: \"\n        \"<https://github.com/langchain-ai/langchain/discussions/18154>\"\n    ),\n    removal=\"1.0\",\n    alternative=(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_450_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_create_openai_tools_runnable'",
      "description": "Function '_create_openai_tools_runnable' on line 450 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 450,
      "code_snippet": "\n\ndef _create_openai_tools_runnable(\n    tool: dict[str, Any] | type[BaseModel] | Callable,\n    llm: Runnable,\n    *,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py_450_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_create_openai_tools_runnable'",
      "description": "Function '_create_openai_tools_runnable' on line 450 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/structured_output/base.py",
      "line_number": 450,
      "code_snippet": "\n\ndef _create_openai_tools_runnable(\n    tool: dict[str, Any] | type[BaseModel] | Callable,\n    llm: Runnable,\n    *,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/retrieval_qa/base.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/retrieval_qa/base.py",
      "line_number": 37,
      "code_snippet": "        \"This class is deprecated. Use the `create_retrieval_chain` constructor \"\n        \"instead. See migration guide here: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\"\n    ),\n)\nclass BaseRetrievalQA(Chain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/retrieval_qa/base.py_215_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 215. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/retrieval_qa/base.py",
      "line_number": 215,
      "code_snippet": "        \"This class is deprecated. Use the `create_retrieval_chain` constructor \"\n        \"instead. See migration guide here: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\"\n    ),\n)\nclass RetrievalQA(BaseRetrievalQA):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/retrieval_qa/base.py_304_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 304. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/retrieval_qa/base.py",
      "line_number": 304,
      "code_snippet": "        \"This class is deprecated. Use the `create_retrieval_chain` constructor \"\n        \"instead. See migration guide here: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\"\n    ),\n)\nclass VectorDBQA(BaseRetrievalQA):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py_173_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 173. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py",
      "line_number": 173,
      "code_snippet": "        \"This function is deprecated. Refer to this guide on retrieval and question \"\n        \"answering with sources: \"\n        \"https://python.langchain.com/docs/how_to/qa_sources/\"\n        \"\\nSee also the following migration guides for replacements \"\n        \"based on `chain_type`:\\n\"\n        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py_176_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 176. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py",
      "line_number": 176,
      "code_snippet": "        \"\\nSee also the following migration guides for replacements \"\n        \"based on `chain_type`:\\n\"\n        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"\n        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py_177_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 177. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py",
      "line_number": 177,
      "code_snippet": "        \"based on `chain_type`:\\n\"\n        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"\n        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n    ),",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py_178_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 178. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py",
      "line_number": 178,
      "code_snippet": "        \"stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\\n\"\n        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n    ),\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py_179_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 179. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/loading.py",
      "line_number": 179,
      "code_snippet": "        \"map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\\n\"\n        \"refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\\n\"\n        \"map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\\n\"\n    ),\n)\ndef load_qa_with_sources_chain(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/base.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/base.py",
      "line_number": 43,
      "code_snippet": "        \"This class is deprecated. Refer to this guide on retrieval and question \"\n        \"answering with sources: \"\n        \"https://python.langchain.com/docs/how_to/qa_sources/\"\n    ),\n)\nclass BaseQAWithSourcesChain(Chain, ABC):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/base.py_224_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 224. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_with_sources/base.py",
      "line_number": 224,
      "code_snippet": "        \"This class is deprecated. Refer to this guide on retrieval and question \"\n        \"answering with sources: \"\n        \"https://python.langchain.com/docs/how_to/qa_sources/\"\n    ),\n)\nclass QAWithSourcesChain(BaseQAWithSourcesChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_symbolic_math/__init__.py_5_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 5. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_symbolic_math/__init__.py",
      "line_number": 5,
      "code_snippet": "    msg = (\n        \"This module has been moved to langchain-experimental. \"\n        \"For more details: https://github.com/langchain-ai/langchain/discussions/11352.\"\n        \"To access this code, install it with `pip install langchain-experimental`.\"\n        \"`from langchain_experimental.llm_symbolic_math.base \"\n        \"import LLMSymbolicMathChain`\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py_116",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_call' on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py",
      "line_number": 116,
      "code_snippet": "    def _call(\n        self,\n        inputs: dict[str, Any],\n        run_manager: CallbackManagerForChainRun | None = None,\n    ) -> dict[str, list]:\n        docs = self.text_splitter.create_documents([inputs[self.input_key]])\n        results = self.llm_chain.generate(\n            [{\"text\": d.page_content} for d in docs],\n            run_manager=run_manager,\n        )\n        qa = [json.loads(res[0].text) for res in results.generations]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py",
      "line_number": 23,
      "code_snippet": "    alternative=(\n        \"example in API reference with more detail: \"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_generation.base.QAGenerationChain.html\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py_116_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in '_call'",
      "description": "Function '_call' on line 116 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py",
      "line_number": 116,
      "code_snippet": "        return [self.output_key]\n\n    def _call(\n        self,\n        inputs: dict[str, Any],\n        run_manager: CallbackManagerForChainRun | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py_116_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_call'",
      "description": "Function '_call' on line 116 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/qa_generation/base.py",
      "line_number": 116,
      "code_snippet": "        return [self.output_key]\n\n    def _call(\n        self,\n        inputs: dict[str, Any],\n        run_manager: CallbackManagerForChainRun | None = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/principles.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/principles.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Constitutional principles.\n\nConstitutional principles from https://arxiv.org/pdf/2212.08073.pdf (Bai et al. 2022)\nUnifiedObjectives v0.2 principles (\"uo-*\") adapted from https://examine.dev/docs/Unified_objectives.pdf (Samwald et al. 2023).\n\"\"\"\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/principles.py_4_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 4. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/principles.py",
      "line_number": 4,
      "code_snippet": "\nConstitutional principles from https://arxiv.org/pdf/2212.08073.pdf (Bai et al. 2022)\nUnifiedObjectives v0.2 principles (\"uo-*\") adapted from https://examine.dev/docs/Unified_objectives.pdf (Samwald et al. 2023).\n\"\"\"\n\nfrom langchain_classic.chains.constitutional_ai.models import ConstitutionalPrinciple",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/base.py_25_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 25. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/constitutional_ai/base.py",
      "line_number": 25,
      "code_snippet": "        \"This class is deprecated and will be removed in langchain 1.0. \"\n        \"See API reference for replacement: \"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/__init__.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/__init__.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Implement a GPT-3 driven browser.\n\nHeavily influenced from https://github.com/nat/natbot\n\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/prompt.py_67_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 67. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/prompt.py",
      "line_number": 67,
      "code_snippet": "------------------\nOBJECTIVE: Find a 2 bedroom house for sale in Anchorage AK for under $750k\nCURRENT URL: https://www.google.com/\nYOUR COMMAND:\nTYPESUBMIT 8 \"anchorage redfin\"\n==================================================",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/prompt.py_96_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 96. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/prompt.py",
      "line_number": 96,
      "code_snippet": "------------------\nOBJECTIVE: Make a reservation for 4 at Dorsia at 8pm\nCURRENT URL: https://www.google.com/\nYOUR COMMAND:\nTYPESUBMIT 8 \"dorsia nyc opentable\"\n==================================================",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/prompt.py_123_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 123. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/prompt.py",
      "line_number": 123,
      "code_snippet": "------------------\nOBJECTIVE: Make a reservation for 4 for dinner at Dorsia in New York City at 8pm\nCURRENT URL: https://www.opentable.com/\nYOUR COMMAND:\nTYPESUBMIT 12 \"dorsia new york city\"\n==================================================",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/base.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/base.py",
      "line_number": 43,
      "code_snippet": "        that hosts this chain.\n\n        See https://docs.langchain.com/oss/python/security-policy for more information.\n\n    Example:\n        ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py_61_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 61. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py",
      "line_number": 61,
      "code_snippet": "        the application.\n\n        See https://docs.langchain.com/oss/python/security-policy for more information.\n    \"\"\"\n\n    def __init__(self) -> None:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py_87_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 87. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py",
      "line_number": 87,
      "code_snippet": "        Args:\n            url: The URL to navigate to. If it does not contain a scheme, it will be\n                prefixed with \"http://\".\n        \"\"\"\n        self.page.goto(url=url if \"://\" in url else \"http://\" + url)\n        self.client = self.page.context.new_cdp_session(self.page)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py_89_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 89. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/natbot/crawler.py",
      "line_number": 89,
      "code_snippet": "                prefixed with \"http://\".\n        \"\"\"\n        self.page.goto(url=url if \"://\" in url else \"http://\" + url)\n        self.client = self.page.context.new_cdp_session(self.page)\n        self.page_element_buffer = {}\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/open_meteo_docs.py_1_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/open_meteo_docs.py",
      "line_number": 1,
      "code_snippet": "OPEN_METEO_DOCS = \"\"\"BASE URL: https://api.open-meteo.com/\n\nAPI Documentation\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/podcast_docs.py_2_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 2. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/podcast_docs.py",
      "line_number": 2,
      "code_snippet": "PODCAST_DOCS = \"\"\"API documentation:\nEndpoint: https://listen-api.listennotes.com/api/v2\nGET /search\n\nThis API is for searching podcasts or episodes.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/tmdb_docs.py_2_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 2. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/tmdb_docs.py",
      "line_number": 2,
      "code_snippet": "TMDB_DOCS = \"\"\"API documentation:\nEndpoint: https://api.themoviedb.org/3\nGET /search/movie\n\nThis API is for searching movies.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/news_docs.py_2_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 2. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/news_docs.py",
      "line_number": 2,
      "code_snippet": "NEWS_DOCS = \"\"\"API documentation:\nEndpoint: https://newsapi.org\nTop headlines /v2/top-headlines\n\nThis endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py_64_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 64. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py",
      "line_number": 64,
      "code_snippet": "            \"This class is deprecated and will be removed in langchain 1.0. \"\n            \"See API reference for replacement: \"\n            \"https://api.python.langchain.com/en/latest/chains/langchain.chains.api.base.APIChain.html\"\n        ),\n        removal=\"1.0\",\n    )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py_83_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 83. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py",
      "line_number": 83,
      "code_snippet": "            what network access it has.\n\n            See https://docs.langchain.com/oss/python/security-policy for more\n            information.\n\n        !!! note",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py_125_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 125. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py",
      "line_number": 125,
      "code_snippet": "        ALLOW_DANGEROUS_REQUESTS = True\n\n        # Subset of spec for https://jsonplaceholder.typicode.com\n        api_spec = \\\"\\\"\\\"\n        openapi: 3.0.0\n        info:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py_132_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 132. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py",
      "line_number": 132,
      "code_snippet": "          version: 1.0.0\n        servers:\n          - url: https://jsonplaceholder.typicode.com\n        paths:\n          /posts:\n            get:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py_214_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 214. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py",
      "line_number": 214,
      "code_snippet": "        \"\"\"Use to limit the domains that can be accessed by the API chain.\n\n        * For example, to limit to just the domain `https://www.example.com`, set\n            `limit_to_domains=[\"https://www.example.com\"]`.\n        * The default value is an empty tuple, which means that no domains are\n            allowed by default. By design this will raise an error on instantiation.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py_215_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 215. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/api/base.py",
      "line_number": 215,
      "code_snippet": "\n        * For example, to limit to just the domain `https://www.example.com`, set\n            `limit_to_domains=[\"https://www.example.com\"]`.\n        * The default value is an empty tuple, which means that no domains are\n            allowed by default. By design this will raise an error on instantiation.\n        * Use a None if you want to allow all domains by default -- this is not",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_checker/__init__.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_checker/__init__.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Chain that tries to verify assumptions before answering a question.\n\nHeavily borrowed from https://github.com/jagilley/fact-checker\n\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_checker/base.py_71_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 71. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_checker/base.py",
      "line_number": 71,
      "code_snippet": "        \"See LangGraph guides for a variety of self-reflection and corrective \"\n        \"strategies for question-answering and other tasks: \"\n        \"https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_bash/__init__.py_5_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 5. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_bash/__init__.py",
      "line_number": 5,
      "code_snippet": "    msg = (\n        \"This module has been moved to langchain-experimental. \"\n        \"For more details: https://github.com/langchain-ai/langchain/discussions/11352.\"\n        \"To access this code, install it with `pip install langchain-experimental`.\"\n        \"`from langchain_experimental.llm_bash.base \"\n        \"import LLMBashChain`\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_math/__init__.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_math/__init__.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Chain that interprets a prompt and executes python code to do math.\n\nHeavily borrowed from https://replit.com/@amasad/gptpy?v=1#main.py\n\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_math/base.py_29_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 29. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_math/base.py",
      "line_number": 29,
      "code_snippet": "        \"This class is deprecated and will be removed in langchain 1.0. \"\n        \"See API reference for replacement: \"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_math.base.LLMMathChain.html\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/stuff.py_110_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 110. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/stuff.py",
      "line_number": 110,
      "code_snippet": "        \"This class is deprecated. Use the `create_stuff_documents_chain` constructor \"\n        \"instead. See migration guide here: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\"\n    ),\n)\nclass StuffDocumentsChain(BaseCombineDocumentsChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/reduce.py_137_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 137. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/reduce.py",
      "line_number": 137,
      "code_snippet": "        \"This class is deprecated. Please see the migration guide here for \"\n        \"a recommended replacement: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\"\n    ),\n)\nclass ReduceDocumentsChain(BaseCombineDocumentsChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/refine.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/refine.py",
      "line_number": 30,
      "code_snippet": "        \"This class is deprecated. Please see the migration guide here for \"\n        \"a recommended replacement: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/refine_docs_chain/\"\n    ),\n)\nclass RefineDocumentsChain(BaseCombineDocumentsChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/map_reduce.py_26_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 26. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/map_reduce.py",
      "line_number": 26,
      "code_snippet": "        \"This class is deprecated. Please see the migration guide here for \"\n        \"a recommended replacement: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\"\n    ),\n)\nclass MapReduceDocumentsChain(BaseCombineDocumentsChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/map_rerank.py_27_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 27. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/map_rerank.py",
      "line_number": 27,
      "code_snippet": "        \"This class is deprecated. Please see the migration guide here for \"\n        \"a recommended replacement: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain/\"\n    ),\n)\nclass MapRerankDocumentsChain(BaseCombineDocumentsChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/base.py_172_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 172. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/combine_documents/base.py",
      "line_number": 172,
      "code_snippet": "    alternative=(\n        \"example in API reference with more detail: \"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.base.AnalyzeDocumentChain.html\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py_268_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 268. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py",
      "line_number": 268,
      "code_snippet": "    This class is deprecated. See below for an example implementation using\n    `create_retrieval_chain`. Additional walkthroughs can be found at\n    https://python.langchain.com/docs/use_cases/question_answering/chat_history\n\n    ```python\n    from langchain_classic.chains import (",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_summarization_checker/base.py_74_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 74. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/llm_summarization_checker/base.py",
      "line_number": 74,
      "code_snippet": "        \"See LangGraph guides for a variety of self-reflection and corrective \"\n        \"strategies for question-answering and other tasks: \"\n        \"https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/\"\n    ),\n    removal=\"1.0\",\n)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/__init__.py_1_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/__init__.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Adapted from https://github.com/jzbjyb/FLARE.\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py_250",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'from_llm' on line 250 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
      "line_number": 250,
      "code_snippet": "    def from_llm(\n        cls,\n        llm: BaseLanguageModel | None,\n        max_generation_len: int = 32,\n        **kwargs: Any,\n    ) -> FlareChain:\n        \"\"\"Creates a FlareChain from a language model.\n\n        Args:\n            llm: Language model to use.\n            max_generation_len: Maximum length of the generated response.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py_103_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 103. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
      "line_number": 103,
      "code_snippet": "    and a response generator.\n\n    See [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983) paper.\n    \"\"\"\n\n    question_generator_chain: Runnable",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py_250_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'from_llm'",
      "description": "Function 'from_llm' on line 250 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
      "line_number": 250,
      "code_snippet": "\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel | None,\n        max_generation_len: int = 32,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py_250_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'from_llm'",
      "description": "Function 'from_llm' on line 250 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/flare/base.py",
      "line_number": 250,
      "code_snippet": "\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel | None,\n        max_generation_len: int = 32,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/conversation/base.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/conversation/base.py",
      "line_number": 23,
      "code_snippet": "\n    This class is deprecated in favor of `RunnableWithMessageHistory`. Please refer\n    to this tutorial for more detail: https://python.langchain.com/docs/tutorials/chatbot/\n\n    `RunnableWithMessageHistory` offers several benefits, including:\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/router/multi_prompt.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/router/multi_prompt.py",
      "line_number": 30,
      "code_snippet": "    message=(\n        \"Please see migration guide here for recommended implementation: \"\n        \"https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\"\n    ),\n)\nclass MultiPromptChain(MultiRouteChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/router/llm_router.py_30_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 30. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/chains/router/llm_router.py",
      "line_number": 30,
      "code_snippet": "        \"Use RunnableLambda to select from multiple prompt templates. See example \"\n        \"in API reference: \"\n        \"https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html\"\n    ),\n)\nclass LLMRouterChain(RouterChain):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/agents/trajectory_eval_chain.py_102_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 102. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/agents/trajectory_eval_chain.py",
      "line_number": 102,
      "code_snippet": "    the sequence of actions taken and their outcomes.\n    Based on the paper \"ReAct: Synergizing Reasoning and Acting in Language Models\"\n    (https://arxiv.org/abs/2210.03629)\n\n    Example:\n    ```python",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/scoring/eval_chain.py_240",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'from_llm' on line 240 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/scoring/eval_chain.py",
      "line_number": 240,
      "code_snippet": "    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        *,\n        prompt: PromptTemplate | None = None,\n        criteria: CRITERIA_TYPE | str | None = None,\n        normalize_by: float | None = None,\n        **kwargs: Any,\n    ) -> ScoreStringEvalChain:\n        \"\"\"Initialize the ScoreStringEvalChain from an LLM.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/scoring/eval_chain.py_240_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'from_llm'",
      "description": "Function 'from_llm' on line 240 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/scoring/eval_chain.py",
      "line_number": 240,
      "code_snippet": "\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        *,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/scoring/prompt.py_5_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 5. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/scoring/prompt.py",
      "line_number": 5,
      "code_snippet": "This prompt is used to score the responses and evaluate how it follows the instructions\nand answers the question. The prompt is based on the paper from\nZheng, et. al. https://arxiv.org/abs/2306.05685\n\"\"\"\n\nfrom langchain_core.prompts.chat import ChatPromptTemplate",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/comparison/eval_chain.py_240",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'from_llm' on line 240 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/comparison/eval_chain.py",
      "line_number": 240,
      "code_snippet": "    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        *,\n        prompt: PromptTemplate | None = None,\n        criteria: CRITERIA_TYPE | str | None = None,\n        **kwargs: Any,\n    ) -> PairwiseStringEvalChain:\n        \"\"\"Initialize the PairwiseStringEvalChain from an LLM.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/comparison/prompt.py_5_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 5. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/comparison/prompt.py",
      "line_number": 5,
      "code_snippet": "This prompt is used to compare two responses and evaluate which one best follows the instructions\nand answers the question. The prompt is based on the paper from\nZheng, et. al. https://arxiv.org/abs/2306.05685\n\"\"\"  # noqa: E501\n\nfrom langchain_core.prompts.chat import ChatPromptTemplate",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/criteria/prompt.py_1_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/evaluation/criteria/prompt.py",
      "line_number": 1,
      "code_snippet": "# Credit to https://github.com/openai/evals/tree/main\n\nfrom langchain_core.prompts import PromptTemplate\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/__init__.py_3_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 3. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/__init__.py",
      "line_number": 3,
      "code_snippet": "\"\"\"Chain that does self ask with search.\n\nHeavily borrowed from https://github.com/ofirpress/self-ask\n\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py_97",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_self_ask_with_search_agent' on line 97 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py",
      "line_number": 97,
      "code_snippet": "def create_self_ask_with_search_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,\n) -> Runnable:\n    \"\"\"Create an agent that uses self-ask with search prompting.\n\n    Args:\n        llm: LLM to use as the agent.\n        tools: List of tools. Should just be of length 1, with that tool having\n            name `Intermediate Answer`",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py_97_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_self_ask_with_search_agent'",
      "description": "Function 'create_self_ask_with_search_agent' on line 97 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py",
      "line_number": 97,
      "code_snippet": "\n\ndef create_self_ask_with_search_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py_97_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_self_ask_with_search_agent'",
      "description": "Function 'create_self_ask_with_search_agent' on line 97 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py",
      "line_number": 97,
      "code_snippet": "\n\ndef create_self_ask_with_search_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py_17",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_openai_tools_agent' on line 17 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py",
      "line_number": 17,
      "code_snippet": "def create_openai_tools_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,\n    strict: bool | None = None,  # noqa: FBT001\n) -> Runnable:\n    \"\"\"Create an agent that uses OpenAI tools.\n\n    Args:\n        llm: LLM to use as the agent.\n        tools: Tools this agent has access to.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py_100_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'create_openai_tools_agent'",
      "description": "Function 'create_openai_tools_agent' on line 17 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py",
      "line_number": 100,
      "code_snippet": "        raise ValueError(msg)\n\n    llm_with_tools = llm.bind(\n        tools=[convert_to_openai_tool(tool, strict=strict) for tool in tools],\n    )\n",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py_17_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_openai_tools_agent'",
      "description": "Function 'create_openai_tools_agent' on line 17 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py",
      "line_number": 17,
      "code_snippet": "\n\ndef create_openai_tools_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py_17_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_openai_tools_agent'",
      "description": "Function 'create_openai_tools_agent' on line 17 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py",
      "line_number": 17,
      "code_snippet": "\n\ndef create_openai_tools_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py_17_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_openai_tools_agent'",
      "description": "Function 'create_openai_tools_agent' on line 17 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_tools/base.py",
      "line_number": 17,
      "code_snippet": "\n\ndef create_openai_tools_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py_75",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration",
      "description": "Function 'save_context' on line 75 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py",
      "line_number": 75,
      "code_snippet": "    def save_context(self, inputs: dict[str, Any], outputs: dict[str, Any]) -> None:\n        \"\"\"Save context from this conversation to buffer. Pruned.\n\n        Args:\n            inputs: Inputs to the agent.\n            outputs: Outputs from the agent.\n        \"\"\"\n        input_str, output_str = self._get_input_output(inputs, outputs)\n        self.chat_memory.add_messages(input_str)  # type: ignore[arg-type]\n        format_to_messages = (\n            format_to_tool_messages",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py_75_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'save_context'",
      "description": "Function 'save_context' on line 75 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py",
      "line_number": 75,
      "code_snippet": "        return {self.memory_key: final_buffer}\n\n    def save_context(self, inputs: dict[str, Any], outputs: dict[str, Any]) -> None:\n        \"\"\"Save context from this conversation to buffer. Pruned.\n\n        Args:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_125",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.invoke' is used in 'SELECT' on line 125 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 125,
      "code_snippet": "        if with_functions:\n            predicted_message = self.llm.invoke(\n                messages,\n                functions=self.functions,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_287",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_openai_functions_agent' on line 287 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 287,
      "code_snippet": "def create_openai_functions_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,\n) -> Runnable:\n    \"\"\"Create an agent that uses OpenAI function calling.\n\n    Args:\n        llm: LLM to use as the agent. Should work with OpenAI function calling,\n            so either be an OpenAI model that supports that or a wrapper of\n            a different model that adds in equivalent support.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_96",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'plan' on line 96 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 96,
      "code_snippet": "    def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        with_functions: bool = True,  # noqa: FBT001,FBT002\n        **kwargs: Any,\n    ) -> AgentAction | AgentFinish:\n        \"\"\"Given input, decided what to do.\n\n        Args:\n            intermediate_steps: Steps the LLM has taken to date,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_372_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'create_openai_functions_agent'",
      "description": "Function 'create_openai_functions_agent' on line 287 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 372,
      "code_snippet": "        )\n        raise ValueError(msg)\n    llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])\n    return (\n        RunnablePassthrough.assign(\n            agent_scratchpad=lambda x: format_to_openai_function_messages(",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_125_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'plan'",
      "description": "Function 'plan' on line 96 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 125,
      "code_snippet": "        messages = prompt.to_messages()\n        if with_functions:\n            predicted_message = self.llm.invoke(\n                messages,\n                functions=self.functions,\n                callbacks=callbacks,",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_287_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_openai_functions_agent'",
      "description": "Function 'create_openai_functions_agent' on line 287 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 287,
      "code_snippet": "\n\ndef create_openai_functions_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_287_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_openai_functions_agent'",
      "description": "Function 'create_openai_functions_agent' on line 287 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 287,
      "code_snippet": "\n\ndef create_openai_functions_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_287_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_openai_functions_agent'",
      "description": "Function 'create_openai_functions_agent' on line 287 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 287,
      "code_snippet": "\n\ndef create_openai_functions_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py_96_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'plan'",
      "description": "Function 'plan' on line 96 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py",
      "line_number": 96,
      "code_snippet": "        return [dict(convert_to_openai_function(t)) for t in self.tools]\n\n    def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py_228",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.invoke' is used in 'SELECT' on line 228 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py",
      "line_number": 228,
      "code_snippet": "        messages = prompt.to_messages()\n        predicted_message = self.llm.invoke(\n            messages,\n            functions=self.functions,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py_204",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'plan' on line 204 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py",
      "line_number": 204,
      "code_snippet": "    def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> list[AgentAction] | AgentFinish:\n        \"\"\"Given input, decided what to do.\n\n        Args:\n            intermediate_steps: Steps the LLM has taken to date,\n                along with observations.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py_228_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'plan'",
      "description": "Function 'plan' on line 204 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py",
      "line_number": 228,
      "code_snippet": "        prompt = self.prompt.format_prompt(**full_inputs)\n        messages = prompt.to_messages()\n        predicted_message = self.llm.invoke(\n            messages,\n            functions=self.functions,\n            callbacks=callbacks,",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py_204_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'plan'",
      "description": "Function 'plan' on line 204 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py",
      "line_number": 204,
      "code_snippet": "        return [tool_selection]\n\n    def plan(\n        self,\n        intermediate_steps: list[tuple[AgentAction, str]],\n        callbacks: Callbacks = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py_18",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_tool_calling_agent' on line 18 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py",
      "line_number": 18,
      "code_snippet": "def create_tool_calling_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,\n    *,\n    message_formatter: MessageFormatter = format_to_tool_messages,\n) -> Runnable:\n    \"\"\"Create an agent that uses tools.\n\n    Args:\n        llm: LLM to use as the agent.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py_18_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_tool_calling_agent'",
      "description": "Function 'create_tool_calling_agent' on line 18 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py",
      "line_number": 18,
      "code_snippet": "\n\ndef create_tool_calling_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py_18_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_tool_calling_agent'",
      "description": "Function 'create_tool_calling_agent' on line 18 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py",
      "line_number": 18,
      "code_snippet": "\n\ndef create_tool_calling_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py_18_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_tool_calling_agent'",
      "description": "Function 'create_tool_calling_agent' on line 18 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py",
      "line_number": 18,
      "code_snippet": "\n\ndef create_tool_calling_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py_166",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'create_structured_chat_agent' on line 166 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py",
      "line_number": 166,
      "code_snippet": "def create_structured_chat_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,\n    tools_renderer: ToolsRenderer = render_text_description_and_args,\n    *,\n    stop_sequence: bool | list[str] = True,\n) -> Runnable:\n    \"\"\"Create an agent aimed at supporting tools with multiple inputs.\n\n    Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py_166_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_structured_chat_agent'",
      "description": "Function 'create_structured_chat_agent' on line 166 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py",
      "line_number": 166,
      "code_snippet": "\n\ndef create_structured_chat_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py_166_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_structured_chat_agent'",
      "description": "Function 'create_structured_chat_agent' on line 166 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py",
      "line_number": 166,
      "code_snippet": "\n\ndef create_structured_chat_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py_166_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_structured_chat_agent'",
      "description": "Function 'create_structured_chat_agent' on line 166 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/structured_chat/base.py",
      "line_number": 166,
      "code_snippet": "\n\ndef create_structured_chat_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/json_chat/base.py_181",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'llm.bind' is used in 'Response' on line 181 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/json_chat/base.py",
      "line_number": 181,
      "code_snippet": "        stop = [\"\\nObservation\"] if stop_sequence is True else stop_sequence\n        llm_to_use = llm.bind(stop=stop)\n    else:\n        llm_to_use = llm",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/json_chat/base.py_14",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'create_json_chat_agent' on line 14 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/json_chat/base.py",
      "line_number": 14,
      "code_snippet": "def create_json_chat_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,\n    stop_sequence: bool | list[str] = True,  # noqa: FBT001,FBT002\n    tools_renderer: ToolsRenderer = render_text_description,\n    template_tool_response: str = TEMPLATE_TOOL_RESPONSE,\n) -> Runnable:\n    r\"\"\"Create an agent that uses JSON to format its logic, build for Chat Models.\n\n    Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/json_chat/base.py_14_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_json_chat_agent'",
      "description": "Function 'create_json_chat_agent' on line 14 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/json_chat/base.py",
      "line_number": 14,
      "code_snippet": "\n\ndef create_json_chat_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: ChatPromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py_115",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_xml_agent' on line 115 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py",
      "line_number": 115,
      "code_snippet": "def create_xml_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,\n    tools_renderer: ToolsRenderer = render_text_description,\n    *,\n    stop_sequence: bool | list[str] = True,\n) -> Runnable:\n    r\"\"\"Create an agent that uses XML to format its logic.\n\n    Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py_115_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_xml_agent'",
      "description": "Function 'create_xml_agent' on line 115 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py",
      "line_number": 115,
      "code_snippet": "\n\ndef create_xml_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py_115_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_xml_agent'",
      "description": "Function 'create_xml_agent' on line 115 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py",
      "line_number": 115,
      "code_snippet": "\n\ndef create_xml_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py_115_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_xml_agent'",
      "description": "Function 'create_xml_agent' on line 115 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/xml/base.py",
      "line_number": 115,
      "code_snippet": "\n\ndef create_xml_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_74",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_get_openai_client' on line 74 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 74,
      "code_snippet": "def _get_openai_client() -> openai.OpenAI:\n    try:\n        import openai\n\n        return openai.OpenAI()\n    except ImportError as e:\n        msg = \"Unable to import openai, please install with `pip install openai`.\"\n        raise ImportError(msg) from e\n    except AttributeError as e:\n        msg = (\n            \"Please make sure you are using a v1.1-compatible version of openai. You \"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_90",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_get_openai_async_client' on line 90 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 90,
      "code_snippet": "def _get_openai_async_client() -> openai.AsyncOpenAI:\n    try:\n        import openai\n\n        return openai.AsyncOpenAI()\n    except ImportError as e:\n        msg = \"Unable to import openai, please install with `pip install openai`.\"\n        raise ImportError(msg) from e\n    except AttributeError as e:\n        msg = (\n            \"Please make sure you are using a v1.1-compatible version of openai. You \"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_245",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_validate_async_client' on line 245 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 245,
      "code_snippet": "    def _validate_async_client(self) -> Self:\n        if self.async_client is None:\n            import openai\n\n            api_key = self.client.api_key\n            self.async_client = openai.AsyncOpenAI(api_key=api_key)\n        return self\n\n    @classmethod\n    def create_assistant(\n        cls,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_589",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_get_response' on line 589 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 589,
      "code_snippet": "    def _get_response(self, run: Any) -> Any:\n        # TODO: Pagination\n\n        if run.status == \"completed\":\n            import openai\n\n            major_version = int(openai.version.VERSION.split(\".\")[0])\n            minor_version = int(openai.version.VERSION.split(\".\")[1])\n            version_gte_1_14 = (major_version > 1) or (\n                major_version == 1 and minor_version >= 14  # noqa: PLR2004\n            )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_589_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_get_response'",
      "description": "Function '_get_response' on line 589 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 589,
      "code_snippet": "        )\n\n    def _get_response(self, run: Any) -> Any:\n        # TODO: Pagination\n\n        if run.status == \"completed\":",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_74_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_openai_client'",
      "description": "Function '_get_openai_client' on line 74 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 74,
      "code_snippet": "\n\ndef _get_openai_client() -> openai.OpenAI:\n    try:\n        import openai\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_90_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_openai_async_client'",
      "description": "Function '_get_openai_async_client' on line 90 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 90,
      "code_snippet": "\n\ndef _get_openai_async_client() -> openai.AsyncOpenAI:\n    try:\n        import openai\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py_589_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_response'",
      "description": "Function '_get_response' on line 589 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/openai_assistant/base.py",
      "line_number": 589,
      "code_snippet": "        )\n\n    def _get_response(self, run: Any) -> Any:\n        # TODO: Pagination\n\n        if run.status == \"completed\":",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/__init__.py_14_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 14. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/__init__.py",
      "line_number": 14,
      "code_snippet": "whether permissions of the given toolkit are appropriate for the application.\n\nSee https://docs.langchain.com/oss/python/security-policy for more information.\n\"\"\"\n\nfrom pathlib import Path",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/__init__.py_126_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 126. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/__init__.py",
      "line_number": 126,
      "code_snippet": "        msg = (\n            f\"{name} has been moved to langchain_experimental. \"\n            \"See https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"for more information.\\n\"\n            f\"Please update your import statement from: `{old_path}` to `{new_path}`.\"\n        )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/__init__.py_1_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/__init__.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf.\"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py_16",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'create_react_agent' on line 16 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py",
      "line_number": 16,
      "code_snippet": "def create_react_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,\n    output_parser: AgentOutputParser | None = None,\n    tools_renderer: ToolsRenderer = render_text_description,\n    *,\n    stop_sequence: bool | list[str] = True,\n) -> Runnable:\n    r\"\"\"Create an agent that uses ReAct prompting.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py_28_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 28. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py",
      "line_number": 28,
      "code_snippet": "\n    Based on paper \"ReAct: Synergizing Reasoning and Acting in Language Models\"\n    (https://arxiv.org/abs/2210.03629)\n\n    !!! warning\n       This implementation is based on the foundational ReAct paper but is older and",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py",
      "line_number": 36,
      "code_snippet": "       `create_react_agent` function from the LangGraph library.\n       See the\n       [reference doc](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)\n       for more information.\n\n    Args:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py_16_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create_react_agent'",
      "description": "Function 'create_react_agent' on line 16 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py",
      "line_number": 16,
      "code_snippet": "\n\ndef create_react_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py_16_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_react_agent'",
      "description": "Function 'create_react_agent' on line 16 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py",
      "line_number": 16,
      "code_snippet": "\n\ndef create_react_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py_16_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'create_react_agent'",
      "description": "Function 'create_react_agent' on line 16 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/agent.py",
      "line_number": 16,
      "code_snippet": "\n\ndef create_react_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/base.py_1_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/react/base.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Chain that implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf.\"\"\"\n\nfrom __future__ import annotations\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/python/__init__.py_11_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 11. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/python/__init__.py",
      "line_number": 11,
      "code_snippet": "            \"This agent relies on python REPL tool under the hood, so to use it \"\n            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/python/__init__.py_12_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 12. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/python/__init__.py",
      "line_number": 12,
      "code_snippet": "            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"\n            f\"`langchain_classic.agents.agent_toolkits.python.{name}` to \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/csv/__init__.py_11_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 11. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/csv/__init__.py",
      "line_number": 11,
      "code_snippet": "            \"This agent relies on python REPL tool under the hood, so to use it \"\n            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/csv/__init__.py_12_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 12. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/csv/__init__.py",
      "line_number": 12,
      "code_snippet": "            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n \"\n            f\"`langchain_classic.agents.agent_toolkits.csv.{name}` to \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/xorbits/__init__.py_11_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 11. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/xorbits/__init__.py",
      "line_number": 11,
      "code_snippet": "            \"This agent relies on python REPL tool under the hood, so to use it \"\n            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/xorbits/__init__.py_12_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 12. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/xorbits/__init__.py",
      "line_number": 12,
      "code_snippet": "            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"\n            f\"`langchain_classic.agents.agent_toolkits.xorbits.{name}` to \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/spark/__init__.py_11_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 11. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/spark/__init__.py",
      "line_number": 11,
      "code_snippet": "            \"This agent relies on python REPL tool under the hood, so to use it \"\n            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/spark/__init__.py_12_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 12. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/spark/__init__.py",
      "line_number": 12,
      "code_snippet": "            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"\n            f\"`langchain_classic.agents.agent_toolkits.spark.{name}` to \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/pandas/__init__.py_11_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 11. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/pandas/__init__.py",
      "line_number": 11,
      "code_snippet": "            \"This agent relies on python REPL tool under the hood, so to use it \"\n            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/pandas/__init__.py_12_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 12. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/pandas/__init__.py",
      "line_number": 12,
      "code_snippet": "            \"safely please sandbox the python REPL. \"\n            \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n            \"and https://github.com/langchain-ai/langchain/discussions/11680\"\n            \"To keep using this code as is, install langchain_experimental and \"\n            \"update your import statement from:\\n\"\n            f\"`langchain_classic.agents.agent_toolkits.pandas.{name}` to \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py",
      "line_number": 31,
      "code_snippet": "        \"tool-calling, persistence of state, and human-in-the-loop workflows. \"\n        \"See API reference for this function for a replacement implementation: \"\n        \"https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent.html \"  # noqa: E501\n        \"Read more here on how to create agents that query vector stores: \"\n        \"https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#agents\"\n    ),",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py_33_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 33. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py",
      "line_number": 33,
      "code_snippet": "        \"https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent.html \"  # noqa: E501\n        \"Read more here on how to create agents that query vector stores: \"\n        \"https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#agents\"\n    ),\n)\ndef create_vectorstore_agent(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py_128_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 128. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py",
      "line_number": 128,
      "code_snippet": "        \"tool-calling, persistence of state, and human-in-the-loop workflows. \"\n        \"See API reference for this function for a replacement implementation: \"\n        \"https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent.html \"  # noqa: E501\n        \"Read more here on how to create agents that query vector stores: \"\n        \"https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#agents\"\n    ),",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py_130_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 130. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/agents/agent_toolkits/vectorstore/base.py",
      "line_number": 130,
      "code_snippet": "        \"https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent.html \"  # noqa: E501\n        \"Read more here on how to create agents that query vector stores: \"\n        \"https://python.langchain.com/docs/how_to/qa_chat_history_how_to/#agents\"\n    ),\n)\ndef create_vectorstore_router_agent(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_903",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'prompt_or_messages' directly embedded in LLM prompt",
      "description": "The function '_run_llm' embeds user input ('prompt_or_messages') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 903,
      "code_snippet": "",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_861",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_run_llm' on line 861 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 861,
      "code_snippet": "def _run_llm(\n    llm: BaseLanguageModel,\n    inputs: dict[str, Any],\n    callbacks: Callbacks,\n    *,\n    tags: list[str] | None = None,\n    input_mapper: Callable[[dict], Any] | None = None,\n    metadata: dict[str, Any] | None = None,\n) -> str | BaseMessage:\n    \"\"\"Run the language model on the example.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_1512",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'run_on_dataset' on line 1512 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 1512,
      "code_snippet": "def run_on_dataset(\n    client: Client | None,\n    dataset_name: str,\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,\n    *,\n    evaluation: smith_eval.RunEvalConfig | None = None,\n    dataset_version: datetime | str | None = None,\n    concurrency_level: int = 5,\n    project_name: str | None = None,\n    project_metadata: dict[str, Any] | None = None,\n    verbose: bool = False,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_600_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 600. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 600,
      "code_snippet": "            \" rather than the output of a single model.\"\n            \" Did you mean to use a StringEvaluator instead?\"\n            \"\\nSee: https://python.langchain.com/docs/guides/evaluation/string/\"\n        )\n        raise NotImplementedError(msg)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_1331_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 1331. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 1331,
      "code_snippet": "    \"    return input_mapper | my_chain\\n\"\n    \"run_on_dataset(..., llm_or_chain_factory=construct_chain)\\n\"\n    \"(See https://api.python.langchain.com/en/latest/schema/\"\n    \"langchain.schema.runnable.base.RunnableLambda.html)\"\n)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_861_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_run_llm'",
      "description": "Function '_run_llm' on line 861 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 861,
      "code_snippet": "\n\ndef _run_llm(\n    llm: BaseLanguageModel,\n    inputs: dict[str, Any],\n    callbacks: Callbacks,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_1512_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_on_dataset'",
      "description": "Function 'run_on_dataset' on line 1512 makes critical security, data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 1512,
      "code_snippet": "\n\ndef run_on_dataset(\n    client: Client | None,\n    dataset_name: str,\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_861_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run_llm'",
      "description": "Function '_run_llm' on line 861 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 861,
      "code_snippet": "\n\ndef _run_llm(\n    llm: BaseLanguageModel,\n    inputs: dict[str, Any],\n    callbacks: Callbacks,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_1512_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_on_dataset'",
      "description": "Function 'run_on_dataset' on line 1512 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 1512,
      "code_snippet": "\n\ndef run_on_dataset(\n    client: Client | None,\n    dataset_name: str,\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_861_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run_llm'",
      "description": "Function '_run_llm' on line 861 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 861,
      "code_snippet": "\n\ndef _run_llm(\n    llm: BaseLanguageModel,\n    inputs: dict[str, Any],\n    callbacks: Callbacks,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py_1512_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'run_on_dataset'",
      "description": "Function 'run_on_dataset' on line 1512 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py",
      "line_number": 1512,
      "code_snippet": "\n\ndef run_on_dataset(\n    client: Client | None,\n    dataset_name: str,\n    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/__init__.py_7_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 7. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/smith/evaluation/__init__.py",
      "line_number": 7,
      "code_snippet": "\nFor more information on the LangSmith API, see the\n[LangSmith API documentation](https://docs.langchain.com/langsmith/home).\n\n**Example**\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/tools/python/__init__.py_9_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 9. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/tools/python/__init__.py",
      "line_number": 9,
      "code_snippet": "        \"This tool has access to a python REPL. \"\n        \"For best practices make sure to sandbox this tool. \"\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\n        \"To keep using this code as is, install langchain_experimental and \"\n        \"update relevant imports replacing 'langchain' with 'langchain_experimental'\"\n    )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py_102",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'from_llm' on line 102 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py",
      "line_number": 102,
      "code_snippet": "    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        *,\n        prompt: BasePromptTemplate | None = None,\n        **kwargs: Any,\n    ) -> \"LLMListwiseRerank\":\n        \"\"\"Create a LLMListwiseRerank document compressor from a language model.\n\n        Args:\n            llm: The language model to use for filtering. **Must implement",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py_43_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 43. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py",
      "line_number": 43,
      "code_snippet": "    \"\"\"Document compressor that uses `Zero-Shot Listwise Document Reranking`.\n\n    Adapted from: https://arxiv.org/pdf/2305.02156.pdf\n\n    `LLMListwiseRerank` uses a language model to rerank a list of documents based on\n    their relevance to a query.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py_102_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'from_llm'",
      "description": "Function 'from_llm' on line 102 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py",
      "line_number": 102,
      "code_snippet": "\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        *,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py_31",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'compress_documents' on line 31 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py",
      "line_number": 31,
      "code_snippet": "    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Callbacks | None = None,\n    ) -> Sequence[Document]:\n        \"\"\"Rerank documents using CrossEncoder.\n\n        Args:\n            documents: A sequence of documents to compress.\n            query: The query to use for compressing the documents.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py_31_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'compress_documents'",
      "description": "Function 'compress_documents' on line 31 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py",
      "line_number": 31,
      "code_snippet": "\n    @override\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py_31_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'compress_documents'",
      "description": "Function 'compress_documents' on line 31 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/langchain/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py",
      "line_number": 31,
      "code_snippet": "\n    @override\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    }
  ],
  "metadata": {}
}