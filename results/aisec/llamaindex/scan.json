{
  "report_type": "static_scan",
  "generated_at": "2026-01-08T18:01:56.039567Z",
  "summary": {
    "target": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core",
    "files_scanned": 504,
    "overall_score": 1.89,
    "confidence": 0.62,
    "duration_seconds": 2.446,
    "findings_count": 499,
    "severity_breakdown": {
      "CRITICAL": 130,
      "HIGH": 250,
      "MEDIUM": 30,
      "LOW": 89,
      "INFO": 0
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 6,
      "confidence": 0.53,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.53,
      "subscores": {
        "model_protection": 41,
        "extraction_defense": 35,
        "supply_chain_security": 25,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "Model registry",
        "Rate limiting",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": [
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.59,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII masking",
        "Consent withdrawal",
        "Right to access",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 1,
      "confidence": 0.42,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 50,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.77,
      "subscores": {
        "LLM01": 0,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 100,
        "LLM07": 55,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 100
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 24 critical",
        "Insecure Output Handling: 31 critical, 100 high",
        "Model Denial of Service: 75 critical, 48 high",
        "Supply Chain Vulnerabilities: 58 high",
        "Insecure Plugin Design: 3 high",
        "Excessive Agency: 13 high, 21 medium",
        "Overreliance: 28 high, 9 medium, 89 low"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context.py_18_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 18. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context.py",
      "line_number": 18,
      "code_snippet": "            \"or pass in modules to local functions/methods/interfaces.\\n\"\n            \"See the docs for updated usage/migration: \\n\"\n            \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n        )\n\n    @classmethod",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context.py_37_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 37. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context.py",
      "line_number": 37,
      "code_snippet": "            \"or pass in modules to local functions/methods/interfaces.\\n\"\n            \"See the docs for updated usage/migration: \\n\"\n            \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n        )\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context.py_47_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 47. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context.py",
      "line_number": 47,
      "code_snippet": "        \"or pass in modules to local functions/methods/interfaces.\\n\"\n        \"See the docs for updated usage/migration: \\n\"\n        \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n    )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/constants.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/constants.py",
      "line_number": 35,
      "code_snippet": "DEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/constants.py_36_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 36. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/constants.py",
      "line_number": 36,
      "code_snippet": "DEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/__init__.py_90_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 90. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/__init__.py",
      "line_number": 90,
      "code_snippet": "\n# best practices for library logging:\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\nlogging.getLogger(__name__).addHandler(NullHandler())\n\n__all__ = [",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utils.py_198_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 198. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utils.py",
      "line_number": 198,
      "code_snippet": "\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utils.py_695_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 695. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utils.py",
      "line_number": 695,
      "code_snippet": "\n        headers = {\n            \"User-Agent\": \"LlamaIndex/0.0 (https://llamaindex.ai; info@llamaindex.ai) llama-index-core/0.0\"\n        }\n        response = requests.get(url, headers=headers, timeout=(60, 60))\n        response.raise_for_status()",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/schema.py_8_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for model serialization",
      "description": "Import of 'pickle' on line 8 for model serialization. This library can execute arbitrary code during deserialization, making it vulnerable to supply chain attacks if loading untrusted models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/schema.py",
      "line_number": 8,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors for PyTorch/TensorFlow\n2. Never deserialize models from untrusted sources\n3. Scan serialized models before loading\n4. Use sandboxed environments for model loading\n5. Implement allowlists for model formats\n6. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/vector_stores/types.py_101_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 101. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/vector_stores/types.py",
      "line_number": 101,
      "code_snippet": "    converted to string before.\n\n    See: https://docs.pydantic.dev/latest/usage/types/#strict-types\n    \"\"\"\n\n    key: str",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py_84",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'prompt_str' directly embedded in LLM prompt",
      "description": "The function '_get_queries' embeds user input ('prompt_str') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py",
      "line_number": 84,
      "code_snippet": "    def _get_queries(self, original_query: str) -> List[QueryBundle]:\n        prompt_str = self.query_gen_prompt.format(\n            num_queries=self.num_queries - 1,\n            query=original_query,\n        )\n        response = self._llm.complete(prompt_str)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py_88",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.complete' is used in 'Response' on line 88 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py",
      "line_number": 88,
      "code_snippet": "        )\n        response = self._llm.complete(prompt_str)\n\n        # Strip code block and assume LLM properly put each query on a newline",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py_83",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_queries' on line 83 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py",
      "line_number": 83,
      "code_snippet": "    def _get_queries(self, original_query: str) -> List[QueryBundle]:\n        prompt_str = self.query_gen_prompt.format(\n            num_queries=self.num_queries - 1,\n            query=original_query,\n        )\n        response = self._llm.complete(prompt_str)\n\n        # Strip code block and assume LLM properly put each query on a newline\n        queries = response.text.strip(\"`\").split(\"\\n\")\n        queries = [q.strip() for q in queries if q.strip()]\n        if self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py_107_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 107. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py",
      "line_number": 107,
      "code_snippet": "\n        The original paper uses k=60 for best results:\n        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n        \"\"\"\n        k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n        fused_scores = {}",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py_83_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_queries'",
      "description": "Function '_get_queries' on line 83 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py",
      "line_number": 83,
      "code_snippet": "            ).template\n\n    def _get_queries(self, original_query: str) -> List[QueryBundle]:\n        prompt_str = self.query_gen_prompt.format(\n            num_queries=self.num_queries - 1,\n            query=original_query,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py_151",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'embed_nodes' on line 151 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py",
      "line_number": 151,
      "code_snippet": "def embed_nodes(\n    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List[float]]:\n    \"\"\"\n    Get embeddings of the given nodes, run embedding model if necessary.\n\n    Args:\n        nodes (Sequence[BaseNode]): The nodes to embed.\n        embed_model (BaseEmbedding): The embedding model to use.\n        show_progress (bool): Whether to show progress bar.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py_187",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'embed_image_nodes' on line 187 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py",
      "line_number": 187,
      "code_snippet": "def embed_image_nodes(\n    nodes: Sequence[ImageNode],\n    embed_model: MultiModalEmbedding,\n    show_progress: bool = False,\n) -> Dict[str, List[float]]:\n    \"\"\"\n    Get image embeddings of the given nodes, run image embedding model if necessary.\n\n    Args:\n        nodes (Sequence[ImageNode]): The nodes to embed.\n        embed_model (MultiModalEmbedding): The embedding model to use.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py_151_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'embed_nodes'",
      "description": "Function 'embed_nodes' on line 151 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py",
      "line_number": 151,
      "code_snippet": "\n\ndef embed_nodes(\n    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List[float]]:\n    \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py_187_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'embed_image_nodes'",
      "description": "Function 'embed_image_nodes' on line 187 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py",
      "line_number": 187,
      "code_snippet": "\n\ndef embed_image_nodes(\n    nodes: Sequence[ImageNode],\n    embed_model: MultiModalEmbedding,\n    show_progress: bool = False,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py_151_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'embed_nodes'",
      "description": "Function 'embed_nodes' on line 151 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py",
      "line_number": 151,
      "code_snippet": "\n\ndef embed_nodes(\n    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List[float]]:\n    \"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py_187_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'embed_image_nodes'",
      "description": "Function 'embed_image_nodes' on line 187 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/utils.py",
      "line_number": 187,
      "code_snippet": "\n\ndef embed_image_nodes(\n    nodes: Sequence[ImageNode],\n    embed_model: MultiModalEmbedding,\n    show_progress: bool = False,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/query_plan.py_45_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 45. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/query_plan.py",
      "line_number": 45,
      "code_snippet": "    \"\"\"\n\n    # NOTE: inspired from https://github.com/jxnl/openai_function_call/pull/3/files\n\n    id: int = Field(..., description=\"ID of the query node.\")\n    query_str: str = Field(..., description=QUERYNODE_QUERY_STR_DESC)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py_198",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'to_langchain_tool' on line 198 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py",
      "line_number": 198,
      "code_snippet": "    def to_langchain_tool(\n        self,\n        **langchain_tool_kwargs: Any,\n    ) -> \"Tool\":\n        \"\"\"To langchain tool.\"\"\"\n        from llama_index.core.bridge.langchain import Tool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return Tool.from_function(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py_213",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'to_langchain_structured_tool' on line 213 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py",
      "line_number": 213,
      "code_snippet": "    def to_langchain_structured_tool(\n        self,\n        **langchain_tool_kwargs: Any,\n    ) -> \"StructuredTool\":\n        \"\"\"To langchain structured tool.\"\"\"\n        from llama_index.core.bridge.langchain import StructuredTool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return StructuredTool.from_function(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py_198_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'to_langchain_tool'",
      "description": "Function 'to_langchain_tool' on line 198 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py",
      "line_number": 198,
      "code_snippet": "        return langchain_tool_kwargs\n\n    def to_langchain_tool(\n        self,\n        **langchain_tool_kwargs: Any,\n    ) -> \"Tool\":",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py_213_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'to_langchain_structured_tool'",
      "description": "Function 'to_langchain_structured_tool' on line 213 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/types.py",
      "line_number": 213,
      "code_snippet": "        )\n\n    def to_langchain_structured_tool(\n        self,\n        **langchain_tool_kwargs: Any,\n    ) -> \"StructuredTool\":",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py_382",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'to_langchain_tool' on line 382 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py",
      "line_number": 382,
      "code_snippet": "    def to_langchain_tool(self, **langchain_tool_kwargs: Any) -> \"Tool\":\n        \"\"\"To langchain tool.\"\"\"\n        from llama_index.core.bridge.langchain import Tool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return Tool.from_function(\n            func=self.fn,\n            coroutine=self.async_fn,\n            **langchain_tool_kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py_395",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'to_langchain_structured_tool' on line 395 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py",
      "line_number": 395,
      "code_snippet": "    def to_langchain_structured_tool(\n        self, **langchain_tool_kwargs: Any\n    ) -> \"StructuredTool\":\n        \"\"\"To langchain structured tool.\"\"\"\n        from llama_index.core.bridge.langchain import StructuredTool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return StructuredTool.from_function(\n            func=self.fn,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py_382_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'to_langchain_tool'",
      "description": "Function 'to_langchain_tool' on line 382 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py",
      "line_number": 382,
      "code_snippet": "        return default_output\n\n    def to_langchain_tool(self, **langchain_tool_kwargs: Any) -> \"Tool\":\n        \"\"\"To langchain tool.\"\"\"\n        from llama_index.core.bridge.langchain import Tool\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py_395_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'to_langchain_structured_tool'",
      "description": "Function 'to_langchain_structured_tool' on line 395 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/tools/function_tool.py",
      "line_number": 395,
      "code_snippet": "        )\n\n    def to_langchain_structured_tool(\n        self, **langchain_tool_kwargs: Any\n    ) -> \"StructuredTool\":\n        \"\"\"To langchain structured tool.\"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/pooling.py_28_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 28. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/pooling.py",
      "line_number": 28,
      "code_snippet": "    @overload\n    # TODO: Remove this `type: ignore` after the false positive problem\n    #  is addressed in mypy: https://github.com/python/mypy/issues/15683 .\n    def cls_pooling(cls, array: \"torch.Tensor\") -> \"torch.Tensor\":  # type: ignore\n        ...\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py_31",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'resolve_embed_model' on line 31 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py",
      "line_number": 31,
      "code_snippet": "def resolve_embed_model(\n    embed_model: Optional[EmbedType] = None,\n    callback_manager: Optional[CallbackManager] = None,\n) -> BaseEmbedding:\n    \"\"\"Resolve embed model.\"\"\"\n    from llama_index.core.settings import Settings\n\n    try:\n        from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings\n    except ImportError:\n        LCEmbeddings = None  # type: ignore",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py_74_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 74. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py",
      "line_number": 74,
      "code_snippet": "                \"\\nConsider using embed_model='local'.\\n\"\n                \"Visit our documentation for more embedding options: \"\n                \"https://developers.llamaindex.ai/python/framework/module_guides/\"\n                \"models/embeddings/\"\n                \"\\n******\"\n            )",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py_90_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 90. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py",
      "line_number": 90,
      "code_snippet": "            raise ImportError(\n                \"`llama-index-embeddings-clip` package not found, \"\n                \"please run `pip install llama-index-embeddings-clip` and `pip install git+https://github.com/openai/CLIP.git`\"\n            )\n\n    if isinstance(embed_model, str):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py_31_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'resolve_embed_model'",
      "description": "Function 'resolve_embed_model' on line 31 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/embeddings/utils.py",
      "line_number": 31,
      "code_snippet": "\n\ndef resolve_embed_model(\n    embed_model: Optional[EmbedType] = None,\n    callback_manager: Optional[CallbackManager] = None,\n) -> BaseEmbedding:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_203",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.chat_store.get_messages' is used in 'UPDATE' on line 203 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 203,
      "code_snippet": "        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n\n    def put(self, message: ChatMessage) -> None:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_216",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat_store.set_messages' is used in 'Response' on line 216 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 216,
      "code_snippet": "        \"\"\"Set chat history.\"\"\"\n        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_220",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat_store.delete_messages' is used in 'Response' on line 220 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 220,
      "code_snippet": "        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n\n    def get_token_count(self) -> int:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_201",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_all' on line 201 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 201,
      "code_snippet": "    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_205",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'put' on line 205 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 205,
      "code_snippet": "    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        await self.chat_store.async_add_message(self.chat_store_key, message)\n\n    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_214",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'set' on line 214 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 214,
      "code_snippet": "    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n\n    def get_token_count(self) -> int:\n        \"\"\"Returns the token count of the memory buffer (excluding the last assistant response).\"\"\"\n        return self._token_count",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_218",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'reset' on line 218 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 218,
      "code_snippet": "    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n\n    def get_token_count(self) -> int:\n        \"\"\"Returns the token count of the memory buffer (excluding the last assistant response).\"\"\"\n        return self._token_count\n\n    def _token_count_for_messages(self, messages: List[ChatMessage]) -> int:\n        \"\"\"Get token count for list of messages.\"\"\"\n        if len(messages) <= 0:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_262",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_summarize_oldest_chat_history' on line 262 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 262,
      "code_snippet": "    def _summarize_oldest_chat_history(\n        self, chat_history_to_be_summarized: List[ChatMessage]\n    ) -> ChatMessage:\n        \"\"\"\n        Use the llm to summarize the messages that do not fit into the\n        buffer.\n        \"\"\"\n        assert self.llm is not None\n\n        # Only summarize if there is new information to be summarized\n        if (",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_205_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'put'",
      "description": "Function 'put' on line 205 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 205,
      "code_snippet": "        return self.chat_store.get_messages(self.chat_store_key)\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_218_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'reset'",
      "description": "Function 'reset' on line 218 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 218,
      "code_snippet": "        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_218_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'reset'",
      "description": "Function 'reset' on line 218 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 218,
      "code_snippet": "        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_262_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_summarize_oldest_chat_history'",
      "description": "Function '_summarize_oldest_chat_history' on line 262 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 262,
      "code_snippet": "        return chat_history_full_text, chat_history_to_be_summarized\n\n    def _summarize_oldest_chat_history(\n        self, chat_history_to_be_summarized: List[ChatMessage]\n    ) -> ChatMessage:\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_201_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get_all'",
      "description": "Function 'get_all' on line 201 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 201,
      "code_snippet": "        return updated_history\n\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py_262_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_summarize_oldest_chat_history'",
      "description": "Function '_summarize_oldest_chat_history' on line 262 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
      "line_number": 262,
      "code_snippet": "        return chat_history_full_text, chat_history_to_be_summarized\n\n    def _summarize_oldest_chat_history(\n        self, chat_history_to_be_summarized: List[ChatMessage]\n    ) -> ChatMessage:\n        \"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_139",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.chat_store.set_messages' is used in 'DELETE' on line 139 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 139,
      "code_snippet": "        \"\"\"Set chat history.\"\"\"\n        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    async def aset(self, messages: List[ChatMessage]) -> None:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_148",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.chat_store.delete_messages' is used in 'DELETE' on line 148 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 148,
      "code_snippet": "        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n\n    async def areset(self) -> None:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_109",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_all' on line 109 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 109,
      "code_snippet": "    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n\n    async def aget_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return await self.chat_store.aget_messages(self.chat_store_key)\n\n    def get(self, input: Optional[str] = None, **kwargs: Any) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_117",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get' on line 117 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 117,
      "code_snippet": "    def get(self, input: Optional[str] = None, **kwargs: Any) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key, **kwargs)\n\n    async def aget(\n        self, input: Optional[str] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return await self.chat_store.aget_messages(self.chat_store_key, **kwargs)\n\n    def put(self, message: ChatMessage) -> None:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_127",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'put' on line 127 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 127,
      "code_snippet": "    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        await self.chat_store.async_add_message(self.chat_store_key, message)\n\n    def set(self, messages: List[ChatMessage]) -> None:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_137",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'set' on line 137 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 137,
      "code_snippet": "    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    async def aset(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        # ensure everything is serialized\n        await self.chat_store.aset_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_146",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'reset' on line 146 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 146,
      "code_snippet": "    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n\n    async def areset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        await self.chat_store.adelete_messages(self.chat_store_key)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_117_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'get'",
      "description": "Function 'get' on line 117 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 117,
      "code_snippet": "        return await self.chat_store.aget_messages(self.chat_store_key)\n\n    def get(self, input: Optional[str] = None, **kwargs: Any) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key, **kwargs)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_127_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'put'",
      "description": "Function 'put' on line 127 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 127,
      "code_snippet": "        return await self.chat_store.aget_messages(self.chat_store_key, **kwargs)\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_146_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'reset'",
      "description": "Function 'reset' on line 146 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 146,
      "code_snippet": "        await self.chat_store.aset_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_146_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'reset'",
      "description": "Function 'reset' on line 146 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 146,
      "code_snippet": "        await self.chat_store.aset_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_109_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get_all'",
      "description": "Function 'get_all' on line 109 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 109,
      "code_snippet": "        \"\"\"Create a chat memory from defaults.\"\"\"\n\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py_117_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get'",
      "description": "Function 'get' on line 117 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/memory/types.py",
      "line_number": 117,
      "code_snippet": "        return await self.chat_store.aget_messages(self.chat_store_key)\n\n    def get(self, input: Optional[str] = None, **kwargs: Any) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key, **kwargs)\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/playground/base.py_47_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 47. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/playground/base.py",
      "line_number": 47,
      "code_snippet": "                chosen from the index when a query is made. A full list of\n                retriever_modes available to each index can be found here:\n                https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retriever_modes.html\n\n        \"\"\"\n        self._validate_indices(indices)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/callbacks/pythonically_printing_base_handler.py_13_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 13. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/callbacks/pythonically_printing_base_handler.py",
      "line_number": 13,
      "code_snippet": "    \"\"\"\n    Callback handler that prints logs in a Pythonic way. That is, not using `print` at all; use the logger instead.\n    See https://stackoverflow.com/a/6918596/1147061 for why you should prefer using a logger over `print`.\n\n    This class is meant to be subclassed, not used directly.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/callbacks/pythonically_printing_base_handler.py_18_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 18. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/callbacks/pythonically_printing_base_handler.py",
      "line_number": 18,
      "code_snippet": "\n    Using this class, your LlamaIndex Callback Handlers can now make use of vanilla Python logging handlers now.\n    One popular choice is https://rich.readthedocs.io/en/stable/logging.html#logging-handler.\n    \"\"\"\n\n    def __init__(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py_22",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'model_dump' on line 22 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py",
      "line_number": 22,
      "code_snippet": "    def model_dump(self, **kwargs: Any) -> Dict[str, Any]:\n        print(\"here\", flush=True)\n        data = super().model_dump(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Keep for backwards compatibility.\"\"\"\n        return self.model_dump(**kwargs)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py_32",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'to_dict' on line 32 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py",
      "line_number": 32,
      "code_snippet": "    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data\n\n    @property\n    @abstractmethod\n    def llm(self) -> LLM:\n        \"\"\"Get LLM.\"\"\"\n\n    @property",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py_22_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'model_dump'",
      "description": "Function 'model_dump' on line 22 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py",
      "line_number": 22,
      "code_snippet": "    \"\"\"Base LLM Predictor.\"\"\"\n\n    def model_dump(self, **kwargs: Any) -> Dict[str, Any]:\n        print(\"here\", flush=True)\n        data = super().model_dump(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py_32_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'to_dict'",
      "description": "Function 'to_dict' on line 32 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py",
      "line_number": 32,
      "code_snippet": "        return self.model_dump(**kwargs)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py_143",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'query_bundle' directly embedded in LLM prompt",
      "description": "The function '_postprocess_nodes' embeds user input ('query_bundle') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py",
      "line_number": 143,
      "code_snippet": "\n            query_text = self.query_embedding_tmpl.format(\n                context_str=node.node.get_content(metadata_mode=MetadataMode.EMBED),\n            )\n            query_embedding = self.embed_model.get_query_embedding(query_text)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py_108",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 108 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py",
      "line_number": 108,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py_108_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 108 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py",
      "line_number": 108,
      "code_snippet": "        return \"EmbeddingRecencyPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py_108_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 108 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py",
      "line_number": 108,
      "code_snippet": "        return \"EmbeddingRecencyPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py_108_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 108 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py",
      "line_number": 108,
      "code_snippet": "        return \"EmbeddingRecencyPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py_174",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.chat' is used in 'Response' on line 174 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py",
      "line_number": 174,
      "code_snippet": "    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py_173",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'run_llm' on line 173 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py",
      "line_number": 173,
      "code_snippet": "    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return await self.llm.achat(messages)\n\n    def _clean_response(self, response: str) -> str:\n        new_response = \"\"\n        for c in response:\n            if not c.isdigit():\n                new_response += \" \"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py_173_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'run_llm'",
      "description": "Function 'run_llm' on line 173 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py",
      "line_number": 173,
      "code_snippet": "        return messages\n\n    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py_173_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_llm'",
      "description": "Function 'run_llm' on line 173 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py",
      "line_number": 173,
      "code_snippet": "        return messages\n\n    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py_173_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'run_llm'",
      "description": "Function 'run_llm' on line 173 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py",
      "line_number": 173,
      "code_snippet": "        return messages\n\n    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py_98",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 98 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py",
      "line_number": 98,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Optimize a node text given the query by shortening the node text.\"\"\"\n        if query_bundle is None:\n            return nodes\n\n        for node_idx in range(len(nodes)):\n            text = nodes[node_idx].node.get_content(metadata_mode=MetadataMode.LLM)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py_98_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 98 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py",
      "line_number": 98,
      "code_snippet": "        return \"SentenceEmbeddingOptimizer\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py_98_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 98 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py",
      "line_number": 98,
      "code_snippet": "        return \"SentenceEmbeddingOptimizer\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node.py_370_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 370. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/node.py",
      "line_number": 370,
      "code_snippet": "    Models struggle to access significant details found\n    in the center of extended contexts. A study\n    (https://arxiv.org/abs/2307.03172) observed that the best\n    performance typically arises when crucial data is positioned\n    at the start or conclusion of the input context. Additionally,\n    as the input context lengthens, performance drops notably, even",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py_62",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 62 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py",
      "line_number": 62,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n            raise ValueError(\"Missing query bundle in extra info.\")\n        if len(nodes) == 0:\n            return []\n\n        query_and_nodes = [",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py_62_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 62 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py",
      "line_number": 62,
      "code_snippet": "        return \"SentenceTransformerRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py_62_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 62 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py",
      "line_number": 62,
      "code_snippet": "        return \"SentenceTransformerRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py_62_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 62 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py",
      "line_number": 62,
      "code_snippet": "        return \"SentenceTransformerRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py_68",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.predict' is used in 'Response' on line 68 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py",
      "line_number": 68,
      "code_snippet": "\n        response = self.llm.predict(pii_prompt, context_str=text, query_str=task_str)\n        splits = response.split(\"Output Mapping:\")\n        text_output = splits[0].strip()",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py_59",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'mask_pii' on line 59 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py",
      "line_number": 59,
      "code_snippet": "    def mask_pii(self, text: str) -> Tuple[str, Dict]:\n        \"\"\"Mask PII in text.\"\"\"\n        pii_prompt = PromptTemplate(self.pii_str_tmpl)\n        # TODO: allow customization\n        task_str = (\n            \"Mask out the PII, replace each PII with a tag, and return the text. \"\n            \"Return the mapping in JSON.\"\n        )\n\n        response = self.llm.predict(pii_prompt, context_str=text, query_str=task_str)\n        splits = response.split(\"Output Mapping:\")",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py_59_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'mask_pii'",
      "description": "Function 'mask_pii' on line 59 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py",
      "line_number": 59,
      "code_snippet": "        return \"PIINodePostprocessor\"\n\n    def mask_pii(self, text: str) -> Tuple[str, Dict]:\n        \"\"\"Mask PII in text.\"\"\"\n        pii_prompt = PromptTemplate(self.pii_str_tmpl)\n        # TODO: allow customization",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py_90",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.predict' is used in 'Response' on line 90 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py",
      "line_number": 90,
      "code_snippet": "            # call each batch independently\n            raw_response = self.llm.predict(\n                self.choice_select_prompt,\n                context_str=fmt_batch_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py_71",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 71 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py",
      "line_number": 71,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n            raise ValueError(\"Query bundle must be provided.\")\n        if len(nodes) == 0:\n            return []\n\n        initial_results: List[NodeWithScore] = []",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py_71_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 71 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py",
      "line_number": 71,
      "code_snippet": "        return \"LLMRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py_71_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 71 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py",
      "line_number": 71,
      "code_snippet": "        return \"LLMRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py_180",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.structured_predict' is used in 'SELECT' on line 180 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py",
      "line_number": 180,
      "code_snippet": "                # call each batch independently\n                result: Union[BaseModel, str] = self.llm.structured_predict(\n                    output_cls=self._document_relevance_list_cls,\n                    prompt=self.choice_select_prompt,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py_143",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_postprocess_nodes' on line 143 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py",
      "line_number": 143,
      "code_snippet": "    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        dispatcher.event(\n            ReRankStartEvent(\n                query=query_bundle,\n                nodes=nodes,\n                top_n=self.top_n,\n                model_name=self.llm.metadata.model_name,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py_143_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 143 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py",
      "line_number": 143,
      "code_snippet": "        return \"StructuredLLMRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py_143_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_postprocess_nodes'",
      "description": "Function '_postprocess_nodes' on line 143 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/postprocessor/structured_llm_rerank.py",
      "line_number": 143,
      "code_snippet": "        return \"StructuredLLMRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/graph_stores/utils.py_4_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 4. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/graph_stores/utils.py",
      "line_number": 4,
      "code_snippet": "Borrowed from Langchain's Neo4j graph utility functions.\n\nhttps://github.com/langchain-ai/langchain/blob/95c3e5f85f8ed8026a11e351b57bfae488d654c4/libs/community/langchain_community/graphs/neo4j_graph.py\n\"\"\"\n\nfrom typing import Any",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/objects/base_node_mapping.py_4_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for model serialization",
      "description": "Import of 'pickle' on line 4 for model serialization. This library can execute arbitrary code during deserialization, making it vulnerable to supply chain attacks if loading untrusted models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/objects/base_node_mapping.py",
      "line_number": 4,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors for PyTorch/TensorFlow\n2. Never deserialize models from untrusted sources\n3. Scan serialized models before loading\n4. Use sandboxed environments for model loading\n5. Implement allowlists for model formats\n6. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/objects/base.py_3_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Use of pickle for model serialization",
      "description": "Import of 'pickle' on line 3 for model serialization. This library can execute arbitrary code during deserialization, making it vulnerable to supply chain attacks if loading untrusted models.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/objects/base.py",
      "line_number": 3,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors for PyTorch/TensorFlow\n2. Never deserialize models from untrusted sources\n3. Scan serialized models before loading\n4. Use sandboxed environments for model loading\n5. Implement allowlists for model formats\n6. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py_190",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'prompt_input_key' directly embedded in LLM prompt",
      "description": "The function 'save_context' embeds user input ('prompt_input_key') directly into an LLM prompt using concatenation. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py",
      "line_number": 190,
      "code_snippet": "",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py_184",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.chat_memory.messages.append' is used in 'INSERT' on line 184 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py",
      "line_number": 184,
      "code_snippet": "\n        self.chat_memory.messages.append(human_message)\n        self.chat_memory.messages.append(ai_message)\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py_184",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.chat_memory.messages.append' is used in 'INSERT' on line 184 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py",
      "line_number": 184,
      "code_snippet": "\n        self.chat_memory.messages.append(human_message)\n        self.chat_memory.messages.append(ai_message)\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py_165",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'save_context' on line 165 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py",
      "line_number": 165,
      "code_snippet": "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = next(iter(outputs.keys()))\n        else:\n            output_key = self.output_key\n\n        # a bit different than existing langchain implementation",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utilities/token_counting.py_2_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 2. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utilities/token_counting.py",
      "line_number": 2,
      "code_snippet": "# Modified from:\n# https://github.com/nyno-ai/openai-token-counter\n\nfrom typing import Any, Callable, Dict, List, Optional\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utilities/gemini_utils.py_16_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 16. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utilities/gemini_utils.py",
      "line_number": 16,
      "code_snippet": "    MessageRole.MODEL: MessageRole.MODEL,\n    ## Gemini has function role, but chat mode only accepts user and model roles.\n    ## https://medium.com/@smallufo/openai-vs-gemini-function-calling-a664f7f2b29f\n    ## Agent response's 'tool/function' role is converted to 'user' role.\n    MessageRole.TOOL: MessageRole.USER,\n    MessageRole.FUNCTION: MessageRole.USER,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utilities/sql_wrapper.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/utilities/sql_wrapper.py",
      "line_number": 23,
      "code_snippet": "\n    Based on langchain SQLDatabase.\n    https://github.com/langchain-ai/langchain/blob/e355606b1100097665207ca259de6dc548d44c78/libs/langchain/langchain/utilities/sql_database.py#L39\n\n    Args:\n        engine (Engine): The SQLAlchemy engine instance to use for database operations.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py_21_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 21. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py",
      "line_number": 21,
      "code_snippet": "\nLLAMA_PACKS_CONTENTS_URL = (\n    \"https://raw.githubusercontent.com/run-llama/llama_index/main/llama-index-packs\"\n)\nLLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama_index/tree/main\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py_24_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 24. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py",
      "line_number": 24,
      "code_snippet": ")\nLLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama_index/tree/main\"\n)\nPY_NAMESPACE = \"llama_index/packs\"\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py_29_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 29. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py",
      "line_number": 29,
      "code_snippet": "\nPATH_TYPE = Union[str, Path]\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\n\nlogger = logging.getLogger(__name__)\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py_34_auth",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Plugin management function without authentication in 'download_module_and_reqs'",
      "description": "Function 'download_module_and_reqs' on line 34 manages plugins (install/enable/execute) without requiring authentication. This allows unauthorized users to manipulate the plugin system, potentially installing malicious plugins or executing arbitrary code.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/pack.py",
      "line_number": 34,
      "code_snippet": "\n\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,",
      "recommendation": "Plugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/dataset.py_17_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 17. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/dataset.py",
      "line_number": 17,
      "code_snippet": "\nLLAMA_INDEX_CONTENTS_URL = (\n    f\"https://raw.githubusercontent.com/run-llama/llama_index/main\"\n)\nLLAMA_DATASETS_PATH = \"/llama-datasets\"\nLLAMA_DATASETS_URL = LLAMA_INDEX_CONTENTS_URL + LLAMA_DATASETS_PATH",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/dataset.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/dataset.py",
      "line_number": 23,
      "code_snippet": "\nLLAMA_DATASETS_LFS_URL = (\n    f\"https://media.githubusercontent.com/media/run-llama/llama-datasets/main\"\n)\n\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/dataset.py_27_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 27. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/dataset.py",
      "line_number": 27,
      "code_snippet": "\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"\n)\nLLAMA_SOURCE_FILES_PATH = \"source_files\"\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py_21_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 21. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py",
      "line_number": 21,
      "code_snippet": ")\n\nLLAMA_HUB_CONTENTS_URL = f\"https://raw.githubusercontent.com/run-llama/llama-hub/main\"\nLLAMA_HUB_PATH = \"/llama_hub\"\nLLAMA_HUB_URL = LLAMA_HUB_CONTENTS_URL + LLAMA_HUB_PATH\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py_28_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 28. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py",
      "line_number": 28,
      "code_snippet": "\nlogger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\n\n\nclass MODULE_TYPE(str, Enum):",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py_93_auth",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Plugin management function without authentication in 'download_module_and_reqs'",
      "description": "Function 'download_module_and_reqs' on line 93 manages plugins (install/enable/execute) without requiring authentication. This allows unauthorized users to manipulate the plugin system, potentially installing malicious plugins or executing arbitrary code.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py",
      "line_number": 93,
      "code_snippet": "\n\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_id: str,",
      "recommendation": "Plugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    },
    {
      "id": "LLM07_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py_179_auth",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Plugin management function without authentication in 'download_llama_module'",
      "description": "Function 'download_llama_module' on line 179 manages plugins (install/enable/execute) without requiring authentication. This allows unauthorized users to manipulate the plugin system, potentially installing malicious plugins or executing arbitrary code.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/download/module.py",
      "line_number": 179,
      "code_snippet": "\n\ndef download_llama_module(\n    module_class: str,\n    llama_hub_url: str = LLAMA_HUB_URL,\n    refresh_cache: bool = False,",
      "recommendation": "Plugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py_91",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm._extend_messages' is used in 'Response' on line 91 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py",
      "line_number": 91,
      "code_snippet": "            messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n            messages = self._llm._extend_messages(messages)\n            chat_response = self._llm.chat(messages, **llm_kwargs)\n",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py_92",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.chat' is used in 'Response' on line 92 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py",
      "line_number": 92,
      "code_snippet": "            messages = self._llm._extend_messages(messages)\n            chat_response = self._llm.chat(messages, **llm_kwargs)\n\n            raw_output = chat_response.message.content or \"\"",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py_98",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.complete' is used in 'Response' on line 98 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py",
      "line_number": 98,
      "code_snippet": "\n            response = self._llm.complete(formatted_prompt, **llm_kwargs)\n\n            raw_output = response.text",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py_82",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '__call__' on line 82 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py",
      "line_number": 82,
      "code_snippet": "    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Model:\n        llm_kwargs = llm_kwargs or {}\n        if self._llm.metadata.is_chat_model:\n            messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n            messages = self._llm._extend_messages(messages)\n            chat_response = self._llm.chat(messages, **llm_kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py_82_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '__call__'",
      "description": "Function '__call__' on line 82 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/llm_program.py",
      "line_number": 82,
      "code_snippet": "        self._prompt = prompt\n\n    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        *args: Any,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_151",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm._extend_messages' is used in 'call(' on line 151 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 151,
      "code_snippet": "        messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n        messages = self._llm._extend_messages(messages)\n\n        agent_response = self._llm.predict_and_call(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_153",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm._extend_messages' is used in 'call(' on line 153 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 153,
      "code_snippet": "\n        agent_response = self._llm.predict_and_call(\n            [tool],\n            chat_history=messages,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_153",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm.predict_and_call' is used in 'call(' on line 153 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 153,
      "code_snippet": "\n        agent_response = self._llm.predict_and_call(\n            [tool],\n            chat_history=messages,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_153",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm.predict_and_call' is used in 'call(' on line 153 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 153,
      "code_snippet": "\n        agent_response = self._llm.predict_and_call(\n            [tool],\n            chat_history=messages,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_218",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.get_tool_calls_from_response' is used in 'Response' on line 218 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 218,
      "code_snippet": "        \"\"\"Process stream.\"\"\"\n        tool_calls = self._llm.get_tool_calls_from_response(\n            chat_response,\n            # error_on_no_tool_call=True",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_283",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm._extend_messages' is used in 'Response' on line 283 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 283,
      "code_snippet": "        messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n        messages = self._llm._extend_messages(messages)\n\n        chat_response_gen = self._llm.stream_chat_with_tools(",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_285",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream_chat_with_tools' is used in 'Response' on line 285 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 285,
      "code_snippet": "\n        chat_response_gen = self._llm.stream_chat_with_tools(\n            [tool],\n            chat_history=messages,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_136",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '__call__' on line 136 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 136,
      "code_snippet": "    def __call__(\n        self,\n        *args: Any,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Union[Model, List[Model]]:\n        # avoid passing in duplicate kwargs\n        llm_kwargs = llm_kwargs or {}\n        llm_kwargs.pop(\"tool_required\", None)\n        llm_kwargs.pop(\"tool_choice\", None)\n        llm_kwargs.pop(\"allow_parallel_tool_calls\", None)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_211",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_process_objects' on line 211 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 211,
      "code_snippet": "    def _process_objects(\n        self,\n        chat_response: ChatResponse,\n        output_cls: Type[Model],\n        cur_objects: Optional[List[Model]] = None,\n    ) -> Union[Model, List[Model]]:\n        \"\"\"Process stream.\"\"\"\n        tool_calls = self._llm.get_tool_calls_from_response(\n            chat_response,\n            # error_on_no_tool_call=True\n            error_on_no_tool_call=False,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_259",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'stream_call' on line 259 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 259,
      "code_snippet": "    def stream_call(\n        self, *args: Any, llm_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> Generator[\n        Union[Model, List[Model], FlexibleModel, List[FlexibleModel]], None, None\n    ]:\n        \"\"\"\n        Stream object.\n\n        Returns a generator returning partials of the same object\n        or a list of objects until it returns.\n        \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_211_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_process_objects'",
      "description": "Function '_process_objects' on line 211 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 211,
      "code_snippet": "            return outputs[0]\n\n    def _process_objects(\n        self,\n        chat_response: ChatResponse,\n        output_cls: Type[Model],",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_136_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '__call__'",
      "description": "Function '__call__' on line 136 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 136,
      "code_snippet": "        self._prompt = prompt\n\n    def __call__(\n        self,\n        *args: Any,\n        llm_kwargs: Optional[Dict[str, Any]] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py_259_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'stream_call'",
      "description": "Function 'stream_call' on line 259 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/function_program.py",
      "line_number": 259,
      "code_snippet": "            return new_cur_objects[0]\n\n    def stream_call(\n        self, *args: Any, llm_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> Generator[\n        Union[Model, List[Model], FlexibleModel, List[FlexibleModel]], None, None",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py_130",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 130 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py",
      "line_number": 130,
      "code_snippet": "\n        response = self._multi_modal_llm.chat(\n            messages=[ChatMessage(role=\"user\", blocks=blocks)],\n            **llm_kwargs,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py_102",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '__call__' on line 102 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py",
      "line_number": 102,
      "code_snippet": "    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        image_documents: Optional[List[Union[ImageBlock, ImageNode]]] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> BaseModel:\n        llm_kwargs = llm_kwargs or {}\n        formatted_prompt = self._prompt.format(llm=self._multi_modal_llm, **kwargs)  # type: ignore\n\n        if image_documents and all(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py_102_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__call__'",
      "description": "Function '__call__' on line 102 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py",
      "line_number": 102,
      "code_snippet": "        self._prompt = prompt\n\n    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        image_documents: Optional[List[Union[ImageBlock, ImageNode]]] = None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py_102_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '__call__'",
      "description": "Function '__call__' on line 102 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py",
      "line_number": 102,
      "code_snippet": "        self._prompt = prompt\n\n    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        image_documents: Optional[List[Union[ImageBlock, ImageNode]]] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py_202",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'llm.get_tool_calls_from_response' is used in 'Response' on line 202 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py",
      "line_number": 202,
      "code_snippet": "        if isinstance(chat_response.message.additional_kwargs.get(\"tool_calls\"), list):\n            tool_calls = llm.get_tool_calls_from_response(\n                chat_response, error_on_no_tool_call=False\n            )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py_165",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'process_streaming_objects' on line 165 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py",
      "line_number": 165,
      "code_snippet": "def process_streaming_objects(\n    chat_response: ChatResponse,\n    output_cls: Type[Model],\n    cur_objects: Optional[Sequence[Model]] = None,\n    allow_parallel_tool_calls: bool = False,\n    flexible_mode: bool = True,\n    llm: Optional[FunctionCallingLLM] = None,\n) -> Union[Model, List[Model], FlexibleModel, List[FlexibleModel]]:\n    \"\"\"\n    Process streaming response into structured objects.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py_39_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 39. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py",
      "line_number": 39,
      "code_snippet": "    \"\"\"Create a list version of an existing Pydantic object.\"\"\"\n    # NOTE: this is directly taken from\n    # https://github.com/jxnl/openai_function_call/blob/main/examples/streaming_multitask/streaming_multitask.py\n    # all credits go to the openai_function_call repo\n\n    name = f\"{base_cls.__name__}List\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py_165_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'process_streaming_objects'",
      "description": "Function 'process_streaming_objects' on line 165 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/program/utils.py",
      "line_number": 165,
      "code_snippet": "\n\ndef process_streaming_objects(\n    chat_response: ChatResponse,\n    output_cls: Type[Model],\n    cur_objects: Optional[Sequence[Model]] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/rich.py_81",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'format_messages' on line 81 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/rich.py",
      "line_number": 81,
      "code_snippet": "    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        del llm  # unused\n        \"\"\"Format the prompt into a list of chat messages.\"\"\"\n        all_kwargs = {\n            **self.kwargs,\n            **kwargs,\n        }\n        mapped_all_kwargs = self._map_all_vars(all_kwargs)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/rich.py_81_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'format_messages'",
      "description": "Function 'format_messages' on line 81 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/rich.py",
      "line_number": 81,
      "code_snippet": "            return Prompt(self.template_str).text(data=mapped_all_kwargs)\n\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        del llm  # unused",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/rich.py_81_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'format_messages'",
      "description": "Function 'format_messages' on line 81 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/rich.py",
      "line_number": 81,
      "code_snippet": "            return Prompt(self.template_str).text(data=mapped_all_kwargs)\n\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        del llm  # unused",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/guidance_utils.py_65_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 65. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/guidance_utils.py",
      "line_number": 65,
      "code_snippet": "    Convert a json schema to guidance output template.\n\n    Implementation based on https://github.com/microsoft/guidance/\\\n        blob/main/notebooks/applications/jsonformer.ipynb\n    Modified to support nested pydantic models.\n    \"\"\"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/default_prompts.py_187_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 187. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/prompts/default_prompts.py",
      "line_number": 187,
      "code_snippet": "\n# NOTE: taken from langchain and adapted\n# https://github.com/langchain-ai/langchain/blob/v0.0.303/libs/langchain/langchain/chains/sql_database/prompt.py\nDEFAULT_TEXT_TO_SQL_TMPL = (\n    \"Given an input question, first create a syntactically correct {dialect} \"\n    \"query to run, then look at the results of the query and return the answer. \"",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_92",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 92 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 92,
      "code_snippet": "        if not self._streaming:\n            response = self._llm.predict(\n                text_qa_template,\n                context_str=truncated_chunks,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_105",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 105 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 105,
      "code_snippet": "        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_98",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 98 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 98,
      "code_snippet": "        else:\n            response = self._llm.stream(\n                text_qa_template,\n                context_str=truncated_chunks,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_105",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 105 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 105,
      "code_snippet": "        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_76",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 76 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 76,
      "code_snippet": "    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        single_text_chunk = \"\\n\".join(text_chunks)\n        truncated_chunks = self._prompt_helper.truncate(\n            prompt=text_qa_template,\n            text_chunks=[single_text_chunk],",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_76_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_response'",
      "description": "Function 'get_response' on line 76 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 76,
      "code_snippet": "        return response\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py_76_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get_response'",
      "description": "Function 'get_response' on line 76 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
      "line_number": 76,
      "code_snippet": "        return response\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py_87",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 87 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py",
      "line_number": 87,
      "code_snippet": "        if not self._streaming:\n            return self._llm.predict(\n                self._input_prompt,\n                query_str=query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py_93",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 93 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py",
      "line_number": 93,
      "code_snippet": "        else:\n            return self._llm.stream(\n                self._input_prompt,\n                query_str=query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py_77",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 77 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py",
      "line_number": 77,
      "code_snippet": "    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        # NOTE: ignore text chunks and previous response\n        del text_chunks\n\n        if not self._streaming:\n            return self._llm.predict(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py_77_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get_response'",
      "description": "Function 'get_response' on line 77 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py",
      "line_number": 77,
      "code_snippet": "            )\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_77",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.structured_predict' is used in 'Response' on line 77 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 77,
      "code_snippet": "        if self._output_cls is not None:\n            answer = self._llm.structured_predict(\n                self._output_cls,\n                self._prompt,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_89",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.structured_predict' is used in 'Response' on line 89 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 89,
      "code_snippet": "            )\n        return StructuredRefineResponse(answer=answer, query_satisfied=True)\n\n    async def acall(self, *args: Any, **kwds: Any) -> StructuredRefineResponse:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_85",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'call(' on line 85 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 85,
      "code_snippet": "        else:\n            answer = self._llm.predict(\n                self._prompt,\n                **kwds,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_89",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 89 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 89,
      "code_snippet": "            )\n        return StructuredRefineResponse(answer=answer, query_satisfied=True)\n\n    async def acall(self, *args: Any, **kwds: Any) -> StructuredRefineResponse:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_342",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 342 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 342,
      "code_snippet": "\n                response = self._llm.stream(\n                    refine_template,\n                    context_msg=cur_text_chunk,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_254",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 254 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 254,
      "code_snippet": "            elif response is None and self._streaming:\n                response = self._llm.stream(\n                    text_qa_template,\n                    context_str=cur_text_chunk,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_268",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 268 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 268,
      "code_snippet": "        if response is None:\n            response = \"Empty Response\"\n        if isinstance(response, str):\n            response = response or \"Empty Response\"",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_270",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 270 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 270,
      "code_snippet": "        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_75",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '__call__' on line 75 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 75,
      "code_snippet": "    def __call__(self, *args: Any, **kwds: Any) -> StructuredRefineResponse:\n        if self._output_cls is not None:\n            answer = self._llm.structured_predict(\n                self._output_cls,\n                self._prompt,\n                **kwds,\n            )\n            if isinstance(answer, BaseModel):\n                answer = answer.model_dump_json()\n        else:\n            answer = self._llm.predict(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_220",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_give_response_single' on line 220 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 220,
      "code_snippet": "    def _give_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        text_chunks = self._prompt_helper.repack(\n            text_qa_template, [text_chunk], llm=self._llm\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_275",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_refine_response_single' on line 275 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 275,
      "code_snippet": "    def _refine_response_single(\n        self,\n        response: RESPONSE_TEXT_TYPE,\n        query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n    ) -> Optional[RESPONSE_TEXT_TYPE]:\n        \"\"\"Refine response.\"\"\"\n        # TODO: consolidate with logic in response/schema.py\n        if isinstance(response, Generator):\n            response = get_response_text(response)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_275_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_refine_response_single'",
      "description": "Function '_refine_response_single' on line 275 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 275,
      "code_snippet": "        return response\n\n    def _refine_response_single(\n        self,\n        response: RESPONSE_TEXT_TYPE,\n        query_str: str,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py_75_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '__call__'",
      "description": "Function '__call__' on line 75 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
      "line_number": 75,
      "code_snippet": "        return StructuredRefineResponse\n\n    def __call__(self, *args: Any, **kwds: Any) -> StructuredRefineResponse:\n        if self._output_cls is not None:\n            answer = self._llm.structured_predict(\n                self._output_cls,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py_157",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.output_cls.model_validate_json' is used in 'Response' on line 157 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py",
      "line_number": 157,
      "code_snippet": "            # convert string to output_cls\n            output = self._llm.output_cls.model_validate_json(str(response_str))\n            return PydanticResponse(\n                output,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py_145",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_prepare_response_output' on line 145 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py",
      "line_number": 145,
      "code_snippet": "    def _prepare_response_output(\n        self,\n        response_str: Optional[RESPONSE_TEXT_TYPE],\n        source_nodes: List[NodeWithScore],\n    ) -> RESPONSE_TYPE:\n        \"\"\"Prepare response object from response string.\"\"\"\n        response_metadata = self._get_metadata_for_response(\n            [node_with_score.node for node_with_score in source_nodes]\n        )\n\n        if isinstance(self._llm, StructuredLLM):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py_145_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_prepare_response_output'",
      "description": "Function '_prepare_response_output' on line 145 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py",
      "line_number": 145,
      "code_snippet": "        return {node.node_id: node.metadata for node in nodes}\n\n    def _prepare_response_output(\n        self,\n        response_str: Optional[RESPONSE_TEXT_TYPE],\n        source_nodes: List[NodeWithScore],",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_148",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'text_chunks' directly embedded in LLM prompt",
      "description": "The function 'get_response' embeds user input ('text_chunks') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 148,
      "code_snippet": "        if self._verbose:\n            print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give final response if there is only one chunk\n        if len(text_chunks) == 1:\n            response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                response = self._llm.stream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_148",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'text_chunks' directly embedded in LLM prompt",
      "description": "The function 'get_response' embeds user input ('text_chunks') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 148,
      "code_snippet": "        if self._verbose:\n            print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give final response if there is only one chunk\n        if len(text_chunks) == 1:\n            response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                response = self._llm.stream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs\n                )\n            else:\n                if self._output_cls is None:\n                    response = self._llm.predict(\n                        summary_template,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_148",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'text_chunks' directly embedded in LLM prompt",
      "description": "The function 'get_response' embeds user input ('text_chunks') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 148,
      "code_snippet": "        if self._verbose:\n            print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give final response if there is only one chunk\n        if len(text_chunks) == 1:\n            response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                response = self._llm.stream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs\n                )\n            else:\n                if self._output_cls is None:\n                    response = self._llm.predict(\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n                else:\n                    response = self._llm.structured_predict(\n                        self._output_cls,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_154",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 154 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 154,
      "code_snippet": "            if self._streaming:\n                response = self._llm.stream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs\n                )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_159",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 159 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 159,
      "code_snippet": "                if self._output_cls is None:\n                    response = self._llm.predict(\n                        summary_template,\n                        context_str=text_chunks[0],",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_165",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.structured_predict' is used in 'Response' on line 165 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 165,
      "code_snippet": "                else:\n                    response = self._llm.structured_predict(\n                        self._output_cls,\n                        summary_template,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_179",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.apredict' is used in 'Response' on line 179 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 179,
      "code_snippet": "                    tasks = [\n                        self._llm.apredict(\n                            summary_template,\n                            context_str=text_chunk,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_188",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.astructured_predict' is used in 'Response' on line 188 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 188,
      "code_snippet": "                    tasks = [\n                        self._llm.astructured_predict(\n                            self._output_cls,\n                            summary_template,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_208",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 208 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 208,
      "code_snippet": "                    summaries = [\n                        self._llm.predict(\n                            summary_template,\n                            context_str=text_chunk,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_217",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.structured_predict' is used in 'Response' on line 217 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 217,
      "code_snippet": "                    summaries = [\n                        self._llm.structured_predict(\n                            self._output_cls,\n                            summary_template,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_134",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 134 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 134,
      "code_snippet": "    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get tree summarize response.\"\"\"\n        summary_template = self._summary_template.partial_format(query_str=query_str)\n        # repack text_chunks so that each chunk fills the context window\n        text_chunks = self._prompt_helper.repack(\n            summary_template, text_chunks=text_chunks, llm=self._llm",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py_134_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get_response'",
      "description": "Function 'get_response' on line 134 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
      "line_number": 134,
      "code_snippet": "            )\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/semantic_similarity.py_23_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 23. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/semantic_similarity.py",
      "line_number": 23,
      "code_snippet": "    Inspired by this paper:\n    - Semantic Answer Similarity for Evaluating Question Answering Models\n        https://arxiv.org/pdf/2108.06130.pdf\n\n    Args:\n        similarity_threshold (float): Embedding similarity threshold for \"passing\".",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_169",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'chat_history' directly embedded in LLM prompt",
      "description": "The function 'synthesize' embeds user input ('chat_history') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 169,
      "code_snippet": "        )\n        fmt_prompt = self._context_template.format(\n            context_str=context_str, query_str=query_bundle.query_str\n        )\n\n        blocks: List[Union[ImageBlock, TextBlock]] = [\n            image_node_to_image_block(image_node.node)\n            for image_node in image_nodes\n            if isinstance(image_node.node, ImageNode)\n        ]\n\n        blocks.append(TextBlock(text=fmt_prompt))\n\n        chat_history = self._memory.get(\n            input=str(query_bundle),\n        )\n\n        if streaming:\n            llm_stream = self._multi_modal_llm.stream_chat(\n                [",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_169",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'chat_history' directly embedded in LLM prompt",
      "description": "The function 'synthesize' embeds user input ('chat_history') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 169,
      "code_snippet": "        )\n        fmt_prompt = self._context_template.format(\n            context_str=context_str, query_str=query_bundle.query_str\n        )\n\n        blocks: List[Union[ImageBlock, TextBlock]] = [\n            image_node_to_image_block(image_node.node)\n            for image_node in image_nodes\n            if isinstance(image_node.node, ImageNode)\n        ]\n\n        blocks.append(TextBlock(text=fmt_prompt))\n\n        chat_history = self._memory.get(\n            input=str(query_bundle),\n        )\n\n        if streaming:\n            llm_stream = self._multi_modal_llm.stream_chat(\n                [\n                    ChatMessage(role=\"system\", content=self._system_prompt),\n                    *chat_history,\n                    ChatMessage(role=\"user\", blocks=blocks),\n                ]\n            )\n            stream_tokens = stream_chat_response_to_tokens(llm_stream)\n            return StreamingResponse(\n                response_gen=stream_tokens,\n                source_nodes=nodes,\n                metadata={\"text_nodes\": text_nodes, \"image_nodes\": image_nodes},\n            )\n        else:\n            llm_response = self._multi_modal_llm.chat(\n                [",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_186",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.stream_chat' is used in 'Response' on line 186 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 186,
      "code_snippet": "        if streaming:\n            llm_stream = self._multi_modal_llm.stream_chat(\n                [\n                    ChatMessage(role=\"system\", content=self._system_prompt),",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_200",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 200 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 200,
      "code_snippet": "        else:\n            llm_response = self._multi_modal_llm.chat(\n                [\n                    ChatMessage(role=\"system\", content=self._system_prompt),",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 158 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 158,
      "code_snippet": "    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        streaming: bool = False,\n    ) -> RESPONSE_TYPE:\n        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_158_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'synthesize'",
      "description": "Function 'synthesize' on line 158 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 158,
      "code_snippet": "        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py_158_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'synthesize'",
      "description": "Function 'synthesize' on line 158 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_context.py",
      "line_number": 158,
      "code_snippet": "        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py_183",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'latest_message' directly embedded in LLM prompt",
      "description": "The function '_condense_question' embeds user input ('latest_message') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py",
      "line_number": 183,
      "code_snippet": "\n        llm_input = self._condense_prompt_template.format(\n            chat_history=chat_history_str, question=latest_message\n        )\n\n        return str(self._llm.complete(llm_input))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py_173",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_condense_question' on line 173 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py",
      "line_number": 173,
      "code_snippet": "    def _condense_question(\n        self, chat_history: List[ChatMessage], latest_message: str\n    ) -> str:\n        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"\n        if self._skip_condense or len(chat_history) == 0:\n            return latest_message\n\n        chat_history_str = messages_to_history_str(chat_history)\n        logger.debug(chat_history_str)\n\n        llm_input = self._condense_prompt_template.format(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py_173_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_condense_question'",
      "description": "Function '_condense_question' on line 173 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py",
      "line_number": 173,
      "code_snippet": "        )\n\n    def _condense_question(\n        self, chat_history: List[ChatMessage], latest_message: str\n    ) -> str:\n        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_question.py_117",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_condense_question' on line 117 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_question.py",
      "line_number": 117,
      "code_snippet": "    def _condense_question(\n        self, chat_history: List[ChatMessage], last_message: str\n    ) -> str:\n        \"\"\"\n        Generate standalone question from conversation context and last message.\n        \"\"\"\n        if not chat_history:\n            # Keep the question as is if there's no conversation context.\n            return last_message\n\n        chat_history_str = messages_to_history_str(chat_history)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_question.py_117_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_condense_question'",
      "description": "Function '_condense_question' on line 117 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/condense_question.py",
      "line_number": 117,
      "code_snippet": "        )\n\n    def _condense_question(\n        self, chat_history: List[ChatMessage], last_message: str\n    ) -> str:\n        \"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/types.py_409",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat' is used in 'Response' on line 409 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/types.py",
      "line_number": 409,
      "code_snippet": "        while message != \"exit\":\n            response = self.chat(message)\n            print(f\"Assistant: {response}\\n\")\n            message = input(\"Human: \")",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/types.py_402",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'chat_repl' on line 402 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/types.py",
      "line_number": 402,
      "code_snippet": "    def chat_repl(self) -> None:\n        \"\"\"Enter interactive chat REPL.\"\"\"\n        print(\"===== Entering Chat REPL =====\")\n        print('Type \"exit\" to exit.\\n')\n        self.reset()\n        message = input(\"Human: \")\n        while message != \"exit\":\n            response = self.chat(message)\n            print(f\"Assistant: {response}\\n\")\n            message = input(\"Human: \")\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/types.py_402_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'chat_repl'",
      "description": "Function 'chat_repl' on line 402 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/types.py",
      "line_number": 402,
      "code_snippet": "        \"\"\"Async version of main chat interface.\"\"\"\n\n    def chat_repl(self) -> None:\n        \"\"\"Enter interactive chat REPL.\"\"\"\n        print(\"===== Entering Chat REPL =====\")\n        print('Type \"exit\" to exit.\\n')",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_97",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'message' directly embedded in LLM prompt",
      "description": "The function 'chat' embeds user input ('message') directly into an LLM prompt using concatenation. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 97,
      "code_snippet": "\n        all_messages = self._prefix_messages + self._memory.get(\n            initial_token_count=initial_token_count\n        )\n\n        chat_response = self._llm.chat(all_messages)\n        ai_message = chat_response.message",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_130",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'message' directly embedded in LLM prompt",
      "description": "The function 'stream_chat' embeds user input ('message') directly into an LLM prompt using concatenation. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 130,
      "code_snippet": "\n        all_messages = self._prefix_messages + self._memory.get(\n            initial_token_count=initial_token_count\n        )\n\n        chat_response = StreamingAgentChatResponse(\n            chat_stream=self._llm.stream_chat(all_messages)\n        )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_101",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.chat' is used in 'Response' on line 101 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 101,
      "code_snippet": "\n        chat_response = self._llm.chat(all_messages)\n        ai_message = chat_response.message\n        self._memory.put(ai_message)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_105",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.chat' is used in 'Response' on line 105 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 105,
      "code_snippet": "\n        return AgentChatResponse(response=str(chat_response.message.content))\n\n    @trace_method(\"chat\")",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_135",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream_chat' is used in 'Response' on line 135 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 135,
      "code_snippet": "        chat_response = StreamingAgentChatResponse(\n            chat_stream=self._llm.stream_chat(all_messages)\n        )\n        thread = Thread(",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_134",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream_chat' is used in 'Response' on line 134 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 134,
      "code_snippet": "\n        chat_response = StreamingAgentChatResponse(\n            chat_stream=self._llm.stream_chat(all_messages)\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_75",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 75 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 75,
      "code_snippet": "    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        self._memory.put(ChatMessage(content=message, role=\"user\"))\n\n        if hasattr(self._memory, \"tokenizer_fn\"):\n            initial_token_count = len(\n                self._memory.tokenizer_fn(\n                    \" \".join(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_108",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 108 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 108,
      "code_snippet": "    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        self._memory.put(ChatMessage(content=message, role=\"user\"))\n\n        if hasattr(self._memory, \"tokenizer_fn\"):\n            initial_token_count = len(\n                self._memory.tokenizer_fn(\n                    \" \".join(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_75_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'chat'",
      "description": "Function 'chat' on line 75 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 75,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_108_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'stream_chat'",
      "description": "Function 'stream_chat' on line 108 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 108,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_75_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 75 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 75,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_108_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 108 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 108,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_75_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'chat'",
      "description": "Function 'chat' on line 75 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 75,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py_108_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'stream_chat'",
      "description": "Function 'stream_chat' on line 108 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
      "line_number": 108,
      "code_snippet": "\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_141",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'latest_message' directly embedded in LLM prompt",
      "description": "The function '_condense_question' embeds user input ('latest_message') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 141,
      "code_snippet": "\n        llm_input = self._condense_prompt_template.format(\n            chat_history=chat_history_str, question=latest_message\n        )\n\n        return str(self._multi_modal_llm.complete(llm_input))\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_248",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'query_str' directly embedded in LLM prompt",
      "description": "The function 'synthesize' embeds user input ('query_str') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 248,
      "code_snippet": "        )\n        fmt_prompt = self._context_prompt_template.format(\n            context_str=context_str, query_str=query_str\n        )\n\n        blocks: List[Union[ImageBlock, TextBlock]] = [\n            image_node_to_image_block(image_node.node)\n            for image_node in image_nodes\n            if isinstance(image_node.node, ImageNode)\n        ]\n\n        blocks.append(TextBlock(text=fmt_prompt))\n\n        chat_history = self._memory.get(\n            input=query_str,\n        )\n\n        if streaming:\n            llm_stream = self._multi_modal_llm.stream_chat(\n                [",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_248",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'query_str' directly embedded in LLM prompt",
      "description": "The function 'synthesize' embeds user input ('query_str') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 248,
      "code_snippet": "        )\n        fmt_prompt = self._context_prompt_template.format(\n            context_str=context_str, query_str=query_str\n        )\n\n        blocks: List[Union[ImageBlock, TextBlock]] = [\n            image_node_to_image_block(image_node.node)\n            for image_node in image_nodes\n            if isinstance(image_node.node, ImageNode)\n        ]\n\n        blocks.append(TextBlock(text=fmt_prompt))\n\n        chat_history = self._memory.get(\n            input=query_str,\n        )\n\n        if streaming:\n            llm_stream = self._multi_modal_llm.stream_chat(\n                [\n                    ChatMessage(role=\"system\", content=self._system_prompt),\n                    *chat_history,\n                    ChatMessage(role=\"user\", blocks=blocks),\n                ]\n            )\n            stream_tokens = stream_chat_response_to_tokens(llm_stream)\n            return StreamingResponse(\n                response_gen=stream_tokens,\n                source_nodes=nodes,\n                metadata={\"text_nodes\": text_nodes, \"image_nodes\": image_nodes},\n            )\n        else:\n            llm_response = self._multi_modal_llm.chat(\n                [",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_265",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.stream_chat' is used in 'Response' on line 265 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 265,
      "code_snippet": "        if streaming:\n            llm_stream = self._multi_modal_llm.stream_chat(\n                [\n                    ChatMessage(role=\"system\", content=self._system_prompt),",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_279",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 279 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 279,
      "code_snippet": "        else:\n            llm_response = self._multi_modal_llm.chat(\n                [\n                    ChatMessage(role=\"system\", content=self._system_prompt),",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_131",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_condense_question' on line 131 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 131,
      "code_snippet": "    def _condense_question(\n        self, chat_history: List[ChatMessage], latest_message: str\n    ) -> str:\n        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"\n        if self._skip_condense or len(chat_history) == 0:\n            return latest_message\n\n        chat_history_str = messages_to_history_str(chat_history)\n        logger.debug(chat_history_str)\n\n        llm_input = self._condense_prompt_template.format(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_237",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 237 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 237,
      "code_snippet": "    def synthesize(\n        self,\n        query_str: str,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        streaming: bool = False,\n    ) -> RESPONSE_TYPE:\n        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_237_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'synthesize'",
      "description": "Function 'synthesize' on line 237 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 237,
      "code_snippet": "        return context_source, context_nodes\n\n    def synthesize(\n        self,\n        query_str: str,\n        nodes: List[NodeWithScore],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_131_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_condense_question'",
      "description": "Function '_condense_question' on line 131 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 131,
      "code_snippet": "        )\n\n    def _condense_question(\n        self, chat_history: List[ChatMessage], latest_message: str\n    ) -> str:\n        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py_237_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'synthesize'",
      "description": "Function 'synthesize' on line 237 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/chat_engine/multi_modal_condense_plus_context.py",
      "line_number": 237,
      "code_snippet": "        return context_source, context_nodes\n\n    def synthesize(\n        self,\n        query_str: str,\n        nodes: List[NodeWithScore],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/base.py_43",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'load_langchain_documents' on line 43 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/base.py",
      "line_number": 43,
      "code_snippet": "    def load_langchain_documents(self, **load_kwargs: Any) -> List[\"LCDocument\"]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n\n\nclass BasePydanticReader(BaseReader, BaseComponent):\n    \"\"\"Serialiable Data Loader with Pydantic.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    is_remote: bool = Field(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/base.py_43_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'load_langchain_documents'",
      "description": "Function 'load_langchain_documents' on line 43 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/base.py",
      "line_number": 43,
      "code_snippet": "        return await asyncio.to_thread(self.load_data, *args, **load_kwargs)\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[\"LCDocument\"]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py_67",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate' on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py",
      "line_number": 67,
      "code_snippet": "    def generate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        tools_str = build_tools_text(tools)\n        query_str = query.query_str\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            tools_str=tools_str,\n            query_str=query_str,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py_72_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'generate'",
      "description": "Function 'generate' on line 67 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py",
      "line_number": 72,
      "code_snippet": "        tools_str = build_tools_text(tools)\n        query_str = query.query_str\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            tools_str=tools_str,\n            query_str=query_str,",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py_67_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'generate'",
      "description": "Function 'generate' on line 67 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py",
      "line_number": 67,
      "code_snippet": "            )\n\n    def generate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        tools_str = build_tools_text(tools)",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/custom.py_38",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.complete' is used in 'Response' on line 38 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/custom.py",
      "line_number": 38,
      "code_snippet": "        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/custom.py_34",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 34 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/custom.py",
      "line_number": 34,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        assert self.messages_to_prompt is not None\n\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/custom.py_34_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'chat'",
      "description": "Function 'chat' on line 34 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/custom.py",
      "line_number": 34,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        assert self.messages_to_prompt is not None\n\n        prompt = self.messages_to_prompt(messages)",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_63",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.structured_predict' is used in 'Response' on line 63 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 63,
      "code_snippet": "\n        output = self.llm.structured_predict(\n            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_79",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.stream_structured_predict' is used in 'Response' on line 79 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 79,
      "code_snippet": "\n        stream_output = self.llm.stream_structured_predict(\n            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_53",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 53 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 53,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat endpoint for LLM.\"\"\"\n        # TODO:\n\n        # NOTE: we are wrapping existing messages in a ChatPromptTemplate to\n        # make this work with our FunctionCallingProgram, even though\n        # the messages don't technically have any variables (they are already formatted)\n\n        chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n        output = self.llm.structured_predict(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_74",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'stream_chat' on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 74,
      "code_snippet": "    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n        stream_output = self.llm.stream_structured_predict(\n            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )\n        for partial_output in stream_output:\n            yield ChatResponse(\n                message=ChatMessage(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_53_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 53 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 53,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat endpoint for LLM.\"\"\"\n        # TODO:\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_74_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'stream_chat'",
      "description": "Function 'stream_chat' on line 74 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 74,
      "code_snippet": "\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        chat_prompt = ChatPromptTemplate(message_templates=messages)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py_53_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'chat'",
      "description": "Function 'chat' on line 53 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
      "line_number": 53,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat endpoint for LLM.\"\"\"\n        # TODO:\n",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py_623",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat' is used in 'Response' on line 623 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py",
      "line_number": 623,
      "code_snippet": "            messages = self._get_messages(prompt, **prompt_args)\n            chat_response = self.chat(messages)\n            output = chat_response.message.content or \"\"\n        else:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py_627",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.complete' is used in 'Response' on line 627 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py",
      "line_number": 627,
      "code_snippet": "            formatted_prompt = self._get_prompt(prompt, **prompt_args)\n            response = self.complete(formatted_prompt, formatted=True)\n            output = response.text\n        parsed_output = self._parse_output(output)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py_589",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'predict' on line 589 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py",
      "line_number": 589,
      "code_snippet": "    def predict(\n        self,\n        prompt: BasePromptTemplate,\n        **prompt_args: Any,\n    ) -> str:\n        \"\"\"\n        Predict for a given prompt.\n\n        Args:\n            prompt (BasePromptTemplate):\n                The prompt to use for prediction.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py_589_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'predict'",
      "description": "Function 'predict' on line 589 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/llm.py",
      "line_number": 589,
      "code_snippet": "\n    @dispatcher.span\n    def predict(\n        self,\n        prompt: BasePromptTemplate,\n        **prompt_args: Any,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_55",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat' is used in 'Response' on line 55 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 55,
      "code_snippet": "        )\n        response = self.chat(**chat_kwargs)\n        return self._validate_chat_with_tools_response(\n            response,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_228",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat_with_tools' is used in 'Response' on line 228 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 228,
      "code_snippet": "\n        response = self.chat_with_tools(\n            tools,\n            user_msg=user_msg,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_255",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.chat_with_tools' is used in 'Response' on line 255 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 255,
      "code_snippet": "            )\n            return AgentChatResponse(response=output_text, sources=tool_outputs)\n        else:\n            if len(tool_outputs) > 1:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_35",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat_with_tools' on line 35 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 35,
      "code_snippet": "    def chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        tool_required: bool = False,  # if required, LLM should only call tools, and not return a response\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Chat with function calling.\"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_202",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'predict_and_call' on line 202 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 202,
      "code_snippet": "    def predict_and_call(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        error_on_no_tool_call: bool = True,\n        error_on_tool_error: bool = False,\n        **kwargs: Any,\n    ) -> \"AgentChatResponse\":",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_202_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'predict_and_call'",
      "description": "Function 'predict_and_call' on line 202 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 202,
      "code_snippet": "        )\n\n    def predict_and_call(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py_202_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'predict_and_call'",
      "description": "Function 'predict_and_call' on line 202 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
      "line_number": 202,
      "code_snippet": "        )\n\n    def predict_and_call(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/utils.py_15",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'resolve_llm' on line 15 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/utils.py",
      "line_number": 15,
      "code_snippet": "def resolve_llm(\n    llm: Optional[LLMType] = None, callback_manager: Optional[CallbackManager] = None\n) -> LLM:\n    \"\"\"Resolve LLM from string or LLM instance.\"\"\"\n    from llama_index.core.settings import Settings\n\n    try:\n        from langchain.base_language import BaseLanguageModel  # pants: no-infer-dep\n    except ImportError:\n        BaseLanguageModel = None  # type: ignore\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/utils.py_118_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 118. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/utils.py",
      "line_number": 118,
      "code_snippet": "\n    NOTE: This is adapted from\n    https://github.com/OpenInterpreter/open-interpreter/blob/5b6080fae1f8c68938a1e4fa8667e3744084ee21/interpreter/utils/parse_partial_json.py\n    \"\"\"\n    # Attempt to parse the string as-is.\n    try:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/callbacks.py_259_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 259. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/callbacks.py",
      "line_number": 259,
      "code_snippet": "\n        # Update the wrapper function to look like the wrapped function.\n        # See e.g. https://github.com/python/cpython/blob/0abf997e75bd3a8b76d920d33cc64d5e6c2d380f/Lib/functools.py#L57\n        for attr in (\n            \"__module__\",\n            \"__name__\",",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/callbacks.py_519_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 519. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/callbacks.py",
      "line_number": 519,
      "code_snippet": "\n        # Update the wrapper function to look like the wrapped function.\n        # See e.g. https://github.com/python/cpython/blob/0abf997e75bd3a8b76d920d33cc64d5e6c2d380f/Lib/functools.py#L57\n        for attr in (\n            \"__module__\",\n            \"__name__\",",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py_149",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'super().chat' is used in 'Response' on line 149 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py",
      "line_number": 149,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        r = super().chat(copy.deepcopy(messages), **kwargs)\n        self.last_chat_messages = messages\n        self.last_called_chat_function.append(\"chat\")",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py_148",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'super().chat' is used in 'Response' on line 148 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py",
      "line_number": 148,
      "code_snippet": "    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        r = super().chat(copy.deepcopy(messages), **kwargs)\n        self.last_chat_messages = messages",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py_148",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 148 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py",
      "line_number": 148,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        r = super().chat(copy.deepcopy(messages), **kwargs)\n        self.last_chat_messages = messages\n        self.last_called_chat_function.append(\"chat\")\n        return r\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        r = super().stream_chat(copy.deepcopy(messages), **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py_148_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'chat'",
      "description": "Function 'chat' on line 148 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llms/mock.py",
      "line_number": 148,
      "code_snippet": "\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        r = super().chat(copy.deepcopy(messages), **kwargs)\n        self.last_chat_messages = messages\n        self.last_called_chat_function.append(\"chat\")",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py_151",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.generate_query' is used in 'Response' on line 151 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py",
      "line_number": 151,
      "code_snippet": "        \"\"\"Get nodes for response.\"\"\"\n        graph_store_query = self.generate_query(query_bundle.query_str)\n        if self._verbose:\n            print_text(f\"Graph Store Query:\\n{graph_store_query}\\n\", color=\"yellow\")",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py_164",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.generate_query' is used in 'Response' on line 164 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py",
      "line_number": 164,
      "code_snippet": "                print_text(\n                    f\"Graph Store Response:\\n{graph_store_response}\\n\",\n                    color=\"yellow\",\n                )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py_167",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.generate_query' is used in 'Response' on line 167 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py",
      "line_number": 167,
      "code_snippet": "                )\n            logger.debug(f\"Graph Store Response:\\n{graph_store_response}\")\n\n            retrieve_event.on_end(payload={EventPayload.RESPONSE: graph_store_response})",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py_125",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_query' on line 125 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py",
      "line_number": 125,
      "code_snippet": "    def generate_query(self, query_str: str) -> str:\n        \"\"\"Generate a Graph Store Query from a query bundle.\"\"\"\n        # Get the query engine query string\n\n        graph_store_query: str = self._llm.predict(\n            self._graph_query_synthesis_prompt,\n            query_str=query_str,\n            schema=self._graph_schema,\n        )\n\n        return graph_store_query",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py_149",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 149 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py",
      "line_number": 149,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        graph_store_query = self.generate_query(query_bundle.query_str)\n        if self._verbose:\n            print_text(f\"Graph Store Query:\\n{graph_store_query}\\n\", color=\"yellow\")\n        logger.debug(f\"Graph Store Query:\\n{graph_store_query}\")\n\n        with self.callback_manager.event(\n            CBEventType.RETRIEVE,\n            payload={EventPayload.QUERY_STR: graph_store_query},\n        ) as retrieve_event:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_152",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 152 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 152,
      "code_snippet": "        sql_query_response = metadata[\"sql_query_response\"]\n        new_query_str = self._llm.predict(\n            self._sql_augment_transform_prompt,\n            query_str=query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_288",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.stream' is used in 'Response' on line 288 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 288,
      "code_snippet": "        if self._streaming:\n            response_gen = self._llm.stream(\n                self._sql_join_synthesis_prompt,\n                query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_308",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 308 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 308,
      "code_snippet": "        else:\n            response_str = self._llm.predict(\n                self._sql_join_synthesis_prompt,\n                query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_147",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 147 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 147,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        query_str = query_bundle.query_str\n        sql_query = metadata[\"sql_query\"]\n        sql_query_response = metadata[\"sql_query_response\"]\n        new_query_str = self._llm.predict(\n            self._sql_augment_transform_prompt,\n            query_str=query_str,\n            sql_query_str=sql_query,\n            sql_response_str=sql_query_response,\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_250",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_query_sql_other' on line 250 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 250,
      "code_snippet": "    def _query_sql_other(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query SQL database + other query engine in sequence.\"\"\"\n        # first query SQL database\n        sql_response = self._sql_query_tool.query_engine.query(query_bundle)\n        if not self._use_sql_join_synthesis:\n            return sql_response\n\n        sql_query = (\n            sql_response.metadata[\"sql_query\"] if sql_response.metadata else None\n        )\n        if self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_147_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run'",
      "description": "Function '_run' on line 147 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 147,
      "code_snippet": "            self._sql_augment_transform_prompt = prompts[\"sql_augment_transform_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        query_str = query_bundle.query_str\n        sql_query = metadata[\"sql_query\"]",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py_147_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run'",
      "description": "Function '_run' on line 147 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
      "line_number": 147,
      "code_snippet": "            self._sql_augment_transform_prompt = prompts[\"sql_augment_transform_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        query_str = query_bundle.query_str\n        sql_query = metadata[\"sql_query\"]",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_133",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 133 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 133,
      "code_snippet": "\n        llm_response = self._multi_modal_llm.chat(\n            [ChatMessage(role=\"user\", blocks=blocks)]\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_161",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 161 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 161,
      "code_snippet": "\n        llm_response = self._multi_modal_llm.chat(\n            [ChatMessage(role=\"user\", blocks=blocks)]\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_111",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'synthesize' on line 111 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 111,
      "code_snippet": "    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]\n        )\n        fmt_prompt = self._text_qa_template.format(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_142",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_get_response_with_images' on line 142 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 142,
      "code_snippet": "    def _get_response_with_images(\n        self,\n        prompt_str: str,\n        image_nodes: List[NodeWithScore],\n    ) -> RESPONSE_TYPE:\n        assert all(isinstance(node.node, ImageNode) for node in image_nodes)\n\n        fmt_prompt = self._image_qa_template.format(\n            query_str=prompt_str,\n        )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_111_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'synthesize'",
      "description": "Function 'synthesize' on line 111 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 111,
      "code_snippet": "        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_142_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_response_with_images'",
      "description": "Function '_get_response_with_images' on line 142 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 142,
      "code_snippet": "        )\n\n    def _get_response_with_images(\n        self,\n        prompt_str: str,\n        image_nodes: List[NodeWithScore],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_111_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'synthesize'",
      "description": "Function 'synthesize' on line 111 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 111,
      "code_snippet": "        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py_142_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_response_with_images'",
      "description": "Function '_get_response_with_images' on line 142 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
      "line_number": 142,
      "code_snippet": "        )\n\n    def _get_response_with_images(\n        self,\n        prompt_str: str,\n        image_nodes: List[NodeWithScore],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/cogniswitch_query_engine.py_24_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 24. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/cogniswitch_query_engine.py",
      "line_number": 24,
      "code_snippet": "        self.apiKey = apiKey\n        self.knowledge_request_endpoint = (\n            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeRequest\"\n        )\n        self.headers = {\n            \"apiKey\": self.apiKey,",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py_138",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._question_gen.generate' is used in 'Response' on line 138 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py",
      "line_number": 138,
      "code_snippet": "        ) as query_event:\n            sub_questions = self._question_gen.generate(self._metadatas, query_bundle)\n\n            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py_134",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 134 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py",
      "line_number": 134,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            sub_questions = self._question_gen.generate(self._metadatas, query_bundle)\n\n            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n\n            if self._verbose:\n                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py_134_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_query'",
      "description": "Function '_query' on line 134 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py",
      "line_number": 134,
      "code_snippet": "        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py_134_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_query'",
      "description": "Function '_query' on line 134 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py",
      "line_number": 134,
      "code_snippet": "        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py_33",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 33 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py",
      "line_number": 33,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec.\"\"\"\n        retrieval_spec = self.generate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)\n        return retriever.retrieve(new_query_bundle)\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec asynchronously.\"\"\"\n        retrieval_spec = await self.agenerate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)\n        return await retriever.aretrieve(new_query_bundle)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py_33_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_retrieve'",
      "description": "Function '_retrieve' on line 33 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py",
      "line_number": 33,
      "code_snippet": "        ...\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec.\"\"\"\n        retrieval_spec = self.generate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py_33_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_retrieve'",
      "description": "Function '_retrieve' on line 33 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py",
      "line_number": 33,
      "code_snippet": "        ...\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec.\"\"\"\n        retrieval_spec = self.generate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py_54",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_query_embedding' is used in 'SELECT' on line 54 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py",
      "line_number": 54,
      "code_snippet": "    ) -> SelectorResult:\n        query_embedding = self._embed_model.get_query_embedding(query.query_str)\n        text_embeddings = [\n            self._embed_model.get_text_embedding(choice.description)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py_56",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding' is used in 'SELECT' on line 56 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py",
      "line_number": 56,
      "code_snippet": "        text_embeddings = [\n            self._embed_model.get_text_embedding(choice.description)\n            for choice in choices\n        ]",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py_51",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_select' on line 51 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py",
      "line_number": 51,
      "code_snippet": "    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        query_embedding = self._embed_model.get_query_embedding(query.query_str)\n        text_embeddings = [\n            self._embed_model.get_text_embedding(choice.description)\n            for choice in choices\n        ]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            query_embedding,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py_51_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_select'",
      "description": "Function '_select' on line 51 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py",
      "line_number": 51,
      "code_snippet": "        \"\"\"Update prompts.\"\"\"\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        query_embedding = self._embed_model.get_query_embedding(query.query_str)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py_51_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_select'",
      "description": "Function '_select' on line 51 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py",
      "line_number": 51,
      "code_snippet": "        \"\"\"Update prompts.\"\"\"\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        query_embedding = self._embed_model.get_query_embedding(query.query_str)",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py_108",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'SELECT' on line 108 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
      "line_number": 108,
      "code_snippet": "        # predict\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            num_choices=len(choices),",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py_205",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'SELECT' on line 205 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
      "line_number": 205,
      "code_snippet": "\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            num_choices=len(choices),",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py_101",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_select' on line 101 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
      "line_number": 101,
      "code_snippet": "    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        choices_text = _build_choices_text(choices)\n\n        # predict\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            num_choices=len(choices),\n            context_list=choices_text,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py_198",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_select' on line 198 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
      "line_number": 198,
      "code_snippet": "    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        context_list = _build_choices_text(choices)\n        max_outputs = self._max_outputs or len(choices)\n\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            num_choices=len(choices),\n            max_outputs=max_outputs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py_101_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_select'",
      "description": "Function '_select' on line 101 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
      "line_number": 101,
      "code_snippet": "            self._prompt = prompts[\"prompt\"]\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py_198_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_select'",
      "description": "Function '_select' on line 198 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
      "line_number": 198,
      "code_snippet": "            self._prompt = prompts[\"prompt\"]\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/response/schema.py_86_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 86. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/response/schema.py",
      "line_number": 86,
      "code_snippet": "\n        Sources:\n            - https://docs.pydantic.dev/1.10/usage/dataclasses/#use-of-stdlib-dataclasses-with-basemodel\n            - https://docs.pydantic.dev/1.10/usage/dataclasses/#initialize-hooks\n        \"\"\"\n        return",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/response/schema.py_87_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 87. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/base/response/schema.py",
      "line_number": 87,
      "code_snippet": "        Sources:\n            - https://docs.pydantic.dev/1.10/usage/dataclasses/#use-of-stdlib-dataclasses-with-basemodel\n            - https://docs.pydantic.dev/1.10/usage/dataclasses/#initialize-hooks\n        \"\"\"\n        return\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py_88",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.73,
      "title": "User input 'num_questions_generated' directly embedded in LLM prompt",
      "description": "The function 'generate_qa_embedding_pairs' embeds user input ('num_questions_generated') directly into an LLM prompt using format_call. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py",
      "line_number": 88,
      "code_snippet": "    for node_id, text in tqdm(node_dict.items()):\n        query = qa_generate_prompt_tmpl.format(\n            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n        )\n        response = llm.complete(query)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py_91",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'llm.complete' is used in 'Response' on line 91 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py",
      "line_number": 91,
      "code_snippet": "        )\n        response = llm.complete(query)\n\n        result = str(response).strip().split(\"\\n\")",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py_72",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'generate_qa_embedding_pairs' on line 72 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py",
      "line_number": 72,
      "code_snippet": "def generate_qa_embedding_pairs(\n    nodes: List[TextNode],\n    llm: Optional[LLM] = None,\n    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,\n    num_questions_per_chunk: int = 2,\n) -> EmbeddingQAFinetuneDataset:\n    \"\"\"Generate examples given a set of nodes.\"\"\"\n    llm = llm or Settings.llm\n    node_dict = {\n        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)\n        for node in nodes",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py_72_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'generate_qa_embedding_pairs'",
      "description": "Function 'generate_qa_embedding_pairs' on line 72 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py",
      "line_number": 72,
      "code_snippet": "\n# generate queries as a convenience function\ndef generate_qa_embedding_pairs(\n    nodes: List[TextNode],\n    llm: Optional[LLM] = None,\n    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py_179",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 179 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py",
      "line_number": 179,
      "code_snippet": "\n        return self._llm.predict(\n            self._answer_insert_prompt,\n            lookahead_response=response,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py_165",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'insert' on line 165 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py",
      "line_number": 165,
      "code_snippet": "    def insert(\n        self,\n        response: str,\n        query_tasks: List[QueryTask],\n        answers: List[str],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Insert answers into response.\"\"\"\n        prev_response = prev_response or \"\"\n\n        query_answer_pairs = \"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py_165_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'insert'",
      "description": "Function 'insert' on line 165 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py",
      "line_number": 165,
      "code_snippet": "            self._answer_insert_prompt = prompts[\"answer_insert_prompt\"]\n\n    def insert(\n        self,\n        response: str,\n        query_tasks: List[QueryTask],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_190",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'query_bundle' directly embedded in LLM prompt",
      "description": "The function '_query' embeds user input ('query_bundle') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 190,
      "code_snippet": "        \"\"\"Query and get response.\"\"\"\n        print_text(f\"Query: {query_bundle.query_str}\\n\", color=\"green\")\n        cur_response = \"\"\n        source_nodes = []\n        for iter in range(self._max_iterations):\n            if self._verbose:\n                print_text(f\"Current response: {cur_response}\\n\", color=\"blue\")\n            # generate \"lookahead response\" that contains \"[Search(query)]\" tags\n            # e.g.\n            # The colors on the flag of Ghana have the following meanings. Red is\n            # for [Search(Ghana flag meaning)],...\n            lookahead_resp = self._llm.predict(\n                self._instruct_prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_252",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'lookahead_resp' directly embedded in LLM prompt",
      "description": "The function '_query' embeds user input ('lookahead_resp') directly into an LLM prompt using concatenation. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 252,
      "code_snippet": "",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_200",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 200 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 200,
      "code_snippet": "            # for [Search(Ghana flag meaning)],...\n            lookahead_resp = self._llm.predict(\n                self._instruct_prompt,\n                query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_257",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 257 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 257,
      "code_snippet": "        # NOTE: at the moment, does not support streaming\n        return Response(response=cur_response, source_nodes=source_nodes)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_188",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 188 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 188,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query and get response.\"\"\"\n        print_text(f\"Query: {query_bundle.query_str}\\n\", color=\"green\")\n        cur_response = \"\"\n        source_nodes = []\n        for iter in range(self._max_iterations):\n            if self._verbose:\n                print_text(f\"Current response: {cur_response}\\n\", color=\"blue\")\n            # generate \"lookahead response\" that contains \"[Search(query)]\" tags\n            # e.g.\n            # The colors on the flag of Ghana have the following meanings. Red is",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_31_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 31. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 31,
      "code_snippet": "\n# These prompts are taken from the FLARE repo:\n# https://github.com/jzbjyb/FLARE/blob/main/src/templates.py\n\nDEFAULT_EXAMPLES = \"\"\"\nQuery: But what are the risks during production of nanomaterials?",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_188_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_query'",
      "description": "Function '_query' on line 188 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 188,
      "code_snippet": "        return relevant_lookahead_resp\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query and get response.\"\"\"\n        print_text(f\"Query: {query_bundle.query_str}\\n\", color=\"green\")\n        cur_response = \"\"",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py_188_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_query'",
      "description": "Function '_query' on line 188 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
      "line_number": 188,
      "code_snippet": "        return relevant_lookahead_resp\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query and get response.\"\"\"\n        print_text(f\"Query: {query_bundle.query_str}\\n\", color=\"green\")\n        cur_response = \"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/file/base.py_225_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 225. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/file/base.py",
      "line_number": 225,
      "code_snippet": "            Default is utf-8.\n        errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        recursive (bool): Whether to recursively search in subdirectories.\n            False by default.\n        filename_as_id (bool): Whether to use the filename as the document id.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/file/base.py_581_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 581. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/readers/file/base.py",
      "line_number": 581,
      "code_snippet": "                Default is utf-8.\n            errors (str): how encoding and decoding errors are to be handled,\n                see https://docs.python.org/3/library/functions.html#open\n            raise_on_error (bool): Whether to raise an error if a file cannot be read.\n            fs (Optional[fsspec.AbstractFileSystem]): File system to use. Defaults\n                to using the local file system. Can be changed to use any remote file system",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py_19_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 19. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py",
      "line_number": 19,
      "code_snippet": "from llama_index.core.utils import get_cache_dir\n\nDEV_DISTRACTOR_URL = \"\"\"https://web.archive.org/web/20250512032701id_/http://curtis.ml.cmu.edu/datasets/\\\nhotpot/hotpot_dev_distractor_v1.json\"\"\"\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py_19_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'http://' found on line 19. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py",
      "line_number": 19,
      "code_snippet": "from llama_index.core.utils import get_cache_dir\n\nDEV_DISTRACTOR_URL = \"\"\"https://web.archive.org/web/20250512032701id_/http://curtis.ml.cmu.edu/datasets/\\\nhotpot/hotpot_dev_distractor_v1.json\"\"\"\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py_25_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 25. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py",
      "line_number": 25,
      "code_snippet": "class HotpotQAEvaluator:\n    \"\"\"\n    Refer to https://hotpotqa.github.io/ for more details on the dataset.\n    \"\"\"\n\n    def _download_datasets(self) -> Dict[str, str]:",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py_162_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 162. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py",
      "line_number": 162,
      "code_snippet": "\n\"\"\"\nUtils from https://github.com/hotpotqa/hotpot/blob/master/hotpot_evaluate_v1.py\n\"\"\"\n\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/beir.py_14_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 14. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/beir.py",
      "line_number": 14,
      "code_snippet": "class BeirEvaluator:\n    \"\"\"\n    Refer to: https://github.com/beir-cellar/beir for a full list of supported datasets\n    and a full description of BEIR.\n    \"\"\"\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/beir.py_35_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 35. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/beir.py",
      "line_number": 35,
      "code_snippet": "            dataset_full_path = os.path.join(cache_dir, \"datasets\", \"BeIR__\" + dataset)\n            if not os.path.exists(dataset_full_path):\n                url = f\"\"\"https://public.ukp.informatik.tu-darmstadt.de/thakur\\\n/BEIR/datasets/{dataset}.zip\"\"\"\n                try:\n                    util.download_and_unzip(url, dataset_full_path)",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py_164",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 164 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py",
      "line_number": 164,
      "code_snippet": "\n        response_obj = self._multi_modal_llm.chat(\n            messages=[ChatMessage(role=\"user\", blocks=image_nodes)],\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py_133",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'evaluate' on line 133 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py",
      "line_number": 133,
      "code_snippet": "    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,\n        contexts: Union[Sequence[str], None] = None,\n        image_paths: Union[List[str], None] = None,\n        image_urls: Union[List[str], None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the response is faithful to the multi-modal contexts.\"\"\"\n        del query  # Unused",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py_133_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'evaluate'",
      "description": "Function 'evaluate' on line 133 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py",
      "line_number": 133,
      "code_snippet": "            self._refine_template = prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py_133_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'evaluate'",
      "description": "Function 'evaluate' on line 133 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py",
      "line_number": 133,
      "code_snippet": "            self._refine_template = prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py_144",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._multi_modal_llm.chat' is used in 'Response' on line 144 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py",
      "line_number": 144,
      "code_snippet": "\n        response_obj = self._multi_modal_llm.chat(\n            messages=[ChatMessage(role=\"user\", blocks=blocks)],\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py_112",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'evaluate' on line 112 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py",
      "line_number": 112,
      "code_snippet": "    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,\n        contexts: Union[Sequence[str], None] = None,\n        image_paths: Union[List[str], None] = None,\n        image_urls: Union[List[str], None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the multi-modal contexts and response are relevant to the query.\"\"\"\n        del kwargs  # Unused",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py_112_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'evaluate'",
      "description": "Function 'evaluate' on line 112 makes critical security decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py",
      "line_number": 112,
      "code_snippet": "            self._refine_template = prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py_112_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'evaluate'",
      "description": "Function 'evaluate' on line 112 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py",
      "line_number": 112,
      "code_snippet": "            self._refine_template = prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py_58",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'model.model_validate' is used in 'Response' on line 58 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py",
      "line_number": 58,
      "code_snippet": "        try:\n            return model.model_validate(self.output)\n        except ValidationError as e:\n            warnings.warn(",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py_84",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'model.model_validate' is used in 'Response' on line 84 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py",
      "line_number": 84,
      "code_snippet": "        try:\n            return model.model_validate(self.structured_response)\n        except ValidationError as e:\n            warnings.warn(",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py_54",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_pydantic_model' on line 54 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py",
      "line_number": 54,
      "code_snippet": "    def get_pydantic_model(self, model: Type[BaseModel]) -> Optional[BaseModel]:\n        if self.output is None:\n            return self.output\n        try:\n            return model.model_validate(self.output)\n        except ValidationError as e:\n            warnings.warn(\n                f\"Conversion of structured response to Pydantic model failed because:\\n\\n{e.title}\\n\\nPlease check the model you provided.\",\n                PydanticConversionWarning,\n            )\n            return None",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py_80",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_pydantic_model' on line 80 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py",
      "line_number": 80,
      "code_snippet": "    def get_pydantic_model(self, model: Type[BaseModel]) -> Optional[BaseModel]:\n        if self.structured_response is None:\n            return self.structured_response\n        try:\n            return model.model_validate(self.structured_response)\n        except ValidationError as e:\n            warnings.warn(\n                f\"Conversion of structured response to Pydantic model failed because:\\n\\n{e.title}\\n\\nPlease check the model you provided.\",\n                PydanticConversionWarning,\n            )\n            return None",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py_54_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'get_pydantic_model'",
      "description": "Function 'get_pydantic_model' on line 54 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py",
      "line_number": 54,
      "code_snippet": "    output: Dict[str, Any]\n\n    def get_pydantic_model(self, model: Type[BaseModel]) -> Optional[BaseModel]:\n        if self.output is None:\n            return self.output\n        try:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py_160",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'build_semantic_nodes_from_documents' on line 160 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py",
      "line_number": 160,
      "code_snippet": "    def build_semantic_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"Build window nodes from documents.\"\"\"\n        all_nodes: List[BaseNode] = []\n        for doc in documents:\n            text = doc.text\n            text_splits = self.sentence_splitter(text)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py_263",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_calculate_distances_between_sentence_groups' on line 263 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py",
      "line_number": 263,
      "code_snippet": "    def _calculate_distances_between_sentence_groups(\n        self, sentences: List[SentenceCombination]\n    ) -> List[float]:\n        distances = []\n        for i in range(len(sentences) - 1):\n            embedding_current = sentences[i][\"combined_sentence_embedding\"]\n            embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n\n            similarity = self.embed_model.similarity(embedding_current, embedding_next)\n\n            distance = 1 - similarity",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py_160_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'build_semantic_nodes_from_documents'",
      "description": "Function 'build_semantic_nodes_from_documents' on line 160 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py",
      "line_number": 160,
      "code_snippet": "        return all_nodes\n\n    def build_semantic_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py_263_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_calculate_distances_between_sentence_groups'",
      "description": "Function '_calculate_distances_between_sentence_groups' on line 263 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py",
      "line_number": 263,
      "code_snippet": "        return sentences\n\n    def _calculate_distances_between_sentence_groups(\n        self, sentences: List[SentenceCombination]\n    ) -> List[float]:\n        distances = []",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py_22_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 22. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py",
      "line_number": 22,
      "code_snippet": "\n    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n    https://docs.sweep.dev/blogs/chunking-2m-files\n    \"\"\"\n\n    language: str = Field(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py_87_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 87. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py",
      "line_number": 87,
      "code_snippet": "                print(\n                    f\"Could not get parser for language {language}. Check \"\n                    \"https://github.com/Goldziher/tree-sitter-language-pack?tab=readme-ov-file#available-languages \"\n                    \"for a list of valid languages.\"\n                )\n                raise",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py_195_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 195. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py",
      "line_number": 195,
      "code_snippet": "                raise ValueError(f\"Could not parse code with language {self.language}.\")\n\n        # TODO: set up auto-language detection using something like https://github.com/yoeo/guesslang.",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_151",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 151 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 151,
      "code_snippet": "            context_msg = selected_node.get_content(metadata_mode=MetadataMode.LLM)\n            cur_response = self._llm.predict(\n                self._refine_template,\n                query_str=query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_309",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 309 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 309,
      "code_snippet": "\n            response = self._llm.predict(\n                query_template,\n                context_list=numbered_node_text,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_329",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 329 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 329,
      "code_snippet": "\n            response = self._llm.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_193",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 193 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 193,
      "code_snippet": "\n            response = self._llm.predict(\n                query_template,\n                context_list=numbered_node_text,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_213",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 213 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 213,
      "code_snippet": "\n            response = self._llm.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_110",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_query_with_selected_node' on line 110 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 110,
      "code_snippet": "    def _query_with_selected_node(\n        self,\n        selected_node: BaseNode,\n        query_bundle: QueryBundle,\n        prev_response: Optional[str] = None,\n        level: int = 0,\n    ) -> str:\n        \"\"\"\n        Get response for selected node.\n\n        If not leaf node, it will recursively call _query on the child nodes.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_161",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_query_level' on line 161 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 161,
      "code_snippet": "    def _query_level(\n        self,\n        cur_node_ids: Dict[int, str],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        query_str = query_bundle.query_str\n        cur_nodes = {\n            index: self._docstore.get_node(node_id)\n            for index, node_id in cur_node_ids.items()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_288",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_select_nodes' on line 288 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 288,
      "code_snippet": "    def _select_nodes(\n        self,\n        cur_node_list: List[BaseNode],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> List[BaseNode]:\n        query_str = query_bundle.query_str\n\n        if self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_110_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_query_with_selected_node'",
      "description": "Function '_query_with_selected_node' on line 110 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 110,
      "code_snippet": "        )\n\n    def _query_with_selected_node(\n        self,\n        selected_node: BaseNode,\n        query_bundle: QueryBundle,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_110_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_query_with_selected_node'",
      "description": "Function '_query_with_selected_node' on line 110 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 110,
      "code_snippet": "        )\n\n    def _query_with_selected_node(\n        self,\n        selected_node: BaseNode,\n        query_bundle: QueryBundle,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_110_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_query_with_selected_node'",
      "description": "Function '_query_with_selected_node' on line 110 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 110,
      "code_snippet": "        )\n\n    def _query_with_selected_node(\n        self,\n        selected_node: BaseNode,\n        query_bundle: QueryBundle,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_161_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_query_level'",
      "description": "Function '_query_level' on line 161 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 161,
      "code_snippet": "            return str(cur_response)\n\n    def _query_level(\n        self,\n        cur_node_ids: Dict[int, str],\n        query_bundle: QueryBundle,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py_288_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_select_nodes'",
      "description": "Function '_select_nodes' on line 288 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
      "line_number": 288,
      "code_snippet": "        return Response(response_str, source_nodes=[])\n\n    def _select_nodes(\n        self,\n        cur_node_list: List[BaseNode],\n        query_bundle: QueryBundle,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py_106",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_get_query_text_embedding_similarities' on line 106 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py",
      "line_number": 106,
      "code_snippet": "    def _get_query_text_embedding_similarities(\n        self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py_106_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_query_text_embedding_similarities'",
      "description": "Function '_get_query_text_embedding_similarities' on line 106 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py",
      "line_number": 106,
      "code_snippet": "        return cast(str, result_response)\n\n    def _get_query_text_embedding_similarities(\n        self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> List[float]:\n        \"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_85",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'INSERT' on line 85 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 85,
      "code_snippet": "\n            summary1 = self._llm.predict(self.summary_prompt, context_str=text_chunk1)\n            node1 = TextNode(text=summary1)\n            self.index_graph.insert(node1, children_nodes=half1)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_97",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'INSERT' on line 97 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 97,
      "code_snippet": "            text_chunk2 = \"\\n\".join(truncated_chunks)\n            summary2 = self._llm.predict(self.summary_prompt, context_str=text_chunk2)\n            node2 = TextNode(text=summary2)\n            self.index_graph.insert(node2, children_nodes=half2)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_173",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'INSERT' on line 173 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 173,
      "code_snippet": "            text_chunk = \"\\n\".join(truncated_chunks)\n            new_summary = self._llm.predict(self.summary_prompt, context_str=text_chunk)\n\n            parent_node.set_content(new_summary)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_140",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 140 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 140,
      "code_snippet": "            )\n            response = self._llm.predict(\n                self.insert_prompt,\n                new_chunk_text=node.get_content(metadata_mode=MetadataMode.LLM),",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_49",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_insert_under_parent_and_consolidate' on line 49 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 49,
      "code_snippet": "    def _insert_under_parent_and_consolidate(\n        self, text_node: BaseNode, parent_node: Optional[BaseNode]\n    ) -> None:\n        \"\"\"\n        Insert node under parent and consolidate.\n\n        Consolidation will happen by dividing up child nodes, and creating a new\n        intermediate layer of nodes.\n\n        \"\"\"\n        # perform insertion",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_116",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_insert_node' on line 116 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 116,
      "code_snippet": "    def _insert_node(\n        self, node: BaseNode, parent_node: Optional[BaseNode] = None\n    ) -> None:\n        \"\"\"Insert node.\"\"\"\n        cur_graph_node_ids = self.index_graph.get_children(parent_node)\n        cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)\n        cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n        # if cur_graph_nodes is empty (start with empty graph), then insert under\n        # parent (insert new root node)\n        if len(cur_graph_nodes) == 0:\n            self._insert_under_parent_and_consolidate(node, parent_node)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_49_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_insert_under_parent_and_consolidate'",
      "description": "Function '_insert_under_parent_and_consolidate' on line 49 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 49,
      "code_snippet": "        self._docstore = docstore or get_default_docstore()\n\n    def _insert_under_parent_and_consolidate(\n        self, text_node: BaseNode, parent_node: Optional[BaseNode]\n    ) -> None:\n        \"\"\"",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py_49_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_insert_under_parent_and_consolidate'",
      "description": "Function '_insert_under_parent_and_consolidate' on line 49 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
      "line_number": 49,
      "code_snippet": "        self._docstore = docstore or get_default_docstore()\n\n    def _insert_under_parent_and_consolidate(\n        self, text_node: BaseNode, parent_node: Optional[BaseNode]\n    ) -> None:\n        \"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_260",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'table_desc_str' directly embedded in LLM prompt",
      "description": "The function '_query' embeds user input ('table_desc_str') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 260,
      "code_snippet": "        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_271",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'sql_query_str' directly embedded in LLM prompt",
      "description": "The function '_query' embeds user input ('sql_query_str') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 271,
      "code_snippet": "        # assume that it's a valid SQL query\n        logger.debug(f\"> Predicted SQL query: {sql_query_str}\")\n\n        raw_response_str, metadata = self._run_with_sql_only_check(sql_query_str)\n\n        metadata[\"sql_query\"] = sql_query_str\n\n        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_262",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 262 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 262,
      "code_snippet": "\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_287",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 287 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 287,
      "code_snippet": "\n        return Response(response=response_str, metadata=metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_278",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 278 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 278,
      "code_snippet": "        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,\n                query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_287",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 287 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 287,
      "code_snippet": "\n        return Response(response=response_str, metadata=metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py_257",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 257 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
      "line_number": 257,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,\n            schema=table_desc_str,\n            dialect=self._sql_database.dialect,\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_170",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'response_str' directly embedded in LLM prompt",
      "description": "The function '_query' embeds user input ('response_str') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 170,
      "code_snippet": "            print_text(\n                f\"> JSONPath Instructions:\\n```\\n{json_path_response_str}\\n```\\n\"\n            )\n\n        json_path_output = self._output_processor(\n            json_path_response_str,\n            self._json_value,\n            **self._output_kwargs,\n        )\n\n        # removes JSONPath: prefix from returned JSON path prompt call\n        if self._json_path_prompt == DEFAULT_JSON_PATH_PROMPT:\n            json_path_response_str = default_output_response_parser(\n                json_path_response_str\n            )\n\n        if self._verbose:\n            print_text(f\"> JSONPath Output: {json_path_output}\\n\")\n\n        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_186",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'json_path_output' directly embedded in LLM prompt",
      "description": "The function '_query' embeds user input ('json_path_output') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 186,
      "code_snippet": "        if self._verbose:\n            print_text(f\"> JSONPath Output: {json_path_output}\\n\")\n\n        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_162",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 162 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 162,
      "code_snippet": "\n        json_path_response_str = self._llm.predict(\n            self._json_path_prompt,\n            schema=schema,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_203",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 203 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 203,
      "code_snippet": "\n        return Response(response=response_str, metadata=response_metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_189",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 189 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 189,
      "code_snippet": "        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,\n                query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_203",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 203 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 203,
      "code_snippet": "\n        return Response(response=response_str, metadata=response_metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_query' on line 158 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 158,
      "code_snippet": "    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        schema = self._get_schema_context()\n\n        json_path_response_str = self._llm.predict(\n            self._json_path_prompt,\n            schema=schema,\n            query_str=query_bundle.query_str,\n        )\n\n        if self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py_158_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_query'",
      "description": "Function '_query' on line 158 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
      "line_number": 158,
      "code_snippet": "        return json.dumps(self._json_schema)\n\n    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        schema = self._get_schema_context()\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_310",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'table_desc_str' directly embedded in LLM prompt",
      "description": "The function 'retrieve_with_metadata' embeds user input ('table_desc_str') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 310,
      "code_snippet": "        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n        if self._verbose:\n            print(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_174",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._embed_model.get_query_embedding' is used in 'Response' on line 174 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 174,
      "code_snippet": "        raw_sql_str = response.strip().strip(\"```sql\").strip(\"```\").strip()\n        query_embedding = self._embed_model.get_query_embedding(query_bundle.query_str)\n        query_embedding_str = str(query_embedding)\n        return raw_sql_str.replace(\"[query_vector]\", query_embedding_str)",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_314",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 314 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 314,
      "code_snippet": "\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_160",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'parse_response_to_sql' on line 160 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 160,
      "code_snippet": "    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:\n            response = response[sql_query_start:]\n            # TODO: move to removeprefix after Python 3.9+\n            if response.startswith(\"SQLQuery:\"):\n                response = response[len(\"SQLQuery:\") :]\n        sql_result_start = response.find(\"SQLResult:\")\n        if sql_result_start != -1:\n            response = response[:sql_result_start]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_301",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve_with_metadata' on line 301 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 301,
      "code_snippet": "    def retrieve_with_metadata(\n        self, str_or_query_bundle: QueryType\n    ) -> Tuple[List[NodeWithScore], Dict]:\n        \"\"\"Retrieve with metadata.\"\"\"\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n        if self._verbose:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_160_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'parse_response_to_sql'",
      "description": "Function 'parse_response_to_sql' on line 160 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 160,
      "code_snippet": "        self._embed_model = embed_model\n\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_160_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'parse_response_to_sql'",
      "description": "Function 'parse_response_to_sql' on line 160 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 160,
      "code_snippet": "        self._embed_model = embed_model\n\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py_160_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'parse_response_to_sql'",
      "description": "Function 'parse_response_to_sql' on line 160 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
      "line_number": 160,
      "code_snippet": "        self._embed_model = embed_model\n\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_159",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 159 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 159,
      "code_snippet": "        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_574",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 574 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 574,
      "code_snippet": "        if handle_llm_prompt_template is not None:\n            response = self._llm.predict(\n                handle_llm_prompt_template,\n                max_keywords=max_items,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_157",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_keywords' on line 157 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 157,
      "code_snippet": "    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,\n            question=query_str,\n        )\n        keywords = extract_keywords_given_response(\n            response, start_token=\"KEYWORDS:\", lowercase=False\n        )\n        return list(keywords)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_190",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 190 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 190,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        node_visited = set()\n        keywords = self._get_keywords(query_bundle.query_str)\n        if self._verbose:\n            print_text(f\"Extracted keywords: {keywords}\\n\", color=\"green\")\n        rel_texts = []\n        cur_rel_map = {}",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_541",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_process_entities' on line 541 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 541,
      "code_snippet": "    def _process_entities(\n        self,\n        query_str: str,\n        handle_fn: Optional[Callable],\n        handle_llm_prompt_template: Optional[BasePromptTemplate],\n        cross_handle_policy: Optional[str] = \"union\",\n        max_items: Optional[int] = 5,\n        result_start_token: str = \"KEYWORDS:\",\n    ) -> List[str]:\n        \"\"\"Get entities from query string.\"\"\"\n        assert cross_handle_policy in [",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_190_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in '_retrieve'",
      "description": "Function '_retrieve' on line 190 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 190,
      "code_snippet": "        return keywords\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_190_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_retrieve'",
      "description": "Function '_retrieve' on line 190 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 190,
      "code_snippet": "        return keywords\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_157_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_keywords'",
      "description": "Function '_get_keywords' on line 157 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 157,
      "code_snippet": "        )\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py_541_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_process_entities'",
      "description": "Function '_process_entities' on line 541 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
      "line_number": 541,
      "code_snippet": "        super().__init__(callback_manager=callback_manager or Settings.callback_manager)\n\n    def _process_entities(\n        self,\n        query_str: str,\n        handle_fn: Optional[Callable],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_160",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 160 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 160,
      "code_snippet": "        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.kg_triplet_extract_template,\n            text=text,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_273",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding' is used in 'INSERT' on line 273 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 273,
      "code_snippet": "        if include_embeddings:\n            set_embedding = self._embed_model.get_text_embedding(triplet_str)\n            self._index_struct.add_to_embedding_dict(str(triplet), set_embedding)\n            self._storage_context.index_store.add_index_struct(self._index_struct)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_315",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding' is used in 'DELETE' on line 315 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 315,
      "code_snippet": "        if include_embeddings:\n            set_embedding = self._embed_model.get_text_embedding(triplet_str)\n            self._index_struct.add_to_embedding_dict(str(triplet), set_embedding)\n            self._storage_context.index_store.add_index_struct(self._index_struct)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_226",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding_batch' is used in 'INSERT' on line 226 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 226,
      "code_snippet": "\n                embed_outputs = self._embed_model.get_text_embedding_batch(\n                    triplet_texts, show_progress=self._show_progress\n                )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_250",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding' is used in 'UPDATE' on line 250 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 250,
      "code_snippet": "                ):\n                    rel_embedding = self._embed_model.get_text_embedding(triplet_str)\n                    self._index_struct.add_to_embedding_dict(triplet_str, rel_embedding)\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_llm_extract_triplets' on line 158 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 158,
      "code_snippet": "    def _llm_extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.kg_triplet_extract_template,\n            text=text,\n        )\n        return self._parse_triplet_response(\n            response, max_length=self._max_object_length\n        )\n\n    @staticmethod",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_204",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_build_index_from_nodes' on line 204 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 204,
      "code_snippet": "    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> KG:\n        \"\"\"Build the index from nodes.\"\"\"\n        # do simple concatenation\n        index_struct = self.index_struct_cls()\n        nodes_with_progress = get_tqdm_iterable(\n            nodes, self._show_progress, \"Processing nodes\"\n        )\n        for n in nodes_with_progress:\n            triplets = self._extract_triplets(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_234",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_insert' on line 234 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 234,
      "code_snippet": "    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        for n in nodes:\n            triplets = self._extract_triplets(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            logger.debug(f\"Extracted triplets: {triplets}\")\n            for triplet in triplets:\n                subj, _, obj = triplet\n                triplet_str = str(triplet)\n                self.upsert_triplet(triplet)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_256",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'upsert_triplet' on line 256 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 256,
      "code_snippet": "    def upsert_triplet(\n        self, triplet: Tuple[str, str, str], include_embeddings: bool = False\n    ) -> None:\n        \"\"\"\n        Insert triplets and optionally embeddings.\n\n        Used for manual insertion of KG triplets (in the form\n        of (subject, relationship, object)).\n\n        Args:\n            triplet (tuple): Knowledge triplet",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_291",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'upsert_triplet_and_node' on line 291 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 291,
      "code_snippet": "    def upsert_triplet_and_node(\n        self,\n        triplet: Tuple[str, str, str],\n        node: BaseNode,\n        include_embeddings: bool = False,\n    ) -> None:\n        \"\"\"\n        Upsert KG triplet and node.\n\n        Calls both upsert_triplet and add_node.\n        Behavior is idempotent; if Node already exists,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_204_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_build_index_from_nodes'",
      "description": "Function '_build_index_from_nodes' on line 204 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 204,
      "code_snippet": "        return results\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> KG:\n        \"\"\"Build the index from nodes.\"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_234_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_insert'",
      "description": "Function '_insert' on line 234 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 234,
      "code_snippet": "        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        for n in nodes:\n            triplets = self._extract_triplets(",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_256_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'upsert_triplet'",
      "description": "Function 'upsert_triplet' on line 256 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 256,
      "code_snippet": "        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def upsert_triplet(\n        self, triplet: Tuple[str, str, str], include_embeddings: bool = False\n    ) -> None:\n        \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_234_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_insert'",
      "description": "Function '_insert' on line 234 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 234,
      "code_snippet": "        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        for n in nodes:\n            triplets = self._extract_triplets(",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_158_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_llm_extract_triplets'",
      "description": "Function '_llm_extract_triplets' on line 158 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 158,
      "code_snippet": "            return self._llm_extract_triplets(text)\n\n    def _llm_extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.kg_triplet_extract_template,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py_204_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_build_index_from_nodes'",
      "description": "Function '_build_index_from_nodes' on line 204 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
      "line_number": 204,
      "code_snippet": "        return results\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> KG:\n        \"\"\"Build the index from nodes.\"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py_158",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 158 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py",
      "line_number": 158,
      "code_snippet": "        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py_156",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_keywords' on line 156 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py",
      "line_number": 156,
      "code_snippet": "    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,\n            question=query_str,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return list(keywords)\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py_156_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_keywords'",
      "description": "Function '_get_keywords' on line 156 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py",
      "line_number": 156,
      "code_snippet": "        )\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py_239",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 239 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py",
      "line_number": 239,
      "code_snippet": "        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.keyword_extract_template,\n            text=text,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py_237",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_extract_keywords' on line 237 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py",
      "line_number": 237,
      "code_snippet": "    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        return extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n\n    async def _async_extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = await self._llm.apredict(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py_237_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_extract_keywords'",
      "description": "Function '_extract_keywords' on line 237 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py",
      "line_number": 237,
      "code_snippet": "    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.keyword_extract_template,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_265",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding_batch' is used in 'INSERT' on line 265 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 265,
      "code_snippet": "            else:\n                embeddings = self._embed_model.get_text_embedding_batch(\n                    node_texts, show_progress=self._show_progress\n                )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_259",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding_batch' is used in 'run(' on line 259 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 259,
      "code_snippet": "            if self._use_async:\n                embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        node_texts, show_progress=self._show_progress",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_276",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding_batch' is used in 'run(' on line 276 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 276,
      "code_snippet": "            if self._use_async:\n                kg_embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        kg_node_texts, show_progress=self._show_progress",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_282",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding_batch' is used in 'INSERT' on line 282 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 282,
      "code_snippet": "            else:\n                kg_embeddings = self._embed_model.get_text_embedding_batch(\n                    kg_node_texts,\n                    show_progress=self._show_progress,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_276",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._embed_model.get_text_embedding_batch' is used in 'run(' on line 276 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 276,
      "code_snippet": "            if self._use_async:\n                kg_embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        kg_node_texts, show_progress=self._show_progress",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_260",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._embed_model.aget_text_embedding_batch' is used in 'run(' on line 260 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 260,
      "code_snippet": "                embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        node_texts, show_progress=self._show_progress\n                    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_277",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._embed_model.aget_text_embedding_batch' is used in 'run(' on line 277 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 277,
      "code_snippet": "                kg_embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        kg_node_texts, show_progress=self._show_progress\n                    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_195",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_insert_nodes' on line 195 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 195,
      "code_snippet": "    def _insert_nodes(self, nodes: Sequence[BaseNode]) -> Sequence[BaseNode]:\n        \"\"\"Insert nodes to the index struct.\"\"\"\n        if len(nodes) == 0:\n            return nodes\n\n        # run transformations on nodes to extract triplets\n        if self._use_async:\n            nodes = asyncio.run(\n                arun_transformations(\n                    nodes, self._kg_extractors, show_progress=self._show_progress\n                )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_195_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute operation without confirmation in '_insert_nodes'",
      "description": "Function '_insert_nodes' on line 195 performs high-risk delete/write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 195,
      "code_snippet": "            return None\n\n    def _insert_nodes(self, nodes: Sequence[BaseNode]) -> Sequence[BaseNode]:\n        \"\"\"Insert nodes to the index struct.\"\"\"\n        if len(nodes) == 0:\n            return nodes",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py_195_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_insert_nodes'",
      "description": "Function '_insert_nodes' on line 195 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
      "line_number": 195,
      "code_snippet": "            return None\n\n    def _insert_nodes(self, nodes: Sequence[BaseNode]) -> Sequence[BaseNode]:\n        \"\"\"Insert nodes to the index struct.\"\"\"\n        if len(nodes) == 0:\n            return nodes",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py_177",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'UPDATE' on line 177 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py",
      "line_number": 177,
      "code_snippet": "                summaries = [\n                    self._llm.predict(self.summary_prompt, context_str=text_chunk)\n                    for text_chunk in text_chunks_progress\n                ]",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py_140",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'build_index_from_nodes' on line 140 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py",
      "line_number": 140,
      "code_snippet": "    def build_index_from_nodes(\n        self,\n        index_graph: IndexGraph,\n        cur_node_ids: Dict[int, str],\n        all_node_ids: Dict[int, str],\n        level: int = 0,\n    ) -> IndexGraph:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        if len(cur_node_ids) <= self.num_children:\n            index_graph.root_nodes = cur_node_ids\n            return index_graph",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py_140_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'build_index_from_nodes'",
      "description": "Function 'build_index_from_nodes' on line 140 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py",
      "line_number": 140,
      "code_snippet": "        return new_node_dict\n\n    def build_index_from_nodes(\n        self,\n        index_graph: IndexGraph,\n        cur_node_ids: Dict[int, str],",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py_140_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'build_index_from_nodes'",
      "description": "Function 'build_index_from_nodes' on line 140 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py",
      "line_number": 140,
      "code_snippet": "        return new_node_dict\n\n    def build_index_from_nodes(\n        self,\n        index_graph: IndexGraph,\n        cur_node_ids: Dict[int, str],",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py_202",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 202 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py",
      "line_number": 202,
      "code_snippet": "            # call each batch independently\n            raw_response = self._llm.predict(\n                self._choice_select_prompt,\n                context_str=fmt_batch_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py_124",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_get_embeddings' on line 124 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py",
      "line_number": 124,
      "code_snippet": "    def _get_embeddings(\n        self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> Tuple[List[float], List[List[float]]]:\n        \"\"\"Get top nodes by similarity to the query.\"\"\"\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n                query_bundle.embedding_strs\n            )\n\n        node_embeddings: List[List[float]] = []\n        nodes_embedded = 0",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py_191",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 191 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py",
      "line_number": 191,
      "code_snippet": "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        node_ids = self._index.index_struct.nodes\n        results = []\n        for idx in range(0, len(node_ids), self._choice_batch_size):\n            node_ids_batch = node_ids[idx : idx + self._choice_batch_size]\n            nodes_batch = self._index.docstore.get_nodes(node_ids_batch)\n\n            query_str = query_bundle.query_str\n            fmt_batch_str = self._format_node_batch_fn(nodes_batch)\n            # call each batch independently",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py_124_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_embeddings'",
      "description": "Function '_get_embeddings' on line 124 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py",
      "line_number": 124,
      "code_snippet": "        return node_with_scores\n\n    def _get_embeddings(\n        self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> Tuple[List[float], List[List[float]]]:\n        \"\"\"Get top nodes by similarity to the query.\"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py_191_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_retrieve'",
      "description": "Function '_retrieve' on line 191 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py",
      "line_number": 191,
      "code_snippet": "        )\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        node_ids = self._index.index_struct.nodes\n        results = []",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_140",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_text_retrieve' on line 140 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 140,
      "code_snippet": "    def _text_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_text_vector_store_empty:\n            if self._vector_store.is_embedding_query:\n                if (\n                    query_bundle.embedding is None\n                    and len(query_bundle.embedding_strs) > 0\n                ):\n                    query_bundle.embedding = (",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_166",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_text_to_image_retrieve' on line 166 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 166,
      "code_snippet": "    def _text_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_image_vector_store_empty:\n            if self._image_vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Text encoder\n                query_bundle.embedding = (\n                    self._image_embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_191",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_image_to_image_retrieve' on line 191 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 191,
      "code_snippet": "    def _image_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_image_vector_store_empty:\n            if self._image_vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Image encoder for image input\n                assert isinstance(self._index.image_embed_model, MultiModalEmbedding)\n                query_bundle.embedding = self._image_embed_model.get_image_embedding(\n                    query_bundle.embedding_image[0]\n                )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_191_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_image_to_image_retrieve'",
      "description": "Function '_image_to_image_retrieve' on line 191 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 191,
      "code_snippet": "        return self._text_to_image_retrieve(str_or_query_bundle)\n\n    def _image_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_166_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_text_to_image_retrieve'",
      "description": "Function '_text_to_image_retrieve' on line 166 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 166,
      "code_snippet": "        return self._text_retrieve(str_or_query_bundle)\n\n    def _text_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_191_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_image_to_image_retrieve'",
      "description": "Function '_image_to_image_retrieve' on line 191 makes critical data_modification decisions based on LLM output without human oversight or verification. This creates risk of incorrect or harmful decisions being executed automatically.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 191,
      "code_snippet": "        return self._text_to_image_retrieve(str_or_query_bundle)\n\n    def _image_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_140_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_text_retrieve'",
      "description": "Function '_text_retrieve' on line 140 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 140,
      "code_snippet": "        return res\n\n    def _text_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_166_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_text_to_image_retrieve'",
      "description": "Function '_text_to_image_retrieve' on line 166 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 166,
      "code_snippet": "        return self._text_retrieve(str_or_query_bundle)\n\n    def _text_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py_191_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_image_to_image_retrieve'",
      "description": "Function '_image_to_image_retrieve' on line 191 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
      "line_number": 191,
      "code_snippet": "        return self._text_to_image_retrieve(str_or_query_bundle)\n\n    def _image_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py_140",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'as_query_engine' on line 140 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py",
      "line_number": 140,
      "code_snippet": "    def as_query_engine(\n        self,\n        llm: Optional[LLMType] = None,\n        **kwargs: Any,\n    ) -> SimpleMultiModalQueryEngine:\n        retriever = cast(MultiModalVectorIndexRetriever, self.as_retriever(**kwargs))\n\n        llm = llm or Settings.llm\n        assert isinstance(llm, (BaseLLM, MultiModalLLM))\n        class_name = llm.class_name()\n        if \"multi\" not in class_name:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py_161",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'as_chat_engine' on line 161 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py",
      "line_number": 161,
      "code_snippet": "    def as_chat_engine(\n        self,\n        chat_mode: ChatMode = ChatMode.BEST,\n        llm: Optional[LLMType] = None,\n        **kwargs: Any,\n    ) -> BaseChatEngine:\n        llm = llm or Settings.llm\n        assert isinstance(llm, (BaseLLM, MultiModalLLM))\n        class_name = llm.class_name()\n        if \"multi\" not in class_name:\n            logger.warning(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py_140_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'as_query_engine'",
      "description": "Function 'as_query_engine' on line 140 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py",
      "line_number": 140,
      "code_snippet": "        )\n\n    def as_query_engine(\n        self,\n        llm: Optional[LLMType] = None,\n        **kwargs: Any,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py_161_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'as_chat_engine'",
      "description": "Function 'as_chat_engine' on line 161 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py",
      "line_number": 161,
      "code_snippet": "        )\n\n    def as_chat_engine(\n        self,\n        chat_mode: ChatMode = ChatMode.BEST,\n        llm: Optional[LLMType] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py_96",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 96 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py",
      "line_number": 96,
      "code_snippet": "            # call each batch independently\n            raw_response = self._llm.predict(\n                self._choice_select_prompt,\n                context_str=fmt_batch_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py_81",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 81 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py",
      "line_number": 81,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        summary_ids = self._index.index_struct.summary_ids\n\n        all_summary_ids: List[str] = []\n        all_relevances: List[float] = []\n        for idx in range(0, len(summary_ids), self._choice_batch_size):\n            summary_ids_batch = summary_ids[idx : idx + self._choice_batch_size]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py_157",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 157 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py",
      "line_number": 157,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        if self._vector_store.is_embedding_query:\n            if query_bundle.embedding is None:\n                query_bundle.embedding = (\n                    self._embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py_81_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_retrieve'",
      "description": "Function '_retrieve' on line 81 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py",
      "line_number": 81,
      "code_snippet": "        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py_157_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_retrieve'",
      "description": "Function '_retrieve' on line 157 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py",
      "line_number": 157,
      "code_snippet": "        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py_44",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_top_k_embeddings_learner' on line 44 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py",
      "line_number": 44,
      "code_snippet": "def get_top_k_embeddings_learner(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    query_mode: VectorStoreQueryMode = VectorStoreQueryMode.SVM,\n) -> Tuple[List[float], List]:\n    \"\"\"\n    Get top embeddings by fitting a learner against query.\n\n    Inspired by Karpathy's SVM demo:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py_55_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 55. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py",
      "line_number": 55,
      "code_snippet": "\n    Inspired by Karpathy's SVM demo:\n    https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb\n\n    Can fit SVM, linear regression, and more.\n",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py_44_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'get_top_k_embeddings_learner'",
      "description": "Function 'get_top_k_embeddings_learner' on line 44 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py",
      "line_number": 44,
      "code_snippet": "\n\ndef get_top_k_embeddings_learner(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py_44_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'get_top_k_embeddings_learner'",
      "description": "Function 'get_top_k_embeddings_learner' on line 44 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py",
      "line_number": 44,
      "code_snippet": "\n\ndef get_top_k_embeddings_learner(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py_110",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.predict' is used in 'Response' on line 110 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py",
      "line_number": 110,
      "code_snippet": "        else:\n            new_query_str = self.llm.predict(\n                self.resynthesis_prompt,\n                query_str=query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py_103",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_resynthesize_query' on line 103 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py",
      "line_number": 103,
      "code_snippet": "    def _resynthesize_query(\n        self, query_str: str, response: str, feedback: Optional[str]\n    ) -> str:\n        \"\"\"Resynthesize query given feedback.\"\"\"\n        if feedback is None:\n            return query_str\n        else:\n            new_query_str = self.llm.predict(\n                self.resynthesis_prompt,\n                query_str=query_str,\n                response=response,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py_103_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_resynthesize_query'",
      "description": "Function '_resynthesize_query' on line 103 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py",
      "line_number": 103,
      "code_snippet": "            return \"Here is a previous bad answer.\\n\" + response\n\n    def _resynthesize_query(\n        self, query_str: str, response: str, feedback: Optional[str]\n    ) -> str:\n        \"\"\"Resynthesize query given feedback.\"\"\"",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_204",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'query_str' directly embedded in LLM prompt",
      "description": "The function '_run' embeds user input ('query_str') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 204,
      "code_snippet": "",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_316",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'query_str' directly embedded in LLM prompt",
      "description": "The function '_run' embeds user input ('query_str') directly into an LLM prompt using f-string. This allows attackers to inject malicious instructions that can override system prompts, leak sensitive data, or manipulate model behavior.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 316,
      "code_snippet": "",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_143",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self._llm.predict' is used in 'run(' on line 143 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 143,
      "code_snippet": "        query_str = query_bundle.query_str\n        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n        embedding_strs = [hypothetical_doc]\n        if self._include_original:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_139",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 139 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 139,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # TODO: support generating multiple hypothetical docs\n        query_str = query_bundle.query_str\n        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n        embedding_strs = [hypothetical_doc]\n        if self._include_original:\n            embedding_strs.extend(query_bundle.embedding_strs)\n        return QueryBundle(\n            query_str=query_str,\n            custom_embedding_strs=embedding_strs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_189",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 189 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 189,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # currently, just get text from the index structure\n        index_summary = cast(str, metadata.get(\"index_summary\", \"None\"))\n\n        # given the text from the index, we can use the query bundle to generate\n        # a new query bundle\n        query_str = query_bundle.query_str\n        new_query_str = self._llm.predict(\n            self._decompose_query_prompt,\n            query_str=query_str,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_297",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_run' on line 297 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 297,
      "code_snippet": "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        index_summary = cast(\n            str,\n            metadata.get(\"index_summary\", \"None\"),\n        )\n        prev_reasoning = cast(Response, metadata.get(\"prev_reasoning\"))\n        fmt_prev_reasoning = f\"\\n{prev_reasoning}\" if prev_reasoning else \"None\"\n\n        # given the text from the index, we can use the query bundle to generate\n        # a new query bundle",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_104_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 104. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 104,
      "code_snippet": "\n    As described in `[Precise Zero-Shot Dense Retrieval without Relevance Labels]\n    (https://arxiv.org/abs/2212.10496)`\n    \"\"\"\n\n    def __init__(",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_139_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run'",
      "description": "Function '_run' on line 139 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 139,
      "code_snippet": "            self._hyde_prompt = prompts[\"hyde_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # TODO: support generating multiple hypothetical docs\n        query_str = query_bundle.query_str",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_189_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run'",
      "description": "Function '_run' on line 189 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 189,
      "code_snippet": "            self._decompose_query_prompt = prompts[\"decompose_query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # currently, just get text from the index structure\n        index_summary = cast(str, metadata.get(\"index_summary\", \"None\"))",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_297_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run'",
      "description": "Function '_run' on line 297 automatically executes actions based on LLM output without checking confidence thresholds or validating output. This may lead to executing incorrect or unreliable suggestions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 297,
      "code_snippet": "            self._step_decompose_query_prompt = prompts[\"step_decompose_query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        index_summary = cast(\n            str,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_139_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run'",
      "description": "Function '_run' on line 139 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 139,
      "code_snippet": "            self._hyde_prompt = prompts[\"hyde_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # TODO: support generating multiple hypothetical docs\n        query_str = query_bundle.query_str",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_189_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run'",
      "description": "Function '_run' on line 189 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 189,
      "code_snippet": "            self._decompose_query_prompt = prompts[\"decompose_query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # currently, just get text from the index structure\n        index_summary = cast(str, metadata.get(\"index_summary\", \"None\"))",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py_297_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_run'",
      "description": "Function '_run' on line 297 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
      "line_number": 297,
      "code_snippet": "            self._step_decompose_query_prompt = prompts[\"step_decompose_query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        index_summary = cast(\n            str,",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/base.py_203",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 203 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/base.py",
      "line_number": 203,
      "code_snippet": "            schema_text = self._get_schema_text()\n            response_str = self._llm.predict(\n                self._schema_extract_prompt,\n                text=text_chunk,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/base.py_192",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits",
      "description": "Function 'insert_datapoint_from_nodes' on line 192 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/base.py",
      "line_number": 192,
      "code_snippet": "    def insert_datapoint_from_nodes(self, nodes: Sequence[BaseNode]) -> None:\n        \"\"\"Extract datapoint from a document and insert it.\"\"\"\n        text_chunks = [\n            node.get_content(metadata_mode=MetadataMode.LLM) for node in nodes\n        ]\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logger.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py_104",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_retrieve' on line 104 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py",
      "line_number": 104,
      "code_snippet": "    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if self._needs_embedding():\n            if query_bundle.embedding is None and len(query_bundle.embedding_strs) > 0:\n                query_bundle.embedding = (\n                    self._embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py_104_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_retrieve'",
      "description": "Function '_retrieve' on line 104 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py",
      "line_number": 104,
      "code_snippet": "\n    @dispatcher.span\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM05_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/prompts.py_15_arbitrary_url",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Untrusted model source: arbitrary_url",
      "description": "Untrusted URL 'https://' found on line 15. Loading models from arbitrary URLs poses significant security risks including malicious model injection, backdoors, and data exfiltration.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/prompts.py",
      "line_number": 15,
      "code_snippet": "# NOTE: these prompts are inspired from langchain's self-query prompt,\n# and adapted to our use case.\n# https://github.com/hwchase17/langchain/tree/main/langchain/chains/query_constructor/prompt.py\n\n\nPREFIX = \"\"\"\\",
      "recommendation": "Secure Model Loading:\n1. Only load models from trusted registries (HuggingFace, official repos)\n2. Verify model checksums/signatures before loading\n3. Use allowlists for permitted models\n4. Implement content scanning for downloaded models\n5. Never allow user-controlled model paths\n6. Use isolated environments for model loading\n7. Maintain an SBOM (Software Bill of Materials) for AI components"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/auto_retriever.py_158",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_retrieval_spec' on line 158 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/auto_retriever.py",
      "line_number": 158,
      "code_snippet": "    def generate_retrieval_spec(\n        self, query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        # prepare input\n        info_str = self._vector_store_info.model_dump_json(indent=4)\n        schema_str = VectorStoreQuerySpec.model_json_schema()\n\n        # call LLM\n        output = self._llm.predict(\n            self._prompt,\n            schema_str=schema_str,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py_85",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_vector_store_query' on line 85 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py",
      "line_number": 85,
      "code_snippet": "    def _get_vector_store_query(self, query_bundle: QueryBundle) -> VectorStoreQuery:\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n                query_bundle.embedding_strs\n            )\n\n        return VectorStoreQuery(\n            query_embedding=query_bundle.embedding,\n            similarity_top_k=self._similarity_top_k,\n            filters=self._filters,\n            **self._retriever_kwargs,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py_85_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in '_get_vector_store_query'",
      "description": "Function '_get_vector_store_query' on line 85 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py",
      "line_number": 85,
      "code_snippet": "        return {k: v for k, v in kwargs.items() if k in valid_params}\n\n    def _get_vector_store_query(self, query_bundle: QueryBundle) -> VectorStoreQuery:\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n                query_bundle.embedding_strs",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py_141",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.predict' is used in 'Response' on line 141 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py",
      "line_number": 141,
      "code_snippet": "\n        response = self.llm.predict(\n            self.text_to_cypher_template,\n            schema=schema,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py_154",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.predict' is used in 'Response' on line 154 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py",
      "line_number": 154,
      "code_snippet": "        if self.summarize_response:\n            summarized_response = self.llm.predict(\n                self.summarization_template,\n                context=str(cleaned_query_output),",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py_137",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve_from_graph' on line 137 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py",
      "line_number": 137,
      "code_snippet": "    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        schema = self._graph_store.get_schema_str()\n        question = query_bundle.query_str\n\n        response = self.llm.predict(\n            self.text_to_cypher_template,\n            schema=schema,\n            question=question,\n        )\n\n        parsed_cypher_query = self._parse_generated_cypher(response)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py_137_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'retrieve_from_graph'",
      "description": "Function 'retrieve_from_graph' on line 137 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py",
      "line_number": 137,
      "code_snippet": "        return None\n\n    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        schema = self._graph_store.get_schema_str()\n        question = query_bundle.query_str\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py_55",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.llm.structured_predict' is used in 'Response' on line 55 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py",
      "line_number": 55,
      "code_snippet": "\n        response = self.llm.structured_predict(\n            self.output_cls, PromptTemplate(question)\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py_52",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve_from_graph' on line 52 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py",
      "line_number": 52,
      "code_snippet": "    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        question = query_bundle.query_str\n\n        response = self.llm.structured_predict(\n            self.output_cls, PromptTemplate(question)\n        )\n\n        cypher_response = self._graph_store.structured_query(\n            self.cypher_query,\n            param_map=response.model_dump(),\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py_52_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'retrieve_from_graph'",
      "description": "Function 'retrieve_from_graph' on line 52 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py",
      "line_number": 52,
      "code_snippet": "        )\n\n    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        question = query_bundle.query_str\n\n        response = self.llm.structured_predict(",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    },
    {
      "id": "LLM02_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py_122",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self._llm.predict' is used in 'Response' on line 122 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py",
      "line_number": 122,
      "code_snippet": "    ) -> List[NodeWithScore]:\n        response = self._llm.predict(\n            self._synonym_prompt,\n            query_str=query_bundle.query_str,",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM04_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py_119",
      "category": "LLM04: Model Denial of Service",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'retrieve_from_graph' on line 119 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py",
      "line_number": 119,
      "code_snippet": "    def retrieve_from_graph(\n        self, query_bundle: QueryBundle, limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        response = self._llm.predict(\n            self._synonym_prompt,\n            query_str=query_bundle.query_str,\n            max_keywords=self._max_keywords,\n        )\n        matches = self._parse_llm_output(response)\n\n        return self._prepare_matches(matches, limit=limit or self._limit)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py_119_missing_verification",
      "category": "LLM09: Overreliance",
      "severity": "LOW",
      "confidence": 0.7,
      "title": "LLM output returned without verification in 'retrieve_from_graph'",
      "description": "Function 'retrieve_from_graph' on line 119 returns LLM output without verification, validation, or oversight mechanisms. Consider adding output validation or including disclaimers.",
      "file_path": "/Users/deo/secscan-cli/llm-sec-eval/repos/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py",
      "line_number": 119,
      "code_snippet": "        return self._get_nodes_with_score(triplets)\n\n    def retrieve_from_graph(\n        self, query_bundle: QueryBundle, limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        response = self._llm.predict(",
      "recommendation": "Add verification mechanisms for LLM output:\n\n1. Implement validation:\n   - Schema validation (Pydantic)\n   - Format checking (regex, parsing)\n   - Constraint verification\n\n2. Add fact-checking:\n   - Cross-reference with trusted sources\n   - Use RAG for grounding\n   - Verify claims against knowledge base\n\n3. Include disclaimers:\n   - \"AI-generated content, verify independently\"\n   - \"For informational purposes only\"\n   - \"Consult professional for critical decisions\"\n\n4. Track accuracy:\n   - Monitor user corrections\n   - Collect feedback\n   - Improve over time"
    }
  ],
  "metadata": {}
}