{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:16:33.890388Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer",
    "files_scanned": 1531,
    "overall_score": 7.14,
    "confidence": 0.7,
    "duration_seconds": 40.356,
    "findings_count": 1647,
    "severity_breakdown": {
      "CRITICAL": 968,
      "HIGH": 481,
      "MEDIUM": 169,
      "LOW": 2,
      "INFO": 27
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.54,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "3 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.52,
      "subscores": {
        "model_protection": 80,
        "extraction_defense": 57,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption at rest",
        "Encryption in transit",
        "OAuth",
        "Rate limiting",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": []
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.64,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII encryption",
        "PII masking",
        "Explicit consent",
        "Consent withdrawal",
        "Right to delete",
        "CCPA compliance",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 27,
      "confidence": 0.57,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 75,
        "audit_logging": 0,
        "incident_response": 50,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.92,
      "subscores": {
        "LLM01": 0,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 0,
        "LLM07": 66,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 0
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 8 critical",
        "Insecure Output Handling: 65 critical",
        "Model Denial of Service: 31 critical",
        "Supply Chain Vulnerabilities: 27 critical, 7 high, 64 medium, 2 low",
        "Sensitive Information Disclosure: 19 critical, 329 high, 23 medium",
        "Insecure Plugin Design: 2 high",
        "Excessive Agency: 30 critical, 44 high, 16 medium",
        "Overreliance: 30 critical, 10 high, 2 medium",
        "Model Theft: 15 high, 8 medium",
        "ML: 758 critical, 56 medium",
        "SQLI: 74 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py_33",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run' flows to 'client.beta.threads.runs.retrieve' on line 33 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
      "line_number": 33,
      "code_snippet": "    while run.status == \"queued\" or run.status == \"in_progress\":\n        run = client.beta.threads.runs.retrieve(\n            thread_id=thread.id,\n            run_id=run.id,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py_109",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run' flows to 'wait_on_run' on line 109 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
      "line_number": 109,
      "code_snippet": "\n        run = wait_on_run(client, run, thread)\n        messages = client.beta.threads.messages.list(\n            thread_id=thread.id, order=\"asc\", after=message.id",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py_85",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
      "line_number": 85,
      "code_snippet": "    # Process each topic individually\n    for topic in topics:\n        thread = client.beta.threads.create()\n        message = client.beta.threads.messages.create(\n            thread_id=thread.id,\n            role=\"user\",\n            content=USER_PROMPT.format(topic=topic),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py_91",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/examples/assistants-api/topics_analyzer.py",
      "line_number": 91,
      "code_snippet": "            content=USER_PROMPT.format(topic=topic),\n        )\n\n        run = client.beta.threads.runs.create(\n            thread_id=thread.id,\n            assistant_id=assistant.id,\n            tools=[",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/shared_configs/enums.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/shared_configs/enums.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass EmbeddingProvider(str, Enum):\n    OPENAI = \"openai\"\n    COHERE = \"cohere\"\n    VOYAGE = \"voyage\"\n    GOOGLE = \"google\"\n    LITELLM = \"litellm\"\n    AZURE = \"azure\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/shared_configs/configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/shared_configs/configs.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any\nfrom typing import List\nfrom urllib.parse import urlparse\n\n# Used for logging\nSLACK_CHANNEL_ID = \"channel_id\"\n\n# Skip model warmup at startup\n# Default to True (skip warmup) if not set, otherwise respect the value",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/constants.py",
      "line_number": 1,
      "code_snippet": "MODEL_WARM_UP_STRING = \"hi \" * 512\n\n\nclass GPUStatus:\n    CUDA = \"cuda\"\n    MAC_MPS = \"mps\"\n    NONE = \"none\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/encoders.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/encoders.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport time\nfrom typing import Any\nfrom typing import TYPE_CHECKING\n\nfrom fastapi import APIRouter\nfrom fastapi import HTTPException\nfrom fastapi import Request\n\nfrom model_server.utils import simple_log_function_time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/utils.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport time\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom collections.abc import Iterator\nfrom functools import wraps\nfrom typing import Any\nfrom typing import cast\nfrom typing import TypeVar\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/main.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport shutil\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\n\nimport sentry_sdk\nimport torch\nimport uvicorn",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server_main.py_12_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'main'",
      "description": "Function 'main' on line 12 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server_main.py",
      "line_number": 12,
      "code_snippet": "\n\ndef main() -> None:\n    \"\"\"Run the MCP server.\"\"\"\n    if not MCP_SERVER_ENABLED:\n        logger.info(\"MCP server is disabled (MCP_SERVER_ENABLED=false)\")",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/setup.py",
      "line_number": 1,
      "code_snippet": "import time\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import DISABLE_INDEX_UPDATE_ON_SWAP\nfrom onyx.configs.app_configs import INTEGRATION_TESTS_MODE\nfrom onyx.configs.app_configs import MANAGED_VESPA\nfrom onyx.configs.app_configs import VESPA_NUM_ATTEMPTS_ON_STARTUP\nfrom onyx.configs.chat_configs import INPUT_PROMPT_YAML\nfrom onyx.configs.constants import KV_REINDEX_KEY",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/main.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport sys\nimport traceback\nimport warnings\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom typing import cast\n\nimport sentry_sdk",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py_103",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 103 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
      "line_number": 103,
      "code_snippet": "    try:\n        result = subprocess.run(cmd)\n        if result.returncode == 0:\n            print(f\"Generated Python client at {output_dir}\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py_68_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'generate_client'",
      "description": "Function 'generate_client' on line 68 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
      "line_number": 68,
      "code_snippet": "\n    return schema\n\n\ndef generate_client(openapi_json_path: str, strip_tags: bool = True) -> None:\n    \"\"\"Generate Python client from OpenAPI schema using openapi-generator.\"\"\"\n    import tempfile\n\n    output_dir = os.path.join(os.path.dirname(openapi_json_path), \"onyx_openapi_client\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py_68_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'generate_client'",
      "description": "Function 'generate_client' on line 68 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
      "line_number": 68,
      "code_snippet": "\n\ndef generate_client(openapi_json_path: str, strip_tags: bool = True) -> None:\n    \"\"\"Generate Python client from OpenAPI schema using openapi-generator.\"\"\"\n    import tempfile\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py_19_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'go'",
      "description": "API endpoint 'go' on line 19 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
      "line_number": 19,
      "code_snippet": "\n\ndef go(filename: str, tagged_for_docs: str | None = None) -> None:\n    \"\"\"Generate OpenAPI schema.\n\n    By default outputs tag-stripped schema (for client generation).",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/onyx_openapi_schema.py",
      "line_number": 1,
      "code_snippet": "# export openapi schema without having to start the actual web server\n\n# helpful tips: https://github.com/fastapi/fastapi/issues/1173\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_feedback_dump.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_feedback_dump.py",
      "line_number": 1,
      "code_snippet": "# This file is used to demonstrate how to use the backend APIs directly\n# to query out feedback for all messages\nimport argparse\nimport logging\nfrom logging import getLogger\nfrom typing import Any\nfrom uuid import UUID\n\nimport requests\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/sources_selection_analysis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/sources_selection_analysis.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom os import listdir\nfrom os.path import isfile\nfrom os.path import join\nfrom typing import Optional",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/orphan_doc_cleanup_script.py_82_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 82. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/orphan_doc_cleanup_script.py",
      "line_number": 82,
      "code_snippet": "                    # Check if document exists in Vespa first\n                    try:\n                        chunks = vespa_index.id_based_retrieval(\n                            chunk_requests=[\n                                VespaChunkRequest(document_id=doc_id, max_chunk_ind=2)\n                            ],",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/orphan_doc_cleanup_script.py_113",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 113 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/orphan_doc_cleanup_script.py",
      "line_number": 113,
      "code_snippet": "                        print(\n                            f\"Error deleting document {doc_id} in Vespa and \"\n                            f\"will not delete from Postgres: {e}\"\n                        )\n                        return None",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/api_inference_sample.py_62_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'new_token' containing sensitive data is being logged on line 62. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/api_inference_sample.py",
      "line_number": 62,
      "code_snippet": "\n            if new_token:\n                print(new_token, end=\"\", flush=True)\n\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/api_inference_sample.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/api_inference_sample.py",
      "line_number": 1,
      "code_snippet": "# This file is used to demonstrate how to use the backend APIs directly\n# In this case, the equivalent of asking a question in Onyx Chat in a new chat session\nimport argparse\nimport json\nimport os\n\nimport requests\n\n\ndef create_new_chat_session(onyx_url: str, api_key: str | None) -> int:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/celery_purge_queue.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/celery_purge_queue.py",
      "line_number": 1,
      "code_snippet": "# Tool to run operations on Celery/Redis in production\n# this is a work in progress and isn't completely put together yet\n# but can serve as a stub for future operations\nimport argparse\nimport logging\nfrom logging import getLogger\n\nfrom redis import Redis\n\nfrom onyx.background.celery.celery_redis import celery_get_queue_length",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/decrypt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/decrypt.py",
      "line_number": 1,
      "code_snippet": "import binascii\nimport json\nimport sys\n\nfrom onyx.utils.encryption import decrypt_bytes_to_string\n\n\ndef decrypt_raw_credential(encrypted_value: str) -> None:\n    \"\"\"Decrypt and display a raw encrypted credential value\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_indexes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_indexes.py",
      "line_number": 1,
      "code_snippet": "# This file is purely for development use, not included in any builds\nimport os\nimport sys\nfrom time import sleep\n\nimport requests\nfrom requests.exceptions import RequestException\n\n# makes it so `PYTHONPATH=.` is not required when running this script\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/test-openapi-key.py_45",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/test-openapi-key.py",
      "line_number": 45,
      "code_snippet": "            {\"role\": \"system\", \"content\": \"Finish the sentence\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        response = client.chat.completions.create(\n            model=model_version,\n            messages=messages,  # type:ignore\n            max_tokens=5,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/force_delete_connector_by_id.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/force_delete_connector_by_id.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\nimport sys\n\nfrom sqlalchemy import delete\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.document import delete_documents_complete__no_commit\nfrom onyx.db.enums import ConnectorCredentialPairStatus\nfrom onyx.db.search_settings import get_active_search_settings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_48",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 48 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 48,
      "code_snippet": "    copy_cmd = f\"docker cp {host_file_path} {container_name}:/tmp/\"\n    subprocess.run(copy_cmd, shell=True, check=True)\n\n    container_file_path = f\"/tmp/{os.path.basename(filename)}\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_56",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 56 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 56,
      "code_snippet": "    )\n    subprocess.run(restore_cmd, shell=True, check=True)\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_27",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 27,
      "code_snippet": "    with open(filename, \"w\") as file:\n        subprocess.run(\n            cmd,\n            shell=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_25_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [67, 88]) and executes code (lines [25, 27, 48]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 25,
      "code_snippet": "\n\ndef save_postgres(filename: str, container_name: str) -> None:\n    logger.notice(\"Attempting to take Postgres snapshot\")\n    cmd = f\"docker exec {container_name} pg_dump -U {POSTGRES_USER} -h {POSTGRES_HOST} -p {POSTGRES_PORT} -W -F t {POSTGRES_DB}\"\n    with open(filename, \"w\") as file:\n        subprocess.run(\n            cmd,\n            shell=True,\n            check=True,",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_23_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'save_postgres'",
      "description": "Function 'save_postgres' on line 23 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 23,
      "code_snippet": "\nlogger = setup_logger()\n\n\ndef save_postgres(filename: str, container_name: str) -> None:\n    logger.notice(\"Attempting to take Postgres snapshot\")\n    cmd = f\"docker exec {container_name} pg_dump -U {POSTGRES_USER} -h {POSTGRES_HOST} -p {POSTGRES_PORT} -W -F t {POSTGRES_DB}\"\n    with open(filename, \"w\") as file:\n        subprocess.run(\n            cmd,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_37_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'load_postgres'",
      "description": "Function 'load_postgres' on line 37 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 37,
      "code_snippet": "            input=f\"{POSTGRES_PASSWORD}\\n\",\n        )\n\n\ndef load_postgres(filename: str, container_name: str) -> None:\n    logger.notice(\"Attempting to load Postgres snapshot\")\n    try:\n        alembic_cfg = Config(\"alembic.ini\")\n        command.upgrade(alembic_cfg, \"head\")\n    except Exception as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_23_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'save_postgres'",
      "description": "Function 'save_postgres' on line 23 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 23,
      "code_snippet": "\n\ndef save_postgres(filename: str, container_name: str) -> None:\n    logger.notice(\"Attempting to take Postgres snapshot\")\n    cmd = f\"docker exec {container_name} pg_dump -U {POSTGRES_USER} -h {POSTGRES_HOST} -p {POSTGRES_PORT} -W -F t {POSTGRES_DB}\"\n    with open(filename, \"w\") as file:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_37_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'load_postgres'",
      "description": "Function 'load_postgres' on line 37 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 37,
      "code_snippet": "\n\ndef load_postgres(filename: str, container_name: str) -> None:\n    logger.notice(\"Attempting to load Postgres snapshot\")\n    try:\n        alembic_cfg = Config(\"alembic.ini\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_56",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 56,
      "code_snippet": "        f\"docker exec {container_name} pg_restore --clean -U {POSTGRES_USER} \"\n        f\"-h localhost -p {POSTGRES_PORT} -d {POSTGRES_DB} -1 -F t {container_file_path}\"\n    )\n    subprocess.run(restore_cmd, shell=True, check=True)\n\n\ndef save_vespa(filename: str) -> None:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py_27",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/save_load_state.py",
      "line_number": 27,
      "code_snippet": "    logger.notice(\"Attempting to take Postgres snapshot\")\n    cmd = f\"docker exec {container_name} pg_dump -U {POSTGRES_USER} -h {POSTGRES_HOST} -p {POSTGRES_PORT} -W -F t {POSTGRES_DB}\"\n    with open(filename, \"w\") as file:\n        subprocess.run(\n            cmd,\n            shell=True,\n            check=True,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/resume_paused_connectors.py_6",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 6. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/resume_paused_connectors.py",
      "line_number": 6,
      "code_snippet": "\nAPI_SERVER_URL = \"http://localhost:3000\"\nAPI_KEY = \"onyx-api-key\"  # API key here, if auth is enabled\n\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/resume_paused_connectors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/resume_paused_connectors.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\nimport requests\n\nAPI_SERVER_URL = \"http://localhost:3000\"\nAPI_KEY = \"onyx-api-key\"  # API key here, if auth is enabled\n\n\ndef resume_paused_connectors(\n    api_server_url: str,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_postgres.py_53",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 53 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_postgres.py",
      "line_number": 53,
      "code_snippet": "\n        print(f\"Deleting all rows from {table_name}...\")\n        cur.execute(f'DELETE FROM \"{table_name}\"')\n\n    # Re-enable triggers",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_postgres.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/reset_postgres.py",
      "line_number": 1,
      "code_snippet": "import os\nimport sys\n\nimport psycopg2\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.engine.sql_engine import get_sqlalchemy_engine\n\n# makes it so `PYTHONPATH=.` is not required when running this script\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/hard_delete_chats.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/hard_delete_chats.py",
      "line_number": 1,
      "code_snippet": "import os\nimport sys\n\n\n# Ensure PYTHONPATH is set up for direct script execution\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nprint(parent_dir)\nsys.path.append(parent_dir)\n\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant  # noqa: E402",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py_8",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py",
      "line_number": 8,
      "code_snippet": "API_SERVER_URL = \"http://localhost:3000\"  # Adjust this to your Onyx server URL\nHEADERS = {\"Content-Type\": \"application/json\"}\nAPI_KEY = \"onyx-api-key\"  # API key here, if auth is enabled\n\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py_103_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'web_credential' containing sensitive data is being logged on line 103. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py",
      "line_number": 103,
      "code_snippet": "        is_public=True,\n    )\n    print(f\"Created Web Credential: {web_credential}\")\n\n    # Create CC pair for Web connector",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py_135_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'github_credential' containing sensitive data is being logged on line 135. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py",
      "line_number": 135,
      "code_snippet": "        is_public=True,\n    )\n    print(f\"Created GitHub Credential: {github_credential}\")\n\n    # Create CC pair for GitHub connector",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/add_connector_creation_script.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import Dict\n\nimport requests\n\nAPI_SERVER_URL = \"http://localhost:3000\"  # Adjust this to your Onyx server URL\nHEADERS = {\"Content-Type\": \"application/json\"}\nAPI_KEY = \"onyx-api-key\"  # API key here, if auth is enabled\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_loadtest.py_185_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'avg_tokens_per_sec' containing sensitive data is being logged on line 185. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_loadtest.py",
      "line_number": 185,
      "code_snippet": "            logger.info(f\"Average Time to Documents: {avg_first_doc:.2f} seconds\")\n            logger.info(f\"Average Time to First Answer: {avg_first_answer:.2f} seconds\")\n            logger.info(f\"Average Tokens/Second: {avg_tokens_per_sec:.2f}\")\n\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_loadtest.py_188_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/admin operation without confirmation in 'main'",
      "description": "Function 'main' on line 188 performs high-risk execute/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/chat_loadtest.py",
      "line_number": 188,
      "code_snippet": "\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Chat Load Testing Tool\")\n    parser.add_argument(\n        \"--url\",",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/dev_run_background_jobs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/dev_run_background_jobs.py",
      "line_number": 1,
      "code_snippet": "import subprocess\nimport threading\n\n\ndef monitor_process(process_name: str, process: subprocess.Popen) -> None:\n    assert process.stdout is not None\n\n    while True:\n        output = process.stdout.readline()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/get_wikidocs.py_80_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "trust_remote_code=True enables arbitrary code execution on line 80.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/get_wikidocs.py",
      "line_number": 80,
      "code_snippet": "        split=\"train\",\n        streaming=True,\n        trust_remote_code=True,\n    )\n\n    # Initialize counters",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/get_wikidocs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/get_wikidocs.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to pull Wikipedia documents from Hugging Face and organize them into zip files.\n\nUsage:\n    python get_wikidocs.py --total 1000 --per-zip 100 --output ./wikidata_zips\n\"\"\"\n\nimport argparse\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/transform_openapi_for_docs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/transform_openapi_for_docs.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nTransform OpenAPI schema for public documentation.\n\nFilters endpoints tagged with \"public\", converts auth to Bearer token,\nand removes internal parameters (tenant_id, db_session).\n\nUsage:\n    python scripts/transform_openapi_for_docs.py -i generated/openapi.json -o openapi_docs.json\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py_500",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'context.run_migrations' is used in 'run(' on line 500 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
      "line_number": 500,
      "code_snippet": "            with context.begin_transaction():\n                context.run_migrations()\n\n            # Commit the transaction to ensure changes are visible to next migration",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py_223_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'do_run_migrations'",
      "description": "Function 'do_run_migrations' on line 223 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
      "line_number": 223,
      "code_snippet": "\n\ndef do_run_migrations(\n    connection: Connection, schema_name: str, create_schema: bool\n) -> None:\n    if create_schema:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py_369_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 369 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
      "line_number": 369,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"\n    NOTE(rkuo): This generates a sql script that can be used to migrate the database ...\n    instead of migrating the db live via an open connection",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py_223_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'do_run_migrations'",
      "description": "Function 'do_run_migrations' on line 223 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
      "line_number": 223,
      "code_snippet": "\n\ndef do_run_migrations(\n    connection: Connection, schema_name: str, create_schema: bool\n) -> None:\n    if create_schema:",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py_369_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 369 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
      "line_number": 369,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"\n    NOTE(rkuo): This generates a sql script that can be used to migrate the database ...\n    instead of migrating the db live via an open connection",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/env.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Literal\nfrom onyx.db.engine.iam_auth import get_iam_auth_token\nfrom onyx.configs.app_configs import USE_IAM_AUTH\nfrom onyx.configs.app_configs import POSTGRES_HOST\nfrom onyx.configs.app_configs import POSTGRES_PORT\nfrom onyx.configs.app_configs import POSTGRES_USER\nfrom onyx.configs.app_configs import AWS_REGION_NAME\nfrom onyx.db.engine.sql_engine import build_connection_string\nfrom onyx.db.engine.tenant_utils import get_all_tenant_ids\nfrom sqlalchemy import event",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py_60_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 60 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
      "line_number": 60,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py_84_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'do_run_migrations'",
      "description": "Function 'do_run_migrations' on line 84 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
      "line_number": 84,
      "code_snippet": "\n\ndef do_run_migrations(connection: Connection) -> None:\n    context.configure(\n        connection=connection,\n        target_metadata=target_metadata,  # type: ignore[arg-type]",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py_60_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 60 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
      "line_number": 60,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py_84_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'do_run_migrations'",
      "description": "Function 'do_run_migrations' on line 84 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
      "line_number": 84,
      "code_snippet": "\n\ndef do_run_migrations(connection: Connection) -> None:\n    context.configure(\n        connection=connection,\n        target_metadata=target_metadata,  # type: ignore[arg-type]",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/env.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom logging.config import fileConfig\nfrom typing import Literal\n\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.schema import SchemaItem\n\nfrom alembic import context",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/a4f6ee863c47_mapping_for_anonymous_user_path.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/a4f6ee863c47_mapping_for_anonymous_user_path.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a4f6ee863c47\"\ndown_revision = \"14a83a331951\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/a4f6ee863c47_mapping_for_anonymous_user_path.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/a4f6ee863c47_mapping_for_anonymous_user_path.py",
      "line_number": 1,
      "code_snippet": "\"\"\"mapping for anonymous user path\n\nRevision ID: a4f6ee863c47\nRevises: 14a83a331951\nCreate Date: 2025-01-04 14:16:58.697451\n\n\"\"\"\n\nimport sqlalchemy as sa\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/ac842f85f932_new_column_user_tenant_mapping.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/ac842f85f932_new_column_user_tenant_mapping.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ac842f85f932\"\ndown_revision = \"34e3630c7f32\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/ac842f85f932_new_column_user_tenant_mapping.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/ac842f85f932_new_column_user_tenant_mapping.py",
      "line_number": 1,
      "code_snippet": "\"\"\"new column user tenant mapping\n\nRevision ID: ac842f85f932\nRevises: 34e3630c7f32\nCreate Date: 2025-03-03 13:30:14.802874\n\n\"\"\"\n\nimport sqlalchemy as sa\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_18",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 18. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 18,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3b9f09038764\"\ndown_revision = \"3b45e0018bf1\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 19,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3b9f09038764\"\ndown_revision = \"3b45e0018bf1\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_35",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 35 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 35,
      "code_snippet": "            raise Exception(\"DB_READONLY_USER or DB_READONLY_PASSWORD is not set\")\n\n        op.execute(\n            text(\n                f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_62",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 62 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 62,
      "code_snippet": "        # the user is dropped in the alembic_tenants migration.\n\n        op.execute(\n            text(\n                f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_37",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 37 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 37,
      "code_snippet": "        op.execute(\n            text(\n                f\"\"\"\n                DO $$\n                BEGIN",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_64",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 64 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 64,
      "code_snippet": "        op.execute(\n            text(\n                f\"\"\"\n            DO $$\n            BEGIN",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b9f09038764_add_read_only_kg_user.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_db_readonly_user\n\nRevision ID: 3b9f09038764\nRevises: 3b45e0018bf1\nCreate Date: 2025-05-11 11:05:11.436977\n\n\"\"\"\n\nfrom sqlalchemy import text\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/14a83a331951_create_usertenantmapping_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/14a83a331951_create_usertenantmapping_table.py",
      "line_number": 1,
      "code_snippet": "import sqlalchemy as sa\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"14a83a331951\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/34e3630c7f32_lowercase_multi_tenant_user_auth.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/34e3630c7f32_lowercase_multi_tenant_user_auth.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"34e3630c7f32\"\ndown_revision = \"a4f6ee863c47\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/34e3630c7f32_lowercase_multi_tenant_user_auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/34e3630c7f32_lowercase_multi_tenant_user_auth.py",
      "line_number": 1,
      "code_snippet": "\"\"\"lowercase multi-tenant user auth\n\nRevision ID: 34e3630c7f32\nRevises: a4f6ee863c47\nCreate Date: 2025-02-26 15:03:01.211894\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3b45e0018bf1\"\ndown_revision = \"ac842f85f932\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3b45e0018bf1\"\ndown_revision = \"ac842f85f932\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic_tenants/versions/3b45e0018bf1_add_new_available_tenant_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add new available tenant table\n\nRevision ID: 3b45e0018bf1\nRevises: ac842f85f932\nCreate Date: 2025-03-06 09:55:18.229910\n\n\"\"\"\n\nimport sqlalchemy as sa\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fad14119fb92_delete_tags_with_wrong_enum.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fad14119fb92_delete_tags_with_wrong_enum.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"fad14119fb92\"\ndown_revision = \"72bdc9929a46\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3c1a7904cd0_remove_userfile_related_deprecated_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3c1a7904cd0_remove_userfile_related_deprecated_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a3c1a7904cd0\"\ndown_revision = \"5c3dca366b35\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3c1a7904cd0_remove_userfile_related_deprecated_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3c1a7904cd0_remove_userfile_related_deprecated_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove userfile related deprecated fields\n\nRevision ID: a3c1a7904cd0\nRevises: 5c3dca366b35\nCreate Date: 2026-01-06 13:00:30.634396\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/473a1a7ca408_add_display_model_names_to_llm_provider.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/473a1a7ca408_add_display_model_names_to_llm_provider.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"473a1a7ca408\"\ndown_revision = \"325975216eb3\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/473a1a7ca408_add_display_model_names_to_llm_provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/473a1a7ca408_add_display_model_names_to_llm_provider.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add display_model_names to llm_provider\n\nRevision ID: 473a1a7ca408\nRevises: 325975216eb3\nCreate Date: 2024-07-25 14:31:02.002917\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da4c21c69164_chosen_assistants_changed_to_jsonb.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da4c21c69164_chosen_assistants_changed_to_jsonb.py",
      "line_number": 1,
      "code_snippet": "\"\"\"chosen_assistants changed to jsonb\n\nRevision ID: da4c21c69164\nRevises: c5b692fa265c\nCreate Date: 2024-08-18 19:06:47.291491\n\n\"\"\"\n\nimport json\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5809c0787398_add_chat_sessions.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5809c0787398_add_chat_sessions.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5809c0787398\"\ndown_revision = \"d929f0c1c6af\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e86866a9c78a_add_persona_to_chat_session.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e86866a9c78a_add_persona_to_chat_session.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"e86866a9c78a\"\ndown_revision = \"80696cf850ae\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/238b84885828_add_foreign_key_to_user__external_user_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/238b84885828_add_foreign_key_to_user__external_user_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"238b84885828\"\ndown_revision = \"a7688ab35c45\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/238b84885828_add_foreign_key_to_user__external_user_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/238b84885828_add_foreign_key_to_user__external_user_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add foreign key to user__external_user_group_id\n\nRevision ID: 238b84885828\nRevises: a7688ab35c45\nCreate Date: 2025-05-19 17:15:33.424584\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2a391f840e85_add_last_refreshed_at_mcp_server.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2a391f840e85_add_last_refreshed_at_mcp_server.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembi.\nrevision = \"2a391f840e85\"\ndown_revision = \"4cebcbc9b2ae\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2a391f840e85_add_last_refreshed_at_mcp_server.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2a391f840e85_add_last_refreshed_at_mcp_server.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add last refreshed at mcp server\n\nRevision ID: 2a391f840e85\nRevises: 4cebcbc9b2ae\nCreate Date: 2025-12-06 15:19:59.766066\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/58c50ef19f08_add_stale_column_to_user__external_user_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/58c50ef19f08_add_stale_column_to_user__external_user_.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"58c50ef19f08\"\ndown_revision = \"7b9b952abdf6\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/58c50ef19f08_add_stale_column_to_user__external_user_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/58c50ef19f08_add_stale_column_to_user__external_user_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add stale column to external user group tables\n\nRevision ID: 58c50ef19f08\nRevises: 7b9b952abdf6\nCreate Date: 2025-06-25 14:08:14.162380\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cf90764725d8_larger_refresh_tokens.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cf90764725d8_larger_refresh_tokens.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"cf90764725d8\"\ndown_revision = \"4794bc13e484\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cf90764725d8_larger_refresh_tokens.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cf90764725d8_larger_refresh_tokens.py",
      "line_number": 1,
      "code_snippet": "\"\"\"larger refresh tokens\n\nRevision ID: cf90764725d8\nRevises: 4794bc13e484\nCreate Date: 2025-04-04 10:56:39.769294\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f17bf3b0d9f1_embedding_provider_by_provider_type.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f17bf3b0d9f1_embedding_provider_by_provider_type.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f17bf3b0d9f1\"\ndown_revision = \"351faebd379d\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f80c6a2550f_add_chat_session_specific_temperature_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f80c6a2550f_add_chat_session_specific_temperature_.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"2f80c6a2550f\"\ndown_revision = \"33ea50e88f24\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f80c6a2550f_add_chat_session_specific_temperature_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f80c6a2550f_add_chat_session_specific_temperature_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add chat session specific temperature override\n\nRevision ID: 2f80c6a2550f\nRevises: 33ea50e88f24\nCreate Date: 2025-01-31 10:30:27.289646\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ffc707a226b4_basic_document_metadata.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ffc707a226b4_basic_document_metadata.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ffc707a226b4\"\ndown_revision = \"30c1d5744104\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ed9e44312505\"\ndown_revision = \"5e6f7a8b9c0d\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ed9e44312505\"\ndown_revision = \"5e6f7a8b9c0d\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ed9e44312505_add_icon_name_field.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add icon_name field\n\nRevision ID: ed9e44312505\nRevises: 5e6f7a8b9c0d\nCreate Date: 2025-12-03 16:35:07.828393\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py_10",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 10. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py",
      "line_number": 10,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f11b408e39d3\"\ndown_revision = \"3bd4c84fe72f\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py_11",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 11. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py",
      "line_number": 11,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f11b408e39d3\"\ndown_revision = \"3bd4c84fe72f\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f11b408e39d3_force_lowercase_all_users.py",
      "line_number": 1,
      "code_snippet": "\"\"\"force lowercase all users\n\nRevision ID: f11b408e39d3\nRevises: 3bd4c84fe72f\nCreate Date: 2025-02-26 17:04:55.683500\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = \"f11b408e39d3\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f7505c5b0284\"\ndown_revision = \"f71470ba9274\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f7505c5b0284\"\ndown_revision = \"f71470ba9274\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7505c5b0284_updated_constraints_for_ccpairs.py",
      "line_number": 1,
      "code_snippet": "\"\"\"updated constraints for ccpairs\n\nRevision ID: f7505c5b0284\nRevises: f71470ba9274\nCreate Date: 2025-04-01 17:50:42.504818\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c0aab6edb6dd\"\ndown_revision = \"35e518e0ddf4\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py_68",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 68 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py",
      "line_number": 68,
      "code_snippet": "            # Update only the connectors linked to this credential\n            # (and which are Slack connectors).\n            op.execute(\n                f\"\"\"\n                UPDATE connector AS c",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0aab6edb6dd_delete_workspace.py",
      "line_number": 1,
      "code_snippet": "\"\"\"delete workspace\n\nRevision ID: c0aab6edb6dd\nRevises: 35e518e0ddf4\nCreate Date: 2024-12-17 14:37:07.660631\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 16,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7b9b952abdf6\"\ndown_revision = \"36e9220ab794\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 17,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7b9b952abdf6\"\ndown_revision = \"36e9220ab794\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_278",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 278 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 278,
      "code_snippet": "        # delete removed entity types\n        if entity_type not in new_entity_type_conversion:\n            op.execute(\n                sa.text(f\"DELETE FROM kg_entity_type WHERE id_name = '{entity_type}'\")\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_290",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 290 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 290,
      "code_snippet": "        ]\n        attributes_str = json.dumps(attributes).replace(\"'\", \"''\")\n        op.execute(\n            sa.text(\n                f\"UPDATE kg_entity_type SET attributes = '{attributes_str}'\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_313",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 313 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 313,
      "code_snippet": "\n        attributes_str = json.dumps(attributes).replace(\"'\", \"''\")\n        op.execute(\n            sa.text(\n                f\"UPDATE kg_entity_type SET attributes = '{attributes_str}'\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_292",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 292 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 292,
      "code_snippet": "        op.execute(\n            sa.text(\n                f\"UPDATE kg_entity_type SET attributes = '{attributes_str}'\"\n                f\"WHERE id_name = '{entity_type}'\"\n            ),",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_315",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 315 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 315,
      "code_snippet": "        op.execute(\n            sa.text(\n                f\"UPDATE kg_entity_type SET attributes = '{attributes_str}'\"\n                f\"WHERE id_name = '{entity_type}'\"\n            ),",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7b9b952abdf6_update_entities.py",
      "line_number": 1,
      "code_snippet": "\"\"\"update-entities\n\nRevision ID: 7b9b952abdf6\nRevises: 36e9220ab794\nCreate Date: 2025-06-23 20:24:08.139201\n\n\"\"\"\n\nimport json\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3781a5eb12cb\"\ndown_revision = \"df46c75b714e\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3781a5eb12cb\"\ndown_revision = \"df46c75b714e\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3781a5eb12cb_add_chunk_stats_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add chunk stats table\n\nRevision ID: 3781a5eb12cb\nRevises: df46c75b714e\nCreate Date: 2025-03-10 10:02:30.586666\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ca04500b9ee8_add_cascade_deletes_to_agent_tables.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ca04500b9ee8_add_cascade_deletes_to_agent_tables.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ca04500b9ee8\"\ndown_revision = \"238b84885828\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ca04500b9ee8_add_cascade_deletes_to_agent_tables.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ca04500b9ee8_add_cascade_deletes_to_agent_tables.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_cascade_deletes_to_agent_tables\n\nRevision ID: ca04500b9ee8\nRevises: 238b84885828\nCreate Date: 2025-05-30 16:03:51.112263\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0ebb1d516877_add_ccpair_deletion_failure_message.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0ebb1d516877_add_ccpair_deletion_failure_message.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"0ebb1d516877\"\ndown_revision = \"52a219fb5233\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0ebb1d516877_add_ccpair_deletion_failure_message.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0ebb1d516877_add_ccpair_deletion_failure_message.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add ccpair deletion failure message\n\nRevision ID: 0ebb1d516877\nRevises: 52a219fb5233\nCreate Date: 2024-09-10 15:03:48.233926\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e1c073d48a3_add_personal_access_token_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e1c073d48a3_add_personal_access_token_table.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5e1c073d48a3\"\ndown_revision = \"09995b8811eb\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e1c073d48a3_add_personal_access_token_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e1c073d48a3_add_personal_access_token_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_personal_access_token_table\n\nRevision ID: 5e1c073d48a3\nRevises: 09995b8811eb\nCreate Date: 2025-10-30 17:30:24.308521\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py",
      "line_number": 19,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"16c37a30adf2\"\ndown_revision = \"0cd424f32b1d\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py",
      "line_number": 20,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"16c37a30adf2\"\ndown_revision = \"0cd424f32b1d\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/16c37a30adf2_user_file_relationship_migration.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migration 3: User file relationship migration\n\nRevision ID: 16c37a30adf2\nRevises: 0cd424f32b1d\nCreate Date: 2025-09-22 09:47:34.175596\n\nThis migration converts folder-based relationships to project-based relationships.\nIt migrates persona__user_folder to persona__user_file and populates project__user_file.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py",
      "line_number": 16,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b7c2b63c4a03\"\ndown_revision = \"f11b408e39d3\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py",
      "line_number": 17,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b7c2b63c4a03\"\ndown_revision = \"f11b408e39d3\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7c2b63c4a03_add_background_reindex_enabled_field.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add background_reindex_enabled field\n\nRevision ID: b7c2b63c4a03\nRevises: f11b408e39d3\nCreate Date: 2024-03-26 12:34:56.789012\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/79acd316403a_add_api_key_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/79acd316403a_add_api_key_table.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"79acd316403a\"\ndown_revision = \"904e5138fffb\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/79acd316403a_add_api_key_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/79acd316403a_add_api_key_table.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"79acd316403a\"\ndown_revision = \"904e5138fffb\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c5eae4a75a1b_add_chat_message__standard_answer_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c5eae4a75a1b_add_chat_message__standard_answer_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add chat_message__standard_answer table\n\nRevision ID: c5eae4a75a1b\nRevises: 0f7ff6d75b57\nCreate Date: 2025-01-15 14:08:49.688998\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e1392f05e840_added_input_prompts.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e1392f05e840_added_input_prompts.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"e1392f05e840\"\ndown_revision = \"08a1eda20fe1\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e1392f05e840_added_input_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e1392f05e840_added_input_prompts.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Added input prompts\n\nRevision ID: e1392f05e840\nRevises: 08a1eda20fe1\nCreate Date: 2024-07-13 19:09:22.556224\n\n\"\"\"\n\nimport fastapi_users_db_sqlalchemy\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"2b90f3af54b8\"\ndown_revision = \"9a0296d7421e\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"2b90f3af54b8\"\ndown_revision = \"9a0296d7421e\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b90f3af54b8_usage_limits.py",
      "line_number": 1,
      "code_snippet": "\"\"\"usage_limits\n\nRevision ID: 2b90f3af54b8\nRevises: 9a0296d7421e\nCreate Date: 2026-01-03 16:55:30.449692\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"df46c75b714e\"\ndown_revision = \"3934b1bc7b62\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"df46c75b714e\"\ndown_revision = \"3934b1bc7b62\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/df46c75b714e_add_default_vision_provider_to_llm_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_default_vision_provider_to_llm_provider\n\nRevision ID: df46c75b714e\nRevises: 3934b1bc7b62\nCreate Date: 2025-03-11 16:20:19.038945\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4d58345da04a_lowercase_user_emails.py_18",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 18. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4d58345da04a_lowercase_user_emails.py",
      "line_number": 18,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"4d58345da04a\"\ndown_revision = \"f1ca58b2f2ec\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4d58345da04a_lowercase_user_emails.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4d58345da04a_lowercase_user_emails.py",
      "line_number": 1,
      "code_snippet": "\"\"\"lowercase_user_emails\n\nRevision ID: 4d58345da04a\nRevises: f1ca58b2f2ec\nCreate Date: 2025-01-29 07:48:46.784041\n\n\"\"\"\n\nimport logging\nfrom typing import cast",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7547d982db8f\"\ndown_revision = \"ef7da92f7213\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7547d982db8f\"\ndown_revision = \"ef7da92f7213\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7547d982db8f_chat_folders.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Chat Folders\n\nRevision ID: 7547d982db8f\nRevises: ef7da92f7213\nCreate Date: 2024-05-02 15:18:56.573347\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"4f8a2b3c1d9e\"\ndown_revision = \"a852cbe15577\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"4f8a2b3c1d9e\"\ndown_revision = \"a852cbe15577\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4f8a2b3c1d9e_add_open_url_tool.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_open_url_tool\n\nRevision ID: 4f8a2b3c1d9e\nRevises: a852cbe15577\nCreate Date: 2025-11-24 12:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a7688ab35c45\"\ndown_revision = \"5c448911b12f\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"a7688ab35c45\"\ndown_revision = \"5c448911b12f\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a7688ab35c45_add_public_external_user_group_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add public_external_user_group table\n\nRevision ID: a7688ab35c45\nRevises: 5c448911b12f\nCreate Date: 2025-05-06 20:55:12.747875\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/475fcefe8826_add_name_to_api_key.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/475fcefe8826_add_name_to_api_key.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"475fcefe8826\"\ndown_revision = \"ecab2b3f1a3b\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c6531f32351_add_back_input_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c6531f32351_add_back_input_prompts.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add back input prompts\n\nRevision ID: 3c6531f32351\nRevises: aeda5f2df4f6\nCreate Date: 2025-01-13 12:49:51.705235\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py_18",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 18. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py",
      "line_number": 18,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"4ee1287bd26a\"\ndown_revision = \"47e5bef3a1d7\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py",
      "line_number": 19,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"4ee1287bd26a\"\ndown_revision = \"47e5bef3a1d7\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4ee1287bd26a_add_multiple_slack_bot_support.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_multiple_slack_bot_support\n\nRevision ID: 4ee1287bd26a\nRevises: 47e5bef3a1d7\nCreate Date: 2024-11-06 13:15:53.302644\n\n\"\"\"\n\nfrom typing import cast\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7206234e012a_add_image_generation_config_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7206234e012a_add_image_generation_config_table.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7206234e012a\"\ndown_revision = \"699221885109\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7206234e012a_add_image_generation_config_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7206234e012a_add_image_generation_config_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add image generation config table\n\nRevision ID: 7206234e012a\nRevises: 699221885109\nCreate Date: 2025-12-21 00:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ec85f2b3c544_remove_last_attempt_status_from_cc_pair.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ec85f2b3c544_remove_last_attempt_status_from_cc_pair.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ec85f2b3c544\"\ndown_revision = \"70f00c45c0f2\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dfbe9e93d3c7_extended_role_for_non_web.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dfbe9e93d3c7_extended_role_for_non_web.py",
      "line_number": 1,
      "code_snippet": "\"\"\"extended_role_for_non_web\n\nRevision ID: dfbe9e93d3c7\nRevises: 9cf5c00f72fe\nCreate Date: 2024-11-16 07:54:18.727906\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bc9771dccadf_create_usage_reports_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bc9771dccadf_create_usage_reports_table.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"bc9771dccadf\"\ndown_revision = \"0568ccf46a6b\"\n\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"9087b548dd69\"\ndown_revision = \"2b90f3af54b8\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9087b548dd69\"\ndown_revision = \"2b90f3af54b8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py_60",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 60 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py",
      "line_number": 60,
      "code_snippet": "\n    # Create new LLM provider for image generation (clone only api_key)\n    result = conn.execute(\n        sa.text(\n            \"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9087b548dd69_seed_default_image_gen_config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"seed_default_image_gen_config\n\nRevision ID: 9087b548dd69\nRevises: 2b90f3af54b8\nCreate Date: 2026-01-05 00:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/213fd978c6d8_notifications.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/213fd978c6d8_notifications.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"213fd978c6d8\"\ndown_revision = \"5fc1f54cc252\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b85f02ec1308_fix_file_type_migration.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b85f02ec1308_fix_file_type_migration.py",
      "line_number": 12,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b85f02ec1308\"\ndown_revision = \"a3bfd0d64902\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b85f02ec1308_fix_file_type_migration.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b85f02ec1308_fix_file_type_migration.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b85f02ec1308\"\ndown_revision = \"a3bfd0d64902\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"c1d2e3f4a5b6\"\ndown_revision = \"b8c9d0e1f2a3\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c1d2e3f4a5b6\"\ndown_revision = \"b8c9d0e1f2a3\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c1d2e3f4a5b6_add_deep_research_tool.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_deep_research_tool\n\nRevision ID: c1d2e3f4a5b6\nRevises: b8c9d0e1f2a3\nCreate Date: 2025-12-18 16:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"9aadf32dfeb4\"\ndown_revision = \"3781a5eb12cb\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9aadf32dfeb4\"\ndown_revision = \"3781a5eb12cb\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9aadf32dfeb4_add_user_files.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add user files\n\nRevision ID: 9aadf32dfeb4\nRevises: 3781a5eb12cb\nCreate Date: 2025-01-26 16:08:21.551022\n\n\"\"\"\n\nimport sqlalchemy as sa\nimport datetime",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fcd135795f21_add_slack_bot_display_type.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fcd135795f21_add_slack_bot_display_type.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"fcd135795f21\"\ndown_revision = \"0a2b51deb0b8\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fcd135795f21_add_slack_bot_display_type.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fcd135795f21_add_slack_bot_display_type.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"fcd135795f21\"\ndown_revision = \"0a2b51deb0b8\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/800f48024ae9_add_id_to_connectorcredentialpair.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/800f48024ae9_add_id_to_connectorcredentialpair.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"800f48024ae9\"\ndown_revision = \"767f1c2a00eb\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c99d76fcd298\"\ndown_revision = \"5c7fdadae813\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py_39",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 39 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py",
      "line_number": 39,
      "code_snippet": "\n    # Delete dependent records first\n    op.execute(\n        f\"\"\"\n        DELETE FROM document_retrieval_feedback",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py_47",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 47 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py",
      "line_number": 47,
      "code_snippet": "    \"\"\"\n    )\n    op.execute(\n        f\"\"\"\n        DELETE FROM chat_message__search_doc",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c99d76fcd298_add_nullable_to_persona_id_in_chat_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add nullable to persona id in Chat Session\n\nRevision ID: c99d76fcd298\nRevises: 5c7fdadae813\nCreate Date: 2024-07-09 19:27:01.579697\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/54a74a0417fc_danswerbot_onyxbot.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/54a74a0417fc_danswerbot_onyxbot.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"54a74a0417fc\"\ndown_revision = \"94dc3d0236f8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/54a74a0417fc_danswerbot_onyxbot.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/54a74a0417fc_danswerbot_onyxbot.py",
      "line_number": 1,
      "code_snippet": "\"\"\"danswerbot -> onyxbot\n\nRevision ID: 54a74a0417fc\nRevises: 94dc3d0236f8\nCreate Date: 2024-12-11 18:05:05.490737\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/351faebd379d_add_curator_fields.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/351faebd379d_add_curator_fields.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"351faebd379d\"\ndown_revision = \"ee3f4b47fad5\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/351faebd379d_add_curator_fields.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/351faebd379d_add_curator_fields.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"351faebd379d\"\ndown_revision = \"ee3f4b47fad5\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7ec9b5b505f_adjust_prompt_length.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7ec9b5b505f_adjust_prompt_length.py",
      "line_number": 1,
      "code_snippet": "\"\"\"adjust prompt length\n\nRevision ID: b7ec9b5b505f\nRevises: abbfec3a5ac5\nCreate Date: 2025-09-10 18:51:15.629197\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b8c9d0e1f2a3\"\ndown_revision = \"a2b3c4d5e6f7\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b8c9d0e1f2a3\"\ndown_revision = \"a2b3c4d5e6f7\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b8c9d0e1f2a3_drop_milestone_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Drop milestone table\n\nRevision ID: b8c9d0e1f2a3\nRevises: a2b3c4d5e6f7\nCreate Date: 2025-12-18\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"efb35676026c\"\ndown_revision = \"0ebb1d516877\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"efb35676026c\"\ndown_revision = \"0ebb1d516877\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/efb35676026c_standard_answer_match_regex_flag.py",
      "line_number": 1,
      "code_snippet": "\"\"\"standard answer match_regex flag\n\nRevision ID: efb35676026c\nRevises: 0ebb1d516877\nCreate Date: 2024-09-11 13:55:46.101149\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/570282d33c49_track_onyxbot_explicitly.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/570282d33c49_track_onyxbot_explicitly.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"570282d33c49\"\ndown_revision = \"7547d982db8f\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/570282d33c49_track_onyxbot_explicitly.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/570282d33c49_track_onyxbot_explicitly.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"570282d33c49\"\ndown_revision = \"7547d982db8f\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7f726bad5367_slack_followup.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7f726bad5367_slack_followup.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7f726bad5367\"\ndown_revision = \"79acd316403a\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7f726bad5367_slack_followup.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7f726bad5367_slack_followup.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7f726bad5367\"\ndown_revision = \"79acd316403a\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5ae8240accb3\"\ndown_revision = \"b558f51620b4\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5ae8240accb3\"\ndown_revision = \"b558f51620b4\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5ae8240accb3_add_research_agent_database_tables_and_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add research agent database tables and chat message research fields\n\nRevision ID: 5ae8240accb3\nRevises: b558f51620b4\nCreate Date: 2025-08-06 14:29:24.691388\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/eaa3b5593925_add_default_slack_channel_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/eaa3b5593925_add_default_slack_channel_config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add default slack channel config\n\nRevision ID: eaa3b5593925\nRevises: 98a5008d8711\nCreate Date: 2025-02-03 18:07:56.552526\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7da543f5672f_add_slackbotconfig_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7da543f5672f_add_slackbotconfig_table.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7da543f5672f\"\ndown_revision = \"febe9eaa0644\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e91df4e935ef_private_personas_documentsets.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e91df4e935ef_private_personas_documentsets.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"e91df4e935ef\"\ndown_revision = \"91fd3b470d1a\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py",
      "line_number": 20,
      "code_snippet": "from onyx.utils.encryption import encrypt_string_to_bytes\n\nrevision = \"3c9a65f1207f\"\ndown_revision = \"1f2a3b4c5d6e\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py_21",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py",
      "line_number": 21,
      "code_snippet": "\nrevision = \"3c9a65f1207f\"\ndown_revision = \"1f2a3b4c5d6e\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3c9a65f1207f_seed_exa_provider_from_env.py",
      "line_number": 1,
      "code_snippet": "\"\"\"seed_exa_provider_from_env\n\nRevision ID: 3c9a65f1207f\nRevises: 1f2a3b4c5d6e\nCreate Date: 2025-11-20 19:18:00.000000\n\n\"\"\"\n\nfrom __future__ import annotations\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a852cbe15577_new_chat_history.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a852cbe15577_new_chat_history.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a852cbe15577\"\ndown_revision = \"6436661d5b65\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a852cbe15577_new_chat_history.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a852cbe15577_new_chat_history.py",
      "line_number": 1,
      "code_snippet": "\"\"\"New Chat History\n\nRevision ID: a852cbe15577\nRevises: 6436661d5b65\nCreate Date: 2025-11-08 15:16:37.781308\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/949b4a92a401_remove_rt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/949b4a92a401_remove_rt.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove rt\n\nRevision ID: 949b4a92a401\nRevises: 1b10e1fda030\nCreate Date: 2024-10-26 13:06:06.937969\n\n\"\"\"\n\nfrom alembic import op\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/64bd5677aeb6_add_image_input_support_to_model_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/64bd5677aeb6_add_image_input_support_to_model_config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add image input support to model config\n\nRevision ID: 64bd5677aeb6\nRevises: b30353be4eec\nCreate Date: 2025-09-28 15:48:12.003612\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py_60",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 60 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py",
      "line_number": 60,
      "code_snippet": "        try:\n            stmt = text(\n                f\"DELETE FROM {table_name} WHERE {id_column} = ANY(:ids)\"\n            ).bindparams(sa.bindparam(\"ids\", value=batch_ids, type_=array_type))\n            result = bind.execute(stmt)",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py_78",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 78 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py",
      "line_number": 78,
      "code_snippet": "    if failed_batches:\n        logger.warning(\n            f\"Failed to delete {len(failed_batches)} batches from {table_name}. \"\n            f\"Total deleted: {total_deleted}/{total_count}\"\n        )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a78dba1080a_user_file_legacy_data_cleanup.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migration 5: User file legacy data cleanup\n\nRevision ID: 3a78dba1080a\nRevises: 7cc3fcc116c1\nCreate Date: 2025-09-22 10:04:27.986294\n\nThis migration removes legacy user-file documents and connector_credential_pairs.\nIt performs bulk deletions of obsolete data after the UUID migration.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c448911b12f_add_content_type_to_userfile.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c448911b12f_add_content_type_to_userfile.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5c448911b12f\"\ndown_revision = \"47a07e1a38f1\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c448911b12f_add_content_type_to_userfile.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c448911b12f_add_content_type_to_userfile.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5c448911b12f\"\ndown_revision = \"47a07e1a38f1\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f71470ba9274_add_prompt_length_limit.py_10",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 10. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f71470ba9274_add_prompt_length_limit.py",
      "line_number": 10,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f71470ba9274\"\ndown_revision = \"6a804aeb4830\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f71470ba9274_add_prompt_length_limit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f71470ba9274_add_prompt_length_limit.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add prompt length limit\n\nRevision ID: f71470ba9274\nRevises: 6a804aeb4830\nCreate Date: 2025-04-01 15:07:14.977435\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = \"f71470ba9274\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"c7e9f4a3b2d1\"\ndown_revision = \"3c9a65f1207f\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c7e9f4a3b2d1\"\ndown_revision = \"3c9a65f1207f\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7e9f4a3b2d1_add_python_tool.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_python_tool\n\nRevision ID: c7e9f4a3b2d1\nRevises: 3c9a65f1207f\nCreate Date: 2025-11-08 00:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5d12a446f5c0_add_api_version_and_deployment_name_to_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5d12a446f5c0_add_api_version_and_deployment_name_to_.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5d12a446f5c0\"\ndown_revision = \"e4334d5b33ba\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5d12a446f5c0_add_api_version_and_deployment_name_to_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5d12a446f5c0_add_api_version_and_deployment_name_to_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add api_version and deployment_name to search settings\n\nRevision ID: 5d12a446f5c0\nRevises: e4334d5b33ba\nCreate Date: 2024-10-08 15:56:07.975636\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a4f23d6b71c8\"\ndown_revision = \"5e1c073d48a3\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"a4f23d6b71c8\"\ndown_revision = \"5e1c073d48a3\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a4f23d6b71c8_add_llm_provider_persona_restrictions.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add llm provider persona restrictions\n\nRevision ID: a4f23d6b71c8\nRevises: 5e1c073d48a3\nCreate Date: 2025-10-21 00:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8818cf73fa1a_drop_include_citations.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8818cf73fa1a_drop_include_citations.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"8818cf73fa1a\"\ndown_revision = \"7ed603b64d5a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8818cf73fa1a_drop_include_citations.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8818cf73fa1a_drop_include_citations.py",
      "line_number": 1,
      "code_snippet": "\"\"\"drop include citations\n\nRevision ID: 8818cf73fa1a\nRevises: 7ed603b64d5a\nCreate Date: 2025-09-02 19:43:50.060680\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"bd2921608c3a\"\ndown_revision = \"797089dfb4d2\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"bd2921608c3a\"\ndown_revision = \"797089dfb4d2\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd2921608c3a_non_nullable_default_persona.py",
      "line_number": 1,
      "code_snippet": "\"\"\"non nullable default persona\n\nRevision ID: bd2921608c3a\nRevises: 797089dfb4d2\nCreate Date: 2024-09-20 10:28:37.992042\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"9a0296d7421e\"\ndown_revision = \"7206234e012a\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9a0296d7421e\"\ndown_revision = \"7206234e012a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9a0296d7421e_add_is_auto_mode_to_llm_provider.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_is_auto_mode_to_llm_provider\n\nRevision ID: 9a0296d7421e\nRevises: 7206234e012a\nCreate Date: 2025-12-17 18:14:29.620981\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/904451035c9b_store_tool_details.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/904451035c9b_store_tool_details.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"904451035c9b\"\ndown_revision = \"3b25685ff73c\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/18b5b2524446_add_is_clarification_to_chat_message.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/18b5b2524446_add_is_clarification_to_chat_message.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"18b5b2524446\"\ndown_revision = \"87c52ec39f84\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/18b5b2524446_add_is_clarification_to_chat_message.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/18b5b2524446_add_is_clarification_to_chat_message.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add is_clarification to chat_message\n\nRevision ID: 18b5b2524446\nRevises: 87c52ec39f84\nCreate Date: 2025-01-16\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c3dca366b35_backend_driven_notification_details.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c3dca366b35_backend_driven_notification_details.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5c3dca366b35\"\ndown_revision = \"9087b548dd69\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c3dca366b35_backend_driven_notification_details.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c3dca366b35_backend_driven_notification_details.py",
      "line_number": 1,
      "code_snippet": "\"\"\"backend driven notification details\n\nRevision ID: 5c3dca366b35\nRevises: 9087b548dd69\nCreate Date: 2026-01-06 16:03:11.413724\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0cd424f32b1d_user_file_data_preparation_and_backfill.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0cd424f32b1d_user_file_data_preparation_and_backfill.py",
      "line_number": 19,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"0cd424f32b1d\"\ndown_revision = \"9b66d3156fc6\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0cd424f32b1d_user_file_data_preparation_and_backfill.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0cd424f32b1d_user_file_data_preparation_and_backfill.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migration 2: User file data preparation and backfill\n\nRevision ID: 0cd424f32b1d\nRevises: 9b66d3156fc6\nCreate Date: 2025-09-22 09:44:42.727034\n\nThis migration populates the new columns added in migration 1.\nIt prepares data for the UUID transition and relationship migration.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b558f51620b4\"\ndown_revision = \"90e3b9af7da4\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b558f51620b4\"\ndown_revision = \"90e3b9af7da4\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b558f51620b4_pause_finished_user_file_connectors.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Pause finished user file connectors\n\nRevision ID: b558f51620b4\nRevises: 90e3b9af7da4\nCreate Date: 2025-08-15 17:17:02.456704\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"03d710ccf29c\"  # Generate a new unique ID\ndown_revision = \"96a5702df6aa\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"03d710ccf29c\"  # Generate a new unique ID\ndown_revision = \"96a5702df6aa\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03d710ccf29c_add_permission_sync_attempt_tables.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add permission sync attempt tables\n\nRevision ID: 03d710ccf29c\nRevises: 96a5702df6aa\nCreate Date: 2025-09-11 13:30:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b156fa702355_chat_reworked.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b156fa702355_chat_reworked.py",
      "line_number": 17,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b156fa702355\"\ndown_revision = \"baf71f781b9e\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b156fa702355_chat_reworked.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b156fa702355_chat_reworked.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Chat Reworked\n\nRevision ID: b156fa702355\nRevises: baf71f781b9e\nCreate Date: 2023-12-12 00:57:41.823371\n\n\"\"\"\n\nimport fastapi_users_db_sqlalchemy\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"2daa494a0851\"\ndown_revision = \"c0fd6e4da83a\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"2daa494a0851\"\ndown_revision = \"c0fd6e4da83a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2daa494a0851_add_group_sync_time.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add-group-sync-time\n\nRevision ID: 2daa494a0851\nRevises: c0fd6e4da83a\nCreate Date: 2024-11-11 10:57:22.991157\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/08a1eda20fe1_add_earliest_indexing_to_connector.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/08a1eda20fe1_add_earliest_indexing_to_connector.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"08a1eda20fe1\"\ndown_revision = \"8a87bd6ec550\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3bfd0d64902_add_chosen_assistants_to_user_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3bfd0d64902_add_chosen_assistants_to_user_table.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a3bfd0d64902\"\ndown_revision = \"ec85f2b3c544\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3bfd0d64902_add_chosen_assistants_to_user_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3bfd0d64902_add_chosen_assistants_to_user_table.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"a3bfd0d64902\"\ndown_revision = \"ec85f2b3c544\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/40926a4dab77_reset_userfile_document_id_migrated_.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/40926a4dab77_reset_userfile_document_id_migrated_.py",
      "line_number": 12,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"40926a4dab77\"\ndown_revision = \"64bd5677aeb6\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/40926a4dab77_reset_userfile_document_id_migrated_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/40926a4dab77_reset_userfile_document_id_migrated_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"reset userfile document_id_migrated field\n\nRevision ID: 40926a4dab77\nRevises: 64bd5677aeb6\nCreate Date: 2025-10-06 16:10:32.898668\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/48d14957fe80_add_support_for_custom_tools.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/48d14957fe80_add_support_for_custom_tools.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"48d14957fe80\"\ndown_revision = \"b85f02ec1308\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/48d14957fe80_add_support_for_custom_tools.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/48d14957fe80_add_support_for_custom_tools.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"48d14957fe80\"\ndown_revision = \"b85f02ec1308\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ecab2b3f1a3b_add_overrides_to_the_chat_session.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ecab2b3f1a3b_add_overrides_to_the_chat_session.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ecab2b3f1a3b\"\ndown_revision = \"38eda64af7fe\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a2b3c4d5e6f7_remove_fast_default_model_name.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a2b3c4d5e6f7_remove_fast_default_model_name.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a2b3c4d5e6f7\"\ndown_revision = \"2a391f840e85\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a2b3c4d5e6f7_remove_fast_default_model_name.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a2b3c4d5e6f7_remove_fast_default_model_name.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"a2b3c4d5e6f7\"\ndown_revision = \"2a391f840e85\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7a70b7664e37_add_model_configuration_table.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7a70b7664e37_add_model_configuration_table.py",
      "line_number": 20,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7a70b7664e37\"\ndown_revision = \"d961aca62eb3\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7a70b7664e37_add_model_configuration_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7a70b7664e37_add_model_configuration_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add model-configuration table\n\nRevision ID: 7a70b7664e37\nRevises: d961aca62eb3\nCreate Date: 2025-04-10 15:00:35.984669\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1f2a3b4c5d6e_add_internet_search_and_content_providers.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1f2a3b4c5d6e_add_internet_search_and_content_providers.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"1f2a3b4c5d6e\"\ndown_revision = \"9drpiiw74ljy\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1f2a3b4c5d6e_add_internet_search_and_content_providers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1f2a3b4c5d6e_add_internet_search_and_content_providers.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add internet search and content provider tables\n\nRevision ID: 1f2a3b4c5d6e\nRevises: 9drpiiw74ljy\nCreate Date: 2025-11-10 19:45:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d1b637d7050a\"\ndown_revision = \"d25168c2beee\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"d1b637d7050a\"\ndown_revision = \"d25168c2beee\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d1b637d7050a_sync_exa_api_key_to_content_provider.py",
      "line_number": 1,
      "code_snippet": "\"\"\"sync_exa_api_key_to_content_provider\n\nRevision ID: d1b637d7050a\nRevises: d25168c2beee\nCreate Date: 2026-01-09 15:54:15.646249\n\n\"\"\"\n\nfrom alembic import op\nfrom sqlalchemy import text",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d961aca62eb3\"\ndown_revision = \"cf90764725d8\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"d961aca62eb3\"\ndown_revision = \"cf90764725d8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d961aca62eb3_update_status_length.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update status length\n\nRevision ID: d961aca62eb3\nRevises: cf90764725d8\nCreate Date: 2025-03-23 16:10:05.683965\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py",
      "line_number": 20,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"505c488f6662\"\ndown_revision = \"d09fc20a3c66\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py_305",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 305 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py",
      "line_number": 305,
      "code_snippet": "                set_clause = \", \".join([f\"{k} = :{k}\" for k in updates.keys()])\n                updates[\"user_id\"] = str(user_id)  # Convert UUID to string for SQL\n                conn.execute(\n                    sa.text(f'UPDATE \"user\" SET {set_clause} WHERE id = :user_id'),\n                    updates,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/505c488f6662_merge_default_assistants_into_unified.py",
      "line_number": 1,
      "code_snippet": "\"\"\"merge_default_assistants_into_unified\n\nRevision ID: 505c488f6662\nRevises: d09fc20a3c66\nCreate Date: 2025-09-09 19:00:56.816626\n\n\"\"\"\n\nimport json\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a8c2065484e6_add_auto_scroll_to_user_model.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a8c2065484e6_add_auto_scroll_to_user_model.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a8c2065484e6\"\ndown_revision = \"abe7378b8217\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a8c2065484e6_add_auto_scroll_to_user_model.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a8c2065484e6_add_auto_scroll_to_user_model.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add auto scroll to user model\n\nRevision ID: a8c2065484e6\nRevises: abe7378b8217\nCreate Date: 2024-11-22 17:34:09.690295\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4b08d97e175a_change_default_prune_freq.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4b08d97e175a_change_default_prune_freq.py",
      "line_number": 12,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"4b08d97e175a\"\ndown_revision = \"d9ec13955951\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7f99be1cb9f5_add_index_for_getting_documents_just_by_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7f99be1cb9f5_add_index_for_getting_documents_just_by_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7f99be1cb9f5\"\ndown_revision = \"78dbe7e38469\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/50b683a8295c_add_additional_retrieval_controls_to_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/50b683a8295c_add_additional_retrieval_controls_to_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"50b683a8295c\"\ndown_revision = \"7da0ae5ad583\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7e490836d179\"\ndown_revision = \"c1d2e3f4a5b6\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7e490836d179\"\ndown_revision = \"c1d2e3f4a5b6\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7e490836d179_nullify_default_system_prompt.py",
      "line_number": 1,
      "code_snippet": "\"\"\"nullify_default_system_prompt\n\nRevision ID: 7e490836d179\nRevises: c1d2e3f4a5b6\nCreate Date: 2025-12-29 16:54:36.635574\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6f4f86aef280_add_queries_and_is_web_fetch_to_.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6f4f86aef280_add_queries_and_is_web_fetch_to_.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"6f4f86aef280\"\ndown_revision = \"03d710ccf29c\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6f4f86aef280_add_queries_and_is_web_fetch_to_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6f4f86aef280_add_queries_and_is_web_fetch_to_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add queries and is web fetch to iteration answer\n\nRevision ID: 6f4f86aef280\nRevises: 03d710ccf29c\nCreate Date: 2025-10-14 18:08:30.920123\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"96a5702df6aa\"\ndown_revision = \"40926a4dab77\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"96a5702df6aa\"\ndown_revision = \"40926a4dab77\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/96a5702df6aa_mcp_tool_enabled.py",
      "line_number": 1,
      "code_snippet": "\"\"\"mcp_tool_enabled\n\nRevision ID: 96a5702df6aa\nRevises: 40926a4dab77\nCreate Date: 2025-10-09 12:10:21.733097\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0816326d83aa_add_federated_connector_tables.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0816326d83aa_add_federated_connector_tables.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add federated connector tables\n\nRevision ID: 0816326d83aa\nRevises: 12635f6655b7\nCreate Date: 2025-06-29 14:09:45.109518\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dba7f71618f5_onyx_custom_tool_flow.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dba7f71618f5_onyx_custom_tool_flow.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"dba7f71618f5\"\ndown_revision = \"d5645c915d0e\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dba7f71618f5_onyx_custom_tool_flow.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dba7f71618f5_onyx_custom_tool_flow.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"dba7f71618f5\"\ndown_revision = \"d5645c915d0e\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7cc3fcc116c1_user_file_uuid_primary_key_swap.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7cc3fcc116c1_user_file_uuid_primary_key_swap.py",
      "line_number": 20,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7cc3fcc116c1\"\ndown_revision = \"16c37a30adf2\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7cc3fcc116c1_user_file_uuid_primary_key_swap.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7cc3fcc116c1_user_file_uuid_primary_key_swap.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migration 4: User file UUID primary key swap\n\nRevision ID: 7cc3fcc116c1\nRevises: 16c37a30adf2\nCreate Date: 2025-09-22 09:54:38.292952\n\nThis migration performs the critical UUID primary key swap on user_file table.\nIt updates all foreign key references to use UUIDs instead of integers.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f1c6478c3fd8_add_pre_defined_feedback.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f1c6478c3fd8_add_pre_defined_feedback.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"f1c6478c3fd8\"\ndown_revision = \"643a84a42a33\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/98a5008d8711_agent_tracking.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/98a5008d8711_agent_tracking.py",
      "line_number": 1,
      "code_snippet": "\"\"\"agent_tracking\n\nRevision ID: 98a5008d8711\nRevises: 2f80c6a2550f\nCreate Date: 2025-01-29 17:00:00.000001\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/52a219fb5233_add_last_synced_and_last_modified_to_document_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/52a219fb5233_add_last_synced_and_last_modified_to_document_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add last synced and last modified to document table\n\nRevision ID: 52a219fb5233\nRevises: f7e58d357687\nCreate Date: 2024-08-28 17:40:46.077470\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d61e513bef0a_add_total_docs_for_index_attempt.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d61e513bef0a_add_total_docs_for_index_attempt.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d61e513bef0a\"\ndown_revision = \"46625e4745d4\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ee3f4b47fad5_added_alternate_model_to_chat_message.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ee3f4b47fad5_added_alternate_model_to_chat_message.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ee3f4b47fad5\"\ndown_revision = \"2d2304e27d8c\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ee3f4b47fad5_added_alternate_model_to_chat_message.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ee3f4b47fad5_added_alternate_model_to_chat_message.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ee3f4b47fad5\"\ndown_revision = \"2d2304e27d8c\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a2b51deb0b8_add_starter_prompts.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a2b51deb0b8_add_starter_prompts.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"0a2b51deb0b8\"\ndown_revision = \"5f4b8568a221\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a2b51deb0b8_add_starter_prompts.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a2b51deb0b8_add_starter_prompts.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"0a2b51deb0b8\"\ndown_revision = \"5f4b8568a221\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"91a0a4d62b14\"\ndown_revision = \"dab04867cd88\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"91a0a4d62b14\"\ndown_revision = \"dab04867cd88\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91a0a4d62b14_milestone.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Milestone\n\nRevision ID: 91a0a4d62b14\nRevises: dab04867cd88\nCreate Date: 2024-12-13 19:03:30.947551\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ec3ec2eabf7b_index_from_beginning.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ec3ec2eabf7b_index_from_beginning.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ec3ec2eabf7b\"\ndown_revision = \"dbaa756c2ccf\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"26b931506ecb\"\ndown_revision = \"2daa494a0851\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"26b931506ecb\"\ndown_revision = \"2daa494a0851\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/26b931506ecb_default_chosen_assistants_to_none.py",
      "line_number": 1,
      "code_snippet": "\"\"\"default chosen assistants to none\n\nRevision ID: 26b931506ecb\nRevises: 2daa494a0851\nCreate Date: 2024-11-12 13:23:29.858995\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f95e36923e6_add_indexing_coordination.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2f95e36923e6_add_indexing_coordination.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_indexing_coordination\n\nRevision ID: 2f95e36923e6\nRevises: 0816326d83aa\nCreate Date: 2025-07-10 16:17:57.762182\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/05c07bf07c00_add_search_doc_relevance_details.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/05c07bf07c00_add_search_doc_relevance_details.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"05c07bf07c00\"\ndown_revision = \"b896bbd0d5a7\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3d1cca026fe8_add_oauth_config_and_user_tokens.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3d1cca026fe8_add_oauth_config_and_user_tokens.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3d1cca026fe8\"\ndown_revision = \"c8a93a2af083\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3d1cca026fe8_add_oauth_config_and_user_tokens.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3d1cca026fe8_add_oauth_config_and_user_tokens.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_oauth_config_and_user_tokens\n\nRevision ID: 3d1cca026fe8\nRevises: c8a93a2af083\nCreate Date: 2025-10-21 13:27:34.274721\n\n\"\"\"\n\nfrom alembic import op\nimport fastapi_users_db_sqlalchemy",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/65bc6e0f8500_remove_kg_subtype_from_db.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/65bc6e0f8500_remove_kg_subtype_from_db.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove kg subtype from db\n\nRevision ID: 65bc6e0f8500\nRevises: cec7ec36c505\nCreate Date: 2025-06-13 10:04:27.705976\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f9b8c7d6e5a4_update_parent_question_id_foreign_key_to_research_agent_iteration.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f9b8c7d6e5a4_update_parent_question_id_foreign_key_to_research_agent_iteration.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f9b8c7d6e5a4\"\ndown_revision = \"bd7c3bf8beba\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f9b8c7d6e5a4_update_parent_question_id_foreign_key_to_research_agent_iteration.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f9b8c7d6e5a4_update_parent_question_id_foreign_key_to_research_agent_iteration.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove foreign key constraints from research_agent_iteration_sub_step\n\nRevision ID: f9b8c7d6e5a4\nRevises: bd7c3bf8beba\nCreate Date: 2025-01-27 12:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/15326fcec57e_introduce_onyx_apis.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/15326fcec57e_introduce_onyx_apis.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"15326fcec57e\"\ndown_revision = \"77d07dffae64\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b329d00a9ea6\"\ndown_revision = \"f9b8c7d6e5a4\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b329d00a9ea6\"\ndown_revision = \"f9b8c7d6e5a4\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b329d00a9ea6_adding_assistant_specific_user_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Adding assistant-specific user preferences\n\nRevision ID: b329d00a9ea6\nRevises: f9b8c7d6e5a4\nCreate Date: 2025-08-26 23:14:44.592985\n\n\"\"\"\n\nfrom alembic import op\nimport fastapi_users_db_sqlalchemy",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"35e518e0ddf4\"\ndown_revision = \"91a0a4d62b14\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"35e518e0ddf4\"\ndown_revision = \"91a0a4d62b14\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e518e0ddf4_properly_cascade.py",
      "line_number": 1,
      "code_snippet": "\"\"\"properly_cascade\n\nRevision ID: 35e518e0ddf4\nRevises: 91a0a4d62b14\nCreate Date: 2024-09-20 21:24:04.891018\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4cebcbc9b2ae_add_tab_index_to_tool_call.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4cebcbc9b2ae_add_tab_index_to_tool_call.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"4cebcbc9b2ae\"\ndown_revision = \"a1b2c3d4e5f6\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/acaab4ef4507_remove_inactive_ccpair_status_on_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/acaab4ef4507_remove_inactive_ccpair_status_on_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"acaab4ef4507\"\ndown_revision = \"b388730a2899\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/acaab4ef4507_remove_inactive_ccpair_status_on_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/acaab4ef4507_remove_inactive_ccpair_status_on_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove inactive ccpair status on downgrade\n\nRevision ID: acaab4ef4507\nRevises: b388730a2899\nCreate Date: 2025-02-16 18:21:41.330212\n\n\"\"\"\n\nfrom alembic import op\nfrom onyx.db.models import ConnectorCredentialPair",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7477a5f5d728_added_model_defaults_for_users.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7477a5f5d728_added_model_defaults_for_users.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7477a5f5d728\"\ndown_revision = \"213fd978c6d8\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f39c5794c10a\"\ndown_revision = \"2cdeff6d8c93\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f39c5794c10a\"\ndown_revision = \"2cdeff6d8c93\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f39c5794c10a_add_background_errors_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add background errors table\n\nRevision ID: f39c5794c10a\nRevises: 2cdeff6d8c93\nCreate Date: 2025-02-12 17:11:14.527876\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b896bbd0d5a7_backfill_is_internet_data_to_false.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b896bbd0d5a7_backfill_is_internet_data_to_false.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b896bbd0d5a7\"\ndown_revision = \"44f856ae2a4a\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py_28",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 28. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py",
      "line_number": 28,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"dbaa756c2ccf\"\ndown_revision = \"7f726bad5367\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py_29",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 29. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py",
      "line_number": 29,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"dbaa756c2ccf\"\ndown_revision = \"7f726bad5367\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dbaa756c2ccf_embedding_models.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Embedding Models\n\nRevision ID: dbaa756c2ccf\nRevises: 7f726bad5367\nCreate Date: 2024-01-25 17:12:31.813160\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2666d766cb9b_google_oauth2.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2666d766cb9b_google_oauth2.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"2666d766cb9b\"\ndown_revision = \"6d387b3196c2\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f32615f71aeb\"\ndown_revision = \"bd2921608c3a\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f32615f71aeb\"\ndown_revision = \"bd2921608c3a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f32615f71aeb_add_custom_headers_to_tools.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add custom headers to tools\n\nRevision ID: f32615f71aeb\nRevises: bd2921608c3a\nCreate Date: 2024-09-12 20:26:38.932377\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e0a68a81d434_add_chat_feedback.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e0a68a81d434_add_chat_feedback.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"e0a68a81d434\"\ndown_revision = \"ae62505e3acc\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b082fec533f0_make_last_attempt_status_nullable.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b082fec533f0_make_last_attempt_status_nullable.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"b082fec533f0\"\ndown_revision = \"df0c7ad8a076\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"8f43500ee275\"\ndown_revision = \"da42808081e3\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"8f43500ee275\"\ndown_revision = \"da42808081e3\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8f43500ee275_add_index.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add index\n\nRevision ID: 8f43500ee275\nRevises: da42808081e3\nCreate Date: 2025-02-24 17:35:33.072714\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2d2304e27d8c_add_above_below_to_persona.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2d2304e27d8c_add_above_below_to_persona.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"2d2304e27d8c\"\ndown_revision = \"4b08d97e175a\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2d2304e27d8c_add_above_below_to_persona.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2d2304e27d8c_add_above_below_to_persona.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"2d2304e27d8c\"\ndown_revision = \"4b08d97e175a\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/be2ab2aa50ee_fix_capitalization.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/be2ab2aa50ee_fix_capitalization.py",
      "line_number": 1,
      "code_snippet": "\"\"\"fix_capitalization\n\nRevision ID: be2ab2aa50ee\nRevises: 369644546676\nCreate Date: 2025-01-10 13:13:26.228960\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4a951134c801_moved_status_to_connector_credential_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4a951134c801_moved_status_to_connector_credential_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"4a951134c801\"\ndown_revision = \"7477a5f5d728\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/77d07dffae64_forcibly_remove_more_enum_types_from_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/77d07dffae64_forcibly_remove_more_enum_types_from_.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"77d07dffae64\"\ndown_revision = \"d61e513bef0a\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dab04867cd88_add_composite_index_to_document_by_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dab04867cd88_add_composite_index_to_document_by_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"dab04867cd88\"\ndown_revision = \"54a74a0417fc\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dab04867cd88_add_composite_index_to_document_by_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/dab04867cd88_add_composite_index_to_document_by_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add composite index to document_by_connector_credential_pair\n\nRevision ID: dab04867cd88\nRevises: 54a74a0417fc\nCreate Date: 2024-12-13 22:43:20.119990\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/699221885109_nullify_default_task_prompt.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/699221885109_nullify_default_task_prompt.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"699221885109\"\ndown_revision = \"7e490836d179\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/699221885109_nullify_default_task_prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/699221885109_nullify_default_task_prompt.py",
      "line_number": 1,
      "code_snippet": "\"\"\"nullify_default_task_prompt\n\nRevision ID: 699221885109\nRevises: 7e490836d179\nCreate Date: 2025-12-30 10:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7a7eee5aa15_add_checkpointing_failure_handling.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7a7eee5aa15_add_checkpointing_failure_handling.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b7a7eee5aa15\"\ndown_revision = \"f39c5794c10a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7a7eee5aa15_add_checkpointing_failure_handling.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b7a7eee5aa15_add_checkpointing_failure_handling.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add checkpointing/failure handling\n\nRevision ID: b7a7eee5aa15\nRevises: f39c5794c10a\nCreate Date: 2025-01-24 15:17:36.763172\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7aea705850d5_added_slack_auto_filter.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7aea705850d5_added_slack_auto_filter.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"7aea705850d5\"\ndown_revision = \"4505fd7302e1\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d387b3196c2_basic_auth.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d387b3196c2_basic_auth.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"6d387b3196c2\"\ndown_revision = \"47433d30de82\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/aeda5f2df4f6_add_pinned_assistants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/aeda5f2df4f6_add_pinned_assistants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add pinned assistants\n\nRevision ID: aeda5f2df4f6\nRevises: c5eae4a75a1b\nCreate Date: 2025-01-09 16:04:10.770636\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"8e1ac4f39a9f\"\ndown_revision = \"9aadf32dfeb4\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"8e1ac4f39a9f\"\ndown_revision = \"9aadf32dfeb4\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8e1ac4f39a9f_enable_contextual_retrieval.py",
      "line_number": 1,
      "code_snippet": "\"\"\"enable contextual retrieval\n\nRevision ID: 8e1ac4f39a9f\nRevises: 9aadf32dfeb4\nCreate Date: 2024-12-20 13:29:09.918661\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3fc5d75723b3_add_doc_metadata_field_in_document_model.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3fc5d75723b3_add_doc_metadata_field_in_document_model.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_doc_metadata_field_in_document_model\n\nRevision ID: 3fc5d75723b3\nRevises: 2f95e36923e6\nCreate Date: 2025-07-28 18:45:37.985406\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9c00a2bccb83_chat_message_agentic.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9c00a2bccb83_chat_message_agentic.py",
      "line_number": 1,
      "code_snippet": "\"\"\"chat_message_agentic\n\nRevision ID: 9c00a2bccb83\nRevises: b7a7eee5aa15\nCreate Date: 2025-02-17 11:15:43.081150\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b75d0a8ffcb_user_file_schema_cleanup.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b75d0a8ffcb_user_file_schema_cleanup.py",
      "line_number": 20,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"2b75d0a8ffcb\"\ndown_revision = \"3a78dba1080a\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b75d0a8ffcb_user_file_schema_cleanup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2b75d0a8ffcb_user_file_schema_cleanup.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migration 6: User file schema cleanup\n\nRevision ID: 2b75d0a8ffcb\nRevises: 3a78dba1080a\nCreate Date: 2025-09-22 10:09:26.375377\n\nThis migration removes legacy columns and tables after data migration is complete.\nIt should only be run after verifying all data has been successfully migrated.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"6756efa39ada\"\ndown_revision = \"5d12a446f5c0\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"6756efa39ada\"\ndown_revision = \"5d12a446f5c0\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6756efa39ada_id_uuid_for_chat_session.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migrate chat_session and chat_message tables to use UUID primary keys\n\nRevision ID: 6756efa39ada\nRevises: 5d12a446f5c0\nCreate Date: 2024-10-15 17:47:44.108537\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/401c1ac29467_add_tables_for_ui_based_llm_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/401c1ac29467_add_tables_for_ui_based_llm_.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"401c1ac29467\"\ndown_revision = \"703313b75876\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a1b2c3d4e5f6\"\ndown_revision = \"a01bf2971c5d\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"a1b2c3d4e5f6\"\ndown_revision = \"a01bf2971c5d\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a1b2c3d4e5f6_add_license_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add license table\n\nRevision ID: a1b2c3d4e5f6\nRevises: a01bf2971c5d\nCreate Date: 2025-12-04 10:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47a07e1a38f1_fix_invalid_model_configurations_state.py_21",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47a07e1a38f1_fix_invalid_model_configurations_state.py",
      "line_number": 21,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"47a07e1a38f1\"\ndown_revision = \"7a70b7664e37\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47a07e1a38f1_fix_invalid_model_configurations_state.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47a07e1a38f1_fix_invalid_model_configurations_state.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Fix invalid model-configurations state\n\nRevision ID: 47a07e1a38f1\nRevises: 7a70b7664e37\nCreate Date: 2025-04-23 15:39:43.159504\n\n\"\"\"\n\nfrom alembic import op\nfrom pydantic import BaseModel, ConfigDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3934b1bc7b62\"\ndown_revision = \"b7c2b63c4a03\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3934b1bc7b62\"\ndown_revision = \"b7c2b63c4a03\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3934b1bc7b62_update_github_connector_repo_name_to_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update GitHub connector repo_name to repositories\n\nRevision ID: 3934b1bc7b62\nRevises: b7c2b63c4a03\nCreate Date: 2025-03-05 10:50:30.516962\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"325975216eb3\"\ndown_revision = \"91ffac7e65b3\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"325975216eb3\"\ndown_revision = \"91ffac7e65b3\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/325975216eb3_add_icon_color_and_icon_shape_to_persona.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add icon_color and icon_shape to Persona\n\nRevision ID: 325975216eb3\nRevises: 91ffac7e65b3\nCreate Date: 2024-07-24 21:29:31.784562\n\n\"\"\"\n\nimport random\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"87c52ec39f84\"\ndown_revision = \"7bd55f264e1b\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"87c52ec39f84\"\ndown_revision = \"7bd55f264e1b\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/87c52ec39f84_update_default_system_prompt.py",
      "line_number": 1,
      "code_snippet": "\"\"\"update_default_system_prompt\n\nRevision ID: 87c52ec39f84\nRevises: 7bd55f264e1b\nCreate Date: 2025-12-05 15:54:06.002452\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8a87bd6ec550_associate_index_attempts_with_ccpair.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8a87bd6ec550_associate_index_attempts_with_ccpair.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"8a87bd6ec550\"\ndown_revision = \"4ea2c93919c1\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b8206b29c5d_add_user_delete_cascades.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b8206b29c5d_add_user_delete_cascades.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"1b8206b29c5d\"\ndown_revision = \"35e6853a51d5\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b8206b29c5d_add_user_delete_cascades.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b8206b29c5d_add_user_delete_cascades.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_user_delete_cascades\n\nRevision ID: 1b8206b29c5d\nRevises: 35e6853a51d5\nCreate Date: 2024-09-18 11:48:59.418726\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9f696734098f_combine_search_and_chat.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9f696734098f_combine_search_and_chat.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9f696734098f\"\ndown_revision = \"a8c2065484e6\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9f696734098f_combine_search_and_chat.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9f696734098f_combine_search_and_chat.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Combine Search and Chat\n\nRevision ID: 9f696734098f\nRevises: a8c2065484e6\nCreate Date: 2024-11-27 15:32:19.694972\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33ea50e88f24_foreign_key_input_prompts.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33ea50e88f24_foreign_key_input_prompts.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"33ea50e88f24\"\ndown_revision = \"a6df6b88ef81\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33ea50e88f24_foreign_key_input_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33ea50e88f24_foreign_key_input_prompts.py",
      "line_number": 1,
      "code_snippet": "\"\"\"foreign key input prompts\n\nRevision ID: 33ea50e88f24\nRevises: a6df6b88ef81\nCreate Date: 2025-01-29 10:54:22.141765\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2acdef638fc2_add_switchover_type_field.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2acdef638fc2_add_switchover_type_field.py",
      "line_number": 16,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"2acdef638fc2\"\ndown_revision = \"a4f23d6b71c8\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2acdef638fc2_add_switchover_type_field.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2acdef638fc2_add_switchover_type_field.py",
      "line_number": 17,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"2acdef638fc2\"\ndown_revision = \"a4f23d6b71c8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da42808081e3_migrate_jira_connectors_to_new_format.py_18",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 18. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da42808081e3_migrate_jira_connectors_to_new_format.py",
      "line_number": 18,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"da42808081e3\"\ndown_revision = \"f13db29f3101\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da42808081e3_migrate_jira_connectors_to_new_format.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/da42808081e3_migrate_jira_connectors_to_new_format.py",
      "line_number": 1,
      "code_snippet": "\"\"\"migrate jira connectors to new format\n\nRevision ID: da42808081e3\nRevises: f13db29f3101\nCreate Date: 2025-02-24 11:24:54.396040\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f5437cc136c5_delete_non_search_assistants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f5437cc136c5_delete_non_search_assistants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"delete non-search assistants\n\nRevision ID: f5437cc136c5\nRevises: eaa3b5593925\nCreate Date: 2025-02-04 16:17:15.677256\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5f4b8568a221_add_removed_documents_to_index_attempt.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5f4b8568a221_add_removed_documents_to_index_attempt.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5f4b8568a221\"\ndown_revision = \"8987770549c0\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/09995b8811eb_add_theme_preference_to_user.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/09995b8811eb_add_theme_preference_to_user.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"09995b8811eb\"\ndown_revision = \"3d1cca026fe8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/09995b8811eb_add_theme_preference_to_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/09995b8811eb_add_theme_preference_to_user.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add theme_preference to user\n\nRevision ID: 09995b8811eb\nRevises: 3d1cca026fe8\nCreate Date: 2025-10-24 08:58:50.246949\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abbfec3a5ac5_merge_prompt_into_persona.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abbfec3a5ac5_merge_prompt_into_persona.py",
      "line_number": 1,
      "code_snippet": "\"\"\"merge prompt into persona\n\nRevision ID: abbfec3a5ac5\nRevises: 8818cf73fa1a\nCreate Date: 2024-12-19 12:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/61ff3651add4_add_permission_syncing.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/61ff3651add4_add_permission_syncing.py",
      "line_number": 17,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"61ff3651add4\"\ndown_revision = \"1b8206b29c5d\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/61ff3651add4_add_permission_syncing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/61ff3651add4_add_permission_syncing.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add Permission Syncing\n\nRevision ID: 61ff3651add4\nRevises: 1b8206b29c5d\nCreate Date: 2024-09-05 13:57:11.770413\n\n\"\"\"\n\nimport fastapi_users_db_sqlalchemy\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/173cae5bba26_port_config_store.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/173cae5bba26_port_config_store.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"173cae5bba26\"\ndown_revision = \"e50154680a5c\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/173cae5bba26_port_config_store.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/173cae5bba26_port_config_store.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"173cae5bba26\"\ndown_revision = \"e50154680a5c\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3795dce87be_migration_confluence_to_be_explicit.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3795dce87be_migration_confluence_to_be_explicit.py",
      "line_number": 14,
      "code_snippet": "from sqlalchemy.sql import table, column\n\nrevision = \"a3795dce87be\"\ndown_revision = \"1f60f60c3401\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3795dce87be_migration_confluence_to_be_explicit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a3795dce87be_migration_confluence_to_be_explicit.py",
      "line_number": 1,
      "code_snippet": "\"\"\"migration confluence to be explicit\n\nRevision ID: a3795dce87be\nRevises: 1f60f60c3401\nCreate Date: 2024-09-01 13:52:12.006740\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d7111c1238cd_remove_document_ids.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d7111c1238cd_remove_document_ids.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"d7111c1238cd\"\ndown_revision = \"465f78d9b7f9\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/72bdc9929a46_permission_auto_sync_framework.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/72bdc9929a46_permission_auto_sync_framework.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"72bdc9929a46\"\ndown_revision = \"475fcefe8826\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/72bdc9929a46_permission_auto_sync_framework.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/72bdc9929a46_permission_auto_sync_framework.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"72bdc9929a46\"\ndown_revision = \"475fcefe8826\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d25168c2beee_tool_name_consistency.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d25168c2beee_tool_name_consistency.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d25168c2beee\"\ndown_revision = \"8405ca81cc83\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d25168c2beee_tool_name_consistency.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d25168c2beee_tool_name_consistency.py",
      "line_number": 1,
      "code_snippet": "\"\"\"tool_name_consistency\n\nRevision ID: d25168c2beee\nRevises: 8405ca81cc83\nCreate Date: 2026-01-11 17:54:40.135777\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f13db29f3101_add_composite_index_for_last_modified_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f13db29f3101_add_composite_index_for_last_modified_.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f13db29f3101\"\ndown_revision = \"acaab4ef4507\"\nbranch_labels: str | None = None\ndepends_on: str | None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e4334d5b33ba_add_deployment_name_to_llmprovider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e4334d5b33ba_add_deployment_name_to_llmprovider.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_deployment_name_to_llmprovider\n\nRevision ID: e4334d5b33ba\nRevises: ac5eaac849f9\nCreate Date: 2024-10-04 09:52:34.896867\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4505fd7302e1_added_is_internet_to_dbdoc.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4505fd7302e1_added_is_internet_to_dbdoc.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"4505fd7302e1\"\ndown_revision = \"c18cdf4b497e\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4505fd7302e1_added_is_internet_to_dbdoc.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4505fd7302e1_added_is_internet_to_dbdoc.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"4505fd7302e1\"\ndown_revision = \"c18cdf4b497e\"\n\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b4ef3ae0bf6e_add_user_oauth_token_to_slack_bot.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b4ef3ae0bf6e_add_user_oauth_token_to_slack_bot.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_user_oauth_token_to_slack_bot\n\nRevision ID: b4ef3ae0bf6e\nRevises: 505c488f6662\nCreate Date: 2025-08-26 17:47:41.788462\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b30353be4eec_add_mcp_auth_performer.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b30353be4eec_add_mcp_auth_performer.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b30353be4eec\"\ndown_revision = \"2b75d0a8ffcb\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b30353be4eec_add_mcp_auth_performer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b30353be4eec_add_mcp_auth_performer.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_mcp_auth_performer\n\nRevision ID: b30353be4eec\nRevises: 2b75d0a8ffcb\nCreate Date: 2025-09-13 14:58:08.413534\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9d97fecfab7f_added_retrieved_docs_to_query_event.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9d97fecfab7f_added_retrieved_docs_to_query_event.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9d97fecfab7f\"\ndown_revision = \"ffc707a226b4\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91fd3b470d1a_remove_documentsource_from_tag.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91fd3b470d1a_remove_documentsource_from_tag.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"91fd3b470d1a\"\ndown_revision = \"173cae5bba26\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91fd3b470d1a_remove_documentsource_from_tag.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91fd3b470d1a_remove_documentsource_from_tag.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"91fd3b470d1a\"\ndown_revision = \"173cae5bba26\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/27c6ecc08586_permission_framework.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/27c6ecc08586_permission_framework.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Permission Framework\n\nRevision ID: 27c6ecc08586\nRevises: 2666d766cb9b\nCreate Date: 2023-05-24 18:45:17.244495\n\n\"\"\"\n\nimport fastapi_users_db_sqlalchemy\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/904e5138fffb_tags.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/904e5138fffb_tags.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"904e5138fffb\"\ndown_revision = \"891cd83c87a8\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b10e1fda030_add_additional_data_to_notifications.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b10e1fda030_add_additional_data_to_notifications.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"1b10e1fda030\"\ndown_revision = \"6756efa39ada\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b10e1fda030_add_additional_data_to_notifications.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1b10e1fda030_add_additional_data_to_notifications.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add additional data to notifications\n\nRevision ID: 1b10e1fda030\nRevises: 6756efa39ada\nCreate Date: 2024-10-15 19:26:44.071259\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8aabb57f3b49_restructure_document_indices.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8aabb57f3b49_restructure_document_indices.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"8aabb57f3b49\"\ndown_revision = \"5e84129c8be3\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8aabb57f3b49_restructure_document_indices.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8aabb57f3b49_restructure_document_indices.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"8aabb57f3b49\"\ndown_revision = \"5e84129c8be3\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6a804aeb4830_duplicated_no_harm_user_file_migration.py_11",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 11. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6a804aeb4830_duplicated_no_harm_user_file_migration.py",
      "line_number": 11,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"6a804aeb4830\"\ndown_revision = \"8e1ac4f39a9f\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6a804aeb4830_duplicated_no_harm_user_file_migration.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6a804aeb4830_duplicated_no_harm_user_file_migration.py",
      "line_number": 1,
      "code_snippet": "\"\"\"duplicated no-harm user file migration\n\nRevision ID: 6a804aeb4830\nRevises: 8e1ac4f39a9f\nCreate Date: 2025-04-01 07:26:10.539362\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = \"6a804aeb4830\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9b66d3156fc6_user_file_schema_additions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9b66d3156fc6_user_file_schema_additions.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migration 1: User file schema additions\n\nRevision ID: 9b66d3156fc6\nRevises: b4ef3ae0bf6e\nCreate Date: 2025-09-22 09:42:06.086732\n\nThis migration adds new columns and tables without modifying existing data.\nIt is safe to run and can be easily rolled back.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bceb1e139447_add_base_url_to_cloudembeddingprovider.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bceb1e139447_add_base_url_to_cloudembeddingprovider.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"bceb1e139447\"\ndown_revision = \"a3795dce87be\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/23957775e5f5_remove_feedback_foreignkey_constraint.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/23957775e5f5_remove_feedback_foreignkey_constraint.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove-feedback-foreignkey-constraint\n\nRevision ID: 23957775e5f5\nRevises: bc9771dccadf\nCreate Date: 2024-06-27 16:04:51.480437\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/027381bce97c_add_shortcut_option_for_users.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/027381bce97c_add_shortcut_option_for_users.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"027381bce97c\"\ndown_revision = \"6fc7886d665d\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/027381bce97c_add_shortcut_option_for_users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/027381bce97c_add_shortcut_option_for_users.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add shortcut option for users\n\nRevision ID: 027381bce97c\nRevises: 6fc7886d665d\nCreate Date: 2025-01-14 12:14:00.814390\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2955778aa44c_add_chunk_count_to_document.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2955778aa44c_add_chunk_count_to_document.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add chunk count to document\n\nRevision ID: 2955778aa44c\nRevises: c0aab6edb6dd\nCreate Date: 2025-01-04 11:39:43.268612\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/62c3a055a141_add_file_names_to_file_connector_config.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/62c3a055a141_add_file_names_to_file_connector_config.py",
      "line_number": 17,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"62c3a055a141\"\ndown_revision = \"3fc5d75723b3\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/62c3a055a141_add_file_names_to_file_connector_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/62c3a055a141_add_file_names_to_file_connector_config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add file names to file connector config\n\nRevision ID: 62c3a055a141\nRevises: 3fc5d75723b3\nCreate Date: 2025-07-30 17:01:24.417551\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/43cbbb3f5e6a_rename_index_origin_to_index_recursively.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/43cbbb3f5e6a_rename_index_origin_to_index_recursively.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"1d6ad76d1f37\"\ndown_revision = \"e1392f05e840\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6436661d5b65_add_created_at_in_project_userfile.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6436661d5b65_add_created_at_in_project_userfile.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"6436661d5b65\"\ndown_revision = \"c7e9f4a3b2d1\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6436661d5b65_add_created_at_in_project_userfile.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6436661d5b65_add_created_at_in_project_userfile.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_created_at_in_project_userfile\n\nRevision ID: 6436661d5b65\nRevises: c7e9f4a3b2d1\nCreate Date: 2025-11-24 11:50:24.536052\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3bd4c84fe72f\"\ndown_revision = \"8f43500ee275\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3bd4c84fe72f\"\ndown_revision = \"8f43500ee275\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3bd4c84fe72f_improved_index.py",
      "line_number": 1,
      "code_snippet": "\"\"\"improved index\n\nRevision ID: 3bd4c84fe72f\nRevises: 8f43500ee275\nCreate Date: 2025-02-26 13:07:56.217791\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py_261",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 261 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py",
      "line_number": 261,
      "code_snippet": "        # print(f\"Successfully updated chunk_stats table for document {old_doc_id} -> {new_doc_id}\")\n        # Update chunk_stats ID field which includes document_id\n        bind.execute(\n            sa.text(\n                \"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py_502",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 502 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py",
      "line_number": 502,
      "code_snippet": "        except Exception as e:\n            logger.warning(\n                f\"Some KG/chunk tables may not exist or failed to delete from: {e}\"\n            )\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/12635f6655b7_drive_canonical_ids.py",
      "line_number": 1,
      "code_snippet": "\"\"\"drive-canonical-ids\n\nRevision ID: 12635f6655b7\nRevises: 58c50ef19f08\nCreate Date: 2025-06-20 14:44:54.241159\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py",
      "line_number": 12,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"93560ba1b118\"\ndown_revision = \"6d562f86c78b\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"93560ba1b118\"\ndown_revision = \"6d562f86c78b\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/93560ba1b118_add_web_ui_option_to_slack_config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add web ui option to slack config\n\nRevision ID: 93560ba1b118\nRevises: 6d562f86c78b\nCreate Date: 2024-11-24 06:36:17.490612\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91ffac7e65b3_add_expiry_time.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/91ffac7e65b3_add_expiry_time.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"91ffac7e65b3\"\ndown_revision = \"795b20b85b4b\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a98909f2757_enable_encrypted_fields.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a98909f2757_enable_encrypted_fields.py",
      "line_number": 19,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"0a98909f2757\"\ndown_revision = \"570282d33c49\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a98909f2757_enable_encrypted_fields.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0a98909f2757_enable_encrypted_fields.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Enable Encrypted Fields\n\nRevision ID: 0a98909f2757\nRevises: 570282d33c49\nCreate Date: 2024-05-05 19:30:34.317972\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"c7bf5721733e\"\ndown_revision = \"027381bce97c\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c7bf5721733e\"\ndown_revision = \"027381bce97c\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c7bf5721733e_add_has_been_indexed_to_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add has_been_indexed to DocumentByConnectorCredentialPair\n\nRevision ID: c7bf5721733e\nRevises: fec3db967bf7\nCreate Date: 2025-01-13 12:39:05.831693\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd7c3bf8beba_migrate_agent_responses_to_research_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd7c3bf8beba_migrate_agent_responses_to_research_.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"bd7c3bf8beba\"\ndown_revision = \"f8a9b2c3d4e5\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd7c3bf8beba_migrate_agent_responses_to_research_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bd7c3bf8beba_migrate_agent_responses_to_research_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"migrate_agent_sub_questions_to_research_iterations\n\nRevision ID: bd7c3bf8beba\nRevises: f8a9b2c3d4e5\nCreate Date: 2025-08-18 11:33:27.098287\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c9e2cd766c29_add_s3_file_store_table.py_23",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 23. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c9e2cd766c29_add_s3_file_store_table.py",
      "line_number": 23,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c9e2cd766c29\"\ndown_revision = \"03bf8be6b53a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c9e2cd766c29_add_s3_file_store_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c9e2cd766c29_add_s3_file_store_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"modify_file_store_for_external_storage\n\nRevision ID: c9e2cd766c29\nRevises: 03bf8be6b53a\nCreate Date: 2025-06-13 14:02:09.867679\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/76b60d407dfb_cc_pair_name_not_unique.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/76b60d407dfb_cc_pair_name_not_unique.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"76b60d407dfb\"\ndown_revision = \"b156fa702355\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_30",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 30. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 30,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"90e3b9af7da4\"\ndown_revision = \"62c3a055a141\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_31",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 31. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 31,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"90e3b9af7da4\"\ndown_revision = \"62c3a055a141\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_67",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 67 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 67,
      "code_snippet": "    bind = op.get_bind()\n    for source, key in LIST_METADATA:\n        bind.execute(\n            sa.text(\n                f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_162",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 162 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 162,
      "code_snippet": "            # delete old document__tags\n            bind = op.get_bind()\n            result = bind.execute(\n                sa.text(\n                    f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_231",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 231 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 231,
      "code_snippet": "\n    while True:\n        batch = bind.execute(\n            sa.text(\n                f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_282",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 282 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 282,
      "code_snippet": "def _get_document_tags(document_id: str) -> list[tuple[int, str, str]]:\n    bind = op.get_bind()\n    result = bind.execute(\n        sa.text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_69",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 69 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 69,
      "code_snippet": "        bind.execute(\n            sa.text(\n                f\"\"\"\n                UPDATE tag\n                SET is_list = true",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_164",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 164 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 164,
      "code_snippet": "            result = bind.execute(\n                sa.text(\n                    f\"\"\"\n                    DELETE FROM document__tag\n                    WHERE document_id = '{document_id}'",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_233",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 233 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 233,
      "code_snippet": "        batch = bind.execute(\n            sa.text(\n                f\"\"\"\n                SELECT DISTINCT document__tag.document_id\n                FROM tag",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_284",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 284 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 284,
      "code_snippet": "    result = bind.execute(\n        sa.text(\n            f\"\"\"\n            SELECT tag.id, tag.tag_key, tag.tag_value\n            FROM tag",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/90e3b9af7da4_tag_fix.py",
      "line_number": 1,
      "code_snippet": "\"\"\"tag-fix\n\nRevision ID: 90e3b9af7da4\nRevises: 62c3a055a141\nCreate Date: 2025-08-01 20:58:14.607624\n\n\"\"\"\n\nimport json\nimport logging",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c5b692fa265c_add_index_attempt_errors_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c5b692fa265c_add_index_attempt_errors_table.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c5b692fa265c\"\ndown_revision = \"4a951134c801\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5b29123cd710_nullable_search_settings_for_historic_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5b29123cd710_nullable_search_settings_for_historic_.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5b29123cd710\"\ndown_revision = \"949b4a92a401\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5b29123cd710_nullable_search_settings_for_historic_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5b29123cd710_nullable_search_settings_for_historic_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"nullable search settings for historic index attempts\n\nRevision ID: 5b29123cd710\nRevises: 949b4a92a401\nCreate Date: 2024-10-30 19:37:59.630704\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abe7378b8217_add_indexing_trigger_to_cc_pair.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abe7378b8217_add_indexing_trigger_to_cc_pair.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"abe7378b8217\"\ndown_revision = \"93560ba1b118\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abe7378b8217_add_indexing_trigger_to_cc_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/abe7378b8217_add_indexing_trigger_to_cc_pair.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add indexing trigger to cc_pair\n\nRevision ID: abe7378b8217\nRevises: 6d562f86c78b\nCreate Date: 2024-11-26 19:09:53.481171\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b388730a2899_nullable_preferences.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b388730a2899_nullable_preferences.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b388730a2899\"\ndown_revision = \"1a03d2c2856b\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b388730a2899_nullable_preferences.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b388730a2899_nullable_preferences.py",
      "line_number": 1,
      "code_snippet": "\"\"\"nullable preferences\n\nRevision ID: b388730a2899\nRevises: 1a03d2c2856b\nCreate Date: 2025-02-17 18:49:22.643902\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/78dbe7e38469_task_tracking.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/78dbe7e38469_task_tracking.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"78dbe7e38469\"\ndown_revision = \"7ccea01261f6\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/78dbe7e38469_task_tracking.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/78dbe7e38469_task_tracking.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"78dbe7e38469\"\ndown_revision = \"7ccea01261f6\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4794bc13e484_update_prompt_length.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4794bc13e484_update_prompt_length.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"4794bc13e484\"\ndown_revision = \"f7505c5b0284\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4794bc13e484_update_prompt_length.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/4794bc13e484_update_prompt_length.py",
      "line_number": 1,
      "code_snippet": "\"\"\"update prompt length\n\nRevision ID: 4794bc13e484\nRevises: f7505c5b0284\nCreate Date: 2025-04-02 11:26:36.180328\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9drpiiw74ljy_add_config_to_federated_connector.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9drpiiw74ljy_add_config_to_federated_connector.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9drpiiw74ljy\"\ndown_revision = \"2acdef638fc2\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9drpiiw74ljy_add_config_to_federated_connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9drpiiw74ljy_add_config_to_federated_connector.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add config to federated_connector\n\nRevision ID: 9drpiiw74ljy\nRevises: 2acdef638fc2\nCreate Date: 2025-11-03 12:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ac5eaac849f9_add_last_pruned_to_connector_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ac5eaac849f9_add_last_pruned_to_connector_table.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ac5eaac849f9\"\ndown_revision = \"46b7a812670f\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ac5eaac849f9_add_last_pruned_to_connector_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ac5eaac849f9_add_last_pruned_to_connector_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add last_pruned to the connector_credential_pair table\n\nRevision ID: ac5eaac849f9\nRevises: 52a219fb5233\nCreate Date: 2024-09-10 15:04:26.437118\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f1ca58b2f2ec_add_passthrough_auth_to_tool.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f1ca58b2f2ec_add_passthrough_auth_to_tool.py",
      "line_number": 16,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"f1ca58b2f2ec\"\ndown_revision: Union[str, None] = \"c7bf5721733e\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/55546a7967ee_assistant_rework.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/55546a7967ee_assistant_rework.py",
      "line_number": 1,
      "code_snippet": "\"\"\"assistant_rework\n\nRevision ID: 55546a7967ee\nRevises: 61ff3651add4\nCreate Date: 2024-09-18 17:00:23.755399\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ccea01261f6_store_chat_retrieval_docs.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ccea01261f6_store_chat_retrieval_docs.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7ccea01261f6\"\ndown_revision = \"a570b80a5f20\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/767f1c2a00eb_count_chat_tokens.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/767f1c2a00eb_count_chat_tokens.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"767f1c2a00eb\"\ndown_revision = \"dba7f71618f5\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/767f1c2a00eb_count_chat_tokens.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/767f1c2a00eb_count_chat_tokens.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"767f1c2a00eb\"\ndown_revision = \"dba7f71618f5\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e50154680a5c_no_source_enum.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e50154680a5c_no_source_enum.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"e50154680a5c\"\ndown_revision = \"fcd135795f21\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e50154680a5c_no_source_enum.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e50154680a5c_no_source_enum.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"e50154680a5c\"\ndown_revision = \"fcd135795f21\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9cf5c00f72fe_add_creator_to_cc_pair.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9cf5c00f72fe_add_creator_to_cc_pair.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"9cf5c00f72fe\"\ndown_revision = \"26b931506ecb\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9cf5c00f72fe_add_creator_to_cc_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/9cf5c00f72fe_add_creator_to_cc_pair.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add creator to cc pair\n\nRevision ID: 9cf5c00f72fe\nRevises: 26b931506ecb\nCreate Date: 2024-11-12 15:16:42.682902\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c8a93a2af083_personalization_user_info.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c8a93a2af083_personalization_user_info.py",
      "line_number": 1,
      "code_snippet": "\"\"\"personalization_user_info\n\nRevision ID: c8a93a2af083\nRevises: 6f4f86aef280\nCreate Date: 2025-10-14 15:59:03.577343\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6fc7886d665d_make_categories_labels_and_many_to_many.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6fc7886d665d_make_categories_labels_and_many_to_many.py",
      "line_number": 1,
      "code_snippet": "\"\"\"make categories labels and many to many\n\nRevision ID: 6fc7886d665d\nRevises: 3c6531f32351\nCreate Date: 2025-01-13 18:12:18.029112\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"fec3db967bf7\"\ndown_revision = \"97dbb53fa8c8\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"fec3db967bf7\"\ndown_revision = \"97dbb53fa8c8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/fec3db967bf7_add_time_updated_to_usergroup_and_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add time_updated to UserGroup and DocumentSet\n\nRevision ID: fec3db967bf7\nRevises: 97dbb53fa8c8\nCreate Date: 2025-01-12 15:49:02.289100\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e84129c8be3_add_docs_indexed_column_to_index_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e84129c8be3_add_docs_indexed_column_to_index_.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5e84129c8be3\"\ndown_revision = \"e6a4bbc13fe4\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e84129c8be3_add_docs_indexed_column_to_index_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e84129c8be3_add_docs_indexed_column_to_index_.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5e84129c8be3\"\ndown_revision = \"e6a4bbc13fe4\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d562f86c78b_remove_default_bot.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d562f86c78b_remove_default_bot.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"6d562f86c78b\"\ndown_revision = \"177de57c21c9\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d562f86c78b_remove_default_bot.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/6d562f86c78b_remove_default_bot.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove default bot\n\nRevision ID: 6d562f86c78b\nRevises: 177de57c21c9\nCreate Date: 2024-11-22 11:51:29.331336\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cec7ec36c505_kgentity_parent.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cec7ec36c505_kgentity_parent.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"cec7ec36c505\"\ndown_revision = \"495cb26ce93e\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cec7ec36c505_kgentity_parent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/cec7ec36c505_kgentity_parent.py",
      "line_number": 1,
      "code_snippet": "\"\"\"kgentity_parent\n\nRevision ID: cec7ec36c505\nRevises: 495cb26ce93e\nCreate Date: 2025-06-07 20:07:46.400770\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c18cdf4b497e_add_standard_answer_tables.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c18cdf4b497e_add_standard_answer_tables.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"c18cdf4b497e\"\ndown_revision = \"3a7802814195\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c18cdf4b497e_add_standard_answer_tables.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c18cdf4b497e_add_standard_answer_tables.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"c18cdf4b497e\"\ndown_revision = \"3a7802814195\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/38eda64af7fe_add_chat_session_sharing.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/38eda64af7fe_add_chat_session_sharing.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"38eda64af7fe\"\ndown_revision = \"776b3bbe9092\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/97dbb53fa8c8_add_syncrecord.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/97dbb53fa8c8_add_syncrecord.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"97dbb53fa8c8\"\ndown_revision = \"be2ab2aa50ee\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/97dbb53fa8c8_add_syncrecord.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/97dbb53fa8c8_add_syncrecord.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add SyncRecord\n\nRevision ID: 97dbb53fa8c8\nRevises: 369644546676\nCreate Date: 2025-01-11 19:39:50.426302\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"bf7a81109301\"\ndown_revision = \"f7a894b06d02\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"bf7a81109301\"\ndown_revision = \"f7a894b06d02\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/bf7a81109301_delete_input_prompts.py",
      "line_number": 1,
      "code_snippet": "\"\"\"delete_input_prompts\n\nRevision ID: bf7a81109301\nRevises: f7a894b06d02\nCreate Date: 2024-12-09 12:00:49.884228\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_22",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 22. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 22,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"495cb26ce93e\"\ndown_revision = \"ca04500b9ee8\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_23",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 23. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 23,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"495cb26ce93e\"\ndown_revision = \"ca04500b9ee8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_47",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 47 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 47,
      "code_snippet": "            raise Exception(\"DB_READONLY_USER or DB_READONLY_PASSWORD is not set\")\n\n        op.execute(\n            text(\n                f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_69",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 69 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 69,
      "code_snippet": "\n    # Grant usage on current schema to readonly user\n    op.execute(\n        text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_479",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 479 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 479,
      "code_snippet": "\n    # Create GIN index for clustering and normalization\n    op.execute(\n        \"CREATE INDEX IF NOT EXISTS idx_kg_entity_clustering_trigrams \"\n        f\"ON kg_entity USING GIN (name {POSTGRES_DEFAULT_SCHEMA}.gin_trgm_ops)\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_492",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 492 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 492,
      "code_snippet": "    truncate_length = 1000\n    function = \"update_kg_entity_name\"\n    op.execute(\n        text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_542",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 542 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 542,
      "code_snippet": "    # Create kg_entity trigger to update kg_entity.name and its trigrams\n    function = \"update_kg_entity_name_from_doc\"\n    op.execute(\n        text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_634",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 634 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 634,
      "code_snippet": "        (\"document\", \"update_kg_entity_name_from_doc\"),\n    ):\n        op.execute(f\"DROP TRIGGER IF EXISTS {function}_trigger ON {table}\")\n        op.execute(f\"DROP FUNCTION IF EXISTS {function}()\")\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_657",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 657 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 657,
      "code_snippet": "\n    # Revoke usage on current schema for the readonly user\n    op.execute(\n        text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_675",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 675 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 675,
      "code_snippet": "        # the user is dropped in the alembic_tenants migration.\n\n        op.execute(\n            text(\n                f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_49",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 49 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 49,
      "code_snippet": "        op.execute(\n            text(\n                f\"\"\"\n                DO $$\n                BEGIN",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_71",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 71 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 71,
      "code_snippet": "    op.execute(\n        text(\n            f\"\"\"\n            DO $$\n            BEGIN",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_494",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 494 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 494,
      "code_snippet": "    op.execute(\n        text(\n            f\"\"\"\n            CREATE OR REPLACE FUNCTION {function}()\n            RETURNS TRIGGER AS $$",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_544",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 544 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 544,
      "code_snippet": "    op.execute(\n        text(\n            f\"\"\"\n            CREATE OR REPLACE FUNCTION {function}()\n            RETURNS TRIGGER AS $$",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_659",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 659 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 659,
      "code_snippet": "    op.execute(\n        text(\n            f\"\"\"\n            DO $$\n            BEGIN",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_677",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 677 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 677,
      "code_snippet": "        op.execute(\n            text(\n                f\"\"\"\n            DO $$\n            BEGIN",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/495cb26ce93e_create_knowlege_graph_tables.py",
      "line_number": 1,
      "code_snippet": "\"\"\"create knowledge graph tables\n\nRevision ID: 495cb26ce93e\nRevises: ca04500b9ee8\nCreate Date: 2025-03-19 08:51:14.341989\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/465f78d9b7f9_larger_access_tokens_for_oauth.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/465f78d9b7f9_larger_access_tokens_for_oauth.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"465f78d9b7f9\"\ndown_revision = \"3c5e35aa9af0\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"94dc3d0236f8\"\ndown_revision = \"bf7a81109301\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"94dc3d0236f8\"\ndown_revision = \"bf7a81109301\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/94dc3d0236f8_make_document_set_description_optional.py",
      "line_number": 1,
      "code_snippet": "\"\"\"make document set description optional\n\nRevision ID: 94dc3d0236f8\nRevises: bf7a81109301\nCreate Date: 2024-12-11 11:26:10.616722\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/797089dfb4d2_persona_start_date.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/797089dfb4d2_persona_start_date.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"797089dfb4d2\"\ndown_revision = \"55546a7967ee\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/797089dfb4d2_persona_start_date.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/797089dfb4d2_persona_start_date.py",
      "line_number": 1,
      "code_snippet": "\"\"\"persona_start_date\n\nRevision ID: 797089dfb4d2\nRevises: 55546a7967ee\nCreate Date: 2024-09-11 14:51:49.785835\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b72ed7a5db0e_remove_description_from_starter_messages.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b72ed7a5db0e_remove_description_from_starter_messages.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"b72ed7a5db0e\"\ndown_revision = \"33cb72ea4d80\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b72ed7a5db0e_remove_description_from_starter_messages.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/b72ed7a5db0e_remove_description_from_starter_messages.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove description from starter messages\n\nRevision ID: b72ed7a5db0e\nRevises: 33cb72ea4d80\nCreate Date: 2024-11-03 15:55:28.944408\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7ed603b64d5a\"\ndown_revision = \"b329d00a9ea6\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7ed603b64d5a\"\ndown_revision = \"b329d00a9ea6\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7ed603b64d5a_add_mcp_server_and_connection_config_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_mcp_server_and_connection_config_models\n\nRevision ID: 7ed603b64d5a\nRevises: b329d00a9ea6\nCreate Date: 2025-07-28 17:35:59.900680\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0fd6e4da83a_add_recent_assistants.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0fd6e4da83a_add_recent_assistants.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"c0fd6e4da83a\"\ndown_revision = \"b72ed7a5db0e\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0fd6e4da83a_add_recent_assistants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/c0fd6e4da83a_add_recent_assistants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add recent assistants\n\nRevision ID: c0fd6e4da83a\nRevises: b72ed7a5db0e\nCreate Date: 2024-11-03 17:28:54.916618\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py",
      "line_number": 12,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"46b7a812670f\"\ndown_revision = \"f32615f71aeb\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"46b7a812670f\"\ndown_revision = \"f32615f71aeb\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/46b7a812670f_fix_user__external_user_group_id_fk.py",
      "line_number": 1,
      "code_snippet": "\"\"\"fix_user__external_user_group_id_fk\n\nRevision ID: 46b7a812670f\nRevises: f32615f71aeb\nCreate Date: 2024-09-23 12:58:03.894038\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ae62505e3acc_add_saml_accounts.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ae62505e3acc_add_saml_accounts.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ae62505e3acc\"\ndown_revision = \"7da543f5672f\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d929f0c1c6af_feedback_feature.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d929f0c1c6af_feedback_feature.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d929f0c1c6af\"\ndown_revision = \"8aabb57f3b49\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d929f0c1c6af_feedback_feature.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d929f0c1c6af_feedback_feature.py",
      "line_number": 16,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"d929f0c1c6af\"\ndown_revision = \"8aabb57f3b49\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0f7ff6d75b57_add_index_to_index_attempt_time_created.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0f7ff6d75b57_add_index_to_index_attempt_time_created.py",
      "line_number": 13,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"0f7ff6d75b57\"\ndown_revision = \"fec3db967bf7\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"33cb72ea4d80\"\ndown_revision = \"5b29123cd710\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"33cb72ea4d80\"\ndown_revision = \"5b29123cd710\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/33cb72ea4d80_single_tool_call_per_message.py",
      "line_number": 1,
      "code_snippet": "\"\"\"single tool call per message\n\nRevision ID: 33cb72ea4d80\nRevises: 5b29123cd710\nCreate Date: 2024-11-01 12:51:01.535003\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5e6f7a8b9c0d\"\ndown_revision = \"4f8a2b3c1d9e\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5e6f7a8b9c0d\"\ndown_revision = \"4f8a2b3c1d9e\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5e6f7a8b9c0d_update_default_persona_prompt.py",
      "line_number": 1,
      "code_snippet": "\"\"\"update_default_persona_prompt\n\nRevision ID: 5e6f7a8b9c0d\nRevises: 4f8a2b3c1d9e\nCreate Date: 2025-11-30 12:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3879338f8ba1_add_tool_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3879338f8ba1_add_tool_table.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3879338f8ba1\"\ndown_revision = \"f1c6478c3fd8\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d5645c915d0e_remove_deletion_attempt_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d5645c915d0e_remove_deletion_attempt_table.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d5645c915d0e\"\ndown_revision = \"8e26726b7683\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3b25685ff73c_move_is_public_to_cc_pair.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3b25685ff73c_move_is_public_to_cc_pair.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3b25685ff73c\"\ndown_revision = \"e0a68a81d434\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3b25685ff73c_move_is_public_to_cc_pair.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3b25685ff73c_move_is_public_to_cc_pair.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"3b25685ff73c\"\ndown_revision = \"e0a68a81d434\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/177de57c21c9_display_custom_llm_models.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/177de57c21c9_display_custom_llm_models.py",
      "line_number": 15,
      "code_snippet": "\nrevision = \"177de57c21c9\"\ndown_revision = \"4ee1287bd26a\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/177de57c21c9_display_custom_llm_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/177de57c21c9_display_custom_llm_models.py",
      "line_number": 1,
      "code_snippet": "\"\"\"display custom llm models\n\nRevision ID: 177de57c21c9\nRevises: 4ee1287bd26a\nCreate Date: 2024-11-21 11:49:04.488677\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a6df6b88ef81_remove_recent_assistants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a6df6b88ef81_remove_recent_assistants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"remove recent assistants\n\nRevision ID: a6df6b88ef81\nRevises: 4d58345da04a\nCreate Date: 2025-01-29 10:25:52.790407\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1a03d2c2856b_add_indexes_to_document__tag.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/1a03d2c2856b_add_indexes_to_document__tag.py",
      "line_number": 12,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"1a03d2c2856b\"\ndown_revision = \"9c00a2bccb83\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8405ca81cc83_notifications_constraint.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8405ca81cc83_notifications_constraint.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"8405ca81cc83\"\ndown_revision = \"a3c1a7904cd0\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8405ca81cc83_notifications_constraint.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/8405ca81cc83_notifications_constraint.py",
      "line_number": 1,
      "code_snippet": "\"\"\"notifications constraint, sort index, and cleanup old notifications\n\nRevision ID: 8405ca81cc83\nRevises: a3c1a7904cd0\nCreate Date: 2026-01-07 16:43:44.855156\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0568ccf46a6b_add_thread_specific_model_selection.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0568ccf46a6b_add_thread_specific_model_selection.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"0568ccf46a6b\"\ndown_revision = \"e209dc5a8156\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0568ccf46a6b_add_thread_specific_model_selection.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/0568ccf46a6b_add_thread_specific_model_selection.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"0568ccf46a6b\"\ndown_revision = \"e209dc5a8156\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"e209dc5a8156\"\ndown_revision = \"48d14957fe80\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"e209dc5a8156\"\ndown_revision = \"48d14957fe80\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e209dc5a8156_added_prune_frequency.py",
      "line_number": 1,
      "code_snippet": "\"\"\"added-prune-frequency\n\nRevision ID: e209dc5a8156\nRevises: 48d14957fe80\nCreate Date: 2024-06-16 16:02:35.273231\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/baf71f781b9e_add_llm_model_version_override_to_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/baf71f781b9e_add_llm_model_version_override_to_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"baf71f781b9e\"\ndown_revision = \"50b683a8295c\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a7802814195_add_alternate_assistant_to_chat_message.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/3a7802814195_add_alternate_assistant_to_chat_message.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"3a7802814195\"\ndown_revision = \"23957775e5f5\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ef7da92f7213_add_files_to_chatmessage.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ef7da92f7213_add_files_to_chatmessage.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"ef7da92f7213\"\ndown_revision = \"401c1ac29467\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ef7da92f7213_add_files_to_chatmessage.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/ef7da92f7213_add_files_to_chatmessage.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"ef7da92f7213\"\ndown_revision = \"401c1ac29467\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7a894b06d02_non_nullbale_slack_bot_id_in_channel_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7a894b06d02_non_nullbale_slack_bot_id_in_channel_.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f7a894b06d02\"\ndown_revision = \"9f696734098f\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7a894b06d02_non_nullbale_slack_bot_id_in_channel_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f7a894b06d02_non_nullbale_slack_bot_id_in_channel_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"non-nullbale slack bot id in channel config\n\nRevision ID: f7a894b06d02\nRevises: 9f696734098f\nCreate Date: 2024-12-06 12:55:42.845723\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py_21",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py",
      "line_number": 21,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"03bf8be6b53a\"\ndown_revision = \"65bc6e0f8500\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py_67",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 67 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py",
      "line_number": 67,
      "code_snippet": "    )\n    op.execute(\n        f\"INSERT INTO key_value_store (key, value) VALUES ('kg_config', '{kg_config_settings}')\"\n    )\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/03bf8be6b53a_rework_kg_config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"rework-kg-config\n\nRevision ID: 03bf8be6b53a\nRevises: 65bc6e0f8500\nCreate Date: 2025-06-16 10:52:34.815335\n\n\"\"\"\n\nimport json\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"f8a9b2c3d4e5\"\ndown_revision = \"5ae8240accb3\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"f8a9b2c3d4e5\"\ndown_revision = \"5ae8240accb3\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/f8a9b2c3d4e5_add_research_answer_purpose_to_chat_message.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add research_answer_purpose to chat_message\n\nRevision ID: f8a9b2c3d4e5\nRevises: 5ae8240accb3\nCreate Date: 2025-01-27 12:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d09fc20a3c66_seed_builtin_tools.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d09fc20a3c66_seed_builtin_tools.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"d09fc20a3c66\"\ndown_revision = \"b7ec9b5b505f\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d09fc20a3c66_seed_builtin_tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/d09fc20a3c66_seed_builtin_tools.py",
      "line_number": 1,
      "code_snippet": "\"\"\"seed_builtin_tools\n\nRevision ID: d09fc20a3c66\nRevises: b7ec9b5b505f\nCreate Date: 2025-09-09 19:32:16.824373\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/80696cf850ae_add_chat_session_to_query_event.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/80696cf850ae_add_chat_session_to_query_event.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"80696cf850ae\"\ndown_revision = \"15326fcec57e\"\nbranch_labels: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/80696cf850ae_add_chat_session_to_query_event.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/80696cf850ae_add_chat_session_to_query_event.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"80696cf850ae\"\ndown_revision = \"15326fcec57e\"\nbranch_labels: None = None\ndepends_on: None = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2cdeff6d8c93_set_built_in_to_default.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2cdeff6d8c93_set_built_in_to_default.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"2cdeff6d8c93\"\ndown_revision = \"f5437cc136c5\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2cdeff6d8c93_set_built_in_to_default.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/2cdeff6d8c93_set_built_in_to_default.py",
      "line_number": 1,
      "code_snippet": "\"\"\"set built in to default\n\nRevision ID: 2cdeff6d8c93\nRevises: f5437cc136c5\nCreate Date: 2025-02-11 14:57:51.308775\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"36e9220ab794\"\ndown_revision = \"c9e2cd766c29\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py_40",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 40 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
      "line_number": 40,
      "code_snippet": "    truncate_length = 1000\n    function = \"update_kg_entity_name\"\n    op.execute(\n        text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py_90",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 90 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
      "line_number": 90,
      "code_snippet": "    # Create kg_entity trigger to update kg_entity.name and its trigrams\n    function = \"update_kg_entity_name_from_doc\"\n    op.execute(\n        text(\n            f\"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py_42",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 42 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
      "line_number": 42,
      "code_snippet": "    op.execute(\n        text(\n            f\"\"\"\n            CREATE OR REPLACE FUNCTION \"{tenant_id}\".{function}()\n            RETURNS TRIGGER AS $$",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py_92",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 92 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
      "line_number": 92,
      "code_snippet": "    op.execute(\n        text(\n            f\"\"\"\n            CREATE OR REPLACE FUNCTION \"{tenant_id}\".{function}()\n            RETURNS TRIGGER AS $$",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/36e9220ab794_update_kg_trigger_functions.py",
      "line_number": 1,
      "code_snippet": "\"\"\"update_kg_trigger_functions\n\nRevision ID: 36e9220ab794\nRevises: c9e2cd766c29\nCreate Date: 2025-06-22 17:33:25.833733\n\n\"\"\"\n\nfrom alembic import op\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"5c7fdadae813\"\ndown_revision = \"efb35676026c\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"5c7fdadae813\"\ndown_revision = \"efb35676026c\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/5c7fdadae813_match_any_keywords_flag_for_standard_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"match_any_keywords flag for standard answers\n\nRevision ID: 5c7fdadae813\nRevises: efb35676026c\nCreate Date: 2024-09-13 18:52:59.256478\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py",
      "line_number": 13,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"7bd55f264e1b\"\ndown_revision = \"e8f0d2a38171\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py",
      "line_number": 14,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"7bd55f264e1b\"\ndown_revision = \"e8f0d2a38171\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/7bd55f264e1b_add_display_name_to_model_configuration.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add display_name to model_configuration\n\nRevision ID: 7bd55f264e1b\nRevises: e8f0d2a38171\nCreate Date: 2025-12-04\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py",
      "line_number": 19,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"e8f0d2a38171\"\ndown_revision = \"ed9e44312505\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py",
      "line_number": 20,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = \"e8f0d2a38171\"\ndown_revision = \"ed9e44312505\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/e8f0d2a38171_add_status_to_mcp_server_and_make_auth_.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add status to mcp server and make auth fields nullable\n\nRevision ID: e8f0d2a38171\nRevises: ed9e44312505\nCreate Date: 2025-11-28 11:15:37.667340\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47e5bef3a1d7_add_persona_categories.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47e5bef3a1d7_add_persona_categories.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"47e5bef3a1d7\"\ndown_revision = \"dfbe9e93d3c7\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47e5bef3a1d7_add_persona_categories.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/47e5bef3a1d7_add_persona_categories.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add persona categories\n\nRevision ID: 47e5bef3a1d7\nRevises: dfbe9e93d3c7\nCreate Date: 2024-11-05 18:55:02.221064\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a01bf2971c5d_update_default_tool_descriptions.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a01bf2971c5d_update_default_tool_descriptions.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision = \"a01bf2971c5d\"\ndown_revision = \"18b5b2524446\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a01bf2971c5d_update_default_tool_descriptions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/a01bf2971c5d_update_default_tool_descriptions.py",
      "line_number": 1,
      "code_snippet": "\"\"\"update_default_tool_descriptions\n\nRevision ID: a01bf2971c5d\nRevises: 87c52ec39f84\nCreate Date: 2025-12-16 15:21:25.656375\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e6853a51d5_server_default_chosen_assistants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/alembic/versions/35e6853a51d5_server_default_chosen_assistants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"server default chosen assistants\n\nRevision ID: 35e6853a51d5\nRevises: c99d76fcd298\nCreate Date: 2024-09-13 13:20:32.885317\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/query_time_check/seed_dummy_docs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/query_time_check/seed_dummy_docs.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nlaunch:\n- api server\n- postgres\n- vespa\n- model server (this is only needed so the api server can startup, no embedding is done)\n\nRun this script to seed the database with dummy documents.\nThen run test_query_times.py to test query times.\n\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_54",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 54 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 54,
      "code_snippet": "\n    subprocess.run(cmd_cp, check=True, capture_output=True)\n\n    # Execute script on pod",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_63",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 63 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 63,
      "code_snippet": "\n    result = subprocess.run(cmd_exec, capture_output=True, text=True, check=True)\n\n    # Show progress messages from stderr",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_142",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 142 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 142,
      "code_snippet": "\n    subprocess.run(cmd_write, check=True, capture_output=True)\n\n    # Execute the script on the pod",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_121",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'execute(' on line 121 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 121,
      "code_snippet": "with engine.connect() as conn:\n    result = conn.execute(\n        text(\n            \"SELECT tenant_id, stripe_customer_id, created_at, active_seats, \"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_151",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 151 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 151,
      "code_snippet": "\n    result = subprocess.run(cmd_exec, capture_output=True, text=True, check=True)\n\n    # Parse JSON output",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_28_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'collect_tenant_data'",
      "description": "Function 'collect_tenant_data' on line 28 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 28,
      "code_snippet": "from scripts.tenant_cleanup.no_bastion_cleanup_utils import find_background_pod\nfrom scripts.tenant_cleanup.no_bastion_cleanup_utils import find_worker_pod\n\n\ndef collect_tenant_data(\n    pod_name: str, context: str | None = None\n) -> list[dict[str, Any]]:\n    \"\"\"Run the understand_tenants script on the data plane pod.\"\"\"\n    print(f\"\\nCollecting tenant data from data plane pod {pod_name}...\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_80_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'collect_control_plane_data_from_pod'",
      "description": "Function 'collect_control_plane_data_from_pod' on line 80 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 80,
      "code_snippet": "        print(f\"stdout: {result.stdout[:500]}\", file=sys.stderr)\n        raise\n\n\ndef collect_control_plane_data_from_pod(\n    pod_name: str, context: str | None = None\n) -> list[dict[str, Any]]:\n    \"\"\"Collect control plane data by running a query on a control plane pod.\"\"\"\n    print(f\"\\nCollecting control plane data from pod {pod_name}...\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_28_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'collect_tenant_data'",
      "description": "Function 'collect_tenant_data' on line 28 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 28,
      "code_snippet": "\n\ndef collect_tenant_data(\n    pod_name: str, context: str | None = None\n) -> list[dict[str, Any]]:\n    \"\"\"Run the understand_tenants script on the data plane pod.\"\"\"",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_80_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'collect_control_plane_data_from_pod'",
      "description": "Function 'collect_control_plane_data_from_pod' on line 80 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 80,
      "code_snippet": "\n\ndef collect_control_plane_data_from_pod(\n    pod_name: str, context: str | None = None\n) -> list[dict[str, Any]]:\n    \"\"\"Collect control plane data by running a query on a control plane pod.\"\"\"",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_63",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 63,
      "code_snippet": "        cmd_exec.extend([\"--context\", context])\n    cmd_exec.extend([\"--\", \"python\", \"/tmp/understand_tenants.py\"])\n\n    result = subprocess.run(cmd_exec, capture_output=True, text=True, check=True)\n\n    # Show progress messages from stderr\n    if result.stderr:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py_142",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_analyze_tenants.py",
      "line_number": 142,
      "code_snippet": "        [\"--\", \"bash\", \"-c\", f\"cat > {script_path} << 'EOF'\\n{query_script}\\nEOF\"]\n    )\n\n    subprocess.run(cmd_write, check=True, capture_output=True)\n\n    # Execute the script on the pod\n    print(\"  Executing control plane query on pod...\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py_94",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on line 94 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
      "line_number": 94,
      "code_snippet": "        if result.returncode != 0:\n            raise RuntimeError(result.stderr)\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py_42_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_connector_deletion'",
      "description": "Function 'run_connector_deletion' on line 42 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
      "line_number": 42,
      "code_snippet": "    with _print_lock:\n        print(*args, **kwargs)\n\n\ndef run_connector_deletion(pod_name: str, tenant_id: str, context: str) -> None:\n    \"\"\"Mark all connector credential pairs for deletion.\n\n    Args:\n        pod_name: Data plane pod to execute deletion on\n        tenant_id: Tenant ID to process",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py_42_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_connector_deletion'",
      "description": "Function 'run_connector_deletion' on line 42 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
      "line_number": 42,
      "code_snippet": "\n\ndef run_connector_deletion(pod_name: str, tenant_id: str, context: str) -> None:\n    \"\"\"Mark all connector credential pairs for deletion.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_mark_connectors.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nMark connectors for deletion script that works WITHOUT bastion access.\nAll queries run directly from pods.\nSupports two-cluster architecture (data plane and control plane in separate clusters).\n\nUsage:\n    PYTHONPATH=. python scripts/tenant_cleanup/no_bastion_mark_connectors.py <tenant_id> \\\n        --data-plane-context <context> --control-plane-context <context> [--force]\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_121",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input 'query' flows to LLM call via f-string in variable 'full_cmd'. Function 'execute_control_plane_query' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 121,
      "code_snippet": "    # Build the SSH command with proper escaping\n    full_cmd = (\n        f\"ssh -i {pem_file_location} ec2-user@{bastion_host} \"\n        f'\"psql {db_url} {psql_flags} -c \\\\\"{query}\\\\\"\"'\n    )\n\n    result = subprocess.run(\n        full_cmd,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_27",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 27,
      "code_snippet": "\n    result = subprocess.run(\n        [\"kubectl\", \"get\", \"po\"], capture_output=True, text=True, check=True\n    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_126",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 126 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 126,
      "code_snippet": "\n    result = subprocess.run(\n        full_cmd,\n        shell=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_107",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 107 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 107,
      "code_snippet": "    Returns:\n        subprocess.CompletedProcess with the result\n\n    Raises:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_97",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'execute_control_plane_query' on line 97 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 97,
      "code_snippet": "def execute_control_plane_query(\n    query: str, tuple_only: bool = False\n) -> subprocess.CompletedProcess:\n    \"\"\"Execute a SQL query against the control plane database via SSH.\n\n    Args:\n        query: The SQL query to execute\n        tuple_only: If True, use psql's tuple-only mode (-t flag) for cleaner output\n\n    Returns:\n        subprocess.CompletedProcess with the result",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_23_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'find_worker_pod'",
      "description": "Function 'find_worker_pod' on line 23 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 23,
      "code_snippet": "    bastion_host: str\n    pem_file_location: str\n\n\ndef find_worker_pod() -> str:\n    \"\"\"Find a user file processing worker pod using kubectl.\"\"\"\n    print(\"Finding user file processing worker pod...\")\n\n    result = subprocess.run(\n        [\"kubectl\", \"get\", \"po\"], capture_output=True, text=True, check=True",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_97_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'execute_control_plane_query'",
      "description": "Function 'execute_control_plane_query' on line 97 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 97,
      "code_snippet": "        pem_file_location=pem_file_location,\n    )\n\n\ndef execute_control_plane_query(\n    query: str, tuple_only: bool = False\n) -> subprocess.CompletedProcess:\n    \"\"\"Execute a SQL query against the control plane database via SSH.\n\n    Args:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_23_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'find_worker_pod'",
      "description": "Function 'find_worker_pod' on line 23 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 23,
      "code_snippet": "\n\ndef find_worker_pod() -> str:\n    \"\"\"Find a user file processing worker pod using kubectl.\"\"\"\n    print(\"Finding user file processing worker pod...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_97_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'execute_control_plane_query'",
      "description": "Function 'execute_control_plane_query' on line 97 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 97,
      "code_snippet": "\n\ndef execute_control_plane_query(\n    query: str, tuple_only: bool = False\n) -> subprocess.CompletedProcess:\n    \"\"\"Execute a SQL query against the control plane database via SSH.",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_149",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 149 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 149,
      "code_snippet": "    print(f\"Fetching tenant status for tenant: {tenant_id}\")\n\n    query = f\"SELECT application_status FROM tenant WHERE tenant_id = '{tenant_id}';\"\n\n    try:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_27",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 27,
      "code_snippet": "    \"\"\"Find a user file processing worker pod using kubectl.\"\"\"\n    print(\"Finding user file processing worker pod...\")\n\n    result = subprocess.run(\n        [\"kubectl\", \"get\", \"po\"], capture_output=True, text=True, check=True\n    )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_126",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 126,
      "code_snippet": "        f'\"psql {db_url} {psql_flags} -c \\\\\"{query}\\\\\"\"'\n    )\n\n    result = subprocess.run(\n        full_cmd,\n        shell=True,\n        check=True,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py_97",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_utils.py",
      "line_number": 97,
      "code_snippet": "        bastion_host=bastion_host,\n        pem_file_location=pem_file_location,\n    )\n\n\ndef execute_control_plane_query(\n    query: str, tuple_only: bool = False\n) -> subprocess.CompletedProcess:\n    \"\"\"Execute a SQL query against the control plane database via SSH.\n\n    Args:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_149",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'message' flows to 'RuntimeError' on line 149 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 149,
      "code_snippet": "            message = result_data.get(\"message\", \"Unknown error\")\n            raise RuntimeError(f\"Failed to get index name: {message}\")\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_274",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'error_details' flows to 'RuntimeError' on line 274 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 274,
      "code_snippet": "                error_details += f\"\\n  ConnectorCredentialPairs: {cc_count}\\n  Documents: {doc_count}\"\n            raise RuntimeError(error_details)\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_335",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'message' flows to 'RuntimeError' on line 335 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 335,
      "code_snippet": "            print(f\"\u2717 {message}\", file=sys.stderr)\n            raise RuntimeError(message)\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_47",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'setup_scripts_on_pod' on line 47 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 47,
      "code_snippet": "def setup_scripts_on_pod(pod_name: str, context: str) -> None:\n    \"\"\"Copy all required scripts to the pod once at the beginning.\n\n    Args:\n        pod_name: Pod to copy scripts to\n        context: kubectl context for the cluster\n    \"\"\"\n    print(\"Setting up scripts on pod (one-time operation)...\")\n\n    script_dir = Path(__file__).parent\n    scripts_to_copy = [",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_47_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'setup_scripts_on_pod'",
      "description": "Function 'setup_scripts_on_pod' on line 47 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 47,
      "code_snippet": "    \"\"\"Handle termination signals by killing active subprocess.\"\"\"\n    sys.exit(1)\n\n\ndef setup_scripts_on_pod(pod_name: str, context: str) -> None:\n    \"\"\"Copy all required scripts to the pod once at the beginning.\n\n    Args:\n        pod_name: Pod to copy scripts to\n        context: kubectl context for the cluster",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_80_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_tenant_index_name'",
      "description": "Function 'get_tenant_index_name' on line 80 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 80,
      "code_snippet": "\n    print(\"\u2713 All scripts copied to pod\")\n\n\ndef get_tenant_index_name(pod_name: str, tenant_id: str, context: str) -> str:\n    \"\"\"Get the default index name for the given tenant by running script on pod.\n\n    Args:\n        pod_name: Data plane pod to execute on\n        tenant_id: Tenant ID to process",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_165_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_tenant_users'",
      "description": "Function 'get_tenant_users' on line 165 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 165,
      "code_snippet": "        )\n        raise\n\n\ndef get_tenant_users(pod_name: str, tenant_id: str, context: str) -> list[str]:\n    \"\"\"Get list of user emails from the tenant's data plane schema.\n\n    Args:\n        pod_name: Data plane pod to execute on\n        tenant_id: Tenant ID to process",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_225_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_documents_deleted'",
      "description": "Function 'check_documents_deleted' on line 225 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 225,
      "code_snippet": "        print(f\"\u26a0 Failed to get users for tenant {tenant_id}: {e}\")\n        return []\n\n\ndef check_documents_deleted(pod_name: str, tenant_id: str, context: str) -> None:\n    \"\"\"Check if all documents and connector credential pairs have been deleted.\n\n    Args:\n        pod_name: Data plane pod to execute on\n        tenant_id: Tenant ID to process",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_292_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'drop_data_plane_schema'",
      "description": "Function 'drop_data_plane_schema' on line 292 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 292,
      "code_snippet": "        )\n        raise\n\n\ndef drop_data_plane_schema(pod_name: str, tenant_id: str, context: str) -> None:\n    \"\"\"Drop the PostgreSQL schema for the given tenant by running script on pod.\n\n    Args:\n        pod_name: Data plane pod to execute on\n        tenant_id: Tenant ID to process",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_47_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_scripts_on_pod'",
      "description": "Function 'setup_scripts_on_pod' on line 47 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 47,
      "code_snippet": "\n\ndef setup_scripts_on_pod(pod_name: str, context: str) -> None:\n    \"\"\"Copy all required scripts to the pod once at the beginning.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_80_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'get_tenant_index_name'",
      "description": "Function 'get_tenant_index_name' on line 80 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 80,
      "code_snippet": "\n\ndef get_tenant_index_name(pod_name: str, tenant_id: str, context: str) -> str:\n    \"\"\"Get the default index name for the given tenant by running script on pod.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_165_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'get_tenant_users'",
      "description": "Function 'get_tenant_users' on line 165 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 165,
      "code_snippet": "\n\ndef get_tenant_users(pod_name: str, tenant_id: str, context: str) -> list[str]:\n    \"\"\"Get list of user emails from the tenant's data plane schema.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_225_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_documents_deleted'",
      "description": "Function 'check_documents_deleted' on line 225 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 225,
      "code_snippet": "\n\ndef check_documents_deleted(pod_name: str, tenant_id: str, context: str) -> None:\n    \"\"\"Check if all documents and connector credential pairs have been deleted.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_292_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'drop_data_plane_schema'",
      "description": "Function 'drop_data_plane_schema' on line 292 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 292,
      "code_snippet": "\n\ndef drop_data_plane_schema(pod_name: str, tenant_id: str, context: str) -> None:\n    \"\"\"Drop the PostgreSQL schema for the given tenant by running script on pod.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_364",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 364 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 364,
      "code_snippet": "        (\n            \"tenant_notification\",\n            f\"DELETE FROM tenant_notification WHERE tenant_id = '{tenant_id}'\",\n        ),\n        (\"tenant_config\", f\"DELETE FROM tenant_config WHERE tenant_id = '{tenant_id}'\"),",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_366",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 366 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 366,
      "code_snippet": "            f\"DELETE FROM tenant_notification WHERE tenant_id = '{tenant_id}'\",\n        ),\n        (\"tenant_config\", f\"DELETE FROM tenant_config WHERE tenant_id = '{tenant_id}'\"),\n        (\"subscription\", f\"DELETE FROM subscription WHERE tenant_id = '{tenant_id}'\"),\n        (\"tenant\", f\"DELETE FROM tenant WHERE tenant_id = '{tenant_id}'\"),",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_368",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 368 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 368,
      "code_snippet": "        (\"tenant_config\", f\"DELETE FROM tenant_config WHERE tenant_id = '{tenant_id}'\"),\n        (\"subscription\", f\"DELETE FROM subscription WHERE tenant_id = '{tenant_id}'\"),\n        (\"tenant\", f\"DELETE FROM tenant WHERE tenant_id = '{tenant_id}'\"),\n    ]\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_375",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 375 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 375,
      "code_snippet": "            print(f\"  Deleting from {table_name}...\")\n\n            if not confirm_step(f\"Delete from {table_name}?\", force):\n                print(f\"  Skipping deletion from {table_name}\")\n                continue",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_tenants.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nTenant cleanup script that works WITHOUT bastion access.\nAll queries run directly from pods.\nSupports two-cluster architecture (data plane and control plane in separate clusters).\n\nUsage:\n    PYTHONPATH=. python scripts/tenant_cleanup/no_bastion_cleanup_tenants.py <tenant_id> \\\n        --data-plane-context <context> --control-plane-context <context> [--force]\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_27",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 27,
      "code_snippet": "\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n\n    # Parse output and find user file processing worker pod",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_56",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 56 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 56,
      "code_snippet": "\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n\n    # Parse output and find suitable pod",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_188",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 188 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 188,
      "code_snippet": "\n        subprocess.run(\n            cmd_write,\n            check=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_198",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 198 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 198,
      "code_snippet": "\n        result = subprocess.run(\n            cmd_exec,\n            capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_160",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'execute(' on line 160 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 160,
      "code_snippet": "with engine.connect() as conn:\n    result = conn.execute(text(\"\"\"{query}\"\"\"))\n\n    # Check if this is a SELECT query",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_105",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'execute_control_plane_query_from_pod' on line 105 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 105,
      "code_snippet": "def execute_control_plane_query_from_pod(\n    pod_name: str, query: str, context: str\n) -> dict:\n    \"\"\"Execute a SQL query against control plane database from within a pod.\n\n    Args:\n        pod_name: The Kubernetes pod name to execute from\n        query: The SQL query to execute\n        context: kubectl context for control plane cluster\n\n    Returns:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_17_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'find_worker_pod'",
      "description": "Function 'find_worker_pod' on line 17 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 17,
      "code_snippet": "class TenantNotFoundInControlPlaneError(Exception):\n    \"\"\"Exception raised when tenant/table is not found in control plane.\"\"\"\n\n\ndef find_worker_pod(context: str) -> str:\n    \"\"\"Find a user file processing worker pod using kubectl.\n\n    Args:\n        context: kubectl context to use\n    \"\"\"",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_46_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'find_background_pod'",
      "description": "Function 'find_background_pod' on line 46 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 46,
      "code_snippet": "\n    raise RuntimeError(\"No running user file processing worker pod found\")\n\n\ndef find_background_pod(context: str) -> str:\n    \"\"\"Find a pod for control plane operations.\n\n    Args:\n        context: kubectl context to use\n    \"\"\"",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_105_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'execute_control_plane_query_from_pod'",
      "description": "Function 'execute_control_plane_query_from_pod' on line 105 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 105,
      "code_snippet": "    response = input(\"Proceed? (y/n): \")\n    return response.lower() == \"y\"\n\n\ndef execute_control_plane_query_from_pod(\n    pod_name: str, query: str, context: str\n) -> dict:\n    \"\"\"Execute a SQL query against control plane database from within a pod.\n\n    Args:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_17_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'find_worker_pod'",
      "description": "Function 'find_worker_pod' on line 17 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 17,
      "code_snippet": "\n\ndef find_worker_pod(context: str) -> str:\n    \"\"\"Find a user file processing worker pod using kubectl.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_46_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'find_background_pod'",
      "description": "Function 'find_background_pod' on line 46 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 46,
      "code_snippet": "\n\ndef find_background_pod(context: str) -> str:\n    \"\"\"Find a pod for control plane operations.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_105_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'execute_control_plane_query_from_pod'",
      "description": "Function 'execute_control_plane_query_from_pod' on line 105 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 105,
      "code_snippet": "\n\ndef execute_control_plane_query_from_pod(\n    pod_name: str, query: str, context: str\n) -> dict:\n    \"\"\"Execute a SQL query against control plane database from within a pod.",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_236",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 236 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 236,
      "code_snippet": "    print(f\"Fetching tenant status for tenant: {tenant_id}\")\n\n    query = f\"SELECT application_status FROM tenant WHERE tenant_id = '{tenant_id}'\"\n\n    result = execute_control_plane_query_from_pod(pod_name, query, context)",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_27",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 27,
      "code_snippet": "\n    cmd = [\"kubectl\", \"get\", \"po\", \"--context\", context]\n\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n\n    # Parse output and find user file processing worker pod\n    lines = result.stdout.strip().split(\"\\n\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py_105",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/no_bastion_cleanup_utils.py",
      "line_number": 105,
      "code_snippet": "    print(f\"\\n{message}\")\n    response = input(\"Proceed? (y/n): \")\n    return response.lower() == \"y\"\n\n\ndef execute_control_plane_query_from_pod(\n    pod_name: str, query: str, context: str\n) -> dict:\n    \"\"\"Execute a SQL query against control plane database from within a pod.\n\n    Args:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_103",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'message' flows to 'RuntimeError' on line 103 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 103,
      "code_snippet": "            message = result_data.get(\"message\", \"Unknown error\")\n            raise RuntimeError(f\"Failed to get index name: {message}\")\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_269",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'error_details' flows to 'RuntimeError' on line 269 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 269,
      "code_snippet": "                error_details += f\"\\n  ConnectorCredentialPairs: {cc_count}\\n  Documents: {doc_count}\"\n            raise RuntimeError(error_details)\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_346",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'message' flows to 'RuntimeError' on line 346 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 346,
      "code_snippet": "            print(f\"\u2717 {message}\", file=sys.stderr)\n            raise RuntimeError(message)\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_45_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_tenant_index_name'",
      "description": "Function 'get_tenant_index_name' on line 45 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 45,
      "code_snippet": "    \"\"\"Handle termination signals by killing active subprocess.\"\"\"\n    sys.exit(1)\n\n\ndef get_tenant_index_name(pod_name: str, tenant_id: str) -> str:\n    \"\"\"Get the default index name for the given tenant by running script on pod.\"\"\"\n    print(f\"Getting default index name for tenant: {tenant_id}\")\n\n    # Get the path to the script\n    script_dir = Path(__file__).parent",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_119_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_tenant_users'",
      "description": "Function 'get_tenant_users' on line 119 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 119,
      "code_snippet": "        )\n        raise\n\n\ndef get_tenant_users(pod_name: str, tenant_id: str) -> list[str]:\n    \"\"\"Get list of user emails from the tenant's data plane schema.\n\n    Args:\n        pod_name: The Kubernetes pod name to execute on\n        tenant_id: The tenant ID to query",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_201_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_documents_deleted'",
      "description": "Function 'check_documents_deleted' on line 201 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 201,
      "code_snippet": "        print(f\"\u26a0 Failed to get users for tenant {tenant_id}: {e}\")\n        return []\n\n\ndef check_documents_deleted(pod_name: str, tenant_id: str) -> None:\n    \"\"\"Check if all documents and connector credential pairs have been deleted.\n\n    Raises RuntimeError if any ConnectorCredentialPairs or Documents remain.\n    \"\"\"\n    print(f\"Checking for remaining documents in tenant: {tenant_id}\")",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_287_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'drop_data_plane_schema'",
      "description": "Function 'drop_data_plane_schema' on line 287 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 287,
      "code_snippet": "        )\n        raise\n\n\ndef drop_data_plane_schema(pod_name: str, tenant_id: str) -> None:\n    \"\"\"Drop the PostgreSQL schema for the given tenant by running script on pod.\"\"\"\n    print(f\"Dropping data plane schema for tenant: {tenant_id}\")\n\n    # Get the path to the cleanup script\n    script_dir = Path(__file__).parent",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_45_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'get_tenant_index_name'",
      "description": "Function 'get_tenant_index_name' on line 45 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 45,
      "code_snippet": "\n\ndef get_tenant_index_name(pod_name: str, tenant_id: str) -> str:\n    \"\"\"Get the default index name for the given tenant by running script on pod.\"\"\"\n    print(f\"Getting default index name for tenant: {tenant_id}\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_119_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'get_tenant_users'",
      "description": "Function 'get_tenant_users' on line 119 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 119,
      "code_snippet": "\n\ndef get_tenant_users(pod_name: str, tenant_id: str) -> list[str]:\n    \"\"\"Get list of user emails from the tenant's data plane schema.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_201_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_documents_deleted'",
      "description": "Function 'check_documents_deleted' on line 201 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 201,
      "code_snippet": "\n\ndef check_documents_deleted(pod_name: str, tenant_id: str) -> None:\n    \"\"\"Check if all documents and connector credential pairs have been deleted.\n\n    Raises RuntimeError if any ConnectorCredentialPairs or Documents remain.",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_287_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'drop_data_plane_schema'",
      "description": "Function 'drop_data_plane_schema' on line 287 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 287,
      "code_snippet": "\n\ndef drop_data_plane_schema(pod_name: str, tenant_id: str) -> None:\n    \"\"\"Drop the PostgreSQL schema for the given tenant by running script on pod.\"\"\"\n    print(f\"Dropping data plane schema for tenant: {tenant_id}\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_386",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 386 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 386,
      "code_snippet": "            print(f\"  Deleting from {table_name}...\")\n\n            if not confirm_step(f\"Delete from {table_name}?\", force):\n                print(f\"  Skipping deletion from {table_name}\")\n                continue",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/cleanup_tenants.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nTenant cleanup script that:\n1. Deletes all documents from Vespa\n2. Drops the data plane PostgreSQL schema\n3. Clean up control plane (tenants, subscription table)\n\nUsage:\n    python backend/scripts/cleanup_tenant.py <tenant_id> [--force]\n    python backend/scripts/cleanup_tenant.py --csv <csv_file_path> [--force]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_39",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 39 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 39,
      "code_snippet": "    print(\"Copying script to pod...\")\n    subprocess.run(\n        [\n            \"kubectl\",",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_52",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 52 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 52,
      "code_snippet": "    print(\"Executing script on pod (this may take a while)...\")\n    result = subprocess.run(\n        [\"kubectl\", \"exec\", pod_name, \"--\", \"python\", \"/tmp/understand_tenants.py\"],\n        capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_102",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'shell=True' on line 102 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 102,
      "code_snippet": "\n    result = subprocess.run(\n        full_cmd,\n        shell=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_113",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 113,
      "code_snippet": "\n    copy_result = subprocess.run(\n        copy_cmd, shell=True, check=True, capture_output=True, text=True\n    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_86_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'rds_password' containing sensitive data is included in a prompt string on line 86. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 86,
      "code_snippet": "        raise ValueError(\"CONTROL_PLANE_RDS_PASSWORD is not set\")\n\n    db_url = f\"postgresql://postgres:{rds_password}@{rds_host}:5432/control\"\n\n    bastion_host = os.environ.get(\"BASTION_HOST\")",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_24_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'collect_tenant_data'",
      "description": "Function 'collect_tenant_data' on line 24 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 24,
      "code_snippet": "\nfrom scripts.tenant_cleanup.cleanup_utils import find_worker_pod\n\n\ndef collect_tenant_data(pod_name: str) -> list[dict[str, Any]]:\n    \"\"\"Run the understand_tenants script on the pod and return the data.\"\"\"\n    print(f\"\\nCollecting tenant data from pod {pod_name}...\")\n\n    # Get the path to the understand_tenants script\n    script_dir = Path(__file__).parent",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_74_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'collect_control_plane_data'",
      "description": "Function 'collect_control_plane_data' on line 74 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 74,
      "code_snippet": "        print(f\"stdout: {result.stdout[:500]}\", file=sys.stderr)\n        raise\n\n\ndef collect_control_plane_data() -> list[dict[str, Any]]:\n    \"\"\"Collect control plane data from the control plane database.\"\"\"\n    print(\"\\nCollecting control plane data...\")\n\n    rds_host = os.environ.get(\"CONTROL_PLANE_RDS_HOST\")\n    if not rds_host:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_24_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'collect_tenant_data'",
      "description": "Function 'collect_tenant_data' on line 24 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 24,
      "code_snippet": "\n\ndef collect_tenant_data(pod_name: str) -> list[dict[str, Any]]:\n    \"\"\"Run the understand_tenants script on the pod and return the data.\"\"\"\n    print(f\"\\nCollecting tenant data from pod {pod_name}...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_74_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'collect_control_plane_data'",
      "description": "Function 'collect_control_plane_data' on line 74 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 74,
      "code_snippet": "\n\ndef collect_control_plane_data() -> list[dict[str, Any]]:\n    \"\"\"Collect control plane data from the control plane database.\"\"\"\n    print(\"\\nCollecting control plane data...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py_113",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/analyze_current_tenants.py",
      "line_number": 113,
      "code_snippet": "    # Copy the CSV file from the bastion to local machine\n    copy_cmd = f\"scp -i {pem_file_location} ec2-user@{bastion_host}:/tmp/control_plane_data.csv .\"\n\n    copy_result = subprocess.run(\n        copy_cmd, shell=True, check=True, capture_output=True, text=True\n    )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_25",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 25 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 25,
      "code_snippet": "    try:\n        result = subprocess.run(\n            [\"kubectl\", \"version\", \"--client\", \"--short\"],\n            capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_36",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 36 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 36,
      "code_snippet": "            # Try to access cluster\n            result = subprocess.run(\n                [\"kubectl\", \"get\", \"ns\"],\n                capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_70",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 70 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 70,
      "code_snippet": "    try:\n        result = subprocess.run(\n            [\"kubectl\", \"get\", \"po\"],\n            capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_113",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 113,
      "code_snippet": "    try:\n        result = subprocess.run(\n            [\"kubectl\", \"exec\", pod_name, \"--\", \"echo\", \"test\"],\n            capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_147",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 147 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 147,
      "code_snippet": "        # Check for control plane DB env vars\n        result = subprocess.run(\n            [\"kubectl\", \"exec\", pod_name, \"--\", \"env\"],\n            capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_20_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_kubectl_access'",
      "description": "Function 'check_kubectl_access' on line 20 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 20,
      "code_snippet": "    print(f\"  {text}\")\n    print(f\"{'=' * 80}\\n\")\n\n\ndef check_kubectl_access() -> bool:\n    \"\"\"Check if kubectl is installed and can access the cluster.\"\"\"\n    print(\"Checking kubectl access...\")\n\n    try:\n        result = subprocess.run(",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_65_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_worker_pods'",
      "description": "Function 'check_worker_pods' on line 65 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 65,
      "code_snippet": "        print(f\"\u274c Error checking kubectl: {e}\")\n        return False\n\n\ndef check_worker_pods() -> tuple[bool, list[str]]:\n    \"\"\"Check if worker pods are running.\"\"\"\n    print(\"\\nChecking for worker pods...\")\n\n    try:\n        result = subprocess.run(",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_108_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_pod_exec_permission'",
      "description": "Function 'check_pod_exec_permission' on line 108 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 108,
      "code_snippet": "        print(f\"\u274c Error checking worker pods: {e}\")\n        return False, []\n\n\ndef check_pod_exec_permission(pod_name: str) -> bool:\n    \"\"\"Check if we can exec into a pod.\"\"\"\n    print(\"\\nChecking pod exec permissions...\")\n\n    try:\n        result = subprocess.run(",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_136_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_pod_db_access'",
      "description": "Function 'check_pod_db_access' on line 136 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 136,
      "code_snippet": "        print(f\"\u274c Error checking exec permission: {e}\")\n        return False\n\n\ndef check_pod_db_access(pod_name: str) -> dict:\n    \"\"\"Check if pod has database environment variables.\"\"\"\n    print(\"\\nChecking database access from pod...\")\n\n    checks = {\n        \"control_plane\": False,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_20_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_kubectl_access'",
      "description": "Function 'check_kubectl_access' on line 20 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 20,
      "code_snippet": "\n\ndef check_kubectl_access() -> bool:\n    \"\"\"Check if kubectl is installed and can access the cluster.\"\"\"\n    print(\"Checking kubectl access...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_65_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_worker_pods'",
      "description": "Function 'check_worker_pods' on line 65 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 65,
      "code_snippet": "\n\ndef check_worker_pods() -> tuple[bool, list[str]]:\n    \"\"\"Check if worker pods are running.\"\"\"\n    print(\"\\nChecking for worker pods...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_108_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_pod_exec_permission'",
      "description": "Function 'check_pod_exec_permission' on line 108 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 108,
      "code_snippet": "\n\ndef check_pod_exec_permission(pod_name: str) -> bool:\n    \"\"\"Check if we can exec into a pod.\"\"\"\n    print(\"\\nChecking pod exec permissions...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_136_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_pod_db_access'",
      "description": "Function 'check_pod_db_access' on line 136 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 136,
      "code_snippet": "\n\ndef check_pod_db_access(pod_name: str) -> dict:\n    \"\"\"Check if pod has database environment variables.\"\"\"\n    print(\"\\nChecking database access from pod...\")\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/check_no_bastion_setup.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nVerification script to check if your environment is ready for no-bastion tenant cleanup.\n\nUsage:\n    python scripts/tenant_cleanup/check_no_bastion_setup.py\n\"\"\"\n\nimport subprocess\nimport sys",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py_99",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on line 99 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
      "line_number": 99,
      "code_snippet": "        if result.returncode != 0:\n            raise RuntimeError(result.stderr)\n\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py_51_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_connector_deletion'",
      "description": "Function 'run_connector_deletion' on line 51 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
      "line_number": 51,
      "code_snippet": "    with _print_lock:\n        print(*args, **kwargs)\n\n\ndef run_connector_deletion(pod_name: str, tenant_id: str) -> None:\n    \"\"\"Mark all connector credential pairs for deletion.\n\n    Args:\n        pod_name: The Kubernetes pod name to execute on\n        tenant_id: The tenant ID",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py_51_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_connector_deletion'",
      "description": "Function 'run_connector_deletion' on line 51 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
      "line_number": 51,
      "code_snippet": "\n\ndef run_connector_deletion(pod_name: str, tenant_id: str) -> None:\n    \"\"\"Mark all connector credential pairs for deletion.\n\n    Args:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nMark connectors for deletion script that:\n1. Finds all connectors for the specified tenant(s)\n2. Cancels any scheduled indexing attempts\n3. Marks each connector credential pair as DELETING\n4. Triggers the cleanup task\n\nUsage:\n    python backend/scripts/tenant_cleanup/mark_connectors_for_deletion.py <tenant_id> [--force] [--concurrency N]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_vespa_schemas.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_vespa_schemas.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Tool to generate all supported schema variations for Onyx Cloud's Vespa database.\n\nUsage:\n\n```\nPYTHONPATH=. python scripts/debugging/onyx_vespa_schemas.py\n```\n\nThen, paste them into the existing vespa schema downloaded from the Vespa console,\nand then re-zip.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_db.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_db.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Onyx Database tool\"\"\"\n\nimport os\n\n# hack to work around excessive use of globals in other functions\nos.environ[\"MULTI_TENANT\"] = \"True\"\n\nif True:  # noqa: E402\n    import csv\n    import argparse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py_159_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'get_user_token' containing sensitive data is being logged on line 159. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py",
      "line_number": 159,
      "code_snippet": "    elif command == OnyxRedisCommand.get_user_token:\n        if not user_email:\n            logger.error(\"You must specify --user-email with get_user_token\")\n            return 1\n        token_key = get_user_token_from_redis(r, user_email)",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py_163_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token_key' containing sensitive data is being logged on line 163. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py",
      "line_number": 163,
      "code_snippet": "        token_key = get_user_token_from_redis(r, user_email)\n        if token_key:\n            print(f\"Token key for user {user_email}: {token_key}\")\n            return 0\n        else:",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py_170_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'delete_user_token' containing sensitive data is being logged on line 170. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py",
      "line_number": 170,
      "code_snippet": "    elif command == OnyxRedisCommand.delete_user_token:\n        if not user_email:\n            logger.error(\"You must specify --user-email with delete_user_token\")\n            return 1\n        if delete_user_token_from_redis(r, user_email, dry_run):",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_redis.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport logging\nimport sys\nimport time\nfrom enum import Enum\nfrom logging import getLogger\nfrom typing import cast\nfrom uuid import UUID\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_list_tenants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/onyx_list_tenants.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\n\"\"\"\nTenant List Script\nSimple script to list the tenant IDs in the database.\nUsed by the parallel migration script to determine how to split work.\n\nUsage:\n\n```",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/lib/logger.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/lib/logger.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport logging\nimport os\nimport sys\n\n# Detect CI environment\nIS_CI = os.getenv(\"CI\", \"\").lower() == \"true\"\nIS_DEBUG = os.getenv(\"DEBUG\", \"\").lower() == \"true\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/litellm/directly_hit_azure_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/debugging/litellm/directly_hit_azure_api.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nDirectly hit Azure OpenAI endpoints for debugging.\n\nThis script bypasses LiteLLM and directly calls Azure OpenAI APIs.\nUses URL and API key constants plus a payload.json in the same directory.\n\nUsage:\n    python directly_hit_azure_api.py\n\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_users.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to fetch user emails from a tenant's data plane schema.\nMust be run on a pod with access to the data plane PostgreSQL database.\n\nUsage:\n    python get_tenant_users.py <tenant_id>\n\nOutput:\n    JSON object with status and users list",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_index_name.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_index_name.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to get the default index name for a tenant.\nDesigned to be run on a heavy worker pod.\n\nUsage:\n    python get_tenant_index_name.py <tenant_id>\n\"\"\"\n\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/check_documents_deleted.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/check_documents_deleted.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to check for remaining ConnectorCredentialPairs and Documents in a tenant's schema.\nMust be run on a pod with access to the data plane PostgreSQL database.\n\nUsage:\n    python check_documents_deleted.py <tenant_id>\n\nOutput:\n    JSON object with status, message, and counts of remaining records",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/execute_connector_deletion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/execute_connector_deletion.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to mark connector credential pairs for deletion.\nRuns on a Kubernetes pod with access to the data plane database.\n\nUsage:\n    # Mark a specific connector for deletion\n    python mark_connector_for_deletion.py <tenant_id> <cc_pair_id>\n\n    # Mark all connectors for deletion",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/cleanup_tenant_schema.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/cleanup_tenant_schema.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to drop a tenant's PostgreSQL schema.\nDesigned to be run on a heavy worker pod.\n\nUsage:\n    python cleanup_tenant_schema.py <tenant_id>\n\"\"\"\n\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_connectors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/get_tenant_connectors.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to fetch connector credential pairs for a tenant.\nRuns on a Kubernetes pod with access to the data plane database.\n\nUsage:\n    python get_tenant_connectors.py <tenant_id>\n\nOutput:\n    JSON to stdout with structure:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/understand_tenants.py_45",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 45 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/understand_tenants.py",
      "line_number": 45,
      "code_snippet": "            # Use a single query to get all data at once\n            query = text(\n                f\"\"\"\n                SELECT\n                    :tenant_id AS tenant_id,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/understand_tenants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/scripts/tenant_cleanup/on_pod_scripts/understand_tenants.py",
      "line_number": 1,
      "code_snippet": "import json\nimport sys\nfrom typing import Any\n\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import ProgrammingError\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.engine.sql_engine import get_session_with_shared_schema\nfrom onyx.db.engine.sql_engine import SqlEngine",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/interfaces.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/interfaces.py",
      "line_number": 1,
      "code_snippet": "import abc\nfrom collections.abc import Generator\nfrom collections.abc import Iterator\nfrom types import TracebackType\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeAlias\nfrom typing import TypeVar\n\nfrom pydantic import BaseModel",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/models.py",
      "line_number": 1,
      "code_snippet": "import sys\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\nfrom typing import cast\n\nfrom pydantic import BaseModel\nfrom pydantic import model_validator\n\nfrom onyx.access.models import ExternalAccess",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/factory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/factory.py",
      "line_number": 1,
      "code_snippet": "import importlib\nfrom typing import Any\nfrom typing import Type\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import INTEGRATION_TESTS_MODE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.configs.llm_configs import get_image_extraction_and_analysis_enabled\nfrom onyx.connectors.credentials_provider import OnyxDBCredentialsProvider",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/credentials_provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/credentials_provider.py",
      "line_number": 1,
      "code_snippet": "import uuid\nfrom types import TracebackType\nfrom typing import Any\n\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy import select\n\nfrom onyx.connectors.interfaces import CredentialsProviderInterface\nfrom onyx.db.engine.sql_engine import get_session_with_tenant\nfrom onyx.db.models import Credential",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/connector_runner.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/connector_runner.py",
      "line_number": 1,
      "code_snippet": "import sys\nimport time\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom typing import Generic\nfrom typing import TypeVar\n\nfrom onyx.connectors.interfaces import BaseConnector\nfrom onyx.connectors.interfaces import CheckpointedConnector\nfrom onyx.connectors.interfaces import CheckpointedConnectorWithPermSync",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/langfuse_tracing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/langfuse_tracing.py",
      "line_number": 1,
      "code_snippet": "from onyx.configs.app_configs import LANGFUSE_PUBLIC_KEY\nfrom onyx.configs.app_configs import LANGFUSE_SECRET_KEY\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\n\ndef setup_langfuse_if_creds_available() -> None:\n    # Check if Langfuse credentials are available\n    if not LANGFUSE_SECRET_KEY or not LANGFUSE_PUBLIC_KEY:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/openinference_tracing_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/openinference_tracing_processor.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport logging\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom collections.abc import Mapping\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/braintrust_tracing_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/braintrust_tracing_processor.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Optional\n\nimport braintrust\nfrom braintrust import NOOP_SPAN\n\nfrom .framework.processor_interface import TracingProcessor\nfrom .framework.span_data import AgentSpanData",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/braintrust_tracing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/braintrust_tracing.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nfrom typing import Any\n\nfrom onyx.configs.app_configs import BRAINTRUST_API_KEY\nfrom onyx.configs.app_configs import BRAINTRUST_PROJECT\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/models.py_134_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 134. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/models.py",
      "line_number": 134,
      "code_snippet": "class EvalProvider(ABC):\n    @abstractmethod\n    def eval(\n        self,\n        task: Callable[[dict[str, Any]], EvalToolResult],\n        configuration: EvalConfigurationOptions,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py_115_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 115. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
      "line_number": 115,
      "code_snippet": "\n    if remote_dataset_name:\n        score = run_eval(\n            configuration=configuration,\n            remote_dataset_name=remote_dataset_name,\n            provider=provider,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py_126_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 126. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
      "line_number": 126,
      "code_snippet": "            )\n        data = load_data_local(local_data_path)\n        score = run_eval(configuration=configuration, data=data, provider=provider)\n\n    return score\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py_131_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'run_remote' executes dangerous operations",
      "description": "Tool function 'run_remote' on line 131 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
      "line_number": 131,
      "code_snippet": "\n    return score\n\n\ndef run_remote(\n    base_url: str,\n    api_key: str,\n    remote_dataset_name: str,\n    search_permissions_email: str,\n    payload: dict[str, Any] | None = None,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval_cli.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nCLI for running evaluations with local configurations.\n\"\"\"\n\nimport argparse\nimport json\nimport logging\nimport os\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py_618_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 618. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py",
      "line_number": 618,
      "code_snippet": "\n\ndef run_eval(\n    configuration: EvalConfigurationOptions,\n    data: list[dict[str, Any]] | None = None,\n    remote_dataset_name: str | None = None,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py_630_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 630. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py",
      "line_number": 630,
      "code_snippet": "        raise ValueError(\"Must specify either data or remote_dataset_name\")\n\n    return provider.eval(\n        task=lambda eval_input: _get_answer_with_tools(eval_input, configuration),\n        configuration=configuration,\n        data=data,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py_103_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'gather_stream_with_tools' executes dangerous operations",
      "description": "Tool function 'gather_stream_with_tools' on line 103 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py",
      "line_number": 103,
      "code_snippet": "    citations: list[CitationInfo] = []\n    timings: EvalTimings | None = None\n\n\ndef gather_stream_with_tools(packets: AnswerStream) -> GatherStreamResult:\n    \"\"\"\n    Gather streaming packets and extract both answer content and tool call information.\n\n    Returns a GatherStreamResult containing the answer and all tools that were called.\n    \"\"\"",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/eval.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom typing import Any\n\nfrom pydantic import BaseModel\nfrom sqlalchemy import Engine\nfrom sqlalchemy import event\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py_318_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 318. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py",
      "line_number": 318,
      "code_snippet": "\n    @abc.abstractmethod\n    def id_based_retrieval(\n        self,\n        chunk_requests: list[VespaChunkRequest],\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py_350_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 350. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py",
      "line_number": 350,
      "code_snippet": "\n    @abc.abstractmethod\n    def hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: Embedding,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py_407_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 407. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py",
      "line_number": 407,
      "code_snippet": "\n    @abc.abstractmethod\n    def admin_retrieval(\n        self,\n        query: str,\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py_433_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 433. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces.py",
      "line_number": 433,
      "code_snippet": "\n    @abc.abstractmethod\n    def random_retrieval(\n        self,\n        filters: IndexFilters,\n        num_to_retrieve: int = 10,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py_311_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 311. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py",
      "line_number": 311,
      "code_snippet": "\n    @abc.abstractmethod\n    def id_based_retrieval(\n        self,\n        chunk_requests: list[DocumentSectionRequest],\n        # TODO(andrei): Make this more strict w.r.t. acl, temporary for now.",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py_344_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 344. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py",
      "line_number": 344,
      "code_snippet": "\n    @abc.abstractmethod\n    def hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: Embedding,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py_386_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 386. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py",
      "line_number": 386,
      "code_snippet": "\n    @abc.abstractmethod\n    def random_retrieval(\n        self,\n        # TODO(andrei): Make this more strict w.r.t. acl, temporary for now.\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/interfaces_new.py",
      "line_number": 1,
      "code_snippet": "import abc\nfrom typing import Self\n\nfrom pydantic import BaseModel\nfrom pydantic import model_validator\n\nfrom onyx.access.models import DocumentAccess\nfrom onyx.configs.constants import PUBLIC_DOC_PAT\nfrom onyx.context.search.enums import QueryType\nfrom onyx.context.search.models import IndexFilters",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/document_index_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/document_index_utils.py",
      "line_number": 1,
      "code_snippet": "import math\nimport uuid\nfrom uuid import UUID\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import ENABLE_MULTIPASS_INDEXING\nfrom onyx.db.models import SearchSettings\nfrom onyx.db.search_settings import get_current_search_settings\nfrom onyx.db.search_settings import get_secondary_search_settings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa_constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa_constants.py",
      "line_number": 1,
      "code_snippet": "from onyx.configs.app_configs import VESPA_CLOUD_URL\nfrom onyx.configs.app_configs import VESPA_CONFIG_SERVER_HOST\nfrom onyx.configs.app_configs import VESPA_HOST\nfrom onyx.configs.app_configs import VESPA_PORT\nfrom onyx.configs.app_configs import VESPA_TENANT_PORT\nfrom onyx.configs.constants import SOURCE_TYPE\n\n# config server\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/models.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport json\nfrom enum import Enum\nfrom typing import Any\nfrom typing import Literal\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/constants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Tool name and ID constants matching frontend definitions.\"\"\"\n\n# Tool names as referenced by tool results / tool calls\nSEARCH_TOOL_NAME = \"run_search\"\nINTERNET_SEARCH_TOOL_NAME = \"run_internet_search\"\nIMAGE_GENERATION_TOOL_NAME = \"run_image_generation\"\nPYTHON_TOOL_NAME = \"run_python\"\nOPEN_URL_TOOL_NAME = \"open_url\"\n\n# In-code tool IDs that also correspond to the tool's name when associated with a persona",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/utils.py",
      "line_number": 1,
      "code_snippet": "import json\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import AZURE_IMAGE_API_KEY\nfrom onyx.db.connector import check_connectors_exist\nfrom onyx.db.document import check_docs_exist\nfrom onyx.db.models import LLMProvider\nfrom onyx.llm.constants import LlmProviderNames\nfrom onyx.llm.utils import find_model_obj",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py_120",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'tool.run' is used in 'run(' on line 120 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py",
      "line_number": 120,
      "code_snippet": "        try:\n            tool_response = tool.run(\n                placement=tool_call.placement,\n                override_kwargs=override_kwargs,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py_97_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_safe_run_single_tool'",
      "description": "Function '_safe_run_single_tool' on line 97 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py",
      "line_number": 97,
      "code_snippet": "\n\ndef _safe_run_single_tool(\n    tool: Tool,\n    tool_call: ToolCallKickoff,\n    override_kwargs: Any,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py_97_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_safe_run_single_tool'",
      "description": "Function '_safe_run_single_tool' on line 97 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py",
      "line_number": 97,
      "code_snippet": "\n\ndef _safe_run_single_tool(\n    tool: Tool,\n    tool_call: ToolCallKickoff,\n    override_kwargs: Any,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py_120",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_runner.py",
      "line_number": 120,
      "code_snippet": "    with function_span(tool.name) as span_fn:\n        span_fn.span_data.input = str(tool_call.tool_args)\n        try:\n            tool_response = tool.run(\n                placement=tool_call.placement,\n                override_kwargs=override_kwargs,\n                **tool_call.tool_args,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_constructor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_constructor.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.oauth_token_manager import OAuthTokenManager\nfrom onyx.chat.emitter import Emitter\nfrom onyx.configs.model_configs import GEN_AI_TEMPERATURE\nfrom onyx.context.search.models import BaseFilters",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\n\nclass ToolChoiceOptions(str, Enum):\n    REQUIRED = \"required\"\n    AUTO = \"auto\"\n    NONE = \"none\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py_43_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_model_info'",
      "description": "Function '_get_model_info' on line 43 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py",
      "line_number": 43,
      "code_snippet": "\n\ndef _get_model_info(model_key: str) -> dict:\n    \"\"\"Get model info from litellm.model_cost.\"\"\"\n    from onyx.llm.litellm_singleton import litellm\n",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py_59_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_extract_provider'",
      "description": "Function '_extract_provider' on line 59 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py",
      "line_number": 59,
      "code_snippet": "\n\ndef _extract_provider(model_key: str) -> str:\n    \"\"\"Extract provider from model key prefix.\"\"\"\n    from onyx.llm.litellm_singleton import litellm\n",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_name_parser.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nLiteLLM Model Name Parser\n\nParses LiteLLM model strings and returns structured metadata for UI display.\nAll metadata comes from litellm's model_cost dictionary. Until this upstream patch to LiteLLM\nis merged (https://github.com/BerriAI/litellm/pull/17330), we use the model_metadata_enrichments.json\nto add these fields at server startup.\n\nEnrichment fields:\n- display_name: Human-friendly name (e.g., \"Claude 3.5 Sonnet\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/constants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nLLM Constants\n\nCentralized constants for LLM providers, vendors, and display names.\n\"\"\"\n\nfrom enum import Enum\n\n\n# Provider names",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/multi_llm.py_269_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in '_completion'",
      "description": "Function '_completion' on line 269 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/multi_llm.py",
      "line_number": 269,
      "code_snippet": "            logger.warning(f\"Failed to track LLM cost: {e}\")\n\n    def _completion(\n        self,\n        prompt: LanguageModelInput,\n        tools: list[dict] | None,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/multi_llm.py_269",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/multi_llm.py",
      "line_number": 269,
      "code_snippet": "                db_session.commit()\n        except Exception as e:\n            # Log but don't fail the LLM call if tracking fails\n            logger.warning(f\"Failed to track LLM cost: {e}\")\n\n    def _completion(\n        self,\n        prompt: LanguageModelInput,\n        tools: list[dict] | None,\n        tool_choice: ToolChoiceOptions | None,\n        stream: bool,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/factory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/factory.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.chat.models import PersonaOverrideConfig\nfrom onyx.configs.model_configs import GEN_AI_TEMPERATURE\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.llm import can_user_access_llm_provider\nfrom onyx.db.llm import fetch_default_provider\nfrom onyx.db.llm import fetch_default_vision_provider",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/utils.py_423_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'get_llm_contextual_cost'",
      "description": "Function 'get_llm_contextual_cost' on line 423 makes critical financial, legal decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/utils.py",
      "line_number": 423,
      "code_snippet": "\n\ndef get_llm_contextual_cost(\n    llm: LLM,\n) -> float:\n    \"\"\"",
      "recommendation": "Critical financial, legal decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/utils.py_483",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/utils.py",
      "line_number": 483,
      "code_snippet": "    num_output_tokens += num_docs * MAX_CONTEXT_TOKENS\n\n    try:\n        usd_per_prompt, usd_per_completion = litellm.cost_per_token(\n            model=llm.config.model_name,\n            prompt_tokens=num_input_tokens,\n            completion_tokens=num_output_tokens,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/cost.py_23_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model_name' is used without version pinning on line 23. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/cost.py",
      "line_number": 23,
      "code_snippet": "\n        # cost_per_token returns (prompt_cost, completion_cost) in USD\n        prompt_cost_usd, completion_cost_usd = litellm.cost_per_token(\n            model=model_name,\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/cost.py_8_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'calculate_llm_cost_cents'",
      "description": "Function 'calculate_llm_cost_cents' on line 8 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/cost.py",
      "line_number": 8,
      "code_snippet": "\n\ndef calculate_llm_cost_cents(\n    model_name: str,\n    prompt_tokens: int,\n    completion_tokens: int,",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_response.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/model_response.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom typing import Any\nfrom typing import List\nfrom typing import TYPE_CHECKING\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/citation_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/citation_processor.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nDynamic Citation Processor for LLM Responses\n\nThis module provides a citation processor that can:\n- Accept citation number to SearchDoc mappings dynamically\n- Process token streams from LLMs to extract citations\n- Optionally replace citation markers with formatted markdown links\n- Emit CitationInfo objects for detected citations (when replacing)\n- Track all seen citations regardless of replacement mode\n- Maintain a list of cited documents in order of first citation",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/models.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom collections.abc import Iterator\nfrom enum import Enum\nfrom typing import Any\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\nfrom onyx.configs.constants import MessageType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/process_message.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/process_message.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nIMPORTANT: familiarize yourself with the design concepts prior to contributing to this file.\nAn overview can be found in the README.md file in this directory.\n\"\"\"\n\nimport re\nimport traceback\nfrom collections.abc import Callable\nfrom uuid import UUID\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_loop.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_loop.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.chat.chat_state import ChatStateContainer\nfrom onyx.chat.chat_utils import create_tool_call_failure_messages\nfrom onyx.chat.citation_processor import CitationMapping\nfrom onyx.chat.citation_processor import DynamicCitationProcessor\nfrom onyx.chat.citation_utils import update_citation_processor_from_tool_response\nfrom onyx.chat.emitter import Emitter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_processing_checker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_processing_checker.py",
      "line_number": 1,
      "code_snippet": "from uuid import UUID\n\nfrom redis.client import Redis\n\n# Redis key prefixes for chat message processing\nPREFIX = \"chatprocessing\"\nFENCE_PREFIX = f\"{PREFIX}_fence\"\nFENCE_TTL = 30 * 60  # 30 minutes\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/save_chat.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/save_chat.py",
      "line_number": 1,
      "code_snippet": "import json\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.context.search.models import CitationDocInfo\nfrom onyx.context.search.models import SearchDoc\nfrom onyx.db.chat import add_search_docs_to_chat_message\nfrom onyx.db.chat import add_search_docs_to_tool_call\nfrom onyx.db.chat import create_db_search_doc",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/stop_signal_checker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/stop_signal_checker.py",
      "line_number": 1,
      "code_snippet": "from uuid import UUID\n\nfrom redis.client import Redis\n\n# Redis key prefixes for chat session stop signals\nPREFIX = \"chatsessionstop\"\nFENCE_PREFIX = f\"{PREFIX}_fence\"\nFENCE_TTL = 10 * 60  # 10 minutes - defensive TTL to prevent memory leaks\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py_567_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'run_llm_step_pkt_generator'",
      "description": "Function 'run_llm_step_pkt_generator' on line 468 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py",
      "line_number": 567,
      "code_snippet": "        stream_start_time = time.monotonic()\n        first_action_recorded = False\n        for packet in llm.stream(\n            prompt=llm_msg_history,\n            tools=tool_definitions,\n            tool_choice=tool_choice,",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py_468_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_llm_step_pkt_generator'",
      "description": "Function 'run_llm_step_pkt_generator' on line 468 makes critical financial, security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py",
      "line_number": 468,
      "code_snippet": "\n\ndef run_llm_step_pkt_generator(\n    history: list[ChatMessageSimple],\n    tool_definitions: list[dict],\n    tool_choice: ToolChoiceOptions,",
      "recommendation": "Critical financial, security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py_567",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/llm_step.py",
      "line_number": 567,
      "code_snippet": "        )\n        stream_start_time = time.monotonic()\n        first_action_recorded = False\n        for packet in llm.stream(\n            prompt=llm_msg_history,\n            tools=tool_definitions,\n            tool_choice=tool_choice,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_state.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_state.py",
      "line_number": 1,
      "code_snippet": "import threading\nimport time\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom queue import Empty\nfrom typing import Any\n\nfrom onyx.chat.citation_processor import CitationMapping\nfrom onyx.chat.emitter import Emitter\nfrom onyx.server.query_and_chat.placement import Placement",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/prompt_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/prompt_utils.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom collections.abc import Sequence\nfrom uuid import UUID\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.persona import get_default_behavior_persona\nfrom onyx.db.user_file import calculate_user_files_token_count\nfrom onyx.file_store.models import FileDescriptor\nfrom onyx.prompts.chat_prompts import CITATION_REMINDER",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/chat_utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport re\nfrom collections.abc import Callable\nfrom typing import cast\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom fastapi.datastructures import Headers\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/citation_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/chat/citation_utils.py",
      "line_number": 1,
      "code_snippet": "import re\n\nfrom onyx.chat.citation_processor import CitationMapping\nfrom onyx.chat.citation_processor import DynamicCitationProcessor\nfrom onyx.context.search.models import SearchDocsResponse\nfrom onyx.tools.built_in_tools import CITEABLE_TOOLS_NAMES\nfrom onyx.tools.models import ToolResponse\n\n\ndef update_citation_processor_from_tool_response(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/access/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/access/models.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\n\nfrom onyx.access.utils import prefix_external_group\nfrom onyx.access.utils import prefix_user_email\nfrom onyx.access.utils import prefix_user_group\nfrom onyx.configs.constants import PUBLIC_DOC_PAT\n\n\n@dataclass(frozen=True)\nclass ExternalAccess:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/access/access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/access/access.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import cast\n\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.orm import Session\n\nfrom onyx.access.models import DocumentAccess\nfrom onyx.access.utils import prefix_user_email\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.configs.constants import PUBLIC_DOC_PAT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_object_helper.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_object_helper.py",
      "line_number": 1,
      "code_snippet": "from abc import ABC\nfrom abc import abstractmethod\n\nfrom celery import Celery\nfrom redis import Redis\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n\nfrom onyx.redis.redis_pool import get_redis_client\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector.py",
      "line_number": 1,
      "code_snippet": "import redis\n\nfrom onyx.redis.redis_connector_delete import RedisConnectorDelete\nfrom onyx.redis.redis_connector_doc_perm_sync import RedisConnectorPermissionSync\nfrom onyx.redis.redis_connector_ext_group_sync import RedisConnectorExternalGroupSync\nfrom onyx.redis.redis_connector_prune import RedisConnectorPrune\nfrom onyx.redis.redis_connector_stop import RedisConnectorStop\nfrom onyx.redis.redis_pool import get_redis_client\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_prune.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_prune.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom typing import cast\nfrom uuid import uuid4\n\nimport redis\nfrom celery import Celery\nfrom pydantic import BaseModel\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_doc_perm_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_doc_perm_sync.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom logging import Logger\nfrom typing import Any\nfrom typing import cast\nfrom typing import NamedTuple\n\nimport redis\nfrom pydantic import BaseModel\nfrom redis.lock import Lock as RedisLock",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_document_set.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_document_set.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import cast\nfrom uuid import uuid4\n\nimport redis\nfrom celery import Celery\nfrom redis import Redis\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_delete.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_delete.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom typing import cast\nfrom uuid import uuid4\n\nimport redis\nfrom celery import Celery\nfrom pydantic import BaseModel\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_ext_group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_ext_group_sync.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import cast\n\nimport redis\nfrom celery import Celery\nfrom pydantic import BaseModel\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import OnyxRedisConstants",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py_424_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 424. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py",
      "line_number": 424,
      "code_snippet": "    token = request.cookies.get(FASTAPI_USERS_AUTH_COOKIE_NAME)\n    if not token:\n        logger.debug(\"No auth token cookie found\")\n        return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py_438_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 438. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py",
      "line_number": 438,
      "code_snippet": "        return json.loads(token_data_str)\n    except json.JSONDecodeError:\n        logger.error(\"Error decoding token data from Redis\")\n        return None\n    except Exception as e:",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_pool.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport functools\nimport json\nimport ssl\nimport threading\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import cast\nfrom typing import Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_usergroup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_usergroup.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import cast\nfrom uuid import uuid4\n\nimport redis\nfrom celery import Celery\nfrom redis import Redis\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_stop.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/redis/redis_connector_stop.py",
      "line_number": 1,
      "code_snippet": "import redis\n\n\nclass RedisConnectorStop:\n    \"\"\"Manages interactions with redis for stop signaling. Should only be accessed\n    through RedisConnector.\"\"\"\n\n    PREFIX = \"connectorstop\"\n    FENCE_PREFIX = f\"{PREFIX}_fence\"\n    FENCE_TTL = 7 * 24 * 60 * 60  # 7 days - defensive TTL to prevent memory leaks",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/disposable_email_validator.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/disposable_email_validator.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nUtility to validate and block disposable/temporary email addresses.\n\nThis module fetches a list of known disposable email domains from a remote source\nand caches them for performance. It's used during user registration to prevent\nabuse from temporary email services.\n\"\"\"\n\nimport threading\nimport time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/email_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/email_utils.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport smtplib\nfrom datetime import datetime\nfrom email.mime.image import MIMEImage\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.utils import formatdate\nfrom email.utils import make_msgid\n\nimport sendgrid  # type: ignore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py_1151_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1151. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py",
      "line_number": 1151,
      "code_snippet": "            try:\n                user, token = user_token\n                logger.info(f\"Processing token refresh request for user {user.email}\")\n\n                # Check if user has OAuth accounts that need refreshing",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py_1174_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1174. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py",
      "line_number": 1174,
      "code_snippet": "                        return await backend.transport.get_login_response(new_token)\n                    except Exception as e:\n                        logger.error(f\"Error refreshing session token: {str(e)}\")\n                        # Fallback to logout and login if refresh fails\n                        await backend.logout(strategy, user, token)",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py_1343_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1343. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py",
      "line_number": 1343,
      "code_snippet": "            user = await fetch_user_for_api_key(hashed_api_key, async_db_session)\n    except ValueError:\n        logger.warning(\"Issue with validating authentication token\")\n        return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py_1493_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_oauth_router'",
      "description": "API endpoint 'get_oauth_router' on line 1493 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py",
      "line_number": 1493,
      "code_snippet": "\n\ndef get_oauth_router(\n    oauth_client: BaseOAuth2,\n    backend: AuthenticationBackend,\n    get_user_manager: UserManagerDependency[models.UP, models.ID],",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py_614",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/users.py",
      "line_number": 614,
      "code_snippet": "                        raise exceptions.UserNotExists()\n\n                except exceptions.UserNotExists:\n                    password = self.password_helper.generate()\n                    user_dict = {\n                        \"email\": account_email,\n                        \"hashed_password\": self.password_helper.hash(password),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/oauth_token_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/oauth_token_manager.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Any\nfrom urllib.parse import urlencode\nfrom uuid import UUID\n\nimport requests\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import OAuthConfig\nfrom onyx.db.models import OAuthUserToken",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py_4",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 4. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
      "line_number": 4,
      "code_snippet": "\n# API Key constants\nAPI_KEY_PREFIX = \"on_\"\nDEPRECATED_API_KEY_PREFIX = \"dn_\"\nAPI_KEY_LENGTH = 192",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py_5",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 5. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
      "line_number": 5,
      "code_snippet": "# API Key constants\nAPI_KEY_PREFIX = \"on_\"\nDEPRECATED_API_KEY_PREFIX = \"dn_\"\nAPI_KEY_LENGTH = 192\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
      "line_number": 13,
      "code_snippet": "\n# Shared header constants\nAPI_KEY_HEADER_NAME = \"Authorization\"\nAPI_KEY_HEADER_ALTERNATIVE_NAME = \"X-Onyx-Authorization\"\nBEARER_PREFIX = \"Bearer \"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
      "line_number": 14,
      "code_snippet": "# Shared header constants\nAPI_KEY_HEADER_NAME = \"Authorization\"\nAPI_KEY_HEADER_ALTERNATIVE_NAME = \"X-Onyx-Authorization\"\nBEARER_PREFIX = \"Bearer \"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/constants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Authentication constants shared across auth modules.\"\"\"\n\n# API Key constants\nAPI_KEY_PREFIX = \"on_\"\nDEPRECATED_API_KEY_PREFIX = \"dn_\"\nAPI_KEY_LENGTH = 192\n\n# PAT constants\nPAT_PREFIX = \"onyx_pat_\"\nPAT_LENGTH = 192",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/captcha.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/captcha.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Captcha verification for user registration.\"\"\"\n\nimport httpx\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\nfrom onyx.configs.app_configs import CAPTCHA_ENABLED\nfrom onyx.configs.app_configs import RECAPTCHA_SCORE_THRESHOLD\nfrom onyx.configs.app_configs import RECAPTCHA_SECRET_KEY\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/schemas.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/schemas.py",
      "line_number": 1,
      "code_snippet": "import uuid\nfrom enum import Enum\n\nfrom fastapi_users import schemas\n\n\nclass UserRole(str, Enum):\n    \"\"\"\n    User roles\n    - Basic can't perform any admin actions",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Shared authentication utilities for bearer token extraction and validation.\"\"\"\n\nfrom collections.abc import Callable\nfrom urllib.parse import unquote\n\nfrom fastapi import Request\n\nfrom onyx.auth.constants import API_KEY_HEADER_ALTERNATIVE_NAME\nfrom onyx.auth.constants import API_KEY_HEADER_NAME\nfrom onyx.auth.constants import API_KEY_PREFIX",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/noauth_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/noauth_user.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Mapping\nfrom typing import Any\nfrom typing import cast\n\nfrom onyx.auth.schemas import UserRole\nfrom onyx.configs.constants import KV_NO_AUTH_USER_PERSONALIZATION_KEY\nfrom onyx.configs.constants import KV_NO_AUTH_USER_PREFERENCES_KEY\nfrom onyx.configs.constants import NO_AUTH_USER_EMAIL\nfrom onyx.configs.constants import NO_AUTH_USER_ID\nfrom onyx.key_value_store.store import KeyValueStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/invited_users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/invited_users.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom onyx.configs.constants import KV_PENDING_USERS_KEY\nfrom onyx.configs.constants import KV_USER_STORE_KEY\nfrom onyx.key_value_store.factory import get_kv_store\nfrom onyx.key_value_store.interface import KvKeyNotFoundError\nfrom onyx.utils.special_types import JSON_ro\n\n\ndef remove_user_from_invited_users(email: str) -> int:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/jwt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/jwt.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom enum import Enum\nfrom functools import lru_cache\nfrom typing import Any\nfrom typing import cast\n\nimport jwt\nimport requests\nfrom cryptography.hazmat.primitives.asymmetric.rsa import RSAPublicKey\nfrom jwt import decode as jwt_decode",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/oauth_refresher.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/auth/oauth_refresher.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nimport httpx\nfrom fastapi_users.manager import BaseUserManager",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/password_validation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/password_validation.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom typing import Any\nfrom typing import IO\n\nfrom onyx.file_processing.extract_file_text import get_file_ext\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py_96",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_summarize_image' on line 96 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py",
      "line_number": 96,
      "code_snippet": "def _summarize_image(\n    encoded_image: str,\n    llm: LLM,\n    query: str | None = None,\n    system_prompt: str | None = None,\n) -> str:\n    \"\"\"Use default LLM (if it is multimodal) to generate a summary of an image.\"\"\"\n\n    messages: list[ChatCompletionMessage] = []\n\n    if system_prompt:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py_121",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py",
      "line_number": 121,
      "code_snippet": "    )\n\n    try:\n        return llm_response_to_string(llm.invoke(messages))\n\n    except Exception as e:\n        error_msg = f\"Summarization failed. Messages: {messages}\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py_96",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/image_summarization.py",
      "line_number": 96,
      "code_snippet": "            context_name,\n        )\n        return None\n\n\ndef _summarize_image(\n    encoded_image: str,\n    llm: LLM,\n    query: str | None = None,\n    system_prompt: str | None = None,\n) -> str:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/file_types.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/file_types.py",
      "line_number": 1,
      "code_snippet": "PRESENTATION_MIME_TYPE = (\n    \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n)\n\nSPREADSHEET_MIME_TYPE = (\n    \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n)\nWORD_PROCESSING_MIME_TYPE = (\n    \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/html_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/html_utils.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom copy import copy\nfrom dataclasses import dataclass\nfrom io import BytesIO\nfrom typing import IO\n\nimport bs4\n\nfrom onyx.configs.app_configs import HTML_BASED_CONNECTOR_TRANSFORM_LINKS_STRATEGY\nfrom onyx.configs.app_configs import PARSE_WITH_TRAFILATURA",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/extract_file_text.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/extract_file_text.py",
      "line_number": 1,
      "code_snippet": "import gc\nimport io\nimport json\nimport os\nimport re\nimport zipfile\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom collections.abc import Sequence\nfrom email.parser import Parser as EmailParser",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/unstructured.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_processing/unstructured.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\nfrom typing import IO\nfrom typing import TYPE_CHECKING\n\nfrom onyx.configs.constants import KV_UNSTRUCTURED_API_KEY\nfrom onyx.key_value_store.factory import get_kv_store\nfrom onyx.key_value_store.interface import KvKeyNotFoundError\nfrom onyx.utils.logger import setup_logger\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/s3_key_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/s3_key_utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nS3 key sanitization utilities for ensuring AWS S3 compatibility.\n\nThis module provides utilities for sanitizing file names to be compatible with\nAWS S3 object key naming guidelines while ensuring uniqueness when significant\nsanitization occurs.\n\nReference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/file_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/file_store.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport tempfile\nimport uuid\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom io import BytesIO\nfrom typing import Any\nfrom typing import cast\nfrom typing import IO\nfrom typing import NotRequired",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/models.py",
      "line_number": 1,
      "code_snippet": "import base64\nfrom enum import Enum\nfrom typing import NotRequired\nfrom typing_extensions import TypedDict  # noreorder\n\nfrom pydantic import BaseModel\n\n\nclass ChatFileType(str, Enum):\n    # Image types only contain the binary data",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/utils.py",
      "line_number": 1,
      "code_snippet": "import base64\nfrom collections.abc import Callable\nfrom io import BytesIO\nfrom typing import cast\nfrom uuid import UUID\n\nimport requests\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import WEB_DOMAIN",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/document_batch_storage.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/file_store/document_batch_storage.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom io import StringIO\nfrom typing import List\nfrom typing import Optional\nfrom typing import TypeAlias\n\nfrom pydantic import BaseModel",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/seeding/load_yamls.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/seeding/load_yamls.py",
      "line_number": 1,
      "code_snippet": "import yaml\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.chat_configs import INPUT_PROMPT_YAML\nfrom onyx.db.input_prompt import insert_input_prompt_if_not_exists\nfrom onyx.utils.logger import setup_logger\n\n\nlogger = setup_logger()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/api_key_usage.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/api_key_usage.py",
      "line_number": 1,
      "code_snippet": "\"\"\"API key and PAT usage tracking for cloud usage limits.\"\"\"\n\nfrom fastapi import Depends\nfrom fastapi import Request\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.api_key import get_hashed_api_key_from_request\nfrom onyx.auth.pat import get_hashed_pat_from_request\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.usage import increment_usage",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/saml.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/saml.py",
      "line_number": 1,
      "code_snippet": "import contextlib\nimport secrets\nimport string\nimport uuid\nfrom typing import Any\nfrom urllib.parse import urlparse\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/usage_limits.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/usage_limits.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Usage limits enforcement for cloud deployments.\"\"\"\n\nfrom collections.abc import Callable\n\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import ANTHROPIC_DEFAULT_API_KEY\nfrom onyx.configs.app_configs import COHERE_DEFAULT_API_KEY\nfrom onyx.configs.app_configs import OPENAI_DEFAULT_API_KEY",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/tenant_usage_limits.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/tenant_usage_limits.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nNon-EE version of tenant usage limit overrides.\n\nIn non-EE deployments, there are no tenant-specific overrides - all tenants\nuse the default limits from environment variables.\n\nThe EE version (ee.onyx.server.tenant_usage_limits) fetches per-tenant\noverrides from the control plane.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/auth_check.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/auth_check.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom fastapi import FastAPI\nfrom fastapi.dependencies.models import Dependant\nfrom starlette.routing import BaseRoute\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.auth.users import current_chat_accessible_user\nfrom onyx.auth.users import current_curator_or_admin_user\nfrom onyx.auth.users import current_limited_user",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/variable_functionality.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/variable_functionality.py",
      "line_number": 1,
      "code_snippet": "import functools\nimport importlib\nimport inspect\nfrom typing import Any\nfrom typing import TypeVar\n\nfrom onyx.configs.app_configs import ENTERPRISE_EDITION_ENABLED\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/b64.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/b64.py",
      "line_number": 1,
      "code_snippet": "import base64\n\n\ndef get_image_type_from_bytes(raw_b64_bytes: bytes) -> str:\n    magic_number = raw_b64_bytes[:4]\n\n    if magic_number.startswith(b\"\\x89PNG\"):\n        mime_type = \"image/png\"\n    elif magic_number.startswith(b\"\\xff\\xd8\"):\n        mime_type = \"image/jpeg\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/subclasses.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/subclasses.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport importlib\nimport os\nimport pkgutil\nimport sys\nfrom types import ModuleType\nfrom typing import List\nfrom typing import Type\nfrom typing import TypeVar",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/threadpool_concurrency.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/threadpool_concurrency.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport collections.abc\nimport concurrent\nimport contextvars\nimport copy\nimport threading\nimport uuid\nfrom collections.abc import Awaitable\nfrom collections.abc import Callable\nfrom collections.abc import Iterator",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/supervisord_watchdog.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/supervisord_watchdog.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\nimport argparse\nimport subprocess\nimport time\n\nfrom onyx.redis.redis_pool import get_redis_client\nfrom onyx.utils.logger import setup_logger\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/text_processing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/text_processing.py",
      "line_number": 1,
      "code_snippet": "import codecs\nimport json\nimport re\nimport string\nfrom urllib.parse import quote\n\nfrom onyx.utils.logger import setup_logger\n\n\nlogger = setup_logger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/sitemap.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/sitemap.py",
      "line_number": 1,
      "code_snippet": "import re\nimport xml.etree.ElementTree as ET\nfrom typing import Set\nfrom urllib.parse import urljoin\n\nimport requests\n\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/object_size_check.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/object_size_check.py",
      "line_number": 1,
      "code_snippet": "import sys\nfrom typing import TypeVar\n\nT = TypeVar(\"T\", dict, list, tuple, set, frozenset)\n\n\ndef deep_getsizeof(obj: T, seen: set[int] | None = None) -> int:\n    \"\"\"Recursively sum size of objects, handling circular references.\"\"\"\n    if seen is None:\n        seen = set()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py_140",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'current_context.run' is used in 'run(' on line 140 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
      "line_number": 140,
      "code_snippet": "        thread = threading.Thread(\n            target=lambda: current_context.run(telemetry_logic), daemon=True\n        )\n        thread.start()",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py_95",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'optional_telemetry' on line 95 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
      "line_number": 95,
      "code_snippet": "def optional_telemetry(\n    record_type: RecordType,\n    data: dict,\n    user_id: str | None = None,\n    tenant_id: str | None = None,  # Allows for override of tenant_id\n) -> None:\n    if DISABLE_TELEMETRY:\n        return\n\n    tenant_id = tenant_id or get_current_tenant_id()\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py_95_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'optional_telemetry'",
      "description": "Function 'optional_telemetry' on line 95 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
      "line_number": 95,
      "code_snippet": "\n\ndef optional_telemetry(\n    record_type: RecordType,\n    data: dict,\n    user_id: str | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/telemetry.py",
      "line_number": 1,
      "code_snippet": "import contextvars\nimport threading\nimport uuid\nfrom enum import Enum\nfrom typing import cast\n\nimport requests\n\nfrom onyx.configs.app_configs import DISABLE_TELEMETRY\nfrom onyx.configs.app_configs import ENTERPRISE_EDITION_ENABLED",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/logger.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/logger.py",
      "line_number": 1,
      "code_snippet": "import contextvars\nimport logging\nimport os\nfrom collections.abc import MutableMapping\nfrom logging.handlers import RotatingFileHandler\nfrom typing import Any\n\nfrom shared_configs.configs import DEV_LOGGING_ENABLED\nfrom shared_configs.configs import LOG_FILE_NAME\nfrom shared_configs.configs import LOG_LEVEL",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/url.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/url.py",
      "line_number": 1,
      "code_snippet": "import ipaddress\nimport socket\nfrom typing import Any\nfrom urllib.parse import parse_qs\nfrom urllib.parse import urlencode\nfrom urllib.parse import urlparse\nfrom urllib.parse import urlunparse\n\nimport requests\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/file.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nimport puremagic\nfrom pydantic import BaseModel\n\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/timing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/timing.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom collections.abc import Iterator\nfrom functools import wraps\nfrom typing import Any\nfrom typing import cast\nfrom typing import TypeVar\n\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/long_term_log.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/long_term_log.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport threading\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nfrom onyx.utils.logger import setup_logger\nfrom onyx.utils.special_types import JSON_ro\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/web_content.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/web_content.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport io\nfrom urllib.parse import unquote\nfrom urllib.parse import urlparse\n\nfrom bs4.dammit import UnicodeDammit\n\nfrom onyx.file_processing.extract_file_text import read_pdf_file\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/gpu_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/gpu_utils.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom functools import lru_cache\n\nimport requests\nfrom retry import retry\n\nfrom onyx.utils.logger import setup_logger\nfrom shared_configs.configs import INDEXING_MODEL_SERVER_HOST\nfrom shared_configs.configs import INDEXING_MODEL_SERVER_PORT\nfrom shared_configs.configs import MODEL_SERVER_HOST",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/headers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/headers.py",
      "line_number": 1,
      "code_snippet": "from typing import TypedDict\n\nfrom fastapi.datastructures import Headers\n\nfrom onyx.configs.model_configs import LITELLM_EXTRA_HEADERS\nfrom onyx.configs.model_configs import LITELLM_PASS_THROUGH_HEADERS\nfrom onyx.configs.tool_configs import CUSTOM_TOOL_PASS_THROUGH_HEADERS\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/middleware.py_31_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'add_onyx_request_id_middleware'",
      "description": "API endpoint 'add_onyx_request_id_middleware' on line 31 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/middleware.py",
      "line_number": 31,
      "code_snippet": "\n\ndef add_onyx_request_id_middleware(\n    app: FastAPI, prefix: str, logger: logging.LoggerAdapter\n) -> None:\n    @app.middleware(\"http\")",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/middleware.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/utils/middleware.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport hashlib\nimport logging\nimport uuid\nfrom collections.abc import Awaitable\nfrom collections.abc import Callable\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom fastapi import FastAPI",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py_190",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'functions' flows to 'run_functions_in_parallel' on line 190 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
      "line_number": 190,
      "code_snippet": "\n    results = run_functions_in_parallel(function_calls=functions)\n    prompts = []\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py_109",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_starter_messages' on line 109 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
      "line_number": 109,
      "code_snippet": "def generate_starter_messages(\n    name: str,\n    description: str,\n    instructions: str,\n    document_set_ids: List[int],\n    generation_count: int,\n    db_session: Session,\n    user: User | None,\n) -> List[StarterMessage]:\n    \"\"\"\n    Generates starter messages by first obtaining categories and then generating messages for each category.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py_44_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 44. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
      "line_number": 44,
      "code_snippet": "    filters = IndexFilters(document_set=doc_sets, access_control_list=acl_filters)\n\n    chunks = document_index.random_retrieval(\n        filters=filters, num_to_retrieve=NUM_PERSONA_PROMPT_GENERATION_CHUNKS\n    )\n    return chunks",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py_145",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/starter_message_creation.py",
      "line_number": 145,
      "code_snippet": "            num_categories=generation_count,\n        )\n\n        category_response = llm.invoke(category_generation_prompt)\n        response_content = llm_response_to_string(category_response)\n        categories = parse_categories(response_content)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py_49",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'model_output' flows to '_extract_usefulness' on line 49 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
      "line_number": 49,
      "code_snippet": "\n    return _extract_usefulness(model_output)\n\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py_14",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'llm_eval_section' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
      "line_number": 14,
      "code_snippet": "def llm_eval_section(\n    query: str,\n    section_content: str,\n    llm: LLM,\n    title: str,\n    metadata: dict[str, str | list[str]],\n) -> bool:\n    def _get_metadata_str(metadata: dict[str, str | list[str]]) -> str:\n        metadata_str = \"\\nMetadata:\\n\"\n        for key, value in metadata.items():\n            value_str = \", \".join(value) if isinstance(value, list) else value",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py_43",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
      "line_number": 43,
      "code_snippet": "            return False\n        return True\n\n    model_output = llm_response_to_string(llm.invoke(prompt))\n\n    # NOTE(rkuo): all this does is print \"Yes useful\" or \"Not useful\"\n    # disabling becuase it's spammy, restore and give more context if this is needed",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py_14",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chunk_usefulness.py",
      "line_number": 14,
      "code_snippet": "from onyx.utils.threadpool_concurrency import run_functions_tuples_in_parallel\n\nlogger = setup_logger()\n\n\ndef llm_eval_section(\n    query: str,\n    section_content: str,\n    llm: LLM,\n    title: str,\n    metadata: dict[str, str | list[str]],",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chat_session_naming.py_32",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/chat_session_naming.py",
      "line_number": 32,
      "code_snippet": "        language_hint_or_empty=language_hint, chat_history=history_str\n    )\n\n    new_name_raw = llm_response_to_string(llm.invoke(prompt))\n\n    new_name = new_name_raw.strip().strip('\"')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py_229",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input 'query' flows to LLM call via format_call in variable 'prompt'. Function 'llm_multilingual_query_expansion' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
      "line_number": 229,
      "code_snippet": "\n    prompt = LANGUAGE_REPHRASE_PROMPT.format(query=query, target_language=language)\n    model_output = llm_response_to_string(\n        llm.invoke(prompt, reasoning_effort=ReasoningEffort.OFF)\n    )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py_226",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'llm_multilingual_query_expansion' on line 226 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
      "line_number": 226,
      "code_snippet": "def llm_multilingual_query_expansion(query: str, language: str) -> str:\n    llm = get_default_llm(timeout=5)\n\n    prompt = LANGUAGE_REPHRASE_PROMPT.format(query=query, target_language=language)\n    model_output = llm_response_to_string(\n        llm.invoke(prompt, reasoning_effort=ReasoningEffort.OFF)\n    )\n    logger.debug(model_output)\n\n    return model_output\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py_138",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
      "line_number": 138,
      "code_snippet": "    messages.append(final_user_msg)\n\n    # Call LLM and return result\n    response = llm.invoke(prompt=messages, reasoning_effort=ReasoningEffort.OFF)\n\n    final_query = response.choice.message.content\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py_215",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
      "line_number": 215,
      "code_snippet": "    messages.append(final_user_msg)\n\n    # Call LLM and return result\n    response = llm.invoke(prompt=messages, reasoning_effort=ReasoningEffort.OFF)\n    content = response.choice.message.content\n\n    # Parse the response - each line is a separate keyword query",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py_231",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
      "line_number": 231,
      "code_snippet": "\n    prompt = LANGUAGE_REPHRASE_PROMPT.format(query=query, target_language=language)\n    model_output = llm_response_to_string(\n        llm.invoke(prompt, reasoning_effort=ReasoningEffort.OFF)\n    )\n    logger.debug(model_output)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py_226",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/query_expansion.py",
      "line_number": 226,
      "code_snippet": "\n    queries = [line.strip() for line in content.strip().split(\"\\n\") if line.strip()]\n    return queries\n\n\ndef llm_multilingual_query_expansion(query: str, language: str) -> str:\n    llm = get_default_llm(timeout=5)\n\n    prompt = LANGUAGE_REPHRASE_PROMPT.format(query=query, target_language=language)\n    model_output = llm_response_to_string(\n        llm.invoke(prompt, reasoning_effort=ReasoningEffort.OFF)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py_26",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'model_output' flows to '_extract_validity' on line 26 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py",
      "line_number": 26,
      "code_snippet": "\n    validity = _extract_validity(model_output)\n\n    return validity",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py_11",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_answer_validity' on line 11 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py",
      "line_number": 11,
      "code_snippet": "def get_answer_validity(\n    query: str,\n    answer: str,\n) -> bool:\n    def _extract_validity(model_output: str) -> bool:\n        if model_output.strip().strip(\"```\").strip().split()[-1].lower() == \"invalid\":\n            return False\n        return True  # If something is wrong, let's not toss away the answer\n\n    llm = get_default_llm()\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py_23",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py",
      "line_number": 23,
      "code_snippet": "    llm = get_default_llm()\n\n    prompt = ANSWER_VALIDITY_PROMPT.format(user_query=query, llm_answer=answer)\n    model_output = llm_response_to_string(llm.invoke(prompt))\n    logger.debug(model_output)\n\n    validity = _extract_validity(model_output)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py_11",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/answer_validation.py",
      "line_number": 11,
      "code_snippet": "\nlogger = setup_logger()\n\n\n@log_function_time()\ndef get_answer_validity(\n    query: str,\n    answer: str,\n) -> bool:\n    def _extract_validity(model_output: str) -> bool:\n        if model_output.strip().strip(\"```\").strip().split()[-1].lower() == \"invalid\":",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/time_filter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/time_filter.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\n\nfrom dateutil.parser import parse\n\nfrom onyx.llm.interfaces import LLM\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py_111",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "User input 'section_text' embedded in LLM prompt",
      "description": "User input 'section_text' flows to LLM call via format_call in variable 'prompt_text'. Function 'classify_section_relevance' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
      "line_number": 111,
      "code_snippet": "    # Build the prompt\n    prompt_text = DOCUMENT_CONTEXT_SELECTION_PROMPT.format(\n        document_title=document_title,\n        main_section=section_text,\n        section_above=section_above_text if section_above_text else \"N/A\",\n        section_below=section_below_text if section_below_text else \"N/A\",\n        user_query=user_query,\n    )\n\n    # Default to MAIN_SECTION_ONLY\n    default_classification = ContextExpansionType.MAIN_SECTION_ONLY\n\n    # Call LLM for classification\n    try:\n        response = llm.invoke(prompt=prompt_text, reasoning_effort=ReasoningEffort.OFF)\n        llm_response = response.choice.message.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py_124",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'llm.invoke' is used in 'SELECT' on line 124 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
      "line_number": 124,
      "code_snippet": "    try:\n        response = llm.invoke(prompt=prompt_text, reasoning_effort=ReasoningEffort.OFF)\n        llm_response = response.choice.message.content\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py_258",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'llm.invoke' is used in 'SELECT' on line 258 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
      "line_number": 258,
      "code_snippet": "    try:\n        response = llm.invoke(prompt=prompt_text, reasoning_effort=ReasoningEffort.OFF)\n        llm_response = response.choice.message.content\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py_90",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'classify_section_relevance' on line 90 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
      "line_number": 90,
      "code_snippet": "def classify_section_relevance(\n    document_title: str,\n    section_text: str,\n    user_query: str,\n    llm: LLM,\n    section_above_text: str | None,\n    section_below_text: str | None,\n) -> ContextExpansionType:\n    \"\"\"Use LLM to classify section relevance and determine context expansion type.\n\n    Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py_124",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
      "line_number": 124,
      "code_snippet": "\n    # Call LLM for classification\n    try:\n        response = llm.invoke(prompt=prompt_text, reasoning_effort=ReasoningEffort.OFF)\n        llm_response = response.choice.message.content\n\n        if not llm_response:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py_258",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/secondary_llm_flows/document_filter.py",
      "line_number": 258,
      "code_snippet": "\n    # Call LLM for selection\n    try:\n        response = llm.invoke(prompt=prompt_text, reasoning_effort=ReasoningEffort.OFF)\n        llm_response = response.choice.message.content\n\n        if not llm_response:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/oauth_utils.py_136_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 136. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/oauth_utils.py",
      "line_number": 136,
      "code_snippet": "        raise ValueError(f\"OAuth state not found in Redis: {state}\")\n\n    # Delete the key after retrieval (one-time use)\n    redis_client.delete(redis_key)\n\n    # Parse and return session",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/oauth_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/oauth_utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Generic OAuth utilities for federated connectors API layer.\"\"\"\n\nimport base64\nimport json\nimport uuid\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\nfrom typing import Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/factory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/factory.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Factory for creating federated connector instances.\"\"\"\n\nimport importlib\nfrom typing import Any\nfrom typing import Type\n\nfrom onyx.configs.constants import FederatedConnectorSource\nfrom onyx.federated_connectors.interfaces import FederatedConnector\nfrom onyx.federated_connectors.registry import FEDERATED_CONNECTOR_CLASS_MAP\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/federated_retrieval.py_131",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 131. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/federated_retrieval.py",
      "line_number": 131,
      "code_snippet": "                credentials = {\n                    \"client_id\": \"bot-context\",  # Placeholder for bot context\n                    \"client_secret\": \"bot-context\",  # Placeholder for bot context\n                }\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/federated_retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/federated_retrieval.py",
      "line_number": 1,
      "code_snippet": "from collections import defaultdict\nfrom collections.abc import Callable\nfrom typing import Any\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/constants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nConstants for natural language processing, including embedding and reranking models.\n\nThis file contains constants moved from model_server to support the gradual migration\nof API-based calls to bypass the model server.\n\"\"\"\n\nfrom shared_configs.enums import EmbeddingProvider\nfrom shared_configs.enums import EmbedTextType\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/utils.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom copy import copy\n\nfrom tokenizers import Encoding  # type: ignore[import-untyped]\nfrom tokenizers import Tokenizer\n\nfrom onyx.configs.model_configs import DOCUMENT_ENCODER_MODEL\nfrom onyx.context.search.models import InferenceChunk",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py_112_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'cleanup_embedding_thread_locals'",
      "description": "Function 'cleanup_embedding_thread_locals' on line 112 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py",
      "line_number": 112,
      "code_snippet": "\n\ndef cleanup_embedding_thread_locals() -> None:\n    \"\"\"Clean up thread-local event loops to prevent memory leaks.\n\n    This should be called after each task completes to ensure that",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py_1206",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py",
      "line_number": 1206,
      "code_snippet": "\n    def _warm_up() -> None:\n        try:\n            embedding_model.encode(texts=[warm_up_str], text_type=EmbedTextType.QUERY)\n            logger.debug(\n                f\"Warm-up complete for encoder model: {embedding_model.model_name}\"\n            )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py_745",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py",
      "line_number": 745,
      "code_snippet": "            api_url=self.api_url,\n            api_version=self.api_version,\n        ) as cloud_model:\n            embeddings = await cloud_model.embed(\n                texts=embed_request.texts,\n                model_name=embed_request.model_name,\n                deployment_name=embed_request.deployment_name,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py_1094",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/natural_language_processing/search_nlp_models.py",
      "line_number": 1094,
      "code_snippet": "                query, passages, self.api_url, self.model_name, self.api_key\n            )\n        else:\n            raise ValueError(f\"Unsupported reranking provider: {self.provider_type}\")\n\n    def predict(self, query: str, passages: list[str]) -> list[float]:\n        # Route between direct API calls and model server calls\n        if self.provider_type is not None:\n            # For API providers, make direct API call\n            loop = asyncio.new_event_loop()\n            try:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/chat_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/chat_configs.py",
      "line_number": 1,
      "code_snippet": "import os\n\nINPUT_PROMPT_YAML = \"./onyx/seeding/input_prompts.yaml\"\nPROMPTS_YAML = \"./onyx/seeding/prompts.yaml\"\nPERSONAS_YAML = \"./onyx/seeding/personas.yaml\"\nNUM_RETURNED_HITS = 50\n# Used for LLM filtering and reranking\n# We want this to be approximately the number of results we want to show on the first page\n# It cannot be too large due to cost and latency implications\nNUM_POSTPROCESSED_RESULTS = 20",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_21",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 21,
      "code_snippet": "IGNORE_FOR_QA = \"ignore_for_qa\"\n# NOTE: deprecated, only used for porting key from old system\nGEN_AI_API_KEY_STORAGE_KEY = \"genai_api_key\"\nPUBLIC_DOC_PAT = \"PUBLIC\"\nID_SEPARATOR = \":;:\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_93",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 93. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 93,
      "code_snippet": "SSL_CERT_FILE = \"bundle.pem\"\n# API Keys\nDANSWER_API_KEY_PREFIX = \"API_KEY__\"\nDANSWER_API_KEY_DUMMY_EMAIL_DOMAIN = \"onyxapikey.ai\"\nUNNAMED_KEY_PLACEHOLDER = \"Unnamed\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_94",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 94. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 94,
      "code_snippet": "# API Keys\nDANSWER_API_KEY_PREFIX = \"API_KEY__\"\nDANSWER_API_KEY_DUMMY_EMAIL_DOMAIN = \"onyxapikey.ai\"\nUNNAMED_KEY_PLACEHOLDER = \"Unnamed\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_100",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 100. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 100,
      "code_snippet": "KV_REINDEX_KEY = \"needs_reindexing\"\nKV_SEARCH_SETTINGS = \"search_settings\"\nKV_UNSTRUCTURED_API_KEY = \"unstructured_api_key\"\nKV_USER_STORE_KEY = \"INVITED_USERS\"\nKV_PENDING_USERS_KEY = \"PENDING_USERS\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_107",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 107. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 107,
      "code_snippet": "KV_CRED_KEY = \"credential_id_{}\"\nKV_GMAIL_CRED_KEY = \"gmail_app_credential\"\nKV_GMAIL_SERVICE_ACCOUNT_KEY = \"gmail_service_account_key\"\nKV_GOOGLE_DRIVE_CRED_KEY = \"google_drive_app_credential\"\nKV_GOOGLE_DRIVE_SERVICE_ACCOUNT_KEY = \"google_drive_service_account_key\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_109",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 109. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 109,
      "code_snippet": "KV_GMAIL_SERVICE_ACCOUNT_KEY = \"gmail_service_account_key\"\nKV_GOOGLE_DRIVE_CRED_KEY = \"google_drive_app_credential\"\nKV_GOOGLE_DRIVE_SERVICE_ACCOUNT_KEY = \"google_drive_service_account_key\"\nKV_GEN_AI_KEY_CHECK_TIME = \"genai_api_key_last_check_time\"\nKV_SETTINGS_KEY = \"onyx_settings\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/constants.py",
      "line_number": 1,
      "code_snippet": "import platform\nimport re\nimport socket\nfrom enum import auto\nfrom enum import Enum\n\n\nONYX_DEFAULT_APPLICATION_NAME = \"Onyx\"\nONYX_DISCORD_URL = \"https://discord.gg/4NA5SbzrWb\"\nONYX_UTM_SOURCE = \"onyx_app\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/app_configs.py_282",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 282. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/app_configs.py",
      "line_number": 282,
      "code_snippet": "REDIS_REPLICA_HOST = os.environ.get(\"REDIS_REPLICA_HOST\") or REDIS_HOST\n\nREDIS_AUTH_KEY_PREFIX = \"fastapi_users_token:\"\n\n# Rate limiting for auth endpoints",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/app_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/app_configs.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport urllib.parse\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import cast\n\nfrom onyx.auth.schemas import AuthBackend\nfrom onyx.configs.constants import AuthType\nfrom onyx.configs.constants import QueryHistoryType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/agent_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/agent_configs.py",
      "line_number": 1,
      "code_snippet": "import os\n\n\nAGENT_DEFAULT_RETRIEVAL_HITS = 15\nAGENT_DEFAULT_RERANKING_HITS = 10\nAGENT_DEFAULT_SUB_QUESTION_MAX_CONTEXT_HITS = 8\nAGENT_DEFAULT_NUM_DOCS_FOR_INITIAL_DECOMPOSITION = 3\nAGENT_DEFAULT_NUM_DOCS_FOR_REFINED_DECOMPOSITION = 5\n\nAGENT_DEFAULT_MAX_STREAMED_DOCS_FOR_INITIAL_ANSWER = 25",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/model_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/model_configs.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\n\n#####\n# Embedding/Reranking Model Configs\n#####\n# Important considerations when choosing models\n# Max tokens count needs to be high considering use case (at least 512)\n# Models used must be MIT or Apache license\n# Inference/Indexing speed",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/onyxbot_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/onyxbot_configs.py",
      "line_number": 1,
      "code_snippet": "import os\n\n#####\n# Onyx Slack Bot Configs\n#####\nONYX_BOT_NUM_RETRIES = int(os.environ.get(\"ONYX_BOT_NUM_RETRIES\", \"5\"))\n# Number of docs to display in \"Reference Documents\"\nONYX_BOT_NUM_DOCS_TO_DISPLAY = int(os.environ.get(\"ONYX_BOT_NUM_DOCS_TO_DISPLAY\", \"5\"))\n# If the LLM fails to answer, Onyx can still show the \"Reference Documents\"\nONYX_BOT_DISABLE_DOCS_ONLY_ANSWER = os.environ.get(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/tool_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/configs/tool_configs.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\n\n\nIMAGE_GENERATION_OUTPUT_FORMAT = os.environ.get(\n    \"IMAGE_GENERATION_OUTPUT_FORMAT\", \"b64_json\"\n)\n\n# if specified, will pass through request headers to the call to API calls made by custom tools\nCUSTOM_TOOL_PASS_THROUGH_HEADERS: list[str] | None = None",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.configs.kg_configs import KG_DEFAULT_MAX_PARENT_RECURSION_DEPTH\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/chat_tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/chat_tools.py",
      "line_number": 1,
      "code_snippet": "# These prompts are to support tool calling. Currently not used in the main flow or via any configs\n# The current generation of LLM is too unreliable for this task.\n# Onyx retrieval call as a tool option\nDANSWER_TOOL_NAME = \"Current Search\"\nDANSWER_TOOL_DESCRIPTION = (\n    \"A search tool that can find information on any topic \"\n    \"including up to date and proprietary knowledge.\"\n)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/filter_extration.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/filter_extration.py",
      "line_number": 1,
      "code_snippet": "# The following prompts are used for extracting filters to apply along with the query in the\n# document index. For example, a filter for dates or a filter by source type such as GitHub\n# or Slack\nfrom onyx.prompts.constants import SOURCES_KEY\n\n\n# Smaller followup prompts in time_filter.py\nTIME_FILTER_PROMPT = \"\"\"\nYou are a tool to identify time filters to apply to a user query for a downstream search \\\napplication. The downstream application is able to use a recency bias or apply a hard cutoff to \\",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/query_validation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/query_validation.py",
      "line_number": 1,
      "code_snippet": "# The following prompts are used for verifying if the user's query can be answered by the current\n# system. Many new users do not understand the design/capabilities of the system and will ask\n# questions that are unanswerable such as aggregations or user specific questions that the system\n# cannot handle, this is used to identify those cases\nfrom onyx.prompts.constants import ANSWERABLE_PAT\nfrom onyx.prompts.constants import GENERAL_SEP_PAT\nfrom onyx.prompts.constants import QUESTION_PAT\nfrom onyx.prompts.constants import THOUGHT_PAT\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/constants.py",
      "line_number": 1,
      "code_snippet": "GENERAL_SEP_PAT = \"--------------\"  # Same length as Langchain's separator\nCODE_BLOCK_PAT = \"```\\n{}\\n```\"\nTRIPLE_BACKTICK = \"```\"\nQUESTION_PAT = \"Query:\"\nFINAL_QUERY_PAT = \"Final Query:\"\nTHOUGHT_PAT = \"Thought:\"\nANSWER_PAT = \"Answer:\"\nANSWERABLE_PAT = \"Answerable:\"\nFINAL_ANSWER_PAT = \"Final Answer:\"\nQUOTE_PAT = \"Quote:\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/chat_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/chat_prompts.py",
      "line_number": 1,
      "code_snippet": "from onyx.prompts.constants import GENERAL_SEP_PAT\n\n# ruff: noqa: E501, W605 start\n\nDATETIME_REPLACEMENT_PAT = \"{{CURRENT_DATETIME}}\"\nCITATION_GUIDANCE_REPLACEMENT_PAT = \"{{CITATION_GUIDANCE}}\"\nALT_DATETIME_REPLACEMENT_PAT = \"[[CURRENT_DATETIME]]\"\nALT_CITATION_GUIDANCE_REPLACEMENT_PAT = \"[[CITATION_GUIDANCE]]\"\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/tool_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/tool_prompts.py",
      "line_number": 1,
      "code_snippet": "# ruff: noqa: E501, W605 start\n# If there are any tools, this section is included, the sections below are for the available tools\nTOOL_SECTION_HEADER = \"\\n\\n# Tools\\n\"\n\n\n# This section is included if there are search type tools, currently internal_search and web_search\nTOOL_DESCRIPTION_SEARCH_GUIDANCE = \"\"\"\nFor questions that can be answered from existing knowledge, answer the user directly without using any tools. If you suspect your knowledge is outdated or for topics where things are rapidly changing, use search tools to get more context.\n\nWhen using any search type tool, do not make any assumptions and stay as faithful to the user's query as possible. \\",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/prompt_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/prompt_utils.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import cast\n\nfrom langchain_core.messages import BaseMessage\n\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.prompts.chat_prompts import ADDITIONAL_INFO\nfrom onyx.prompts.chat_prompts import ALT_CITATION_GUIDANCE_REPLACEMENT_PAT\nfrom onyx.prompts.chat_prompts import ALT_DATETIME_REPLACEMENT_PAT\nfrom onyx.prompts.chat_prompts import CITATION_GUIDANCE_REPLACEMENT_PAT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/search_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/search_prompts.py",
      "line_number": 1,
      "code_snippet": "# How it works and rationale:\n# First - this works best emprically across multiple LLMs, some of this is back-explaining reasons based on results.\n#\n# The system prompt is kept simple and as similar to typical system prompts as possible to stay within training distribution.\n# The history is passed through as a list of messages, this should allow the LLM to more easily understand what is going on.\n# The special tokens and separators let the LLM more easily disregard no longer relevant past messages.\n# The last message is dynamically created and has a detailed description of the actual task.\n# This is based on the assumption that users give much more varied requests in their prompts and LLMs are well adjusted to this.\n# The proximity of the instructions and the lack of any breaks should also let the LLM follow the task more clearly.\n#",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/direct_qa_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/direct_qa_prompts.py",
      "line_number": 1,
      "code_snippet": "# The following prompts are used for the initial response before a chat history exists\n# It is used also for the one shot direct QA flow\nimport json\n\nfrom onyx.prompts.constants import DEFAULT_IGNORE_STATEMENT\nfrom onyx.prompts.constants import FINAL_QUERY_PAT\nfrom onyx.prompts.constants import GENERAL_SEP_PAT\nfrom onyx.prompts.constants import QUESTION_PAT\nfrom onyx.prompts.constants import THOUGHT_PAT\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/contextual_retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/contextual_retrieval.py",
      "line_number": 1,
      "code_snippet": "# NOTE: the prompt separation is partially done for efficiency; previously I tried\n# to do it all in one prompt with sequential format() calls but this will cause a backend\n# error when the document contains any {} as python will expect the {} to be filled by\n# format() arguments\n\n# ruff: noqa: E501, W605 start\nCONTEXTUAL_RAG_PROMPT1 = \"\"\"<document>\n{document}\n</document>\nHere is the chunk we want to situate within the whole document\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py_674",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 674 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py",
      "line_number": 674,
      "code_snippet": "\"\"\"\n\nSOURCE_DETECTION_PROMPT = f\"\"\"\nYou are an expert in generating, understanding and analyzing SQL statements.\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py_732",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 732 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py",
      "line_number": 732,
      "code_snippet": "\"\"\".strip()\n\nENTITY_SOURCE_DETECTION_PROMPT = f\"\"\"\nYou are an expert in generating, understanding and analyzing SQL statements.\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py_839",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 839 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py",
      "line_number": 839,
      "code_snippet": "\n\nSIMPLE_SQL_PROMPT = f\"\"\"\nYou are an expert in generating a SQL statement that only uses ONE TABLE that captures RELATIONSHIPS \\\nbetween TWO ENTITIES. The table has the following structure:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py_995",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 995 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py",
      "line_number": 995,
      "code_snippet": "\"\"\".strip()\n\nSIMPLE_ENTITY_SQL_PROMPT = f\"\"\"\nYou are an expert in generating a SQL statement that only uses ONE TABLE that captures ENTITIES \\\nand their attributes and other data. The table has the following structure:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/kg_prompts.py",
      "line_number": 1,
      "code_snippet": "# Standards\nSEPARATOR_LINE = \"-------\"\nSEPARATOR_LINE_LONG = \"---------------\"\nNO_EXTRACTION = \"No extraction of knowledge graph objects was feasible.\"\nYES = \"yes\"\nNO = \"no\"\n\n# Framing/Support/Template Prompts\nENTITY_TYPE_SETTING_PROMPT = f\"\"\"\n{SEPARATOR_LINE}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/prompt_template.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/prompt_template.py",
      "line_number": 1,
      "code_snippet": "import re\n\nfrom onyx.prompts.prompt_utils import replace_current_datetime_tag\n\n\nclass PromptTemplate:\n    \"\"\"\n    A class for building prompt templates with placeholders.\n    Useful when building templates with json schemas, as {} will not work with f-strings.\n    Unlike string.replace, this class will raise an error if the fields are missing.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/kg_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/kg_config.py",
      "line_number": 1,
      "code_snippet": "from onyx.configs.constants import KV_KG_CONFIG_KEY\nfrom onyx.key_value_store.factory import get_kv_store\nfrom onyx.key_value_store.interface import KvKeyNotFoundError\nfrom onyx.kg.models import KGConfigSettings\nfrom onyx.server.kg.models import EnableKGConfigRequest\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/image_generation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/image_generation.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import selectinload\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import ImageGenerationConfig\nfrom onyx.db.models import LLMProvider\nfrom onyx.db.models import ModelConfiguration\nfrom onyx.llm.utils import get_max_input_tokens\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/auth.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import AsyncGenerator\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import Dict\nfrom typing import TypeVar\n\nfrom fastapi import Depends\nfrom fastapi_users.models import ID\nfrom fastapi_users.models import UP\nfrom fastapi_users_db_sqlalchemy import SQLAlchemyUserDatabase",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/deletion_attempt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/deletion_attempt.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy.orm import Session\n\nfrom onyx.db.index_attempt import get_last_attempt\nfrom onyx.db.models import ConnectorCredentialPair\nfrom onyx.db.models import IndexingStatus\nfrom onyx.db.search_settings import get_current_search_settings\n\n\ndef check_deletion_attempt_is_allowed(\n    connector_credential_pair: ConnectorCredentialPair,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/user_preferences.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/user_preferences.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom uuid import UUID\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import delete\nfrom sqlalchemy import desc\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/pat.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/pat.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Database operations for Personal Access Tokens.\"\"\"\n\nimport asyncio\nfrom datetime import datetime\nfrom datetime import timezone\nfrom uuid import UUID\n\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.ext.asyncio import AsyncSession",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/release_notes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/release_notes.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Database functions for release notes functionality.\"\"\"\n\nfrom urllib.parse import urlencode\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.schemas import UserRole\nfrom onyx.configs.app_configs import INSTANCE_TYPE\nfrom onyx.configs.constants import DANSWER_API_KEY_DUMMY_EMAIL_DOMAIN",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/enums.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/enums.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum as PyEnum\n\n\nclass IndexingStatus(str, PyEnum):\n    NOT_STARTED = \"not_started\"\n    IN_PROGRESS = \"in_progress\"\n    SUCCESS = \"success\"\n    CANCELED = \"canceled\"\n    FAILED = \"failed\"\n    COMPLETED_WITH_ERRORS = \"completed_with_errors\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/sync_record.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/sync_record.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import and_\nfrom sqlalchemy import desc\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.enums import SyncStatus\nfrom onyx.db.enums import SyncType\nfrom onyx.db.models import SyncRecord",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tasks.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nfrom sqlalchemy import desc\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.sql import delete\n\nfrom onyx.configs.app_configs import JOB_TIMEOUT\nfrom onyx.db.engine.time_utils import get_db_current_time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/connector_credential_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/connector_credential_pair.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\nfrom typing import TypeVarTuple\n\nfrom fastapi import HTTPException\nfrom sqlalchemy import delete\nfrom sqlalchemy import desc\nfrom sqlalchemy import exists\nfrom sqlalchemy import Select\nfrom sqlalchemy import select",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/models.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport json\nfrom typing import Any\nfrom typing import Literal\nfrom typing import NotRequired\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import validates\nfrom typing_extensions import TypedDict  # noreorder",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/token_limit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/token_limit.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import TokenRateLimitScope\nfrom onyx.db.models import TokenRateLimit\nfrom onyx.db.models import TokenRateLimit__UserGroup\nfrom onyx.server.token_rate_limits.models import TokenRateLimitArgs\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py_227_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_generate_slack_user'",
      "description": "Function '_generate_slack_user' on line 227 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
      "line_number": 227,
      "code_snippet": "\n\ndef _generate_slack_user(email: str) -> User:\n    fastapi_users_pw_helper = PasswordHelper()\n    password = fastapi_users_pw_helper.generate()\n    hashed_pass = fastapi_users_pw_helper.hash(password)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py_273_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_generate_ext_permissioned_user'",
      "description": "Function '_generate_ext_permissioned_user' on line 273 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
      "line_number": 273,
      "code_snippet": "\n\ndef _generate_ext_permissioned_user(email: str) -> User:\n    fastapi_users_pw_helper = PasswordHelper()\n    password = fastapi_users_pw_helper.generate()\n    hashed_pass = fastapi_users_pw_helper.hash(password)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py_227_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in '_generate_slack_user'",
      "description": "API endpoint '_generate_slack_user' on line 227 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
      "line_number": 227,
      "code_snippet": "\n\ndef _generate_slack_user(email: str) -> User:\n    fastapi_users_pw_helper = PasswordHelper()\n    password = fastapi_users_pw_helper.generate()\n    hashed_pass = fastapi_users_pw_helper.hash(password)",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py_273_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in '_generate_ext_permissioned_user'",
      "description": "API endpoint '_generate_ext_permissioned_user' on line 273 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
      "line_number": 273,
      "code_snippet": "\n\ndef _generate_ext_permissioned_user(email: str) -> User:\n    fastapi_users_pw_helper = PasswordHelper()\n    password = fastapi_users_pw_helper.generate()\n    hashed_pass = fastapi_users_pw_helper.hash(password)",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py_229",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
      "line_number": 229,
      "code_snippet": "\ndef _generate_slack_user(email: str) -> User:\n    fastapi_users_pw_helper = PasswordHelper()\n    password = fastapi_users_pw_helper.generate()\n    hashed_pass = fastapi_users_pw_helper.hash(password)\n    return User(\n        email=email,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py_275",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/users.py",
      "line_number": 275,
      "code_snippet": "\ndef _generate_ext_permissioned_user(email: str) -> User:\n    fastapi_users_pw_helper = PasswordHelper()\n    password = fastapi_users_pw_helper.generate()\n    hashed_pass = fastapi_users_pw_helper.hash(password)\n    return User(\n        email=email,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py_405_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'credential' containing sensitive data is being logged on line 405. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py",
      "line_number": 405,
      "code_snippet": "\n    if force:\n        logger.warning(f\"Force deleting credential {credential_id}\")\n    else:\n        logger.notice(f\"Deleting credential {credential_id}\")",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py_407_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'credential' containing sensitive data is being logged on line 407. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py",
      "line_number": 407,
      "code_snippet": "        logger.warning(f\"Force deleting credential {credential_id}\")\n    else:\n        logger.notice(f\"Deleting credential {credential_id}\")\n\n    _cleanup_credential__user_group_relationships__no_commit(db_session, credential_id)",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/credentials.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom sqlalchemy import exists\nfrom sqlalchemy import Select\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.sql.expression import and_\nfrom sqlalchemy.sql.expression import or_\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/oauth_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/oauth_config.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom uuid import UUID\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import OAuthConfig\nfrom onyx.db.models import OAuthUserToken\nfrom onyx.db.models import Tool\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/swap_index.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/swap_index.py",
      "line_number": 1,
      "code_snippet": "import time\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import VESPA_NUM_ATTEMPTS_ON_STARTUP\nfrom onyx.configs.constants import KV_REINDEX_KEY\nfrom onyx.db.connector_credential_pair import get_connector_credential_pairs\nfrom onyx.db.connector_credential_pair import resync_cc_pair\nfrom onyx.db.document import delete_all_documents_for_connector_credential_pair\nfrom onyx.db.enums import ConnectorCredentialPairStatus",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/index_attempt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/index_attempt.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import TypeVarTuple\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import delete\nfrom sqlalchemy import desc\nfrom sqlalchemy import func",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py_267",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'remove_chat_message_feedback' on line 267 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py",
      "line_number": 267,
      "code_snippet": "def remove_chat_message_feedback(\n    chat_message_id: int,\n    user_id: UUID | None,\n    db_session: Session,\n) -> None:\n    \"\"\"Remove all feedback for a chat message.\"\"\"\n    chat_message = get_chat_message(\n        chat_message_id=chat_message_id, user_id=user_id, db_session=db_session\n    )\n\n    if chat_message.message_type != MessageType.ASSISTANT:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py_267_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/network operation without confirmation in 'remove_chat_message_feedback'",
      "description": "Function 'remove_chat_message_feedback' on line 267 performs high-risk delete/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py",
      "line_number": 267,
      "code_snippet": "\n\ndef remove_chat_message_feedback(\n    chat_message_id: int,\n    user_id: UUID | None,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py_267_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'remove_chat_message_feedback'",
      "description": "Function 'remove_chat_message_feedback' on line 267 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py",
      "line_number": 267,
      "code_snippet": "\n\ndef remove_chat_message_feedback(\n    chat_message_id: int,\n    user_id: UUID | None,\n    db_session: Session,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py_281",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/feedback.py",
      "line_number": 281,
      "code_snippet": "        raise ValueError(\"Can only remove feedback from LLM Outputs\")\n\n    # Delete all feedback for this message\n    db_session.query(ChatMessageFeedback).filter(\n        ChatMessageFeedback.chat_message_id == chat_message_id\n    ).delete()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/notification.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/notification.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom uuid import UUID\n\nfrom sqlalchemy import cast\nfrom sqlalchemy import select\nfrom sqlalchemy.dialects import postgresql\nfrom sqlalchemy.dialects.postgresql import insert\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.sql import func",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tools.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom uuid import UUID\n\nfrom sqlalchemy import func\nfrom sqlalchemy import or_\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/saml.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/saml.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom typing import cast\nfrom uuid import UUID\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import selectinload\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/constants.py",
      "line_number": 1,
      "code_snippet": "SLACK_BOT_PERSONA_PREFIX = \"__slack_bot_persona__\"\nDEFAULT_PERSONA_SLACK_CHANNEL_NAME = \"DEFAULT_SLACK_CHANNEL\"\n\nCONNECTOR_VALIDATION_ERROR_MESSAGE_PREFIX = \"ConnectorValidationError:\"\n\n\n# Sentinel value to distinguish between \"not provided\" and \"explicitly set to None\"\nclass UnsetType:\n    def __repr__(self) -> str:\n        return \"<UNSET>\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/pydantic_type.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/pydantic_type.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\nfrom typing import Optional\nfrom typing import Type\n\nfrom pydantic import BaseModel\nfrom sqlalchemy.dialects.postgresql import JSONB\nfrom sqlalchemy.types import TypeDecorator\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py_304",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'sync_model_configurations' on line 304 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
      "line_number": 304,
      "code_snippet": "def sync_model_configurations(\n    db_session: Session,\n    provider_name: str,\n    models: list[dict],\n) -> int:\n    \"\"\"Sync model configurations for a dynamic provider (OpenRouter, Bedrock, Ollama).\n\n    This inserts NEW models from the source API without overwriting existing ones.\n    User preferences (is_visible, max_input_tokens) are preserved for existing models.\n\n    Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py_304_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'sync_model_configurations'",
      "description": "Function 'sync_model_configurations' on line 304 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
      "line_number": 304,
      "code_snippet": "\n\ndef sync_model_configurations(\n    db_session: Session,\n    provider_name: str,\n    models: list[dict],",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py_340",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
      "line_number": 340,
      "code_snippet": "                    llm_provider_id=provider.id,\n                    name=model_name,\n                    is_visible=False,\n                    max_input_tokens=model.get(\"max_input_tokens\"),\n                    supports_image_input=model.get(\"supports_image_input\", False),\n                    display_name=model.get(\"display_name\"),\n                )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py_341",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
      "line_number": 341,
      "code_snippet": "                    name=model_name,\n                    is_visible=False,\n                    max_input_tokens=model.get(\"max_input_tokens\"),\n                    supports_image_input=model.get(\"supports_image_input\", False),\n                    display_name=model.get(\"display_name\"),\n                )\n                .on_conflict_do_nothing()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py_342",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/llm.py",
      "line_number": 342,
      "code_snippet": "                    is_visible=False,\n                    max_input_tokens=model.get(\"max_input_tokens\"),\n                    supports_image_input=model.get(\"supports_image_input\", False),\n                    display_name=model.get(\"display_name\"),\n                )\n                .on_conflict_do_nothing()\n            )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/persona.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/persona.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom datetime import datetime\nfrom enum import Enum\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom sqlalchemy import exists\nfrom sqlalchemy import func\nfrom sqlalchemy import not_\nfrom sqlalchemy import or_",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chunk.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chunk.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\n\nfrom sqlalchemy import delete\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import ChunkStats\nfrom onyx.indexing.models import UpdatableChunkData\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py_67",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'insert_api_key' on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py",
      "line_number": 67,
      "code_snippet": "def insert_api_key(\n    db_session: Session, api_key_args: APIKeyArgs, user_id: uuid.UUID | None\n) -> ApiKeyDescriptor:\n    std_password_helper = PasswordHelper()\n\n    # Get tenant_id from context var (will be default schema for single tenant)\n    tenant_id = get_current_tenant_id()\n\n    api_key = generate_api_key(tenant_id)\n    api_key_user_id = uuid.uuid4()\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py_67_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'insert_api_key'",
      "description": "Function 'insert_api_key' on line 67 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py",
      "line_number": 67,
      "code_snippet": "\n\ndef insert_api_key(\n    db_session: Session, api_key_args: APIKeyArgs, user_id: uuid.UUID | None\n) -> ApiKeyDescriptor:\n    std_password_helper = PasswordHelper()",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py_67_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'insert_api_key'",
      "description": "Function 'insert_api_key' on line 67 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py",
      "line_number": 67,
      "code_snippet": "\n\ndef insert_api_key(\n    db_session: Session, api_key_args: APIKeyArgs, user_id: uuid.UUID | None\n) -> ApiKeyDescriptor:\n    std_password_helper = PasswordHelper()",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py_83",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/api_key.py",
      "line_number": 83,
      "code_snippet": "        id=api_key_user_id,\n        email=get_api_key_fake_email(display_name, str(api_key_user_id)),\n        # a random password for the \"user\"\n        hashed_password=std_password_helper.hash(std_password_helper.generate()),\n        is_active=True,\n        is_superuser=False,\n        is_verified=True,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/entity_type.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/entity_type.py",
      "line_number": 1,
      "code_snippet": "from collections import defaultdict\n\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.connector import fetch_unique_document_sources\nfrom onyx.db.document import DocumentSource\nfrom onyx.db.models import Connector\nfrom onyx.db.models import KGEntityType\nfrom onyx.kg.models import KGAttributeEntityOption",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_148",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'ChatMessage__SearchDoc.chat_message_id.is_' is used in 'DELETE' on line 148 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 148,
      "code_snippet": "        .outerjoin(ChatMessage__SearchDoc)\n        .filter(ChatMessage__SearchDoc.chat_message_id.is_(None))\n        .all()\n    )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_422",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'ChatMessage.chat_session_id.in_' is used in 'execute(' on line 422 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 422,
      "code_snippet": "        select(ChatMessage)\n        .where(ChatMessage.chat_session_id.in_(chat_session_ids))\n        .order_by(nullsfirst(ChatMessage.parent_message_id))\n    )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_409",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_chat_messages_by_sessions' on line 409 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 409,
      "code_snippet": "def get_chat_messages_by_sessions(\n    chat_session_ids: list[UUID],\n    user_id: UUID | None,\n    db_session: Session,\n    skip_permission_check: bool = False,\n) -> Sequence[ChatMessage]:\n    if not skip_permission_check:\n        for chat_session_id in chat_session_ids:\n            get_chat_session_by_id(\n                chat_session_id=chat_session_id, user_id=user_id, db_session=db_session\n            )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_498",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_chat_messages_by_session' on line 498 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 498,
      "code_snippet": "def get_chat_messages_by_session(\n    chat_session_id: UUID,\n    user_id: UUID | None,\n    db_session: Session,\n    skip_permission_check: bool = False,\n    prefetch_top_two_level_tool_calls: bool = True,\n) -> list[ChatMessage]:\n    if not skip_permission_check:\n        # bug if we ever call this expecting the permission check to not be skipped\n        get_chat_session_by_id(\n            chat_session_id=chat_session_id, user_id=user_id, db_session=db_session",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_952",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'update_db_session_with_messages' on line 952 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 952,
      "code_snippet": "def update_db_session_with_messages(\n    db_session: Session,\n    chat_message_id: int,\n    chat_session_id: UUID,\n    message: str | None = None,\n    message_type: str | None = None,\n    token_count: int | None = None,\n    error: str | None = None,\n    update_parent_message: bool = True,\n    files: list[FileDescriptor] | None = None,\n    reasoning_tokens: str | None = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_96_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute operation without confirmation in 'get_chat_sessions_by_user'",
      "description": "Function 'get_chat_sessions_by_user' on line 96 performs high-risk delete/write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 96,
      "code_snippet": "# Retrieves chat sessions by user\n# Chat sessions do not include onyxbot flows\ndef get_chat_sessions_by_user(\n    user_id: UUID | None,\n    deleted: bool | None,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_144_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'delete_orphaned_search_docs'",
      "description": "Function 'delete_orphaned_search_docs' on line 144 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 144,
      "code_snippet": "\n\ndef delete_orphaned_search_docs(db_session: Session) -> None:\n    orphaned_docs = (\n        db_session.query(DBSearchDoc)\n        .outerjoin(ChatMessage__SearchDoc)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_156_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/execute/network operation without confirmation in 'delete_messages_and_files_from_chat_session'",
      "description": "Function 'delete_messages_and_files_from_chat_session' on line 156 performs high-risk delete/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 156,
      "code_snippet": "\n\ndef delete_messages_and_files_from_chat_session(\n    chat_session_id: UUID, db_session: Session\n) -> None:\n    # Select messages older than cutoff_time with files",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_409_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'get_chat_messages_by_sessions'",
      "description": "Function 'get_chat_messages_by_sessions' on line 409 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 409,
      "code_snippet": "\n\ndef get_chat_messages_by_sessions(\n    chat_session_ids: list[UUID],\n    user_id: UUID | None,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_498_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'get_chat_messages_by_session'",
      "description": "Function 'get_chat_messages_by_session' on line 498 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 498,
      "code_snippet": "\n\ndef get_chat_messages_by_session(\n    chat_session_id: UUID,\n    user_id: UUID | None,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_533_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'get_or_create_root_message'",
      "description": "Function 'get_or_create_root_message' on line 533 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 533,
      "code_snippet": "\n\ndef get_or_create_root_message(\n    chat_session_id: UUID,\n    db_session: Session,\n) -> ChatMessage:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_952_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'update_db_session_with_messages'",
      "description": "Function 'update_db_session_with_messages' on line 952 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 952,
      "code_snippet": "\n\ndef update_db_session_with_messages(\n    db_session: Session,\n    chat_message_id: int,\n    chat_session_id: UUID,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_96_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_chat_sessions_by_user'",
      "description": "Function 'get_chat_sessions_by_user' on line 96 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 96,
      "code_snippet": "# Retrieves chat sessions by user\n# Chat sessions do not include onyxbot flows\ndef get_chat_sessions_by_user(\n    user_id: UUID | None,\n    deleted: bool | None,\n    db_session: Session,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_144_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_orphaned_search_docs'",
      "description": "Function 'delete_orphaned_search_docs' on line 144 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 144,
      "code_snippet": "\n\ndef delete_orphaned_search_docs(db_session: Session) -> None:\n    orphaned_docs = (\n        db_session.query(DBSearchDoc)\n        .outerjoin(ChatMessage__SearchDoc)",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_156_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_messages_and_files_from_chat_session'",
      "description": "Function 'delete_messages_and_files_from_chat_session' on line 156 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 156,
      "code_snippet": "\n\ndef delete_messages_and_files_from_chat_session(\n    chat_session_id: UUID, db_session: Session\n) -> None:\n    # Select messages older than cutoff_time with files",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_952_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'update_db_session_with_messages'",
      "description": "Function 'update_db_session_with_messages' on line 952 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 952,
      "code_snippet": "\n\ndef update_db_session_with_messages(\n    db_session: Session,\n    chat_message_id: int,\n    chat_session_id: UUID,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_533_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'get_or_create_root_message'",
      "description": "Function 'get_or_create_root_message' on line 533 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 533,
      "code_snippet": "\n\ndef get_or_create_root_message(\n    chat_session_id: UUID,\n    db_session: Session,\n) -> ChatMessage:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_146",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 146,
      "code_snippet": "\ndef delete_orphaned_search_docs(db_session: Session) -> None:\n    orphaned_docs = (\n        db_session.query(DBSearchDoc)\n        .outerjoin(ChatMessage__SearchDoc)\n        .filter(ChatMessage__SearchDoc.chat_message_id.is_(None))\n        .all()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_421",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 421,
      "code_snippet": "                chat_session_id=chat_session_id, user_id=user_id, db_session=db_session\n            )\n    stmt = (\n        select(ChatMessage)\n        .where(ChatMessage.chat_session_id.in_(chat_session_ids))\n        .order_by(nullsfirst(ChatMessage.parent_message_id))\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_512",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 512,
      "code_snippet": "        )\n\n    stmt = (\n        select(ChatMessage)\n        .where(ChatMessage.chat_session_id == chat_session_id)\n        .order_by(nullsfirst(ChatMessage.parent_message_id))\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_966",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 966,
      "code_snippet": "    commit: bool = False,\n) -> ChatMessage:\n    chat_message = (\n        db_session.query(ChatMessage)\n        .filter(\n            ChatMessage.id == chat_message_id,\n            ChatMessage.chat_session_id == chat_session_id,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_126",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 126,
      "code_snippet": "\n    if not include_failed_chats:\n        non_system_message_exists_subq = (\n            exists()\n            .where(ChatMessage.chat_session_id == ChatSession.id)\n            .where(ChatMessage.message_type != MessageType.SYSTEM)\n            .correlate(ChatSession)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_539",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 539,
      "code_snippet": ") -> ChatMessage:\n    try:\n        root_message: ChatMessage | None = (\n            db_session.query(ChatMessage)\n            .filter(\n                ChatMessage.chat_session_id == chat_session_id,\n                ChatMessage.parent_message_id.is_(None),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_148",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 148,
      "code_snippet": "    orphaned_docs = (\n        db_session.query(DBSearchDoc)\n        .outerjoin(ChatMessage__SearchDoc)\n        .filter(ChatMessage__SearchDoc.chat_message_id.is_(None))\n        .all()\n    )\n    for doc in orphaned_docs:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py_952",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat.py",
      "line_number": 952,
      "code_snippet": "        is_relevant=saved_search_doc.is_relevant,\n        relevance_explanation=saved_search_doc.relevance_explanation,\n    )\n\n\ndef update_db_session_with_messages(\n    db_session: Session,\n    chat_message_id: int,\n    chat_session_id: UUID,\n    message: str | None = None,\n    message_type: str | None = None,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/indexing_coordination.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/indexing_coordination.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Database-based indexing coordination to replace Redis fencing.\"\"\"\n\nfrom pydantic import BaseModel\nfrom sqlalchemy import select\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.engine.time_utils import get_db_current_time\nfrom onyx.db.enums import IndexingStatus\nfrom onyx.db.index_attempt import count_error_rows_for_index_attempt",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py_18_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'search_chat_sessions'",
      "description": "Function 'search_chat_sessions' on line 18 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py",
      "line_number": 18,
      "code_snippet": "\n\ndef search_chat_sessions(\n    user_id: UUID | None,\n    db_session: Session,\n    query: Optional[str] = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py_18_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'search_chat_sessions'",
      "description": "Function 'search_chat_sessions' on line 18 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py",
      "line_number": 18,
      "code_snippet": "\n\ndef search_chat_sessions(\n    user_id: UUID | None,\n    db_session: Session,\n    query: Optional[str] = None,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py_85",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py",
      "line_number": 85,
      "code_snippet": "    )\n\n    message_session_ids = (\n        select(ChatMessage.chat_session_id)\n        .join(ChatSession, ChatMessage.chat_session_id == ChatSession.id)\n        .where(*base_conditions)\n        .where(message_tsv.op(\"@@\")(ts_query))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py_18",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/chat_search.py",
      "line_number": 18,
      "code_snippet": "\nfrom onyx.db.models import ChatMessage\nfrom onyx.db.models import ChatSession\n\n\ndef search_chat_sessions(\n    user_id: UUID | None,\n    db_session: Session,\n    query: Optional[str] = None,\n    page: int = 1,\n    page_size: int = 10,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/mcp.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/mcp.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom typing import cast\nfrom uuid import UUID\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import delete\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.orm.attributes import flag_modified\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/search_settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/search_settings.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import and_\nfrom sqlalchemy import delete\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.model_configs import DEFAULT_DOCUMENT_ENCODER_MODEL\nfrom onyx.configs.model_configs import DOCUMENT_ENCODER_MODEL\nfrom onyx.context.search.models import SavedSearchSettings\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.llm import fetch_embedding_provider",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/utils.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Any\n\nfrom psycopg2 import errorcodes\nfrom psycopg2 import OperationalError\nfrom pydantic import BaseModel\nfrom sqlalchemy import inspect\n\nfrom onyx.db.models import Base\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/document_set.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/document_set.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom typing import cast\nfrom uuid import UUID\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import delete\nfrom sqlalchemy import exists\nfrom sqlalchemy import func\nfrom sqlalchemy import or_\nfrom sqlalchemy import Select",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/federated.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/federated.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import Any\nfrom uuid import UUID\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.orm import selectinload\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import FederatedConnectorSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/document.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/document.py",
      "line_number": 1,
      "code_snippet": "import contextlib\nimport time\nfrom collections.abc import Generator\nfrom collections.abc import Iterable\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\n\nfrom sqlalchemy import and_",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/connector.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import cast\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import exists\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import aliased\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/slack_channel_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/slack_channel_config.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom typing import Any\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.chat_configs import MAX_CHUNKS_FED_TO_CHAT\nfrom onyx.context.search.enums import RecencyBiasSetting\nfrom onyx.db.constants import DEFAULT_PERSONA_SLACK_CHANNEL_NAME",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/permission_sync_attempt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/permission_sync_attempt.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Permission sync attempt CRUD operations and utilities.\n\nThis module contains all CRUD operations for both DocPermissionSyncAttempt\nand ExternalGroupPermissionSyncAttempt models, along with shared utilities.\n\"\"\"\n\nfrom typing import Any\nfrom typing import cast\n\nfrom sqlalchemy import delete",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/entities.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/entities.py",
      "line_number": 1,
      "code_snippet": "import uuid\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nfrom sqlalchemy import func\nfrom sqlalchemy import literal\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/input_prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/input_prompt.py",
      "line_number": 1,
      "code_snippet": "from uuid import UUID\n\nfrom fastapi import HTTPException\nfrom sqlalchemy import or_\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import aliased\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import AUTH_TYPE\nfrom onyx.configs.constants import AuthType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/projects.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/projects.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport uuid\nfrom typing import List\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom fastapi import UploadFile\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom sqlalchemy import func",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/web_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/web_search.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import InternetContentProvider\nfrom onyx.db.models import InternetSearchProvider\nfrom shared_configs.enums import WebContentProviderType\nfrom shared_configs.enums import WebSearchProviderType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/usage.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/usage.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Database interactions for tenant usage tracking (cloud usage limits).\"\"\"\n\nfrom datetime import datetime\nfrom datetime import timezone\nfrom enum import Enum\n\nfrom pydantic import BaseModel\nfrom sqlalchemy import select\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tag.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/tag.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import delete\nfrom sqlalchemy import or_\nfrom sqlalchemy import select\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/relationships.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/relationships.py",
      "line_number": 1,
      "code_snippet": "from typing import List\n\nfrom sqlalchemy import or_\nfrom sqlalchemy.dialects import postgresql\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\nfrom sqlalchemy.orm import Session\n\nimport onyx.db.document as dbdocument\nfrom onyx.db.models import KGEntity\nfrom onyx.db.models import KGEntityExtractionStaging",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/user_file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/user_file.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom uuid import UUID\n\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import UserFile\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/slack_bot.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/slack_bot.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import SlackBot\n\n\ndef insert_slack_bot(\n    db_session: Session,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/key_value_store/store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/key_value_store/store.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import cast\n\nfrom redis.client import Redis\n\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.models import KVStore\nfrom onyx.key_value_store.interface import KeyValueStore\nfrom onyx.key_value_store.interface import KvKeyNotFoundError\nfrom onyx.redis.redis_pool import get_redis_client",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/models.py",
      "line_number": 1,
      "code_snippet": "import contextlib\nfrom collections.abc import Generator\nfrom typing import Optional\nfrom typing import Protocol\nfrom typing import TYPE_CHECKING\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\nfrom onyx.access.models import DocumentAccess",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/chunker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/chunker.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom chonkie import SentenceChunker\n\nfrom onyx.configs.app_configs import AVERAGE_SUMMARY_EMBEDDINGS\nfrom onyx.configs.app_configs import BLURB_SIZE\nfrom onyx.configs.app_configs import LARGE_CHUNK_RATIO\nfrom onyx.configs.app_configs import MINI_CHUNK_SIZE\nfrom onyx.configs.app_configs import SKIP_METADATA_IN_CHUNK\nfrom onyx.configs.app_configs import USE_CHUNK_SUMMARY",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py_173",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'title_embeddings' flows to 'title_embed_dict.update' on line 173 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py",
      "line_number": 173,
      "code_snippet": "            )\n            title_embed_dict.update(\n                {\n                    title: vector",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py_113_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'embed_chunks'",
      "description": "Function 'embed_chunks' on line 113 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py",
      "line_number": 113,
      "code_snippet": "\n    @log_function_time()\n    def embed_chunks(\n        self,\n        chunks: list[DocAwareChunk],\n        tenant_id: str | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py_113_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'embed_chunks'",
      "description": "Function 'embed_chunks' on line 113 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py",
      "line_number": 113,
      "code_snippet": "\n    @log_function_time()\n    def embed_chunks(\n        self,\n        chunks: list[DocAwareChunk],\n        tenant_id: str | None = None,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/embedder.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom collections import defaultdict\n\nfrom onyx.connectors.models import ConnectorFailure\nfrom onyx.connectors.models import ConnectorStopSignal\nfrom onyx.connectors.models import DocumentFailure\nfrom onyx.db.models import SearchSettings\nfrom onyx.indexing.indexing_heartbeat import IndexingHeartbeatInterface",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py_469_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'add_document_summaries'",
      "description": "Function 'add_document_summaries' on line 469 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py",
      "line_number": 469,
      "code_snippet": "\n\ndef add_document_summaries(\n    chunks_by_doc: list[DocAwareChunk],\n    llm: LLM,\n    tokenizer: BaseTokenizer,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py_495",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py",
      "line_number": 495,
      "code_snippet": "    # So we just pass the full prompt without caching\n    summary_prompt = DOCUMENT_SUMMARY_PROMPT.format(document=doc_content)\n    doc_summary = llm_response_to_string(\n        llm.invoke(summary_prompt, max_tokens=MAX_CONTEXT_TOKENS)\n    )\n\n    for chunk in chunks_by_doc:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py_536",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py",
      "line_number": 536,
      "code_snippet": "        # This happens if the document is too long AND document summaries are turned off\n        # In this case we compute a doc summary using the LLM\n        doc_info = llm_response_to_string(\n            llm.invoke(\n                DOCUMENT_SUMMARY_PROMPT.format(document=doc_content),\n                max_tokens=MAX_CONTEXT_TOKENS,\n            )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py_559",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/indexing_pipeline.py",
      "line_number": 559,
      "code_snippet": "            )\n\n            chunk.chunk_context = llm_response_to_string(\n                llm.invoke(\n                    processed_prompt,\n                    max_tokens=MAX_CONTEXT_TOKENS,\n                )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/dr_loop.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/dr_loop.py",
      "line_number": 1,
      "code_snippet": "# TODO: Notes for potential extensions and future improvements:\n# 1. Allow tools that aren't search specific tools\n# 2. Use user provided custom prompts\n# 3. Save the plan for replay\n\nfrom collections.abc import Callable\nfrom typing import cast\n\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/utils.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom onyx.deep_research.dr_mock_tools import GENERATE_REPORT_TOOL_NAME\nfrom onyx.deep_research.dr_mock_tools import THINK_TOOL_NAME\nfrom onyx.deep_research.models import SpecialToolCalls\nfrom onyx.llm.model_response import ChatCompletionDeltaToolCall\nfrom onyx.llm.model_response import Delta",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/dr_mock_tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/deep_research/dr_mock_tools.py",
      "line_number": 1,
      "code_snippet": "GENERATE_PLAN_TOOL_NAME = \"generate_plan\"\n\nRESEARCH_AGENT_IN_CODE_ID = \"ResearchAgent\"\nRESEARCH_AGENT_TOOL_NAME = \"research_agent\"\nRESEARCH_AGENT_TASK_KEY = \"task\"\n\nGENERATE_REPORT_TOOL_NAME = \"generate_report\"\n\nTHINK_TOOL_NAME = \"think_tool\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/api.py",
      "line_number": 1,
      "code_snippet": "\"\"\"MCP server with FastAPI wrapper.\"\"\"\n\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import FastAPI\nfrom fastapi import Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom fastapi.responses import Response",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utility helpers for the Onyx MCP server.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\n\nimport httpx\nfrom fastmcp.server.auth.auth import AccessToken\nfrom fastmcp.server.dependencies import get_access_token\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/tools/search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/mcp_server/tools/search.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Search tools for MCP server - document and web search.\"\"\"\n\nfrom typing import Any\n\nfrom onyx.mcp_server.api import mcp_server\nfrom onyx.mcp_server.utils import get_api_server_url\nfrom onyx.mcp_server.utils import get_http_client\nfrom onyx.mcp_server.utils import require_access_token\nfrom onyx.utils.logger import setup_logger\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/adapters/user_file_indexing_adapter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/adapters/user_file_indexing_adapter.py",
      "line_number": 1,
      "code_snippet": "import contextlib\nimport datetime\nimport time\nfrom collections.abc import Generator\nfrom uuid import UUID\n\nfrom sqlalchemy import select\nfrom sqlalchemy.exc import OperationalError\nfrom sqlalchemy.orm import selectinload\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/adapters/document_indexing_adapter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/indexing/adapters/document_indexing_adapter.py",
      "line_number": 1,
      "code_snippet": "import contextlib\nfrom collections.abc import Generator\n\nfrom sqlalchemy.engine.util import TransactionalContext\nfrom sqlalchemy.orm import Session\n\nfrom onyx.access.access import get_access_for_documents\nfrom onyx.access.models import DocumentAccess\nfrom onyx.configs.constants import DEFAULT_BOOST\nfrom onyx.connectors.models import Document",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/seeding/chat_history_seeding.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/seeding/chat_history_seeding.py",
      "line_number": 1,
      "code_snippet": "import random\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom logging import getLogger\n\nfrom onyx.configs.constants import MessageType\nfrom onyx.db.chat import create_chat_session\nfrom onyx.db.chat import create_new_chat_message\nfrom onyx.db.chat import get_or_create_root_message\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/connection_warmup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/connection_warmup.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import text\n\nfrom onyx.db.engine.async_sql_engine import get_sqlalchemy_async_engine\nfrom onyx.db.engine.sql_engine import get_sqlalchemy_engine\n\n\nasync def warm_up_connections(\n    sync_connections_to_warm_up: int = 20, async_connections_to_warm_up: int = 20\n) -> None:\n    sync_postgres_engine = get_sqlalchemy_engine()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/async_sql_engine.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/async_sql_engine.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom typing import AsyncContextManager\n\nimport asyncpg  # type: ignore\nfrom fastapi import HTTPException\nfrom sqlalchemy import event\nfrom sqlalchemy import pool\nfrom sqlalchemy.ext.asyncio import AsyncEngine",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/iam_auth.py_14",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_iam_auth_token' on line 14 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/iam_auth.py",
      "line_number": 14,
      "code_snippet": "def get_iam_auth_token(\n    host: str, port: str, user: str, region: str = \"us-east-2\"\n) -> str:\n    \"\"\"\n    Generate an IAM authentication token using boto3.\n    \"\"\"\n    client = boto3.client(\"rds\", region_name=region)\n    token = client.generate_db_auth_token(\n        DBHostname=host, Port=int(port), DBUsername=user\n    )\n    return token",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/iam_auth.py_21",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/iam_auth.py",
      "line_number": 21,
      "code_snippet": "    Generate an IAM authentication token using boto3.\n    \"\"\"\n    client = boto3.client(\"rds\", region_name=region)\n    token = client.generate_db_auth_token(\n        DBHostname=host, Port=int(port), DBUsername=user\n    )\n    return token",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/sql_engine.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/engine/sql_engine.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nimport threading\nimport time\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom typing import Any\n\nfrom fastapi import HTTPException\nfrom sqlalchemy import event",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/_deprecated/pg_file_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/db/_deprecated/pg_file_store.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Kept around since it's used in the migration to move to S3/MinIO\"\"\"\n\nimport tempfile\nfrom io import BytesIO\nfrom typing import IO\n\nfrom psycopg2.extensions import connection\nfrom sqlalchemy import text  # NEW: for SQL large-object helpers\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/dr_tool_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/dr_tool_prompts.py",
      "line_number": 1,
      "code_snippet": "GENERATE_PLAN_TOOL_NAME = \"generate_plan\"\n\n\nGENERATE_REPORT_TOOL_NAME = \"generate_report\"\n\n\nRESEARCH_AGENT_TOOL_NAME = \"research_agent\"\n\n\n# This is to ensure that even the non-reasoning models can have an ok time with this more complex flow.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/research_agent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/research_agent.py",
      "line_number": 1,
      "code_snippet": "from onyx.prompts.deep_research.dr_tool_prompts import GENERATE_REPORT_TOOL_NAME\nfrom onyx.prompts.deep_research.dr_tool_prompts import THINK_TOOL_NAME\n\n\nMAX_RESEARCH_CYCLES = 3\n\n# ruff: noqa: E501, W605 start\nRESEARCH_AGENT_PROMPT = f\"\"\"\nYou are a highly capable, thoughtful, and precise research agent that conducts research on a specific topic. Prefer being thorough in research over being helpful. Be curious but stay strictly on topic. \\\nYou iteratively call the tools available to you including {{available_tools}} until you have completed your research at which point you call the {GENERATE_REPORT_TOOL_NAME} tool.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/orchestration_layer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/prompts/deep_research/orchestration_layer.py",
      "line_number": 1,
      "code_snippet": "from onyx.prompts.deep_research.dr_tool_prompts import GENERATE_PLAN_TOOL_NAME\nfrom onyx.prompts.deep_research.dr_tool_prompts import GENERATE_REPORT_TOOL_NAME\nfrom onyx.prompts.deep_research.dr_tool_prompts import RESEARCH_AGENT_TOOL_NAME\nfrom onyx.prompts.deep_research.dr_tool_prompts import THINK_TOOL_NAME\n\n\n# ruff: noqa: E501, W605 start\nCLARIFICATION_PROMPT = f\"\"\"\nYou are a clarification agent that runs prior to deep research. Assess whether you need to ask clarifying questions, or if the user has already provided enough information for you to start research. \\\nCRITICAL - Never directly answer the user's query, you must only ask clarifying questions or call the `{GENERATE_PLAN_TOOL_NAME}` tool.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/vespa/vespa_interactions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/vespa/vespa_interactions.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom collections.abc import Generator\n\nfrom onyx.document_index.vespa.chunk_retrieval import get_chunks_via_visit_api\nfrom onyx.document_index.vespa.chunk_retrieval import VespaChunkRequest\nfrom onyx.document_index.vespa.index import IndexFilters\nfrom onyx.kg.models import KGChunkFormat\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/setup/kg_default_entity_definitions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/setup/kg_default_entity_definitions.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.db.entity_type import KGEntityType\nfrom onyx.db.kg_config import get_kg_config_settings\nfrom onyx.db.kg_config import validate_kg_settings\nfrom onyx.kg.models import KGAttributeEntityOption\nfrom onyx.kg.models import KGAttributeImplicationProperty",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/clustering/clustering.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/clustering/clustering.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Generator\nfrom typing import cast\n\nfrom rapidfuzz.fuzz import ratio\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy import func\nfrom sqlalchemy import text\n\nfrom onyx.background.celery.tasks.kg_processing.utils import extend_lock",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/clustering/normalizations.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/clustering/normalizations.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections import defaultdict\nfrom typing import cast\n\nimport numpy as np\nfrom rapidfuzz.distance.DamerauLevenshtein import normalized_similarity\nfrom sqlalchemy import desc\nfrom sqlalchemy import Float\nfrom sqlalchemy import func\nfrom sqlalchemy import MetaData",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py_371",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'chunk_batch_results' flows to 'result.deep_extracted_entities.update' on line 371 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
      "line_number": 371,
      "code_snippet": "        if chunk_batch_results is not None:\n            result.deep_extracted_entities.update(\n                chunk_batch_results.deep_extracted_entities\n            )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py_374",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'chunk_batch_results' flows to 'result.deep_extracted_relationships.update' on line 374 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
      "line_number": 374,
      "code_snippet": "            )\n            result.deep_extracted_relationships.update(\n                chunk_batch_results.deep_extracted_relationships\n            )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py_496",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'parsed_result' flows to 'KGDocumentDeepExtractionResults' on line 496 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
      "line_number": 496,
      "code_snippet": "        parsed_result = json.loads(cleaned_response)\n        return KGDocumentDeepExtractionResults(\n            classification_result=None,\n            deep_extracted_entities=set(parsed_result.get(\"entities\", [])),",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py_420",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
      "line_number": 420,
      "code_snippet": "    # classify with LLM\n    llm = get_default_llm()\n    try:\n        raw_classification_result = llm_response_to_string(llm.invoke(prompt))\n        classification_result = (\n            raw_classification_result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py_484",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/extraction_utils.py",
      "line_number": 484,
      "code_snippet": "    # extract with LLM\n    llm = get_default_llm()\n    try:\n        raw_extraction_result = llm_response_to_string(llm.invoke(prompt))\n        cleaned_response = (\n            raw_extraction_result.replace(\"{{\", \"{\")\n            .replace(\"}}\", \"}\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/formatting_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/utils/formatting_utils.py",
      "line_number": 1,
      "code_snippet": "import re\n\nfrom onyx.db.kg_config import KGConfigSettings\nfrom onyx.kg.models import KGPerson\n\n\ndef format_entity_id(entity_id_name: str) -> str:\n    return make_entity_id(*split_entity_id(entity_id_name))\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/resets/reset_vespa.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/resets/reset_vespa.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Any\n\nfrom redis.lock import Lock as RedisLock\n\nfrom onyx.background.celery.tasks.kg_processing.utils import extend_lock\nfrom onyx.configs.constants import CELERY_GENERIC_BEAT_LOCK_TIMEOUT\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.db.document import get_num_chunks_for_document\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/extractions/extraction_processing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/kg/extractions/extraction_processing.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Any\n\nfrom redis.lock import Lock as RedisLock\n\nfrom onyx.background.celery.tasks.kg_processing.utils import extend_lock\nfrom onyx.configs.constants import CELERY_GENERIC_BEAT_LOCK_TIMEOUT\nfrom onyx.db.connector import get_kg_enabled_connectors\nfrom onyx.db.document import get_document_updated_at\nfrom onyx.db.document import get_skipped_kg_documents",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py_310_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 310. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py",
      "line_number": 310,
      "code_snippet": "\n        with get_session_with_current_tenant() as db_session:\n            return slack_retrieval(\n                query,\n                access_token,\n                db_session,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py_153",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Base64 High Entropy String",
      "description": "detect-secrets found a potential Base64 High Entropy String on line 153. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py",
      "line_number": 153,
      "code_snippet": "                description=\"Slack app client secret from your Slack app configuration\",\n                required=True,\n                example=\"1a2b3c4d5e6f7g8h9i0j1k2l3m4n5o6p\",\n                secret=True,\n            ),",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/federated_connectors/slack/federated_connector.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any\nfrom urllib.parse import urlencode\n\nimport requests\nfrom pydantic import ValidationError\nfrom slack_sdk import WebClient\nfrom typing_extensions import override",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/config.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import SlackChannelConfig\nfrom onyx.db.slack_channel_config import (\n    fetch_slack_channel_config_for_channel_or_default,\n)\nfrom onyx.db.slack_channel_config import fetch_slack_channel_configs\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\nfrom onyx.configs.constants import MessageType\n\n\nclass ChannelType(str, Enum):\n    \"\"\"Slack channel types.\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/constants.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\nLIKE_BLOCK_ACTION_ID = \"feedback-like\"\nDISLIKE_BLOCK_ACTION_ID = \"feedback-dislike\"\nSHOW_EVERYONE_ACTION_ID = \"show-everyone\"\nKEEP_TO_YOURSELF_ACTION_ID = \"keep-to-yourself\"\nCONTINUE_IN_WEB_UI_ACTION_ID = \"continue-in-web-ui\"\nFEEDBACK_DOC_BUTTON_BLOCK_ACTION_ID = \"feedback-doc-button\"\nIMMEDIATE_RESOLVED_BUTTON_ACTION_ID = \"immediate-resolved-button\"\nFOLLOWUP_BUTTON_ACTION_ID = \"followup-button\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/formatting.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/formatting.py",
      "line_number": 1,
      "code_snippet": "from mistune import Markdown  # type: ignore[import-untyped]\nfrom mistune import Renderer\n\n\ndef format_slack_message(message: str | None) -> str:\n    return Markdown(renderer=SlackRenderer()).render(message)\n\n\nclass SlackRenderer(Renderer):\n    SPECIALS: dict[str, str] = {\"&\": \"&amp;\", \"<\": \"&lt;\", \">\": \"&gt;\"}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_250",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postMessage'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 250,
      "code_snippet": "        try:\n            response = client.chat_postMessage(\n                channel=channel,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_273",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postMessage'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 273,
      "code_snippet": "            # Try again wtihout blocks containing url\n            response = client.chat_postMessage(\n                channel=channel,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_287",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postEphemeral'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 287,
      "code_snippet": "            try:\n                response = client.chat_postEphemeral(\n                    channel=channel,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_311",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'text' embedded in LLM prompt",
      "description": "User input parameter 'text' is directly passed to LLM API call 'client.chat_postEphemeral'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 311,
      "code_snippet": "                # Try again wtihout blocks containing url\n                response = client.chat_postEphemeral(\n                    channel=channel,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_759",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM API call 'super().run_message_listeners'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 759,
      "code_snippet": "        with self._set_tenant_context():\n            super().run_message_listeners(message, raw_message)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_146",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 146,
      "code_snippet": "def rephrase_slack_message(msg: str) -> str:\n    llm = get_default_llm(timeout=5)\n    prompt = SLACK_LANGUAGE_REPHRASE_PROMPT.format(query=msg)\n    model_output = llm_response_to_string(llm.invoke(prompt))\n    logger.debug(model_output)\n\n    return model_output",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_233",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 233,
      "code_snippet": "    tries=ONYX_BOT_NUM_RETRIES,\n    delay=0.25,\n    backoff=2,\n    logger=cast(logging.Logger, logger),\n)\ndef respond_in_thread_or_channel(\n    client: WebClient,\n    channel: str,\n    thread_ts: str | None,\n    text: str | None = None,\n    blocks: list[Block] | None = None,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py_757",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/utils.py",
      "line_number": 757,
      "code_snippet": "\n    def process_message(self) -> None:\n        with self._set_tenant_context():\n            super().process_message()\n\n    def run_message_listeners(self, message: dict, raw_message: str) -> None:\n        with self._set_tenant_context():\n            super().run_message_listeners(message, raw_message)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/blocks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/blocks.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import cast\n\nimport pytz\nimport timeago  # type: ignore\nfrom slack_sdk.models.blocks import ActionsBlock\nfrom slack_sdk.models.blocks import Block\nfrom slack_sdk.models.blocks import ButtonElement\nfrom slack_sdk.models.blocks import ContextBlock\nfrom slack_sdk.models.blocks import DividerBlock",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/listener.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/listener.py",
      "line_number": 1,
      "code_snippet": "import os\nimport signal\nimport sys\nimport threading\nimport time\nfrom collections.abc import Callable\nfrom contextvars import Token\nfrom threading import Event\nfrom types import FrameType\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py_556",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'client.web_client.chat_delete' is used in 'DELETE' on line 556 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
      "line_number": 556,
      "code_snippet": "    if not immediate:\n        response = client.web_client.chat_delete(\n            channel=channel_id,\n            ts=message_ts,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py_362",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'handle_slack_feedback' on line 362 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
      "line_number": 362,
      "code_snippet": "def handle_slack_feedback(\n    feedback_id: str,\n    feedback_type: str,\n    feedback_msg_reminder: str,\n    client: WebClient,\n    user_id_to_post_confirmation: str,\n    channel_id_to_post_confirmation: str,\n    thread_ts_to_post_confirmation: str,\n) -> None:\n    message_id, doc_id, doc_rank = decompose_action_id(feedback_id)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py_535_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/network operation without confirmation in 'handle_followup_resolved_button'",
      "description": "Function 'handle_followup_resolved_button' on line 535 performs high-risk delete/write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
      "line_number": 535,
      "code_snippet": "\n\ndef handle_followup_resolved_button(\n    req: SocketModeRequest,\n    client: TenantSocketModeClient,\n    immediate: bool = False,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py_535_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'handle_followup_resolved_button'",
      "description": "Function 'handle_followup_resolved_button' on line 535 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
      "line_number": 535,
      "code_snippet": "\n\ndef handle_followup_resolved_button(\n    req: SocketModeRequest,\n    client: TenantSocketModeClient,\n    immediate: bool = False,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py_424",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
      "line_number": 424,
      "code_snippet": "        LIKE_BLOCK_ACTION_ID,\n        DISLIKE_BLOCK_ACTION_ID,\n    ]:\n        client.chat_postEphemeral(\n            channel=channel_id_to_post_confirmation,\n            user=user_id_to_post_confirmation,\n            thread_ts=thread_ts_to_post_confirmation,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py_556",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_buttons.py",
      "line_number": 556,
      "code_snippet": "\n    # Delete the message with the option to mark resolved\n    if not immediate:\n        response = client.web_client.chat_delete(\n            channel=channel_id,\n            ts=message_ts,\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_96",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'client.chat_deleteScheduledMessage' is used in 'DELETE' on line 96 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 96,
      "code_snippet": "    try:\n        client.chat_deleteScheduledMessage(\n            channel=channel, scheduled_message_id=msg_id  # type:ignore\n        )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_50_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'schedule_feedback_reminder'",
      "description": "Function 'schedule_feedback_reminder' on line 50 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 50,
      "code_snippet": "\n\ndef schedule_feedback_reminder(\n    details: SlackMessageInfo, include_followup: bool, client: WebClient\n) -> str | None:\n    logger = setup_logger(extra={SLACK_CHANNEL_ID: details.channel_to_respond})",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_90_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/network operation without confirmation in 'remove_scheduled_feedback_reminder'",
      "description": "Function 'remove_scheduled_feedback_reminder' on line 90 performs high-risk delete/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 90,
      "code_snippet": "\n\ndef remove_scheduled_feedback_reminder(\n    client: WebClient, channel: str | None, msg_id: str\n) -> None:\n    logger = setup_logger(extra={SLACK_CHANNEL_ID: channel})",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_90_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'remove_scheduled_feedback_reminder'",
      "description": "Function 'remove_scheduled_feedback_reminder' on line 90 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 90,
      "code_snippet": "\n\ndef remove_scheduled_feedback_reminder(\n    client: WebClient, channel: str | None, msg_id: str\n) -> None:\n    logger = setup_logger(extra={SLACK_CHANNEL_ID: channel})",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_60",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 60,
      "code_snippet": "        return None\n\n    try:\n        permalink = client.chat_getPermalink(\n            channel=details.channel_to_respond,\n            message_ts=details.msg_to_respond,  # type:ignore\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_72",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 72,
      "code_snippet": "    future = now + datetime.timedelta(minutes=ONYX_BOT_FEEDBACK_REMINDER)\n\n    try:\n        response = client.chat_scheduleMessage(\n            channel=details.sender_id,  # type:ignore\n            post_at=int(future.timestamp()),\n            blocks=[",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py_96",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_message.py",
      "line_number": 96,
      "code_snippet": "    logger = setup_logger(extra={SLACK_CHANNEL_ID: channel})\n\n    try:\n        client.chat_deleteScheduledMessage(\n            channel=channel, scheduled_message_id=msg_id  # type:ignore\n        )\n        logger.info(\"Scheduled feedback reminder deleted\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_regular_answer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/onyxbot/slack/handlers/handle_regular_answer.py",
      "line_number": 1,
      "code_snippet": "import functools\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import Optional\nfrom typing import TypeVar\n\nfrom retry import retry\nfrom slack_sdk import WebClient\n\nfrom onyx.chat.models import ChatBasicResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/store.py",
      "line_number": 1,
      "code_snippet": "from onyx.configs.app_configs import DISABLE_USER_KNOWLEDGE\nfrom onyx.configs.app_configs import ONYX_QUERY_HISTORY_TYPE\nfrom onyx.configs.app_configs import SHOW_EXTRA_CONNECTORS\nfrom onyx.configs.constants import KV_SETTINGS_KEY\nfrom onyx.configs.constants import OnyxRedisLocks\nfrom onyx.key_value_store.factory import get_kv_store\nfrom onyx.key_value_store.interface import KvKeyNotFoundError\nfrom onyx.redis.redis_pool import get_redis_client\nfrom onyx.server.settings.models import Settings\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\n\nfrom pydantic import BaseModel\n\nfrom onyx.configs.constants import NotificationType\nfrom onyx.configs.constants import QueryHistoryType\nfrom onyx.db.models import Notification as NotificationDBModel\nfrom shared_configs.configs import POSTGRES_DEFAULT_SCHEMA\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/settings/api.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.auth.users import current_user\nfrom onyx.auth.users import is_user_admin",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/onyx_api/ingestion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/onyx_api/ingestion.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_curator_or_admin_user\nfrom onyx.configs.constants import DEFAULT_CC_PAIR_ID",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/api_key/api.py_38_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'regenerate_existing_api_key'",
      "description": "API endpoint 'regenerate_existing_api_key' on line 38 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/api_key/api.py",
      "line_number": 38,
      "code_snippet": "\n@router.post(\"/{api_key_id}/regenerate\")\ndef regenerate_existing_api_key(\n    api_key_id: int,\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/runtime/onyx_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/runtime/onyx_runtime.py",
      "line_number": 1,
      "code_snippet": "import io\nfrom typing import cast\n\nfrom PIL import Image\n\nfrom onyx.background.celery.tasks.beat_schedule import CLOUD_BEAT_MULTIPLIER_DEFAULT\nfrom onyx.background.celery.tasks.beat_schedule import (\n    CLOUD_DOC_PERMISSION_SYNC_MULTIPLIER_DEFAULT,\n)\nfrom onyx.configs.constants import CLOUD_BUILD_FENCE_LOOKUP_TABLE_INTERVAL_DEFAULT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/long_term_logs/long_term_logs_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/long_term_logs/long_term_logs_api.py",
      "line_number": 1,
      "code_snippet": "import json\nimport shutil\nimport tempfile\nimport zipfile\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/kg/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/kg/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.configs.constants import TMP_DRALPHA_PERSONA_NAME\nfrom onyx.configs.kg_configs import KG_BETA_ASSISTANT_DESCRIPTION\nfrom onyx.context.search.enums import RecencyBiasSetting\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.entities import get_entity_stats_by_grounded_source_name",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/models.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\nfrom typing import TYPE_CHECKING\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import Field\nfrom pydantic import field_validator",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py_585_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 585. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py",
      "line_number": 585,
      "code_snippet": "        token = request.cookies.get(FASTAPI_USERS_AUTH_COOKIE_NAME)\n        if not token:\n            logger.debug(\"No auth token cookie found\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py_608_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 608. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py",
      "line_number": 608,
      "code_snippet": "\n    except Exception as e:\n        logger.error(f\"Error retrieving token expiration from Redis: {e}\")\n        return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/users.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport io\nimport re\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import cast\n\nfrom email_validator import EmailNotValidError\nfrom email_validator import EmailUndeliverableError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/search_settings.py_245_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'api_key' containing sensitive data is being logged on line 245. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/search_settings.py",
      "line_number": 245,
      "code_snippet": ") -> bool:\n    api_key = get_unstructured_api_key()\n    print(api_key)\n    return api_key is not None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/search_settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/search_settings.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import status\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.auth.users import current_user\nfrom onyx.configs.app_configs import DISABLE_INDEX_UPDATE_ON_SWAP\nfrom onyx.context.search.models import SavedSearchSettings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/validate_tokens.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/validate_tokens.py",
      "line_number": 1,
      "code_snippet": "import requests\nfrom fastapi import HTTPException\n\nfrom onyx.configs.constants import SLACK_USER_TOKEN_PREFIX\n\nSLACK_API_URL = \"https://slack.com/api/auth.test\"\nSLACK_CONNECTIONS_OPEN_URL = \"https://slack.com/api/apps.connections.open\"\n\n\ndef validate_bot_token(bot_token: str) -> bool:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/administrative.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/administrative.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import cast\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/slack_bot.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/slack_bot.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.configs.constants import MilestoneRecordType\nfrom onyx.db.constants import SLACK_BOT_PERSONA_PREFIX\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.models import ChannelConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py_316",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'from_models' on line 316 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py",
      "line_number": 316,
      "code_snippet": "    def from_models(\n        cls,\n        cc_pair_model: ConnectorCredentialPair,\n        latest_deletion_attempt: DeletionAttemptSnapshot | None,\n        number_of_index_attempts: int,\n        last_index_attempt: IndexAttempt | None,\n        num_docs_indexed: int,  # not ideal, but this must be computed separately\n        is_editable_for_current_user: bool,\n        indexing: bool,\n        last_permission_sync_attempt_status: PermissionSyncStatus | None = None,\n        permission_syncing: bool = False,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py_316_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'from_models'",
      "description": "Function 'from_models' on line 316 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py",
      "line_number": 316,
      "code_snippet": "\n    @classmethod\n    def from_models(\n        cls,\n        cc_pair_model: ConnectorCredentialPair,\n        latest_deletion_attempt: DeletionAttemptSnapshot | None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/models.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom datetime import datetime\nfrom datetime import timezone\nfrom datetime import UTC\nfrom enum import Enum\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeVar\nfrom uuid import UUID\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/standard_oauth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/standard_oauth.py",
      "line_number": 1,
      "code_snippet": "import json\nimport uuid\nfrom typing import Annotated\nfrom typing import cast\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import Request",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py_38_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 38. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py",
      "line_number": 38,
      "code_snippet": "\n    user_acl_filters = build_access_filters_for_user(user, db_session)\n    inference_chunks = document_index.id_based_retrieval(\n        chunk_requests=[VespaChunkRequest(document_id=document_id)],\n        filters=IndexFilters(access_control_list=user_acl_filters),\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py_88_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 88. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py",
      "line_number": 88,
      "code_snippet": "    )\n\n    inference_chunks = document_index.id_based_retrieval(\n        chunk_requests=[chunk_request],\n        filters=IndexFilters(access_control_list=user_acl_filters),\n        batch_retrieval=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py_29_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_document_info'",
      "description": "API endpoint 'get_document_info' on line 29 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py",
      "line_number": 29,
      "code_snippet": "# as a different path\n@router.get(\"/document-size-info\")\ndef get_document_info(\n    document_id: str = Query(...),\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py_72_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_chunk_info'",
      "description": "API endpoint 'get_chunk_info' on line 72 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py",
      "line_number": 72,
      "code_snippet": "\n@router.get(\"/chunk-info\")\ndef get_chunk_info(\n    document_id: str = Query(...),\n    chunk_id: int = Query(...),\n    user: User | None = Depends(current_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/document.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_user\nfrom onyx.context.search.models import IndexFilters\nfrom onyx.context.search.preprocessing.access_filters import (\n    build_access_filters_for_user,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/cc_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/cc_pair.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom http import HTTPStatus\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi.responses import JSONResponse\nfrom sqlalchemy import select\nfrom sqlalchemy.exc import IntegrityError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/connector.py",
      "line_number": 1,
      "code_snippet": "import json\nimport math\nimport mimetypes\nimport os\nimport zipfile\nfrom io import BytesIO\nfrom typing import Any\nfrom typing import cast\n\nfrom fastapi import APIRouter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/credential.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/documents/credential.py",
      "line_number": 1,
      "code_snippet": "import json\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import File\nfrom fastapi import Form\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import UploadFile\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py_220",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'chat_session.current_alternate_model.lower' is used in 'UPDATE' on line 220 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
      "line_number": 220,
      "code_snippet": "            and LlmProviderNames.ANTHROPIC\n            in chat_session.current_alternate_model.lower()\n        ):\n            if update_thread_req.temperature_override > 1:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py_195",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'update_chat_session_temperature' on line 195 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
      "line_number": 195,
      "code_snippet": "def update_chat_session_temperature(\n    update_thread_req: UpdateChatSessionTemperatureRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),\n) -> None:\n    chat_session = get_chat_session_by_id(\n        chat_session_id=update_thread_req.chat_session_id,\n        user_id=user.id if user is not None else None,\n        db_session=db_session,\n    )\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py_195_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'update_chat_session_temperature'",
      "description": "Function 'update_chat_session_temperature' on line 195 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
      "line_number": 195,
      "code_snippet": "\n@router.put(\"/update-chat-session-temperature\")\ndef update_chat_session_temperature(\n    update_thread_req: UpdateChatSessionTemperatureRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/chat_backend.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport json\nimport os\nfrom collections.abc import Generator\nfrom datetime import timedelta\nfrom uuid import UUID\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py_274",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'model.time_created.isoformat' is used in 'UPDATE' on line 274 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
      "line_number": 274,
      "code_snippet": "            persona_id=model.persona_id,\n            time_created=model.time_created.isoformat(),\n            time_updated=model.time_updated.isoformat(),\n            shared_status=model.shared_status,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py_275",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'model.time_updated.isoformat' is used in 'UPDATE' on line 275 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
      "line_number": 275,
      "code_snippet": "            time_created=model.time_created.isoformat(),\n            time_updated=model.time_updated.isoformat(),\n            shared_status=model.shared_status,\n            current_alternate_model=model.current_alternate_model,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py_269_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'from_model'",
      "description": "Function 'from_model' on line 269 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
      "line_number": 269,
      "code_snippet": "\n    @classmethod\n    def from_model(cls, model: ChatSession) -> \"ChatSessionDetails\":\n        return cls(\n            id=model.id,\n            name=model.description,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py_269_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'from_model'",
      "description": "Function 'from_model' on line 269 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
      "line_number": 269,
      "code_snippet": "\n    @classmethod\n    def from_model(cls, model: ChatSession) -> \"ChatSessionDetails\":\n        return cls(\n            id=model.id,\n            name=model.description,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Any\nfrom typing import TYPE_CHECKING\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom pydantic import model_validator\n\nfrom onyx.chat.models import PersonaOverrideConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/token_limit.py_71_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_fetch_global_usage'",
      "description": "Function '_fetch_global_usage' on line 71 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/token_limit.py",
      "line_number": 71,
      "code_snippet": "\n\ndef _fetch_global_usage(\n    cutoff_time: datetime, db_session: Session\n) -> Sequence[tuple[datetime, int]]:\n    \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/streaming_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/streaming_utils.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport re\n\nfrom onyx.context.search.models import SavedSearchDoc\nfrom onyx.context.search.models import SearchDoc\nfrom onyx.server.query_and_chat.placement import Placement\nfrom onyx.server.query_and_chat.streaming_models import AgentResponseDelta\nfrom onyx.server.query_and_chat.streaming_models import AgentResponseStart\nfrom onyx.server.query_and_chat.streaming_models import CitationInfo",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py_62_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 62. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py",
      "line_number": 62,
      "code_snippet": "        )\n    if not query or query.strip() == \"\":\n        matching_chunks = document_index.random_retrieval(filters=final_filters)\n    else:\n        matching_chunks = document_index.admin_retrieval(\n            query=query, filters=final_filters",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py_64_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 64. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py",
      "line_number": 64,
      "code_snippet": "        matching_chunks = document_index.random_retrieval(filters=final_filters)\n    else:\n        matching_chunks = document_index.admin_retrieval(\n            query=query, filters=final_filters\n        )\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/query_backend.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_curator_or_admin_user\nfrom onyx.auth.users import current_user\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.context.search.models import IndexFilters\nfrom onyx.context.search.models import SearchDoc",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/streaming_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/streaming_models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Annotated\nfrom typing import Literal\nfrom typing import Union\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\nfrom onyx.context.search.models import SearchDoc\nfrom onyx.server.query_and_chat.placement import Placement",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/session_loading.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/query_and_chat/session_loading.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom typing import cast\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import MessageType\nfrom onyx.context.search.models import SavedSearchDoc\nfrom onyx.context.search.models import SearchDoc\nfrom onyx.db.chat import get_db_search_doc_by_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py_314_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_authorize_url'",
      "description": "API endpoint 'get_authorize_url' on line 314 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py",
      "line_number": 314,
      "code_snippet": "\n@router.get(\"/{id}/authorize\")\ndef get_authorize_url(\n    id: int,\n    user: User = Depends(current_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py_451_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_user_oauth_status'",
      "description": "API endpoint 'get_user_oauth_status' on line 451 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py",
      "line_number": 451,
      "code_snippet": "\n@router.get(\"/oauth-status\")\ndef get_user_oauth_status(\n    user: User = Depends(current_user),\n    db_session: Session = Depends(get_session),\n) -> list[UserOAuthStatus]:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/federated/api.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\nfrom uuid import UUID\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi import Response\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/pat/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/pat/api.py",
      "line_number": 1,
      "code_snippet": "\"\"\"API endpoints for Personal Access Tokens.\"\"\"\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_user\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.models import User",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/web_search/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/web_search/api.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Response\nfrom sqlalchemy.dialects.postgresql import insert\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/models.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import TYPE_CHECKING\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\nfrom pydantic import field_validator\n\nfrom onyx.llm.utils import get_max_input_tokens\nfrom onyx.llm.utils import litellm_thinks_model_supports_image_input\nfrom onyx.llm.utils import model_is_reasoning_model",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_575",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_bedrock_available_models' on line 575 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 575,
      "code_snippet": "def get_bedrock_available_models(\n    request: BedrockModelsRequest,\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),\n) -> list[BedrockFinalModelResponse]:\n    \"\"\"Fetch available Bedrock models for a specific region and credentials.\n\n    Returns model IDs with display names from AWS. Prefers inference profiles\n    (for cross-region support) over base models when available.\n    \"\"\"\n    try:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_662_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'profile_name' containing sensitive data is included in a prompt string on line 662. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 662,
      "code_snippet": "                        model_info[profile_id] = {\n                            \"display_name\": (\n                                f\"{profile_name} ({region})\"\n                                if profile_name\n                                else generate_bedrock_display_name(profile_id)",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_575_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network/admin operation without confirmation in 'get_bedrock_available_models'",
      "description": "Function 'get_bedrock_available_models' on line 575 performs high-risk write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 575,
      "code_snippet": "\n@admin_router.post(\"/bedrock/available-models\")\ndef get_bedrock_available_models(\n    request: BedrockModelsRequest,\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_737_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_get_ollama_available_model_names'",
      "description": "Function '_get_ollama_available_model_names' on line 737 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 737,
      "code_snippet": "\n\ndef _get_ollama_available_model_names(api_base: str) -> set[str]:\n    \"\"\"Fetch available model names from Ollama server.\"\"\"\n    tags_url = f\"{api_base}/api/tags\"\n    try:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_531_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_provider_contextual_cost'",
      "description": "API endpoint 'get_provider_contextual_cost' on line 531 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 531,
      "code_snippet": "\n@admin_router.get(\"/provider-contextual-cost\")\ndef get_provider_contextual_cost(\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),\n) -> list[LLMCost]:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_575_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_bedrock_available_models'",
      "description": "API endpoint 'get_bedrock_available_models' on line 575 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 575,
      "code_snippet": "\n@admin_router.post(\"/bedrock/available-models\")\ndef get_bedrock_available_models(\n    request: BedrockModelsRequest,\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_755_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'get_ollama_available_models'",
      "description": "API endpoint 'get_ollama_available_models' on line 755 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 755,
      "code_snippet": "\n@admin_router.post(\"/ollama/available-models\")\ndef get_ollama_available_models(\n    request: OllamaModelsRequest,\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_621",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 621,
      "code_snippet": "                continue\n\n            available_models.add(model_id)\n            input_modalities = model.get(\"inputModalities\", [])\n            model_info[model_id] = {\n                \"display_name\": model.get(\"modelName\", model_id),\n                \"supports_image_input\": \"IMAGE\" in input_modalities,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py_623",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/api.py",
      "line_number": 623,
      "code_snippet": "            available_models.add(model_id)\n            input_modalities = model.get(\"inputModalities\", [])\n            model_info[model_id] = {\n                \"display_name\": model.get(\"modelName\", model_id),\n                \"supports_image_input\": \"IMAGE\" in input_modalities,\n            }\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/llm/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nLLM Provider Utilities\n\nUtilities for dynamic LLM providers (Bedrock, Ollama, OpenRouter):\n- Display name generation from model identifiers\n- Model validation and filtering\n- Vision/reasoning capability inference\n\"\"\"\n\nimport re",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/image_generation/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/image_generation/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.image_generation import create_image_generation_config__no_commit\nfrom onyx.db.image_generation import delete_image_generation_config__no_commit\nfrom onyx.db.image_generation import get_all_image_generation_configs",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/embedding/api.py_34_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network/admin operation without confirmation in 'test_embedding_configuration'",
      "description": "Function 'test_embedding_configuration' on line 34 performs high-risk network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/embedding/api.py",
      "line_number": 34,
      "code_snippet": "\n@admin_router.post(\"/test-embedding\")\ndef test_embedding_configuration(\n    test_llm_request: TestEmbeddingRequest,\n    _: User | None = Depends(current_admin_user),\n) -> None:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/embedding/api.py_34_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'test_embedding_configuration'",
      "description": "API endpoint 'test_embedding_configuration' on line 34 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/embedding/api.py",
      "line_number": 34,
      "code_snippet": "\n@admin_router.post(\"/test-embedding\")\ndef test_embedding_configuration(\n    test_llm_request: TestEmbeddingRequest,\n    _: User | None = Depends(current_admin_user),\n) -> None:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nImplement monitoring to detect extraction attempts:\n\n1. Log all model access:\n   - Request metadata (IP, user, timestamp)\n   - Query patterns and frequency\n   - Response characteristics\n\n2. Anomaly detection:\n   - Unusual query patterns\n   - High-frequency requests from single source\n   - Systematic probing of model boundaries\n\n3. Set up alerts:\n   - Threshold-based alerts (requests/hour)\n   - Pattern-based alerts (systematic extraction)\n   - Geographic anomalies\n\n4. Response analysis:\n   - Track what outputs are returned\n   - Detect if sensitive info is leaked\n   - Monitor for data exfiltration patterns\n\n5. Implement honeypots:\n   - Fake endpoints to detect scanners\n   - Canary tokens in responses\n   - Deception techniques"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/embedding/api.py_52",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/manage/embedding/api.py",
      "line_number": 52,
      "code_snippet": "            query_prefix=None,\n            passage_prefix=None,\n        )\n        test_model.encode([\"Testing Embedding\"], text_type=EmbedTextType.QUERY)\n\n    except ValueError as e:\n        error_msg = f\"Not a valid embedding model. Exception thrown: {e}\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/web_search/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/web_search/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_user\nfrom onyx.configs.constants import PUBLIC_API_TAGS\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.models import User\nfrom onyx.db.web_search import fetch_active_web_content_provider",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/release_notes/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/release_notes/constants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Constants for release notes functionality.\"\"\"\n\n# GitHub source\nGITHUB_RAW_BASE_URL = (\n    \"https://raw.githubusercontent.com/onyx-dot-app/documentation/main\"\n)\nGITHUB_CHANGELOG_RAW_URL = f\"{GITHUB_RAW_BASE_URL}/changelog.mdx\"\n\n# Base URL for changelog documentation (used for notification links)\nDOCS_CHANGELOG_BASE_URL = \"https://docs.onyx.app/changelog\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/release_notes/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/release_notes/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utility functions for release notes parsing and caching.\"\"\"\n\nimport re\nfrom datetime import datetime\nfrom datetime import timezone\n\nimport httpx\nfrom sqlalchemy.orm import Session\n\nfrom onyx import __version__",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_510",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'move_chat_session' on line 510 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 510,
      "code_snippet": "def move_chat_session(\n    project_id: int,\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),\n) -> Response:\n    user_id = user.id if user is not None else None\n    chat_session = (\n        db_session.query(ChatSession)\n        .filter(ChatSession.id == body.chat_session_id, ChatSession.user_id == user_id)\n        .one_or_none()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_530",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'remove_chat_session' on line 530 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 530,
      "code_snippet": "def remove_chat_session(\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),\n) -> Response:\n    user_id = user.id if user is not None else None\n    chat_session = (\n        db_session.query(ChatSession)\n        .filter(ChatSession.id == body.chat_session_id, ChatSession.user_id == user_id)\n        .one_or_none()\n    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_510_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'move_chat_session'",
      "description": "Function 'move_chat_session' on line 510 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 510,
      "code_snippet": "\n@router.post(\"/{project_id}/move_chat_session\")\ndef move_chat_session(\n    project_id: int,\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_530_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/network operation without confirmation in 'remove_chat_session'",
      "description": "Function 'remove_chat_session' on line 530 performs high-risk delete/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 530,
      "code_snippet": "\n@router.post(\"/remove_chat_session\")\ndef remove_chat_session(\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_530_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'remove_chat_session'",
      "description": "Function 'remove_chat_session' on line 530 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 530,
      "code_snippet": "\n@router.post(\"/remove_chat_session\")\ndef remove_chat_session(\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_510_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'move_chat_session'",
      "description": "API endpoint 'move_chat_session' on line 510 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 510,
      "code_snippet": "\n@router.post(\"/{project_id}/move_chat_session\")\ndef move_chat_session(\n    project_id: int,\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_530_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'remove_chat_session'",
      "description": "API endpoint 'remove_chat_session' on line 530 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 530,
      "code_snippet": "\n@router.post(\"/remove_chat_session\")\ndef remove_chat_session(\n    body: ChatSessionRequest,\n    user: User | None = Depends(current_user),\n    db_session: Session = Depends(get_session),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_518",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 518,
      "code_snippet": ") -> Response:\n    user_id = user.id if user is not None else None\n    chat_session = (\n        db_session.query(ChatSession)\n        .filter(ChatSession.id == body.chat_session_id, ChatSession.user_id == user_id)\n        .one_or_none()\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py_537",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/api.py",
      "line_number": 537,
      "code_snippet": ") -> Response:\n    user_id = user.id if user is not None else None\n    chat_session = (\n        db_session.query(ChatSession)\n        .filter(ChatSession.id == body.chat_session_id, ChatSession.user_id == user_id)\n        .one_or_none()\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/projects_file_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/projects/projects_file_utils.py",
      "line_number": 1,
      "code_snippet": "from math import ceil\n\nfrom fastapi import UploadFile\nfrom PIL import Image\nfrom PIL import ImageOps\nfrom PIL import UnidentifiedImageError\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import Field\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/document_set/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/document_set/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_curator_or_admin_user\nfrom onyx.auth.users import current_user\nfrom onyx.background.celery.versioned_apps.client import app as client_app\nfrom onyx.configs.constants import OnyxCeleryPriority",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py_198_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/execute/network/admin operation without confirmation in 'make_oauth_provider'",
      "description": "Function 'make_oauth_provider' on line 198 performs high-risk delete/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py",
      "line_number": 198,
      "code_snippet": "\n\ndef make_oauth_provider(\n    mcp_server: DbMCPServer,\n    user_id: str,\n    return_path: str,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py_246",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py",
      "line_number": 246,
      "code_snippet": "        # requests CAN block here for up to a minute if the user doesn't resolve the OAuth flow\n        # Run the blocking blpop operation in a thread pool to avoid blocking the event loop\n        loop = asyncio.get_running_loop()\n        pop = await loop.run_in_executor(\n            None, lambda: r.blpop([key], timeout=OAUTH_WAIT_SECONDS)\n        )\n        # TODO: gracefully handle \"user says no\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py_503",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/mcp/api.py",
      "line_number": 503,
      "code_snippet": "    loop = asyncio.get_running_loop()\n\n    async def wait_auth_url() -> str | None:\n        raw = await loop.run_in_executor(\n            None,\n            lambda: r.blpop([key_auth_url(str(user.id))], timeout=OAUTH_WAIT_SECONDS),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/input_prompt/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/input_prompt/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.auth.users import current_user\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.input_prompt import disable_input_prompt_for_user\nfrom onyx.db.input_prompt import fetch_input_prompt_by_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/default_assistant/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/default_assistant/api.py",
      "line_number": 1,
      "code_snippet": "\"\"\"API endpoints for default assistant configuration.\"\"\"\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_admin_user\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.models import User",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/persona/api.py_497_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'build_assistant_prompts'",
      "description": "API endpoint 'build_assistant_prompts' on line 497 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/persona/api.py",
      "line_number": 497,
      "code_snippet": "\n@basic_router.post(\"/assistant-prompt-refresh\")\ndef build_assistant_prompts(\n    generate_persona_prompt_request: GenerateStarterMessageRequest,\n    db_session: Session = Depends(get_session),\n    user: User | None = Depends(current_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/persona/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/persona/api.py",
      "line_number": 1,
      "code_snippet": "from uuid import UUID\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Query\nfrom fastapi import UploadFile\nfrom pydantic import BaseModel\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/oauth_config/api.py_155_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'initiate_oauth_flow'",
      "description": "API endpoint 'initiate_oauth_flow' on line 155 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/oauth_config/api.py",
      "line_number": 155,
      "code_snippet": "\n@router.post(\"/initiate\")\ndef initiate_oauth_flow(\n    request: OAuthInitiateRequest,\n    db_session: Session = Depends(get_session),\n    user: User | None = Depends(current_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/oauth_config/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/oauth_config/api.py",
      "line_number": 1,
      "code_snippet": "\"\"\"API endpoints for OAuth configuration management.\"\"\"\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.oauth_token_manager import OAuthTokenManager\nfrom onyx.auth.users import current_curator_or_admin_user\nfrom onyx.auth.users import current_user",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/notifications/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/notifications/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.users import current_user\nfrom onyx.db.engine.sql_engine import get_session\nfrom onyx.db.models import User\nfrom onyx.db.notification import dismiss_notification\nfrom onyx.db.notification import get_notification_by_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/tool/tool_visibility.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/tool/tool_visibility.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Tool visibility configuration and utility functions.\"\"\"\n\nfrom pydantic import BaseModel\n\nfrom onyx.db.models import Tool\nfrom onyx.tools.constants import OPEN_URL_TOOL_ID\n\n# Tool class name constant for OktaProfileTool (not in main constants.py as it's hidden)\nOKTA_PROFILE_TOOL_ID = \"OktaProfileTool\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/tool/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/server/features/tool/api.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\n\nfrom onyx.auth.schemas import UserRole\nfrom onyx.auth.users import current_curator_or_admin_user",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/enums.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/enums.py",
      "line_number": 1,
      "code_snippet": "\"\"\"NOTE: this needs to be separate from models.py because of circular imports.\nBoth search/models.py and db/models.py import enums from this file AND\nsearch/models.py imports from db/models.py.\"\"\"\n\nfrom enum import Enum\n\n\nclass RecencyBiasSetting(str, Enum):\n    FAVOR_RECENT = \"favor_recent\"  # 2x decay rate\n    BASE_DECAY = \"base_decay\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/models.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\nfrom pydantic import field_validator\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/utils.py_77",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/utils.py",
      "line_number": 77,
      "code_snippet": "        server_port=MODEL_SERVER_PORT,\n    )\n\n    query_embedding = model.encode(queries, text_type=EmbedTextType.QUERY)\n    return query_embedding\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/pipeline.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/pipeline.py",
      "line_number": 1,
      "code_snippet": "from collections import defaultdict\nfrom datetime import datetime\nfrom uuid import UUID\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.context.search.models import BaseFilters\nfrom onyx.context.search.models import ChunkIndexRequest\nfrom onyx.context.search.models import ChunkSearchRequest\nfrom onyx.context.search.models import IndexFilters",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py_98_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 98. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py",
      "line_number": 98,
      "code_snippet": "    hybrid_alpha = query_request.hybrid_alpha or HYBRID_ALPHA\n\n    top_chunks = document_index.hybrid_retrieval(\n        query=query_request.query,\n        query_embedding=query_embedding,\n        final_keywords=query_request.query_keywords,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py_188_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 188. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py",
      "line_number": 188,
      "code_snippet": "    filters = IndexFilters(access_control_list=None)\n\n    retrieved_chunks = document_index.id_based_retrieval(\n        chunk_requests=chunk_requests,\n        filters=filters,\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/retrieval/search_runner.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom uuid import UUID\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.chat_configs import HYBRID_ALPHA\nfrom onyx.configs.chat_configs import NUM_RETURNED_HITS\nfrom onyx.context.search.models import ChunkIndexRequest\nfrom onyx.context.search.models import IndexFilters\nfrom onyx.context.search.models import InferenceChunk",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search.py",
      "line_number": 1,
      "code_snippet": "import json\nimport re\nimport time\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import cast\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import ValidationError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py_136",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'extract_date_range_from_query' on line 136 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py",
      "line_number": 136,
      "code_snippet": "def extract_date_range_from_query(\n    query: str,\n    llm: LLM,\n    default_search_days: int,\n) -> int:\n    query_lower = query.lower()\n\n    if re.search(r\"\\btoday(?:\\'?s)?\\b\", query_lower):\n        return 0\n\n    if re.search(r\"\\byesterday\\b\", query_lower):",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py_193",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py",
      "line_number": 193,
      "code_snippet": "\n    try:\n        prompt = SLACK_DATE_EXTRACTION_PROMPT.format(query=query)\n        response = llm_response_to_string(llm.invoke(prompt))\n\n        response_clean = _parse_llm_code_block_response(response)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py_584",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py",
      "line_number": 584,
      "code_snippet": "    )\n\n    try:\n        response = llm_response_to_string(llm.invoke(prompt))\n\n        response_clean = _parse_llm_code_block_response(response)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py_136",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/context/search/federated/slack_search_utils.py",
      "line_number": 136,
      "code_snippet": "    # - \"recent messages\" -> content_word_count = 0 -> pure recency\n    # - \"golf scores last saturday\" -> content_word_count = 3 (golf, scores, saturday) -> not pure recency\n    return content_word_count < 2\n\n\ndef extract_date_range_from_query(\n    query: str,\n    llm: LLM,\n    default_search_days: int,\n) -> int:\n    query_lower = query.lower()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/auto_update_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/auto_update_service.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Service for fetching and syncing LLM model configurations from GitHub.\n\nThis service manages Auto mode LLM providers, where models and configuration\nare managed centrally via a GitHub-hosted JSON file. In Auto mode:\n- Model list is controlled by GitHub config\n- Model visibility is controlled by GitHub config\n- Default model is controlled by GitHub config\n- Admin only needs to provide API credentials\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/constants.py_36",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 36. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/constants.py",
      "line_number": 36,
      "code_snippet": "\nOLLAMA_PROVIDER_NAME = \"ollama_chat\"\nOLLAMA_API_KEY_CONFIG_KEY = \"OLLAMA_API_KEY\"\n\n# OpenRouter",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/constants.py",
      "line_number": 1,
      "code_snippet": "OPENAI_PROVIDER_NAME = \"openai\"\n# Curated list of OpenAI models to show by default in the UI\nOPENAI_VISIBLE_MODEL_NAMES = {\n    \"gpt-5\",\n    \"gpt-5-mini\",\n    \"o1\",\n    \"o3-mini\",\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_187",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'litellm.model_cost.keys' is used in 'UPDATE' on line 187 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 187,
      "code_snippet": "    # Also extract from model_cost for any models not in the sets\n    for key in litellm.model_cost.keys():\n        if key.startswith(\"vertex_ai/\"):\n            model_name = key.replace(\"vertex_ai/\", \"\")",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_130",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous code_execution sink",
      "description": "LLM output from 'model.lower' is used in 'compile(' on line 130 without sanitization. This creates a code_execution vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 130,
      "code_snippet": "    def is_valid_model(model: str) -> bool:\n        model_lower = model.lower()\n        return not any(\n            ex in model_lower for ex in excluded_terms",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required\n4. Use allowlists for permitted operations\n5. Consider structured output formats (JSON) instead"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_167",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_vertexai_model_names' on line 167 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 167,
      "code_snippet": "def get_vertexai_model_names() -> list[str]:\n    \"\"\"Get Vertex AI model names dynamically from litellm model_cost.\"\"\"\n    import litellm\n\n    # Combine all vertex model sets\n    vertex_models: set[str] = set()\n    vertex_model_sets = [\n        \"vertex_chat_models\",\n        \"vertex_language_models\",\n        \"vertex_anthropic_models\",\n        \"vertex_llama3_models\",",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_102_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Direct execution of LLM-generated code in 'get_openai_model_names'",
      "description": "Function 'get_openai_model_names' on line 102 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 102,
      "code_snippet": "\n    return False\n\n\ndef get_openai_model_names() -> list[str]:\n    \"\"\"Get OpenAI model names dynamically from litellm.\"\"\"\n    import re\n    import litellm\n\n    # TODO: remove these lists once we have a comprehensive model configuration page",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_167_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'get_vertexai_model_names'",
      "description": "Function 'get_vertexai_model_names' on line 167 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 167,
      "code_snippet": "\n\ndef get_vertexai_model_names() -> list[str]:\n    \"\"\"Get Vertex AI model names dynamically from litellm model_cost.\"\"\"\n    import litellm\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_102_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_openai_model_names'",
      "description": "Function 'get_openai_model_names' on line 102 makes critical legal, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 102,
      "code_snippet": "\n\ndef get_openai_model_names() -> list[str]:\n    \"\"\"Get OpenAI model names dynamically from litellm.\"\"\"\n    import re\n    import litellm",
      "recommendation": "Critical legal, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_167_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_vertexai_model_names'",
      "description": "Function 'get_vertexai_model_names' on line 167 makes critical financial, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 167,
      "code_snippet": "\n\ndef get_vertexai_model_names() -> list[str]:\n    \"\"\"Get Vertex AI model names dynamically from litellm model_cost.\"\"\"\n    import litellm\n",
      "recommendation": "Critical financial, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_129_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'is_valid_model'",
      "description": "Function 'is_valid_model' on line 129 makes critical legal decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 129,
      "code_snippet": "    date_pattern = re.compile(r\"-\\d{4}\")\n\n    def is_valid_model(model: str) -> bool:\n        model_lower = model.lower()\n        return not any(\n            ex in model_lower for ex in excluded_terms",
      "recommendation": "Critical legal decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/well_known_providers/llm_provider_options.py",
      "line_number": 1,
      "code_snippet": "import json\nimport pathlib\n\nfrom onyx.llm.constants import LlmProviderNames\nfrom onyx.llm.constants import PROVIDER_DISPLAY_NAMES\nfrom onyx.llm.constants import WELL_KNOWN_PROVIDER_NAMES\nfrom onyx.llm.utils import get_max_input_tokens\nfrom onyx.llm.utils import model_supports_image_input\nfrom onyx.llm.well_known_providers.auto_update_models import LLMRecommendations\nfrom onyx.llm.well_known_providers.auto_update_service import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/cache_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/cache_manager.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Cache manager for storing and retrieving prompt cache metadata.\"\"\"\n\nimport hashlib\nimport json\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom onyx.configs.model_configs import PROMPT_CACHE_REDIS_TTL_MULTIPLIER\nfrom onyx.key_value_store.store import PgRedisKVStore\nfrom onyx.llm.interfaces import LanguageModelInput",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/__init__.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Prompt caching framework for LLM providers.\n\nThis module provides a framework for enabling prompt caching across different\nLLM providers. It supports both implicit caching (automatic provider-side caching)\nand explicit caching (with cache metadata management).\n\"\"\"\n\nfrom onyx.llm.prompt_cache.cache_manager import CacheManager\nfrom onyx.llm.prompt_cache.cache_manager import generate_cache_key_hash\nfrom onyx.llm.prompt_cache.models import CacheMetadata",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/processor.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Main processor for prompt caching.\"\"\"\n\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom onyx.configs.model_configs import ENABLE_PROMPT_CACHING\nfrom onyx.llm.interfaces import LLMConfig\nfrom onyx.llm.models import LanguageModelInput\nfrom onyx.llm.prompt_cache.cache_manager import generate_cache_key_hash\nfrom onyx.llm.prompt_cache.models import CacheMetadata",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/utils.py",
      "line_number": 1,
      "code_snippet": "# pyright: reportMissingTypeStubs=false\n\"\"\"Utility functions for prompt caching.\"\"\"\n\nimport json\nfrom collections.abc import Callable\nfrom collections.abc import Sequence\nfrom typing import Any\n\nfrom onyx.llm.models import ChatCompletionMessage\nfrom onyx.llm.models import LanguageModelInput",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_137",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'litellm.model_cost[model_key].update' is used in 'UPDATE' on line 137 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 137,
      "code_snippet": "                # Update existing entry with our metadata\n                litellm.model_cost[model_key].update(metadata)\n            else:\n                # Model not in litellm.model_cost - add it with just our metadata",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_112",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'load_model_metadata_enrichments' on line 112 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 112,
      "code_snippet": "def load_model_metadata_enrichments() -> None:\n    \"\"\"\n    Load model metadata enrichments from JSON file and merge into litellm.model_cost.\n\n    This adds model_vendor, display_name, and model_version fields\n    to litellm's model_cost dict. These fields are used by the UI to display\n    models grouped by vendor with human-friendly names.\n\n    Once LiteLLM accepts our upstream PR to add these fields natively,\n    this function and the JSON file can be removed.\n    \"\"\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_22_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'register_ollama_models'",
      "description": "Function 'register_ollama_models' on line 21 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 22,
      "code_snippet": "# TODO: We might not need to register ollama_chat in addition to ollama but let's just do it for good measure for now.\ndef register_ollama_models() -> None:\n    litellm.register_model(\n        model_cost={\n            # GPT-OSS models\n            \"ollama_chat/gpt-oss:120b-cloud\": {\"supports_function_calling\": True},",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_112_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write operation without confirmation in 'load_model_metadata_enrichments'",
      "description": "Function 'load_model_metadata_enrichments' on line 112 performs high-risk delete/write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 112,
      "code_snippet": "\n\ndef load_model_metadata_enrichments() -> None:\n    \"\"\"\n    Load model metadata enrichments from JSON file and merge into litellm.model_cost.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_21_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'register_ollama_models'",
      "description": "Function 'register_ollama_models' on line 21 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 21,
      "code_snippet": "\n# TODO: We might not need to register ollama_chat in addition to ollama but let's just do it for good measure for now.\ndef register_ollama_models() -> None:\n    litellm.register_model(\n        model_cost={\n            # GPT-OSS models",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_112_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'load_model_metadata_enrichments'",
      "description": "Function 'load_model_metadata_enrichments' on line 112 makes critical financial, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 112,
      "code_snippet": "\n\ndef load_model_metadata_enrichments() -> None:\n    \"\"\"\n    Load model metadata enrichments from JSON file and merge into litellm.model_cost.\n",
      "recommendation": "Critical financial, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/config.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom pathlib import Path\n\nimport litellm\n\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/monkey_patches.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/litellm_singleton/monkey_patches.py",
      "line_number": 1,
      "code_snippet": "import json\nimport time\nimport uuid\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import TypedDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/__init__.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Provider adapters for prompt caching.\"\"\"\n\nfrom onyx.llm.prompt_cache.providers.anthropic import AnthropicPromptCacheProvider\nfrom onyx.llm.prompt_cache.providers.base import PromptCacheProvider\nfrom onyx.llm.prompt_cache.providers.factory import get_provider_adapter\nfrom onyx.llm.prompt_cache.providers.noop import NoOpPromptCacheProvider\nfrom onyx.llm.prompt_cache.providers.openai import OpenAIPromptCacheProvider\nfrom onyx.llm.prompt_cache.providers.vertex import VertexAIPromptCacheProvider\n\n__all__ = [",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/vertex.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/vertex.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Vertex AI provider adapter for prompt caching.\"\"\"\n\nfrom collections.abc import Sequence\n\nfrom onyx.llm.interfaces import LanguageModelInput\nfrom onyx.llm.models import ChatCompletionMessage\nfrom onyx.llm.prompt_cache.models import CacheMetadata\nfrom onyx.llm.prompt_cache.providers.base import PromptCacheProvider\nfrom onyx.llm.prompt_cache.utils import prepare_messages_with_cacheable_transform\nfrom onyx.llm.prompt_cache.utils import revalidate_message_from_original",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/anthropic.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/llm/prompt_cache/providers/anthropic.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Anthropic provider adapter for prompt caching.\"\"\"\n\nfrom collections.abc import Sequence\n\nfrom onyx.llm.interfaces import LanguageModelInput\nfrom onyx.llm.models import ChatCompletionMessage\nfrom onyx.llm.prompt_cache.models import CacheMetadata\nfrom onyx.llm.prompt_cache.providers.base import PromptCacheProvider\nfrom onyx.llm.prompt_cache.utils import prepare_messages_with_cacheable_transform\nfrom onyx.llm.prompt_cache.utils import revalidate_message_from_original",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/fake_tools/research_agent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/fake_tools/research_agent.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import cast\n\nfrom onyx.chat.chat_state import ChatStateContainer\nfrom onyx.chat.chat_utils import create_tool_call_failure_messages\nfrom onyx.chat.citation_processor import CitationMapping\nfrom onyx.chat.citation_processor import DynamicCitationProcessor\nfrom onyx.chat.citation_utils import collapse_citations\nfrom onyx.chat.citation_utils import update_citation_processor_from_tool_response\nfrom onyx.chat.emitter import Emitter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search_like_tool_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search_like_tool_utils.py",
      "line_number": 1,
      "code_snippet": "from onyx.connectors.models import Document\nfrom onyx.connectors.models import IndexingDocument\nfrom onyx.connectors.models import Section\n\n\nFINAL_CONTEXT_DOCUMENTS_ID = \"final_context_documents\"\nFINAL_SEARCH_QUERIES_ID = \"final_search_queries\"\nSEARCH_INFERENCE_SECTIONS_ID = \"search_inference_sections\"\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/utils.py",
      "line_number": 1,
      "code_snippet": "import json\n\nfrom onyx.context.search.models import InferenceSection\n\n\ndef convert_inference_sections_to_llm_string(\n    top_sections: list[InferenceSection],\n    citation_start: int = 1,\n    limit: int | None = None,\n    include_source_type: bool = True,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/models.py",
      "line_number": 1,
      "code_snippet": "from abc import abstractmethod\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom enum import Enum\n\nfrom pydantic import BaseModel\nfrom pydantic import field_validator\n\nfrom onyx.utils.url import normalize_url\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/providers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/providers.py",
      "line_number": 1,
      "code_snippet": "from onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.models import InternetSearchProvider\nfrom onyx.db.web_search import fetch_active_web_content_provider\nfrom onyx.db.web_search import fetch_active_web_search_provider\nfrom onyx.tools.tool_implementations.open_url.firecrawl import FirecrawlClient\nfrom onyx.tools.tool_implementations.open_url.models import (\n    WebContentProvider,\n)\nfrom onyx.tools.tool_implementations.open_url.onyx_web_crawler import OnyxWebCrawler\nfrom onyx.tools.tool_implementations.web_search.clients.exa_client import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/utils.py",
      "line_number": 1,
      "code_snippet": "from onyx.configs.constants import DocumentSource\nfrom onyx.context.search.models import InferenceChunk\nfrom onyx.context.search.models import InferenceSection\nfrom onyx.tools.tool_implementations.open_url.models import WebContent\nfrom onyx.tools.tool_implementations.web_search.models import WEB_SEARCH_PREFIX\nfrom onyx.tools.tool_implementations.web_search.models import WebSearchResult\n\n\ndef filter_web_search_results_with_no_title_or_snippet(\n    results: list[WebSearchResult],",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/web_search_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/web_search_tool.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\nfrom typing import cast\n\nfrom sqlalchemy.orm import Session\nfrom typing_extensions import override\n\nfrom onyx.chat.emitter import Emitter\nfrom onyx.context.search.models import SearchDocsResponse\nfrom onyx.context.search.utils import convert_inference_sections_to_search_docs",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/knowledge_graph/knowledge_graph_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/knowledge_graph/knowledge_graph_tool.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.chat.emitter import Emitter\nfrom onyx.db.kg_config import get_kg_config_settings\nfrom onyx.server.query_and_chat.placement import Placement\nfrom onyx.tools.interface import Tool\nfrom onyx.tools.models import ToolResponse\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/onyx_web_crawler.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/onyx_web_crawler.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom collections.abc import Sequence\n\nfrom onyx.file_processing.html_utils import ParsedHTML\nfrom onyx.file_processing.html_utils import web_html_cleanup\nfrom onyx.tools.tool_implementations.open_url.models import (\n    WebContent,\n)\nfrom onyx.tools.tool_implementations.open_url.models import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/open_url_tool.py_642_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 642. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/open_url_tool.py",
      "line_number": 642,
      "code_snippet": "\n        try:\n            chunks = self._document_index.id_based_retrieval(\n                chunk_requests=chunk_requests,\n                filters=filters,\n                batch_retrieval=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/open_url_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/open_url_tool.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom collections import defaultdict\nfrom typing import Any\n\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\nfrom typing_extensions import override\n\nfrom onyx.chat.emitter import Emitter\nfrom onyx.context.search.models import IndexFilters",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/url_normalization.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/open_url/url_normalization.py",
      "line_number": 1,
      "code_snippet": "\"\"\"URL normalization for OpenURL tool.\n\nEach connector implements normalize_url() as a class method to normalize URLs to match\nthe canonical Document.id format used during ingestion. This ensures OpenURL can find\nindexed documents.\n\nUsage:\n    normalized = normalize_url(\"https://docs.google.com/document/d/123/edit\")\n    # Returns: \"https://docs.google.com/document/d/123\"\n\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/python/python_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/python/python_tool.py",
      "line_number": 1,
      "code_snippet": "import mimetypes\nfrom io import BytesIO\nfrom typing import Any\nfrom typing import cast\n\nfrom pydantic import TypeAdapter\nfrom sqlalchemy.orm import Session\nfrom typing_extensions import override\n\nfrom onyx.chat.emitter import Emitter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/python/code_interpreter_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/python/code_interpreter_client.py",
      "line_number": 1,
      "code_snippet": "from typing import Literal\nfrom typing import TypedDict\n\nimport requests\nfrom pydantic import BaseModel\n\nfrom onyx.configs.app_configs import CODE_INTERPRETER_BASE_URL\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/images/image_generation_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/images/image_generation_tool.py",
      "line_number": 1,
      "code_snippet": "import json\nimport threading\nfrom typing import Any\nfrom typing import cast\n\nimport requests\nfrom sqlalchemy.orm import Session\nfrom typing_extensions import override\n\nfrom onyx.chat.emitter import Emitter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_client.py_119_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [107]) and executes code (lines [119]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_client.py",
      "line_number": 119,
      "code_snippet": "#         # Message ID counter\n#         self._message_id_counter = 0\n\n#         # For stdio transport\n#         self.process: Optional[subprocess.Popen] = None\n\n\ndef _create_mcp_client_function_runner(\n    function: Callable[[ClientSession], Awaitable[T]],\n    server_url: str,",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_client.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nMCP (Model Context Protocol) Client Implementation\n\nThis module provides a proper MCP client that follows the JSON-RPC 2.0 specification\nand handles connection initialization, session management, and protocol communication.\n\"\"\"\n\nfrom collections.abc import Awaitable\nfrom collections.abc import Callable\nfrom enum import Enum",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/mcp/mcp_tool.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\n\nfrom onyx.chat.emitter import Emitter\nfrom onyx.db.enums import MCPAuthenticationType\nfrom onyx.db.enums import MCPTransport\nfrom onyx.db.models import MCPConnectionConfig\nfrom onyx.db.models import MCPServer\nfrom onyx.server.query_and_chat.placement import Placement\nfrom onyx.server.query_and_chat.streaming_models import CustomToolDelta",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_tool.py_388_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 388. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_tool.py",
      "line_number": 388,
      "code_snippet": "            )\n\n            chunks = slack_retrieval(\n                query=chunk_request,\n                access_token=access_token,\n                db_session=db_session,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_tool.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_tool.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nAn explanation of the search tool found below:\n\nStep 1: Queries\n- The LLM will generate some queries based on the chat history for what it thinks are the best things to search for.\nThis has a pretty generic prompt so it's not perfectly tuned for search but provides breadth and also the LLM can often break up\nthe query into multiple searches which the other flows do not do. Exp: Compare the sales process between company X and Y can be\nbroken up into \"sales process company X\" and \"sales process company Y\".\n- A specifial prompt and history is used to generate another query which is best tuned for a semantic/hybrid search pipeline.\n- A small set of keyword emphasized queries are also generated to cover additional breadth. This is important for cases where",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/constants.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Constants for search tool implementations.\"\"\"\n\n# Query Expansion and Fusion Weights\n# Taking an opinionated stance on the weights, no chance users can do a good job customizing this.\n# The dedicated rephrased/extracted semantic query is likely the best for hybrid search\nLLM_SEMANTIC_QUERY_WEIGHT = 1.3\n# The keyword expansions provide more breadth through a different search ranking function\n# This one is likely to produce the most different results.\nLLM_KEYWORD_QUERY_WEIGHT = 1.0\n# This is also lower because it is the LLM generated query without the custom instructions specifically for this purpose.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py_176_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 176. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py",
      "line_number": 176,
      "code_snippet": "\n        try:\n            chunks_above = document_index.id_based_retrieval(\n                chunk_requests=[above_request],\n                filters=filters,\n                batch_retrieval=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py_198_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 198. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py",
      "line_number": 198,
      "code_snippet": "\n        try:\n            chunks_below = document_index.id_based_retrieval(\n                chunk_requests=[below_request],\n                filters=filters,\n                batch_retrieval=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/search/search_utils.py",
      "line_number": 1,
      "code_snippet": "from collections import defaultdict\nfrom collections.abc import Callable\nfrom typing import TypeVar\n\nfrom onyx.context.search.models import ContextExpansionType\nfrom onyx.context.search.models import IndexFilters\nfrom onyx.context.search.models import InferenceChunk\nfrom onyx.context.search.models import InferenceSection\nfrom onyx.context.search.utils import inference_section_from_chunks\nfrom onyx.document_index.interfaces import DocumentIndex",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool_prompts.py",
      "line_number": 1,
      "code_snippet": "from onyx.prompts.constants import GENERAL_SEP_PAT\n\nDONT_USE_TOOL = \"Don't use tool\"\nUSE_TOOL = \"Use tool\"\n\n\n\"\"\"Prompts to determine if we should use a custom tool or not.\"\"\"\n\n\nSHOULD_USE_CUSTOM_TOOL_SYSTEM_PROMPT = (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/openapi_parsing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/openapi_parsing.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom pydantic import BaseModel\n\nREQUEST_BODY = \"requestBody\"\n\n\nclass PathSpec(BaseModel):\n    path: str",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py_337_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4o'' is used without version pinning on line 337. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py",
      "line_number": 337,
      "code_snippet": "\n    openai_client = openai.OpenAI()\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py_336",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py",
      "line_number": 336,
      "code_snippet": "        dynamic_schema_info=None,\n    )\n\n    openai_client = openai.OpenAI()\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py_337",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/custom/custom_tool.py",
      "line_number": 337,
      "code_snippet": "    )\n\n    openai_client = openai.OpenAI()\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/google_pse_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/google_pse_client.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import Any\n\nimport requests\nfrom fastapi import HTTPException\n\nfrom onyx.tools.tool_implementations.web_search.models import (\n    WebSearchProvider,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/exa_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/exa_client.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections.abc import Sequence\n\nfrom exa_py import Exa\nfrom exa_py.api import HighlightsContentsOptions\nfrom fastapi import HTTPException\n\nfrom onyx.connectors.cross_connector_utils.miscellaneous_utils import time_str_to_utc\nfrom onyx.tools.tool_implementations.open_url.models import WebContent\nfrom onyx.tools.tool_implementations.open_url.models import WebContentProvider",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/searxng_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tools/tool_implementations/web_search/clients/searxng_client.py",
      "line_number": 1,
      "code_snippet": "import requests\nfrom fastapi import HTTPException\n\nfrom onyx.tools.tool_implementations.web_search.models import (\n    WebSearchProvider,\n)\nfrom onyx.tools.tool_implementations.web_search.models import (\n    WebSearchResult,\n)\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_709_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 709. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 709,
      "code_snippet": "        return vespa_document_index.delete(document_id=doc_id, chunk_count=chunk_count)\n\n    def id_based_retrieval(\n        self,\n        chunk_requests: list[VespaChunkRequest],\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_735_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 735. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 735,
      "code_snippet": "                )\n            )\n        return vespa_document_index.id_based_retrieval(\n            chunk_requests=generic_chunk_requests,\n            filters=filters,\n            batch_retrieval=batch_retrieval,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_742_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 742. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 742,
      "code_snippet": "\n    @log_function_time(print_only=True, debug_only=True)\n    def hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: Embedding,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_777_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 777. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 777,
      "code_snippet": "            else QueryType.SEMANTIC\n        )\n        return vespa_document_index.hybrid_retrieval(\n            query,\n            query_embedding,\n            final_keywords,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_787_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 787. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 787,
      "code_snippet": "        )\n\n    def admin_retrieval(\n        self,\n        query: str,\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_1018_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 1018. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 1018,
      "code_snippet": "        logger.info(\"Batch deletion completed\")\n\n    def random_retrieval(\n        self,\n        filters: IndexFilters,\n        num_to_retrieve: int = 10,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_1038_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 1038. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 1038,
      "code_snippet": "            httpx_client=self.httpx_client,\n        )\n        return vespa_document_index.random_retrieval(\n            filters=filters,\n            num_to_retrieve=num_to_retrieve,\n        )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py_923",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 923 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/index.py",
      "line_number": 923,
      "code_snippet": "            # Construct the query to fetch document IDs\n            query_params = {\n                \"yql\": f'select id from sources * where tenant_id contains \"{tenant_id}\";',\n                \"offset\": str(offset),\n                \"hits\": str(limit),",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py_500_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 500. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
      "line_number": 500,
      "code_snippet": "                    )\n\n    def id_based_retrieval(\n        self,\n        chunk_requests: list[DocumentSectionRequest],\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py_519_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 519. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
      "line_number": 519,
      "code_snippet": "        if batch_retrieval:\n            return cleanup_content_for_chunks(\n                batch_search_api_retrieval(\n                    index_name=self._index_name,\n                    chunk_requests=sanitized_chunk_requests,\n                    filters=filters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py_529_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 529. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
      "line_number": 529,
      "code_snippet": "            )\n        return cleanup_content_for_chunks(\n            parallel_visit_api_retrieval(\n                index_name=self._index_name,\n                chunk_requests=sanitized_chunk_requests,\n                filters=filters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py_539_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 539. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
      "line_number": 539,
      "code_snippet": "        )\n\n    def hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: Embedding,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py_600_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 600. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
      "line_number": 600,
      "code_snippet": "        return cleanup_content_for_chunks(query_vespa(params))\n\n    def random_retrieval(\n        self,\n        filters: IndexFilters,\n        num_to_retrieve: int = 100,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py_570_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'ranking_profile' containing sensitive data is being logged on line 570. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/vespa_document_index.py",
      "line_number": 570,
      "code_snippet": "        )\n\n        logger.info(f\"Selected ranking profile: {ranking_profile}\")\n\n        logger.debug(f\"Query YQL: {yql}\")",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py_291_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 291. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py",
      "line_number": 291,
      "code_snippet": "\n\ndef parallel_visit_api_retrieval(\n    index_name: str,\n    chunk_requests: list[VespaChunkRequest],\n    filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py_440_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 440. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py",
      "line_number": 440,
      "code_snippet": "\n\ndef batch_search_api_retrieval(\n    index_name: str,\n    chunk_requests: list[VespaChunkRequest],\n    filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py_488_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 488. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py",
      "line_number": 488,
      "code_snippet": "        logger.debug(f\"Retrieving {len(uncapped_requests)} uncapped requests\")\n        retrieved_chunks.extend(\n            parallel_visit_api_retrieval(\n                index_name, uncapped_requests, filters, get_large_chunks\n            )\n        )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/chunk_retrieval.py",
      "line_number": 1,
      "code_snippet": "import json\nimport string\nfrom collections.abc import Callable\nfrom collections.abc import Mapping\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\n\nimport httpx",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/kg_interactions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/kg_interactions.py",
      "line_number": 1,
      "code_snippet": "from onyx.db.document import get_document_kg_entities_and_relationships\nfrom onyx.db.document import get_num_chunks_for_document\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.document_index.vespa.index import KGUChunkUpdateRequest\nfrom onyx.document_index.vespa.index import VespaIndex\nfrom onyx.utils.logger import setup_logger\nfrom shared_configs.configs import MULTI_TENANT\n\nlogger = setup_logger()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/client.py_75",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 75 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/client.py",
      "line_number": 75,
      "code_snippet": "        response = self._client.indices.create(index=self._index_name, body=body)\n        if not response.get(\"acknowledged\", False):\n            raise RuntimeError(f\"Failed to create index {self._index_name}.\")\n        response_index = response.get(\"index\", \"\")\n        if response_index != self._index_name:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/client.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Any\n\nfrom opensearchpy import OpenSearch\nfrom opensearchpy.exceptions import TransportError\n\nfrom onyx.configs.app_configs import OPENSEARCH_ADMIN_PASSWORD\nfrom onyx.configs.app_configs import OPENSEARCH_ADMIN_USERNAME\nfrom onyx.configs.app_configs import OPENSEARCH_HOST\nfrom onyx.configs.app_configs import OPENSEARCH_REST_API_PORT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/constants.py",
      "line_number": 1,
      "code_snippet": "# Size of the dynamic list used to consider elements during kNN graph creation.\n# Higher values improve search quality but increase indexing time. Values\n# typically range between 100 - 512.\nEF_CONSTRUCTION = 256\n# Number of bi-directional links per element. Higher values improve search\n# quality but increase memory footprint. Values typically range between 12 - 48.\nM = 32  # Increased for better accuracy.\n\n# Default value for the maximum number of tokens a chunk can hold, if none is\n# specified when creating an index.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_295_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 295. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 295,
      "code_snippet": "        raise NotImplementedError(\"[ANDREI]: Update is not implemented for OpenSearch.\")\n\n    def id_based_retrieval(\n        self,\n        chunk_requests: list[VespaChunkRequest],\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_311_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 311. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 311,
      "code_snippet": "        ]\n\n        return self._real_index.id_based_retrieval(\n            section_requests, filters, batch_retrieval\n        )\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_315_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 315. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 315,
      "code_snippet": "        )\n\n    def hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: Embedding,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_336_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 336. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 336,
      "code_snippet": "            query_type = QueryType.SEMANTIC  # Default to semantic for hybrid.\n\n        return self._real_index.hybrid_retrieval(\n            query=query,\n            query_embedding=query_embedding,\n            final_keywords=final_keywords,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_346_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 346. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 346,
      "code_snippet": "        )\n\n    def admin_retrieval(\n        self,\n        query: str,\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_357_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 357. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 357,
      "code_snippet": "        )\n\n    def random_retrieval(\n        self,\n        filters: IndexFilters,\n        num_to_retrieve: int = 100,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_362_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 362. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 362,
      "code_snippet": "        num_to_retrieve: int = 100,\n    ) -> list[InferenceChunk]:\n        return self._real_index.random_retrieval(\n            filters=filters,\n            num_to_retrieve=num_to_retrieve,\n            dirty=None,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_499_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 499. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 499,
      "code_snippet": "        # raises.\n\n    def id_based_retrieval(\n        self,\n        chunk_requests: list[DocumentSectionRequest],\n        filters: IndexFilters,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_535_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 535. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 535,
      "code_snippet": "        return results\n\n    def hybrid_retrieval(\n        self,\n        query: str,\n        query_embedding: Embedding,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_566_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 566. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 566,
      "code_snippet": "        return inference_chunks\n\n    def random_retrieval(\n        self,\n        filters: IndexFilters,\n        num_to_retrieve: int = 100,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/opensearch_document_index.py",
      "line_number": 1,
      "code_snippet": "import json\n\nimport httpx\n\nfrom onyx.configs.chat_configs import TITLE_CONTENT_RATIO\nfrom onyx.connectors.cross_connector_utils.miscellaneous_utils import (\n    get_experts_stores_representations,\n)\nfrom onyx.context.search.enums import QueryType\nfrom onyx.context.search.models import IndexFilters",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/search.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom onyx.document_index.interfaces_new import TenantState\nfrom onyx.document_index.opensearch.constants import SEARCH_CONTENT_KEYWORD_WEIGHT\nfrom onyx.document_index.opensearch.constants import SEARCH_CONTENT_PHRASE_WEIGHT\nfrom onyx.document_index.opensearch.constants import SEARCH_CONTENT_VECTOR_WEIGHT\nfrom onyx.document_index.opensearch.constants import SEARCH_TITLE_KEYWORD_WEIGHT\nfrom onyx.document_index.opensearch.constants import SEARCH_TITLE_VECTOR_WEIGHT\nfrom onyx.document_index.opensearch.schema import CHUNK_INDEX_FIELD_NAME\nfrom onyx.document_index.opensearch.schema import CONTENT_FIELD_NAME",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/schema.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/opensearch/schema.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import Self\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\nfrom pydantic import field_serializer\nfrom pydantic import field_validator\nfrom pydantic import model_serializer",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/shared_utils/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/shared_utils/utils.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import cast\n\nimport httpx\n\nfrom onyx.configs.app_configs import MANAGED_VESPA\nfrom onyx.configs.app_configs import VESPA_CLOUD_CERT_PATH\nfrom onyx.configs.app_configs import VESPA_CLOUD_KEY_PATH\nfrom onyx.configs.app_configs import VESPA_REQUEST_TIMEOUT\nfrom onyx.document_index.vespa_constants import VESPA_APP_CONTAINER_URL",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/shared_utils/vespa_request_builders.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/document_index/vespa/shared_utils/vespa_request_builders.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\n\nfrom onyx.configs.constants import INDEX_SEPARATOR\nfrom onyx.context.search.models import IndexFilters\nfrom onyx.document_index.interfaces import VespaChunkRequest\nfrom onyx.document_index.vespa_constants import ACCESS_CONTROL_LIST\nfrom onyx.document_index.vespa_constants import CHUNK_ID\nfrom onyx.document_index.vespa_constants import DOC_UPDATED_AT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_73_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 73. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 73,
      "code_snippet": "    \"\"\"\n\n    def eval(\n        self,\n        task: Callable[[dict[str, Any]], EvalToolResult],\n        configuration: EvalConfigurationOptions,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_102_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 102. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 102,
      "code_snippet": "            input_data = item.get(\"input\", {})\n\n            # Check if this is a multi-turn eval (has 'messages' array)\n            if \"messages\" in input_data:\n                self._run_multi_turn_eval(\n                    i, total, item, multi_turn_task, passed, failed, no_assertion",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_104_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 104. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 104,
      "code_snippet": "            # Check if this is a multi-turn eval (has 'messages' array)\n            if \"messages\" in input_data:\n                self._run_multi_turn_eval(\n                    i, total, item, multi_turn_task, passed, failed, no_assertion\n                )\n            else:",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_108_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 108. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 108,
      "code_snippet": "                )\n            else:\n                self._run_single_turn_eval(\n                    i, total, item, task, passed, failed, no_assertion\n                )\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_132_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 132. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 132,
      "code_snippet": "        return EvalationAck(success=(failed[0] == 0))\n\n    def _run_single_turn_eval(\n        self,\n        i: int,\n        total: int,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_174_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 174. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 174,
      "code_snippet": "            logger.exception(f\"Error running eval for input: {message}\")\n\n    def _run_multi_turn_eval(\n        self,\n        i: int,\n        total: int,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/local.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nLocal eval provider that runs evaluations and outputs results to the CLI.\nNo external dependencies like Braintrust required.\n\"\"\"\n\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom onyx.evals.models import EvalationAck\nfrom onyx.evals.models import EvalConfigurationOptions",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/braintrust.py_106_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 106. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/braintrust.py",
      "line_number": 106,
      "code_snippet": "\nclass BraintrustEvalProvider(EvalProvider):\n    def eval(\n        self,\n        task: Callable[[dict[str, Any]], EvalToolResult],\n        configuration: EvalConfigurationOptions,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/braintrust.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/providers/braintrust.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import Any\nfrom typing import Union\n\nfrom braintrust import Eval\nfrom braintrust import EvalCase\nfrom braintrust import init_dataset\nfrom braintrust import Score\n\nfrom onyx.configs.app_configs import BRAINTRUST_MAX_CONCURRENCY",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/one_off/create_braintrust_dataset.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/evals/one_off/create_braintrust_dataset.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to create a Braintrust dataset from the DR Master Question & Metric Sheet CSV.\n\nThis script:\n1. Parses the CSV file\n2. Filters records where \"Should we use it\" is TRUE and \"web-only\" is in categories\n3. Creates a Braintrust dataset with Question as input and research_type metadata\n\nUsage:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/framework/provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/framework/provider.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport threading\nimport uuid\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/framework/spans.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/tracing/framework/spans.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport abc\nimport contextvars\nfrom types import TracebackType\nfrom typing import Any\nfrom typing import Generic\nfrom typing import TypeVar\n\nfrom typing_extensions import TypedDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/freshdesk/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/freshdesk/connector.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import List\n\nimport requests\nfrom retry import retry\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/guru/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/guru/connector.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nimport requests\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.cross_connector_utils.miscellaneous_utils import time_str_to_utc",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/productboard/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/productboard/connector.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\nfrom itertools import chain\nfrom typing import Any\nfrom typing import cast\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dateutil import parser\nfrom retry import retry\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/imap/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/imap/models.py",
      "line_number": 1,
      "code_snippet": "import email\nfrom datetime import datetime\nfrom email.message import Message\nfrom enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass Header(str, Enum):\n    SUBJECT_HEADER = \"subject\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/imap/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/imap/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport email\nimport imaplib\nimport os\nimport re\nfrom datetime import datetime\nfrom datetime import timezone\nfrom email.message import Message\nfrom email.utils import parseaddr\nfrom enum import Enum",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_184",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_manage_async_retrieval' on line 184 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 184,
      "code_snippet": "def _manage_async_retrieval(\n    token: str,\n    requested_start_date_string: str,\n    channel_names: list[str],\n    server_ids: list[int],\n    start: datetime | None = None,\n    end: datetime | None = None,\n) -> Iterable[Document]:\n    # parse requested_start_date_string to datetime\n    pull_date: datetime | None = (\n        datetime.strptime(requested_start_date_string, \"%Y-%m-%d\").replace(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_227",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'run_and_yield' on line 227 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 227,
      "code_snippet": "    def run_and_yield() -> Iterable[Document]:\n        loop = asyncio.new_event_loop()\n        try:\n            # Get the async generator\n            async_gen = _async_fetch()\n            # Convert to AsyncIterator\n            async_iter = async_gen.__aiter__()\n            while True:\n                try:\n                    # Create a coroutine by calling anext with the async iterator\n                    next_coro = anext(async_iter)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_184_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 184. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 184,
      "code_snippet": "\n\ndef _manage_async_retrieval(\n    token: str,\n    requested_start_date_string: str,\n    channel_names: list[str],",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_282_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 282. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 282,
      "code_snippet": "    ) -> GenerateDocumentsOutput:\n        doc_batch = []\n        for doc in _manage_async_retrieval(\n            token=self.discord_bot_token,\n            requested_start_date_string=self.requested_start_date_string,\n            channel_names=self.channel_names,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_184_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '_manage_async_retrieval'",
      "description": "Function '_manage_async_retrieval' on line 184 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 184,
      "code_snippet": "\n            yield _convert_message_to_document(thread_message, sections)\n\n\ndef _manage_async_retrieval(\n    token: str,\n    requested_start_date_string: str,\n    channel_names: list[str],\n    server_ids: list[int],\n    start: datetime | None = None,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_227_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'run_and_yield'",
      "description": "Function 'run_and_yield' on line 227 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 227,
      "code_snippet": "                    yield doc\n\n    def run_and_yield() -> Iterable[Document]:\n        loop = asyncio.new_event_loop()\n        try:\n            # Get the async generator",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_184_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '_manage_async_retrieval'",
      "description": "Function '_manage_async_retrieval' on line 184 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 184,
      "code_snippet": "\n\ndef _manage_async_retrieval(\n    token: str,\n    requested_start_date_string: str,\n    channel_names: list[str],",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_227_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_and_yield'",
      "description": "Function 'run_and_yield' on line 227 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 227,
      "code_snippet": "                    yield doc\n\n    def run_and_yield() -> Iterable[Document]:\n        loop = asyncio.new_event_loop()\n        try:\n            # Get the async generator",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discord/connector.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom collections.abc import AsyncIterable\nfrom collections.abc import Iterable\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\n\nfrom discord import Client\nfrom discord.channel import TextChannel",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gitlab/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gitlab/connector.py",
      "line_number": 1,
      "code_snippet": "import fnmatch\nimport itertools\nfrom collections import deque\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import TypeVar\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/drupal_wiki/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/drupal_wiki/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Generic\nfrom typing import List\nfrom typing import Optional\nfrom typing import TypeVar\n\nfrom pydantic import BaseModel\n\nfrom onyx.connectors.interfaces import ConnectorCheckpoint\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/drupal_wiki/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/drupal_wiki/connector.py",
      "line_number": 1,
      "code_snippet": "import mimetypes\nfrom io import BytesIO\nfrom typing import Any\n\nimport requests\nfrom typing_extensions import override\n\nfrom onyx.configs.app_configs import CONTINUE_ON_CONNECTOR_FAILURE\nfrom onyx.configs.app_configs import DRUPAL_WIKI_ATTACHMENT_SIZE_THRESHOLD\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/xenforo/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/xenforo/connector.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nThis is the XenforoConnector class. It is used to connect to a Xenforo forum and load or update documents from the forum.\n\nTo use this class, you need to provide the URL of the Xenforo forum board you want to connect to when creating an instance\nof the class. The URL should be a string that starts with 'http://' or 'https://', followed by the domain name of the\nforum, followed by the board name. For example:\n\n    base_url = 'https://www.example.com/forum/boards/some-topic/'\n\nThe `load_from_state` method is used to load documents from the forum. It takes an optional `state` parameter, which",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slab/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slab/connector.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom urllib.parse import urljoin\n\nimport requests\nfrom dateutil import parser",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/cross_connector_utils/miscellaneous_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/cross_connector_utils/miscellaneous_utils.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import TypeVar\nfrom urllib.parse import urljoin\nfrom urllib.parse import urlparse\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/cross_connector_utils/rate_limit_wrapper.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/cross_connector_utils/rate_limit_wrapper.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Callable\nfrom functools import wraps\nfrom typing import Any\nfrom typing import cast\nfrom typing import TypeVar\n\nimport requests\n\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/file/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/file/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom datetime import datetime\nfrom datetime import timezone\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import IO\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.configs.constants import FileOrigin",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/web/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/web/connector.py",
      "line_number": 1,
      "code_snippet": "import ipaddress\nimport random\nimport socket\nimport time\nfrom datetime import datetime\nfrom datetime import timezone\nfrom enum import Enum\nfrom typing import Any\nfrom typing import cast\nfrom typing import Tuple",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_auth.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\n\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials as OAuthCredentials\nfrom google.oauth2.service_account import Credentials as ServiceAccountCredentials\n\nfrom onyx.configs.app_configs import OAUTH_GOOGLE_DRIVE_CLIENT_ID\nfrom onyx.configs.app_configs import OAUTH_GOOGLE_DRIVE_CLIENT_SECRET\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_kv.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_kv.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import cast\nfrom urllib.parse import parse_qs\nfrom urllib.parse import ParseResult\nfrom urllib.parse import urlparse\n\nfrom google.oauth2.credentials import Credentials as OAuthCredentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow  # type: ignore\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_113_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 113. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 113,
      "code_snippet": "\n\ndef _execute_single_retrieval(\n    retrieval_function: Callable,\n    continue_on_404_or_403: bool = False,\n    **request_kwargs: Any,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_136_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 136. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 136,
      "code_snippet": "                )\n                request_kwargs.pop(\"pageToken\")\n                return _execute_single_retrieval(\n                    retrieval_function,\n                    continue_on_404_or_403,\n                    **request_kwargs,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_166_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 166. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 166,
      "code_snippet": "\n\ndef execute_single_retrieval(\n    retrieval_function: Callable,\n    list_key: str | None = None,\n    continue_on_404_or_403: bool = False,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_172_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 172. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 172,
      "code_snippet": "    **request_kwargs: Any,\n) -> Iterator[GoogleDriveFileType]:\n    results = _execute_single_retrieval(\n        retrieval_function,\n        continue_on_404_or_403,\n        **request_kwargs,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_188_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 188. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 188,
      "code_snippet": "# execute_paginated_retrieval_with_max_pages instead if you want\n# the early stop + yield None after max_num_pages behavior.\ndef execute_paginated_retrieval(\n    retrieval_function: Callable,\n    list_key: str | None = None,\n    continue_on_404_or_403: bool = False,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_194_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 194. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 194,
      "code_snippet": "    **kwargs: Any,\n) -> Iterator[GoogleDriveFileType]:\n    for item in _execute_paginated_retrieval(\n        retrieval_function,\n        list_key,\n        continue_on_404_or_403,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_211_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 211. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 211,
      "code_snippet": "    **kwargs: Any,\n) -> Iterator[GoogleDriveFileType | str]:\n    yield from _execute_paginated_retrieval(\n        retrieval_function,\n        list_key,\n        continue_on_404_or_403,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_220_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 220. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 220,
      "code_snippet": "\n\ndef _execute_paginated_retrieval(\n    retrieval_function: Callable,\n    list_key: str | None = None,\n    continue_on_404_or_403: bool = False,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_250_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 250. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 250,
      "code_snippet": "        if next_page_token:\n            request_kwargs[PAGE_TOKEN_KEY] = next_page_token\n        results = _execute_single_retrieval(\n            retrieval_function,\n            continue_on_404_or_403,\n            **request_kwargs,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/google_utils.py",
      "line_number": 1,
      "code_snippet": "import re\nimport socket\nimport time\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom enum import Enum\nfrom typing import Any\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/resources.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/resources.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import Any\n\nfrom google.auth.exceptions import RefreshError\nfrom google.oauth2.credentials import Credentials as OAuthCredentials\nfrom google.oauth2.service_account import Credentials as ServiceAccountCredentials\nfrom googleapiclient.discovery import build  # type: ignore[import-untyped]\nfrom googleapiclient.discovery import Resource\n\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/shared_constants.py_24",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 24. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/shared_constants.py",
      "line_number": 24,
      "code_snippet": "DB_CREDENTIALS_DICT_TOKEN_KEY = \"google_tokens\"\n# This is the service account key\nDB_CREDENTIALS_DICT_SERVICE_ACCOUNT_KEY = \"google_service_account_key\"\n# The email saved for both auth types\nDB_CREDENTIALS_PRIMARY_ADMIN_KEY = \"google_primary_admin\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/shared_constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_utils/shared_constants.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum as PyEnum\n\nfrom onyx.configs.constants import DocumentSource\n\n# NOTE: do not need https://www.googleapis.com/auth/documents.readonly\n# this is counted under `/auth/drive.readonly`\nGOOGLE_SCOPES = {\n    DocumentSource.GOOGLE_DRIVE: [\n        \"https://www.googleapis.com/auth/drive.readonly\",\n        \"https://www.googleapis.com/auth/drive.metadata.readonly\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/document360/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/document360/connector.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import List\nfrom typing import Optional\n\nimport requests\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/linear/connector.py_138",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 138. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/linear/connector.py",
      "line_number": 138,
      "code_snippet": "            self.linear_api_key = cast(str, credentials[\"linear_api_key\"])\n        elif \"access_token\" in credentials:\n            self.linear_api_key = \"Bearer \" + cast(str, credentials[\"access_token\"])\n        else:\n            # May need to handle case in the future if the OAuth flow expires",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/linear/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/linear/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom urllib.parse import urlparse\n\nimport requests\nfrom typing_extensions import override",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/client.py",
      "line_number": 1,
      "code_snippet": "import base64\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom urllib.parse import urljoin\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import HTTPError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/utils.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\nfrom urllib.parse import urlparse\n\nfrom bs4 import BeautifulSoup\nfrom playwright.sync_api import sync_playwright\n\nfrom onyx.file_processing.html_utils import web_html_cleanup\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py_83_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'credentials' containing sensitive data is being logged on line 83. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py",
      "line_number": 83,
      "code_snippet": "\n    def load_credentials(self, credentials: dict[str, Any]) -> dict[str, Any] | None:\n        logger.info(\"Loading Highspot credentials\")\n        self.highspot_url = credentials.get(\"highspot_url\")\n        self.key = credentials.get(\"highspot_key\")",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py_440_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'credentials' containing sensitive data is being logged on line 440. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py",
      "line_number": 440,
      "code_snippet": "            return self.client.health_check()\n        except Exception as e:\n            logger.error(f\"Failed to validate credentials: {str(e)}\")\n            return False\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/highspot/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom datetime import datetime\nfrom io import BytesIO\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bookstack/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bookstack/client.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nimport requests\n\n\nclass BookStackClientRequestFailedError(ConnectionError):\n    def __init__(self, status: int, error: str) -> None:\n        self.status_code = status\n        self.error = error\n        super().__init__(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bookstack/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bookstack/connector.py",
      "line_number": 1,
      "code_snippet": "import html\nimport time\nfrom collections.abc import Callable\nfrom datetime import datetime\nfrom typing import Any\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.bookstack.client import BookStackApiClient\nfrom onyx.connectors.bookstack.client import BookStackClientRequestFailedError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/clickup/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/clickup/connector.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import Optional\n\nimport requests\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.cross_connector_utils.rate_limit_wrapper import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zulip/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zulip/utils.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Optional\nfrom urllib.parse import quote\n\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zulip/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zulip/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nimport tempfile\nimport urllib.parse\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/testrail/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/testrail/connector.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import ClassVar\nfrom typing import Optional\n\nimport requests",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/notion/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/notion/connector.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom typing import Optional\nfrom urllib.parse import parse_qs\nfrom urllib.parse import urlparse\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/hubspot/rate_limit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/hubspot/rate_limit.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport time\nfrom collections.abc import Callable\nfrom typing import Any\nfrom typing import TypeVar\n\nfrom onyx.connectors.cross_connector_utils.rate_limit_wrapper import (\n    rate_limit_builder,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/hubspot/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/hubspot/connector.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom typing import TypeVar\n\nimport requests",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/fireflies/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/fireflies/connector.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import cast\nfrom typing import List\n\nimport requests\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/jira/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/jira/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Module with custom fields processing functions\"\"\"\n\nimport os\nfrom typing import Any\nfrom typing import List\nfrom urllib.parse import urlparse\n\nfrom jira import JIRA\nfrom jira.resources import CustomFieldOption\nfrom jira.resources import Issue",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/jira/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/jira/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport json\nimport os\nfrom collections.abc import Callable\nfrom collections.abc import Iterable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/github/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/github/utils.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import cast\n\nfrom github import Github\nfrom github.Repository import Repository\n\nfrom onyx.access.models import ExternalAccess\nfrom onyx.connectors.github.models import SerializedRepository\nfrom onyx.utils.logger import setup_logger\nfrom onyx.utils.variable_functionality import fetch_versioned_implementation",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/github/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/github/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom enum import Enum\nfrom typing import Any\nfrom typing import cast\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/outline/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/outline/client.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nimport requests\nfrom requests.exceptions import ConnectionError as RequestsConnectionError\nfrom requests.exceptions import RequestException\nfrom requests.exceptions import Timeout\n\nfrom onyx.configs.app_configs import REQUEST_TIMEOUT_SECONDS\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/outline/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/outline/connector.py",
      "line_number": 1,
      "code_snippet": "import html\nimport time\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.cross_connector_utils.miscellaneous_utils import time_str_to_utc\nfrom onyx.connectors.exceptions import ConnectorValidationError\nfrom onyx.connectors.exceptions import CredentialExpiredError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zendesk/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/zendesk/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport time\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom typing import Any\nfrom typing import cast\n\nimport requests\nfrom pydantic import BaseModel\nfrom requests.exceptions import HTTPError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/onyx_confluence.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/onyx_confluence.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\n# README (notes on Confluence pagination):\n\nWe've noticed that the `search/users` and `users/memberof` endpoints for Confluence Cloud use offset-based pagination as\nopposed to cursor-based. We also know that page-retrieval uses cursor-based pagination.\n\nOur default pagination strategy right now for cloud is to assume cursor-based.\nHowever, if you notice that a cloud API is not being properly paginated (i.e., if the `_links.next` is not appearing in the\nreturned payload), then you can force offset-based pagination.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/utils.py",
      "line_number": 1,
      "code_snippet": "import math\nimport time\nfrom collections.abc import Callable\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import cast",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py_286_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 286. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py",
      "line_number": 286,
      "code_snippet": "        expand = \",\".join(_COMMENT_EXPANSION_FIELDS)\n\n        for comment in self.confluence_client.paginated_cql_retrieval(\n            cql=comment_cql,\n            expand=expand,\n        ):",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py_399_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 399. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py",
      "line_number": 399,
      "code_snippet": "\n        try:\n            for attachment in self.confluence_client.paginated_cql_retrieval(\n                cql=attachment_query,\n                expand=\",\".join(_ATTACHMENT_EXPANSION_FIELDS),\n            ):",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py_590_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 590. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py",
      "line_number": 590,
      "code_snippet": "            checkpoint.next_page_url = next_page_url\n\n        for page in self.confluence_client.paginated_page_retrieval(\n            cql_url=page_query_url,\n            limit=self.batch_size,\n            next_page_callback=store_next_page_url,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/confluence/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any\nfrom urllib.parse import quote\n\nfrom atlassian.errors import ApiError  # type: ignore\nfrom requests.exceptions import HTTPError\nfrom typing_extensions import override",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/sharepoint/connector.py_151",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 151. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/sharepoint/connector.py",
      "line_number": 151,
      "code_snippet": "\nclass SharepointAuthMethod(Enum):\n    CLIENT_SECRET = \"client_secret\"\n    CERTIFICATE = \"certificate\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/sharepoint/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/sharepoint/connector.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport copy\nimport html\nimport io\nimport os\nimport re\nimport time\nfrom collections import deque\nfrom collections.abc import Generator\nfrom datetime import datetime",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_site/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_site/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nfrom typing import Any\nfrom typing import cast\n\nfrom bs4 import BeautifulSoup\nfrom bs4 import Tag\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import Field\nfrom pydantic.alias_generators import to_camel\n\n\nclass Body(BaseModel):\n    content_type: str",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/utils.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom http import HTTPStatus\n\nfrom office365.graph_client import GraphClient  # type: ignore[import-untyped]\nfrom office365.teams.channels.channel import Channel  # type: ignore[import-untyped]\nfrom office365.teams.channels.channel import ConversationMember\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/teams/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport os\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\n\nimport msal  # type: ignore\nfrom office365.graph_client import GraphClient  # type: ignore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discourse/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/discourse/connector.py",
      "line_number": 1,
      "code_snippet": "import time\nimport urllib.parse\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\n\nimport requests\nfrom pydantic import BaseModel\nfrom requests import Response",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/dropbox/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/dropbox/connector.py",
      "line_number": 1,
      "code_snippet": "from datetime import timezone\nfrom io import BytesIO\nfrom typing import Any\n\nfrom dropbox import Dropbox  # type: ignore[import-untyped]\nfrom dropbox.exceptions import ApiError  # type: ignore[import-untyped]\nfrom dropbox.exceptions import AuthError\nfrom dropbox.files import FileMetadata  # type: ignore[import-untyped]\nfrom dropbox.files import FolderMetadata\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/doc_conversion.py_156_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'user_data' containing sensitive data is being logged on line 156. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/doc_conversion.py",
      "line_number": 156,
      "code_snippet": "        and expert_info.display_name is None\n    ):\n        logger.warning(f\"No identifying information found for user {user_data}\")\n        return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/doc_conversion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/doc_conversion.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom typing import Any\nfrom typing import cast\n\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.cross_connector_utils.miscellaneous_utils import time_str_to_utc\nfrom onyx.connectors.models import BasicExpertInfo\nfrom onyx.connectors.models import Document\nfrom onyx.connectors.models import ImageSection\nfrom onyx.connectors.models import TextSection",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py_126",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "SQL injection: % formatting",
      "description": "SQL query on line 126 uses % formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py",
      "line_number": 126,
      "code_snippet": "\n            # Main table for storing Salesforce objects\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE IF NOT EXISTS salesforce_objects (",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py_176",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 176 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py",
      "line_number": 176,
      "code_snippet": "                index_name: str, create_statement: str\n            ) -> None:\n                cursor.execute(\n                    f\"SELECT name FROM sqlite_master WHERE type='index' AND name='{index_name}'\"\n                )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py_339",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 339 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py",
      "line_number": 339,
      "code_snippet": "\n                    # Get directly updated objects of parent types - using index on object_type\n                    cursor.execute(\n                        f\"\"\"\n                        SELECT id FROM salesforce_objects",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py_351",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 351 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py",
      "line_number": 351,
      "code_snippet": "                    # Get parent objects of updated objects - using optimized relationship_types table\n                    cursor.execute(\n                        f\"\"\"\n                        SELECT DISTINCT parent_id\n                        FROM relationship_types",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/sqlite_functions.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport json\nimport os\nimport sqlite3\nimport time\nfrom collections.abc import Iterator\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import cast\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/salesforce_calls.py_75",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 75 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/salesforce_calls.py",
      "line_number": 75,
      "code_snippet": "    queryable_fields: set[str], sf_type: str, time_filter: str\n) -> str:\n    query = f\"SELECT {', '.join(queryable_fields)} FROM {sf_type}{time_filter}\"\n    return query\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/salesforce_calls.py_83",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 83 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/salesforce_calls.py",
      "line_number": 83,
      "code_snippet": ") -> str:\n    query = (\n        f\"SELECT {', '.join(queryable_fields)} FROM {sf_type} WHERE Id = '{object_id}'\"\n    )\n    return query",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/salesforce_calls.py_103",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 103 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/salesforce_calls.py",
      "line_number": 103,
      "code_snippet": "    \"\"\"\n    try:\n        query = f\"SELECT Count() FROM {sf_type}{time_filter} LIMIT 1\"\n        result = sf_client.query(query)\n        if result[\"totalSize\"] == 0:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/utils.py_51_assignment",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Hardcoded Generic API Key detected in assignment",
      "description": "Hardcoded Generic API Key found in assignment on line 51. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version control history, or compiled binaries.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/utils.py",
      "line_number": 51,
      "code_snippet": "\n\n_CHECKSUM_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ012345\"\n_LOOKUP = {format(i, \"05b\"): _CHECKSUM_CHARS[i] for i in range(32)}\n",
      "recommendation": "Remove hardcoded secrets immediately:\n1. Use environment variables: os.getenv('API_KEY')\n2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault\n3. Use configuration files (never commit to git): config.ini, .env\n4. Rotate the exposed secret immediately\n5. Scan git history for leaked secrets: git-secrets, truffleHog\n6. Add secret scanning to CI/CD pipeline"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/utils.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nNAME_FIELD = \"Name\"\nMODIFIED_FIELD = \"LastModifiedDate\"\nID_FIELD = \"Id\"\nACCOUNT_OBJECT_TYPE = \"Account\"\nUSER_OBJECT_TYPE = \"User\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/connector.py_1130",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 1130 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/connector.py",
      "line_number": 1130,
      "code_snippet": "        doc_metadata_list: list[SlimDocument] = []\n        for parent_object_type in self.parent_object_list:\n            query = f\"SELECT Id FROM {parent_object_type}\"\n            query_result = self.sf_client.safe_query_all(query)\n            doc_metadata_list.extend(",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/connector.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport gc\nimport json\nimport os\nimport sys\nimport tempfile\nimport time\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/onyx_salesforce.py_142",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 142 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/onyx_salesforce.py",
      "line_number": 142,
      "code_snippet": "            fields = relationships_to_fields[child_relationship]\n            fields_fragment = \",\".join(fields)\n            query += f\"(SELECT {fields_fragment} FROM {child_relationship} LIMIT {SUBQUERY_LIMIT}), \"\n\n        query = query.rstrip(\", \")",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/onyx_salesforce.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/onyx_salesforce.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Any\n\nfrom simple_salesforce import Salesforce\nfrom simple_salesforce import SFType\nfrom simple_salesforce.exceptions import SalesforceRefusedRequest\n\nfrom onyx.connectors.cross_connector_utils.rate_limit_wrapper import (\n    rate_limit_builder,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/utils.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom functools import lru_cache\nfrom functools import wraps\nfrom typing import Any\nfrom typing import cast\n\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/onyx_slack_web_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/onyx_slack_web_client.py",
      "line_number": 1,
      "code_snippet": "import threading\nimport time\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\nfrom urllib.request import Request\n\nfrom redis import Redis\nfrom redis.lock import Lock as RedisLock\nfrom slack_sdk import WebClient",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/onyx_retry_handler.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/slack/onyx_retry_handler.py",
      "line_number": 1,
      "code_snippet": "import random\nfrom typing import cast\nfrom typing import Optional\n\nfrom redis import Redis\nfrom slack_sdk.http_retry.handler import RetryHandler\nfrom slack_sdk.http_retry.request import HttpRequest\nfrom slack_sdk.http_retry.response import HttpResponse\nfrom slack_sdk.http_retry.state import RetryState\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/axero/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/axero/connector.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nimport requests\nfrom pydantic import BaseModel\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/loopio/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/loopio/connector.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session  # type: ignore\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gitbook/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gitbook/connector.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom urllib.parse import urljoin\n\nimport requests\n\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.interfaces import GenerateDocumentsOutput",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bitbucket/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bitbucket/utils.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport time\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nimport httpx",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bitbucket/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/bitbucket/connector.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport copy\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import TYPE_CHECKING\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/blob/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/blob/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nimport time\nfrom collections.abc import Mapping\nfrom datetime import datetime\nfrom datetime import timezone\nfrom io import BytesIO\nfrom numbers import Integral\nfrom typing import Any\nfrom typing import Optional\nfrom urllib.parse import quote",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gong/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gong/connector.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport time\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\n\nimport requests",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/asana/asana_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/asana/asana_api.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom typing import Dict\n\nimport asana  # type: ignore\n\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/asana/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/asana/connector.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom typing import Any\n\nfrom onyx.configs.app_configs import CONTINUE_ON_CONNECTOR_FAILURE\nfrom onyx.configs.app_configs import INDEX_BATCH_SIZE\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.connectors.asana import asana_api\nfrom onyx.connectors.interfaces import GenerateDocumentsOutput\nfrom onyx.connectors.interfaces import LoadConnector\nfrom onyx.connectors.interfaces import PollConnector",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py_122",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'generator.run' is used in 'run(' on line 122 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py",
      "line_number": 122,
      "code_snippet": "    generator = FamilyFileGeneratorInMemory(url, name, \"Y\", \"Y\")\n    generator.run()\n    if generator.family_definition is None:\n        raise ValueError(\"Family definition was not generated.\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py_107_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'generate_family_class'",
      "description": "Function 'generate_family_class' on line 107 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py",
      "line_number": 107,
      "code_snippet": "\n@functools.lru_cache(maxsize=None)\ndef generate_family_class(url: str, name: str) -> type[family.Family]:\n    \"\"\"Generate a family file for a given URL and name.\n\n    Args:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/family.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport builtins\nimport functools\nimport itertools\nimport tempfile\nfrom typing import Any\nfrom unittest import mock\nfrom urllib.parse import urlparse\nfrom urllib.parse import urlunparse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/wiki.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/mediawiki/wiki.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport datetime\nimport itertools\nimport tempfile\nfrom collections.abc import Generator\nfrom collections.abc import Iterator\nfrom typing import Any\nfrom typing import cast\nfrom typing import ClassVar",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/coda/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/coda/connector.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py_297_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 297. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py",
      "line_number": 297,
      "code_snippet": "    try:\n        thread = next(\n            execute_single_retrieval(\n                retrieval_function=gmail_service.users().threads().get,\n                list_key=None,\n                userId=user_email,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py_399_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 399. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py",
      "line_number": 399,
      "code_snippet": "            admin_service = get_admin_service(self.creds, self.primary_admin_email)\n            emails = []\n            for user in execute_paginated_retrieval(\n                retrieval_function=admin_service.users().list,\n                list_key=\"users\",\n                fields=USER_FIELDS,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/gmail/connector.py",
      "line_number": 1,
      "code_snippet": "from base64 import urlsafe_b64decode\nfrom collections.abc import Callable\nfrom collections.abc import Iterator\nfrom typing import Any\nfrom typing import cast\nfrom typing import Dict\n\nfrom google.oauth2.credentials import Credentials as OAuthCredentials\nfrom google.oauth2.service_account import Credentials as ServiceAccountCredentials\nfrom googleapiclient.errors import HttpError  # type: ignore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/egnyte/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/egnyte/connector.py",
      "line_number": 1,
      "code_snippet": "import io\nimport os\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import IO\nfrom urllib.parse import quote\n\nfrom pydantic import Field",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/section_extraction.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/section_extraction.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom pydantic import BaseModel\n\nfrom onyx.connectors.google_utils.resources import GoogleDocsService\nfrom onyx.connectors.models import TextSection\n\nHEADING_DELIMITER = \"\\n\"\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Any\n\nfrom pydantic import BaseModel\nfrom pydantic import ConfigDict\nfrom pydantic import field_serializer\nfrom pydantic import field_validator\n\nfrom onyx.connectors.interfaces import ConnectorCheckpoint\nfrom onyx.connectors.interfaces import SecondsSinceUnixEpoch",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/doc_conversion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/doc_conversion.py",
      "line_number": 1,
      "code_snippet": "import io\nfrom collections.abc import Callable\nfrom datetime import datetime\nfrom typing import Any\nfrom typing import cast\nfrom urllib.parse import urlparse\nfrom urllib.parse import urlunparse\n\nfrom googleapiclient.errors import HttpError  # type: ignore\nfrom googleapiclient.http import MediaIoBaseDownload  # type: ignore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/constants.py",
      "line_number": 1,
      "code_snippet": "UNSUPPORTED_FILE_TYPE_CONTENT = \"\"  # keep empty for now\nDRIVE_FOLDER_TYPE = \"application/vnd.google-apps.folder\"\nDRIVE_SHORTCUT_TYPE = \"application/vnd.google-apps.shortcut\"\nDRIVE_FILE_TYPE = \"application/vnd.google-apps.file\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_383_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 383. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 383,
      "code_snippet": "        for is_admin in [True, False]:\n            query = \"isAdmin=true\" if is_admin else \"isAdmin=false\"\n            for user in execute_paginated_retrieval(\n                retrieval_function=admin_service.users().list,\n                list_key=\"users\",\n                fields=USER_FIELDS,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_475_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 475. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 475,
      "code_snippet": "        return get_available_drive_id\n\n    def _impersonate_user_for_retrieval(\n        self,\n        user_email: str,\n        field_type: DriveFileFieldType,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_688_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 688. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 688,
      "code_snippet": "        curr_stage.stage = DriveRetrievalStage.DONE\n\n    def _manage_service_account_retrieval(\n        self,\n        field_type: DriveFileFieldType,\n        checkpoint: GoogleDriveCheckpoint,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_763_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 763. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 763,
      "code_snippet": "\n        user_retrieval_gens = [\n            self._impersonate_user_for_retrieval(\n                email,\n                field_type,\n                checkpoint,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_1009_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 1009. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 1009,
      "code_snippet": "            )\n\n    def _checkpointed_retrieval(\n        self,\n        retrieval_method: CredentialedRetrievalMethod,\n        field_type: DriveFileFieldType,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_1074_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 1074. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 1074,
      "code_snippet": "            yield file\n\n    def _manage_oauth_retrieval(\n        self,\n        field_type: DriveFileFieldType,\n        checkpoint: GoogleDriveCheckpoint,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_1494",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 1494. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 1494,
      "code_snippet": "    DB_CREDENTIALS_DICT_TOKEN_KEY = \"google_tokens\"\n    # This is the service account key\n    DB_CREDENTIALS_DICT_SERVICE_ACCOUNT_KEY = \"google_service_account_key\"\n    # The email saved for both auth types\n    DB_CREDENTIALS_PRIMARY_ADMIN_KEY = \"google_primary_admin\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/connector.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport json\nimport os\nimport sys\nimport threading\nfrom collections.abc import Callable\nfrom collections.abc import Generator\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom enum import Enum",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py_100_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 100. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py",
      "line_number": 100,
      "code_snippet": "        query += f\" and '{parent_id}' in parents\"\n\n    for file in execute_paginated_retrieval(\n        retrieval_function=service.files().list,\n        list_key=\"files\",\n        continue_on_404_or_403=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py_136_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 136. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py",
      "line_number": 136,
      "code_snippet": "    kwargs = {ORDER_BY_KEY: GoogleFields.MODIFIED_TIME.value}\n\n    for file in execute_paginated_retrieval(\n        retrieval_function=service.files().list,\n        list_key=\"files\",\n        continue_on_404_or_403=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py_246_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 246. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py",
      "line_number": 246,
      "code_snippet": "        folder_query = f\"mimeType = '{DRIVE_FOLDER_TYPE}'\"\n        folder_query += \" and trashed = false\"\n        for folder in execute_paginated_retrieval(\n            retrieval_function=service.files().list,\n            list_key=\"files\",\n            continue_on_404_or_403=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/google_drive/file_retrieval.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom enum import Enum\n\nfrom googleapiclient.discovery import Resource  # type: ignore\nfrom googleapiclient.errors import HttpError  # type: ignore\n\nfrom onyx.connectors.google_drive.constants import DRIVE_FOLDER_TYPE",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/shelve_stuff/old_test_salesforce_shelves.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/shelve_stuff/old_test_salesforce_shelves.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport os\nimport shutil\n\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_functions import find_ids_by_type\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_functions import (\n    get_affected_parent_ids_by_type,\n)\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_functions import get_child_ids\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_functions import get_record",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/shelve_stuff/shelve_functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/connectors/salesforce/shelve_stuff/shelve_functions.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport shelve\n\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_utils import (\n    get_child_to_parent_shelf_path,\n)\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_utils import get_id_type_shelf_path\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_utils import get_object_shelf_path\nfrom onyx.connectors.salesforce.shelve_stuff.shelve_utils import (\n    get_parent_to_child_shelf_path,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_redis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_redis.py",
      "line_number": 1,
      "code_snippet": "# These are helper objects for tracking the keys we need to write in redis\nimport json\nfrom typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom redis import Redis\n\nfrom onyx.background.celery.configs.base import CELERY_SEPARATOR\nfrom onyx.configs.constants import OnyxCeleryPriority",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_utils.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\nfrom collections.abc import Iterator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import cast\n\nimport httpx\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_k8s_probe.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/celery_k8s_probe.py",
      "line_number": 1,
      "code_snippet": "# script to use as a kubernetes readiness / liveness probe\n\nimport argparse\nimport sys\nimport time\nfrom pathlib import Path\n\n\ndef main_readiness(filename: str) -> int:\n    \"\"\"Checks if the file exists.\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/memory_monitoring.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/memory_monitoring.py",
      "line_number": 1,
      "code_snippet": "# backend/onyx/background/celery/memory_monitoring.py\nimport logging\nimport os\nfrom logging.handlers import RotatingFileHandler\n\nimport psutil\n\nfrom onyx.utils.logger import is_running_in_container\nfrom onyx.utils.logger import setup_logger\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/job_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/job_client.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Custom client that works similarly to Dask, but simpler and more lightweight.\nDask jobs behaved very strangely - they would die all the time, retries would\nnot follow the expected behavior, etc.\n\nNOTE: cannot use Celery directly due to\nhttps://github.com/celery/celery/issues/7007#issuecomment-1740139367\"\"\"\n\nimport multiprocessing as mp\nimport sys\nimport traceback",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py_537",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'connector_runner.run' is used in 'run(' on line 537 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py",
      "line_number": 537,
      "code_snippet": "            )\n            for document_batch, failure, next_checkpoint in connector_runner.run(\n                checkpoint\n            ):",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py_309",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'connector_document_extraction' on line 309 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py",
      "line_number": 309,
      "code_snippet": "def connector_document_extraction(\n    app: Celery,\n    index_attempt_id: int,\n    cc_pair_id: int,\n    search_settings_id: int,\n    tenant_id: str,\n    callback: IndexingHeartbeatInterface | None = None,\n) -> None:\n    \"\"\"Extract documents from connector and queue them for indexing pipeline processing.\n\n    This is the first part of the split indexing process that runs the connector",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py_309_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'connector_document_extraction'",
      "description": "Function 'connector_document_extraction' on line 309 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py",
      "line_number": 309,
      "code_snippet": "\n\ndef connector_document_extraction(\n    app: Celery,\n    index_attempt_id: int,\n    cc_pair_id: int,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/run_docfetching.py",
      "line_number": 1,
      "code_snippet": "import sys\nimport time\nimport traceback\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\n\nfrom celery import Celery\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/checkpointing_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/checkpointing_utils.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timedelta\nfrom io import BytesIO\n\nfrom sqlalchemy import and_\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import FileOrigin\nfrom onyx.configs.constants import NUM_DAYS_TO_KEEP_CHECKPOINTS\nfrom onyx.connectors.interfaces import BaseConnector",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/memory_tracer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/indexing/memory_tracer.py",
      "line_number": 1,
      "code_snippet": "import tracemalloc\n\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\nDANSWER_TRACEMALLOC_FRAMES = 10\n\n\nclass MemoryTracer:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass DocProcessingContext(BaseModel):\n    tenant_id: str\n    cc_pair_id: int\n    search_settings_id: int\n    index_attempt_id: int",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/beat_schedule.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/beat_schedule.py",
      "line_number": 1,
      "code_snippet": "import copy\nfrom datetime import timedelta\nfrom typing import Any\n\nfrom celery.schedules import crontab\n\nfrom onyx.configs.app_configs import AUTO_LLM_CONFIG_URL\nfrom onyx.configs.app_configs import AUTO_LLM_UPDATE_INTERVAL_SECONDS\nfrom onyx.configs.app_configs import ENTERPRISE_EDITION_ENABLED\nfrom onyx.configs.app_configs import SCHEDULED_EVAL_DATASET_NAMES",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/configs/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/configs/base.py",
      "line_number": 1,
      "code_snippet": "# docs: https://docs.celeryq.dev/en/stable/userguide/configuration.html\nimport urllib.parse\n\nfrom onyx.configs.app_configs import CELERY_BROKER_POOL_LIMIT\nfrom onyx.configs.app_configs import CELERY_RESULT_EXPIRES\nfrom onyx.configs.app_configs import REDIS_DB_NUMBER_CELERY\nfrom onyx.configs.app_configs import REDIS_DB_NUMBER_CELERY_RESULT_BACKEND\nfrom onyx.configs.app_configs import REDIS_HEALTH_CHECK_INTERVAL\nfrom onyx.configs.app_configs import REDIS_HOST\nfrom onyx.configs.app_configs import REDIS_PASSWORD",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/kg_processing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/kg_processing.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_process_init",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/light.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/light.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_ready\nfrom celery.signals import worker_shutdown",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/user_file_processing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/user_file_processing.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_process_init",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/background.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/background.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_process_init",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/heavy.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/heavy.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_ready",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/app_base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/app_base.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport multiprocessing\nimport os\nimport time\nfrom typing import Any\nfrom typing import cast\n\nimport sentry_sdk\nfrom celery import bootsteps  # type: ignore\nfrom celery import Task",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/docprocessing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/docprocessing.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_process_init",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/primary.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/primary.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom typing import Any\nfrom typing import cast\n\nfrom celery import bootsteps  # type: ignore\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/beat.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/beat.py",
      "line_number": 1,
      "code_snippet": "from datetime import timedelta\nfrom typing import Any\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery.beat import PersistentScheduler  # type: ignore\nfrom celery.signals import beat_init\nfrom celery.utils.log import get_task_logger\n\nimport onyx.background.celery.apps.app_base as app_base",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/docfetching.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/apps/docfetching.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import signals\nfrom celery import Task\nfrom celery.apps.worker import Worker\nfrom celery.signals import celeryd_init\nfrom celery.signals import worker_init\nfrom celery.signals import worker_ready",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/periodic/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/periodic/tasks.py",
      "line_number": 1,
      "code_snippet": "#####\n# Periodic Tasks\n#####\nimport json\nfrom typing import Any\n\nfrom celery import shared_task\nfrom celery.contrib.abortable import AbortableTask  # type: ignore\nfrom celery.exceptions import TaskRevokedError\nfrom sqlalchemy import inspect",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/evals/tasks.py_116_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 116. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/evals/tasks.py",
      "line_number": 116,
      "code_snippet": "            )\n\n            result = run_eval(\n                configuration=configuration,\n                remote_dataset_name=dataset_name,\n            )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/evals/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/evals/tasks.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nfrom celery import shared_task\nfrom celery import Task\n\nfrom onyx.configs.app_configs import BRAINTRUST_API_KEY\nfrom onyx.configs.app_configs import JOB_TIMEOUT\nfrom onyx.configs.app_configs import SCHEDULED_EVAL_DATASET_NAMES",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docfetching/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docfetching/tasks.py",
      "line_number": 1,
      "code_snippet": "import multiprocessing\nimport os\nimport time\nimport traceback\nfrom time import sleep\n\nimport sentry_sdk\nfrom celery import Celery\nfrom celery import shared_task\nfrom celery import Task",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docfetching/task_creation_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docfetching/task_creation_utils.py",
      "line_number": 1,
      "code_snippet": "from uuid import uuid4\n\nfrom celery import Celery\nfrom redis import Redis\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n\nfrom onyx.background.celery.apps.app_base import task_logger\nfrom onyx.configs.constants import DANSWER_REDIS_FUNCTION_LOCK_PREFIX\nfrom onyx.configs.constants import OnyxCeleryPriority",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_253",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'rds.generate_tasks' is used in 'UPDATE' on line 253 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 253,
      "code_snippet": "    # Add all documents that need to be updated into the queue\n    result = rds.generate_tasks(\n        VESPA_SYNC_MAX_TASKS, celery_app, db_session, r, lock_beat, tenant_id\n    )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_329",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'rug.generate_tasks' is used in 'UPDATE' on line 329 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 329,
      "code_snippet": "    )\n    result = rug.generate_tasks(\n        VESPA_SYNC_MAX_TASKS, celery_app, db_session, r, lock_beat, tenant_id\n    )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_287",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'try_generate_user_group_sync_tasks' on line 287 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 287,
      "code_snippet": "def try_generate_user_group_sync_tasks(\n    celery_app: Celery,\n    usergroup_id: int,\n    db_session: Session,\n    r: Redis,\n    lock_beat: RedisLock,\n    tenant_id: str,\n) -> int | None:\n    lock_beat.reacquire()\n\n    rug = RedisUserGroup(tenant_id, usergroup_id)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_210_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write operation without confirmation in 'try_generate_document_set_sync_tasks'",
      "description": "Function 'try_generate_document_set_sync_tasks' on line 210 performs high-risk delete/write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 210,
      "code_snippet": "\n\ndef try_generate_document_set_sync_tasks(\n    celery_app: Celery,\n    document_set_id: int,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_287_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/network operation without confirmation in 'try_generate_user_group_sync_tasks'",
      "description": "Function 'try_generate_user_group_sync_tasks' on line 287 performs high-risk delete/write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 287,
      "code_snippet": "\n\ndef try_generate_user_group_sync_tasks(\n    celery_app: Celery,\n    usergroup_id: int,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_210_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'try_generate_document_set_sync_tasks'",
      "description": "Function 'try_generate_document_set_sync_tasks' on line 210 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 210,
      "code_snippet": "\n\ndef try_generate_document_set_sync_tasks(\n    celery_app: Celery,\n    document_set_id: int,\n    db_session: Session,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_287_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'try_generate_user_group_sync_tasks'",
      "description": "Function 'try_generate_user_group_sync_tasks' on line 287 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 287,
      "code_snippet": "\n\ndef try_generate_user_group_sync_tasks(\n    celery_app: Celery,\n    usergroup_id: int,\n    db_session: Session,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py_329",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 329,
      "code_snippet": "    task_logger.info(\n        f\"RedisUserGroup.generate_tasks starting. usergroup_id={usergroup.id}\"\n    )\n    result = rug.generate_tasks(\n        VESPA_SYNC_MAX_TASKS, celery_app, db_session, r, lock_beat, tenant_id\n    )\n    if result is None:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/document_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/vespa/document_sync.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import cast\nfrom uuid import uuid4\n\nfrom celery import Celery\nfrom redis import Redis\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import DB_YIELD_PER_DEFAULT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/user_file_processing/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/user_file_processing/tasks.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport time\nfrom typing import Any\nfrom uuid import UUID\n\nimport httpx\nimport sqlalchemy as sa\nfrom celery import shared_task\nfrom celery import Task\nfrom redis.lock import Lock as RedisLock",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/shared/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/shared/tasks.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom enum import Enum\nfrom http import HTTPStatus\n\nimport httpx\nfrom celery import shared_task\nfrom celery import Task\nfrom celery.exceptions import SoftTimeLimitExceeded\nfrom redis import Redis\nfrom tenacity import RetryError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py_327",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'redis_connector.delete.generate_tasks' is used in 'UPDATE' on line 327 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py",
      "line_number": 327,
      "code_snippet": "        )\n        tasks_generated = redis_connector.delete.generate_tasks(\n            app, db_session, lock_beat\n        )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py_235_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute operation without confirmation in 'try_generate_document_cc_pair_cleanup_tasks'",
      "description": "Function 'try_generate_document_cc_pair_cleanup_tasks' on line 235 performs high-risk delete/write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py",
      "line_number": 235,
      "code_snippet": "\n\ndef try_generate_document_cc_pair_cleanup_tasks(\n    app: Celery,\n    cc_pair_id: int,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py_235_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'try_generate_document_cc_pair_cleanup_tasks'",
      "description": "Function 'try_generate_document_cc_pair_cleanup_tasks' on line 235 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py",
      "line_number": 235,
      "code_snippet": "\n\ndef try_generate_document_cc_pair_cleanup_tasks(\n    app: Celery,\n    cc_pair_id: int,\n    db_session: Session,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/connector_deletion/tasks.py",
      "line_number": 1,
      "code_snippet": "import traceback\nfrom datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\n\nfrom celery import Celery\nfrom celery import shared_task\nfrom celery import Task\nfrom celery.exceptions import SoftTimeLimitExceeded",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/pruning/tasks.py_400_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/execute/network operation without confirmation in 'connector_pruning_generator_task'",
      "description": "Function 'connector_pruning_generator_task' on line 400 performs high-risk delete/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/pruning/tasks.py",
      "line_number": 400,
      "code_snippet": "    bind=True,\n)\ndef connector_pruning_generator_task(\n    self: Task,\n    cc_pair_id: int,\n    connector_id: int,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/pruning/tasks.py_400_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'connector_pruning_generator_task'",
      "description": "Function 'connector_pruning_generator_task' on line 400 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/pruning/tasks.py",
      "line_number": 400,
      "code_snippet": "    bind=True,\n)\ndef connector_pruning_generator_task(\n    self: Task,\n    cc_pair_id: int,\n    connector_id: int,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/pruning/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/pruning/tasks.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom uuid import uuid4\n\nfrom celery import Celery\nfrom celery import shared_task",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/kg_processing/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/kg_processing/tasks.py",
      "line_number": 1,
      "code_snippet": "import time\n\nfrom celery import shared_task\nfrom celery import Task\nfrom celery.exceptions import SoftTimeLimitExceeded\nfrom redis.lock import Lock as RedisLock\n\nfrom onyx.background.celery.apps.app_base import task_logger\nfrom onyx.background.celery.tasks.kg_processing.utils import (\n    is_kg_clustering_only_requirements_met,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/kg_processing/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/kg_processing/utils.py",
      "line_number": 1,
      "code_snippet": "import time\n\nfrom redis.lock import Lock as RedisLock\n\nfrom onyx.configs.constants import OnyxRedisLocks\nfrom onyx.db.document import check_for_documents_needing_kg_processing\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.kg_config import get_kg_config_settings\nfrom onyx.db.kg_config import is_kg_config_settings_enabled_valid\nfrom onyx.db.models import KGEntityExtractionStaging",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/tasks.py",
      "line_number": 1,
      "code_snippet": "import gc\nimport os\nimport time\nimport traceback\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/utils.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom redis import Redis\nfrom redis.exceptions import LockError\nfrom redis.lock import Lock as RedisLock\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import DISABLE_INDEX_UPDATE_ON_SWAP",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/heartbeat.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/docprocessing/heartbeat.py",
      "line_number": 1,
      "code_snippet": "import contextvars\nimport threading\n\nfrom sqlalchemy import update\n\nfrom onyx.configs.constants import INDEXING_WORKER_HEARTBEAT_INTERVAL\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.models import IndexAttempt\nfrom onyx.utils.logger import setup_logger\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/monitoring/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/onyx/background/celery/tasks/monitoring/tasks.py",
      "line_number": 1,
      "code_snippet": "import json\nimport time\nfrom collections.abc import Callable\nfrom datetime import timedelta\nfrom itertools import islice\nfrom typing import Any\nfrom typing import cast\nfrom typing import Literal\n\nimport psutil",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/legacy/onyx_torch_model.py_67_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "torch.load without weights_only=True can execute arbitrary code on line 67.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/legacy/onyx_torch_model.py",
      "line_number": 67,
      "code_snippet": "#             device = torch.device(\"cpu\")\n\n#         model.load_state_dict(torch.load(model_path, map_location=device))\n#         model = model.to(device)\n\n#         model.device = device",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/legacy/onyx_torch_model.py_139_unsafe_load",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Unsafe model loading pattern",
      "description": "torch.load without weights_only=True can execute arbitrary code on line 139.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/model_server/legacy/onyx_torch_model.py",
      "line_number": 139,
      "code_snippet": "#             )\n#         )\n#         state_dict = torch.load(\n#             os.path.join(repo_dir, \"pytorch_model.pt\"),\n#             map_location=device,\n#             weights_only=True,",
      "recommendation": "Secure Model Loading:\n1. Set trust_remote_code=False\n2. Use torch.load(..., weights_only=True)\n3. Verify model sources and checksums\n4. Use safetensors format instead of pickle"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/post_query_censoring.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/post_query_censoring.py",
      "line_number": 1,
      "code_snippet": "from ee.onyx.db.connector_credential_pair import get_all_auto_sync_cc_pairs\nfrom ee.onyx.external_permissions.sync_params import get_all_censoring_enabled_sources\nfrom ee.onyx.external_permissions.sync_params import get_source_perm_sync_config\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.context.search.pipeline import InferenceChunk\nfrom onyx.db.engine.sql_engine import get_session_with_current_tenant\nfrom onyx.db.models import User\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/access/access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/access/access.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy.orm import Session\n\nfrom ee.onyx.db.external_perm import fetch_external_groups_for_user\nfrom ee.onyx.db.external_perm import fetch_public_external_group_ids\nfrom ee.onyx.db.user_group import fetch_user_groups_for_documents\nfrom ee.onyx.db.user_group import fetch_user_groups_for_user\nfrom ee.onyx.external_permissions.sync_params import get_source_perm_sync_config\nfrom onyx.access.access import (\n    _get_access_for_documents as get_access_for_documents_without_groups,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/auth/users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/auth/users.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nimport jwt\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi import status\n\nfrom ee.onyx.configs.app_configs import SUPER_CLOUD_API_KEY\nfrom ee.onyx.configs.app_configs import SUPER_USERS",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/seeding.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/seeding.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom copy import deepcopy\nfrom typing import List\nfrom typing import Optional\n\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.db.standard_answer import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenant_usage_limits.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenant_usage_limits.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Tenant-specific usage limit overrides from the control plane (EE version).\"\"\"\n\nimport requests\n\nfrom ee.onyx.server.tenants.access import generate_data_plane_token\nfrom onyx.configs.app_configs import CONTROL_PLANE_API_BASE_URL\nfrom onyx.server.tenant_usage_limits import TenantUsageLimitOverrides\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/encryption.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/encryption.py",
      "line_number": 1,
      "code_snippet": "from functools import lru_cache\nfrom os import urandom\n\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import padding\nfrom cryptography.hazmat.primitives.ciphers import algorithms\nfrom cryptography.hazmat.primitives.ciphers import Cipher\nfrom cryptography.hazmat.primitives.ciphers import modes\n\nfrom onyx.configs.app_configs import ENCRYPTION_KEY_SECRET",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/posthog_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/posthog_client.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\nfrom urllib.parse import unquote\n\nfrom posthog import Posthog\n\nfrom ee.onyx.configs.app_configs import MARKETING_POSTHOG_API_KEY\nfrom ee.onyx.configs.app_configs import POSTHOG_API_KEY\nfrom ee.onyx.configs.app_configs import POSTHOG_HOST\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/license.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/utils/license.py",
      "line_number": 1,
      "code_snippet": "\"\"\"RSA-4096 license signature verification utilities.\"\"\"\n\nimport base64\nimport json\nimport os\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom cryptography.exceptions import InvalidSignature\nfrom cryptography.hazmat.primitives import hashes",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/configs/app_configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/configs/app_configs.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\n\n\n#####\n# Auto Permission Sync\n#####\n# should generally only be used for sources that support polling of permissions\n# e.g. can pull in only permission changes rather than having to go through all\n# documents every time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/user_group.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/user_group.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom operator import and_\nfrom uuid import UUID\n\nfrom fastapi import HTTPException\nfrom sqlalchemy import delete\nfrom sqlalchemy import func\nfrom sqlalchemy import Select\nfrom sqlalchemy import select\nfrom sqlalchemy import update",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/usage_export.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/usage_export.py",
      "line_number": 1,
      "code_snippet": "import uuid\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom typing import IO\nfrom typing import Optional\n\nfrom fastapi_users_db_sqlalchemy import UUID_ID\nfrom sqlalchemy import cast\nfrom sqlalchemy.dialects.postgresql import UUID\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/connector_credential_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/connector_credential_pair.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import delete\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.db.connector_credential_pair import get_connector_credential_pair\nfrom onyx.db.enums import AccessType\nfrom onyx.db.enums import ConnectorCredentialPairStatus\nfrom onyx.db.models import Connector\nfrom onyx.db.models import ConnectorCredentialPair\nfrom onyx.db.models import UserGroup__ConnectorCredentialPair",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/token_limit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/token_limit.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\n\nfrom sqlalchemy import exists\nfrom sqlalchemy import Row\nfrom sqlalchemy import Select\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import aliased\nfrom sqlalchemy.orm import Session\n\nfrom onyx.configs.app_configs import DISABLE_AUTH",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py_27_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_build_filter_conditions'",
      "description": "Function '_build_filter_conditions' on line 27 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
      "line_number": 27,
      "code_snippet": "\n\ndef _build_filter_conditions(\n    start_time: datetime | None,\n    end_time: datetime | None,\n    feedback_filter: QAFeedbackType | None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py_93_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'get_page_of_chat_sessions'",
      "description": "Function 'get_page_of_chat_sessions' on line 93 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
      "line_number": 93,
      "code_snippet": "\n\ndef get_page_of_chat_sessions(\n    start_time: datetime | None,\n    end_time: datetime | None,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py_133_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'fetch_chat_sessions_eagerly_by_time'",
      "description": "Function 'fetch_chat_sessions_eagerly_by_time' on line 133 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
      "line_number": 133,
      "code_snippet": "\n\ndef fetch_chat_sessions_eagerly_by_time(\n    start: datetime,\n    end: datetime,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py_113",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
      "line_number": 113,
      "code_snippet": "    )\n\n    stmt = (\n        select(ChatSession)\n        .join(subquery, ChatSession.id == subquery.c.id)\n        .outerjoin(ChatMessage, ChatSession.id == ChatMessage.chat_session_id)\n        .options(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py_161",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
      "line_number": 161,
      "code_snippet": "    )\n\n    query = (\n        db_session.query(ChatSession)\n        .join(subquery, ChatSession.id == subquery.c.id)\n        .outerjoin(ChatMessage, ChatSession.id == ChatMessage.chat_session_id)\n        .options(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py_49",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/query_history.py",
      "line_number": 49,
      "code_snippet": "\n    if feedback_filter is not None:\n        feedback_subq = (\n            select(ChatMessage.chat_session_id)\n            .join(ChatMessageFeedback)\n            .group_by(ChatMessage.chat_session_id)\n            .having(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/saml.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/saml.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom typing import cast\nfrom uuid import UUID\n\nfrom sqlalchemy import and_\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import selectinload\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/standard_answer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/standard_answer.py",
      "line_number": 1,
      "code_snippet": "import re\nimport string\nfrom collections.abc import Sequence\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import StandardAnswer\nfrom onyx.db.models import StandardAnswerCategory\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/external_perm.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/external_perm.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Sequence\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom sqlalchemy import delete\nfrom sqlalchemy import select\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import Session\n\nfrom onyx.access.utils import build_ext_group_name_for_onyx",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/document_set.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/document_set.py",
      "line_number": 1,
      "code_snippet": "from uuid import UUID\n\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.models import ConnectorCredentialPair\nfrom onyx.db.models import DocumentSet\nfrom onyx.db.models import DocumentSet__ConnectorCredentialPair\nfrom onyx.db.models import DocumentSet__User\nfrom onyx.db.models import DocumentSet__UserGroup\nfrom onyx.db.models import User__UserGroup",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/document.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/document.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\n\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom onyx.access.models import ExternalAccess\nfrom onyx.access.utils import build_ext_group_name_for_onyx\nfrom onyx.configs.constants import DocumentSource\nfrom onyx.db.models import Document as DbDocument",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/license.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/license.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Database and cache operations for the license table.\"\"\"\n\nfrom datetime import datetime\n\nfrom sqlalchemy import func\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.server.license.models import LicenseMetadata\nfrom ee.onyx.server.license.models import LicensePayload",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_112",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'ChatMessage.chat_session_id.label' is used in 'WHERE' on line 112 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 112,
      "code_snippet": "        db_session.query(\n            ChatMessage.chat_session_id.label(\"chat_session_id\"),\n            func.min(ChatMessage.id).label(\"chat_message_id\"),\n        )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_23_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'fetch_query_analytics'",
      "description": "Function 'fetch_query_analytics' on line 23 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 23,
      "code_snippet": "\n\ndef fetch_query_analytics(\n    start: datetime.datetime,\n    end: datetime.datetime,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_58_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/execute/network operation without confirmation in 'fetch_per_user_query_analytics'",
      "description": "Function 'fetch_per_user_query_analytics' on line 58 performs high-risk delete/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 58,
      "code_snippet": "\n\ndef fetch_per_user_query_analytics(\n    start: datetime.datetime,\n    end: datetime.datetime,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_96_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'fetch_onyxbot_analytics'",
      "description": "Function 'fetch_onyxbot_analytics' on line 96 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 96,
      "code_snippet": "\n\ndef fetch_onyxbot_analytics(\n    start: datetime.datetime,\n    end: datetime.datetime,\n    db_session: Session,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_185_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'fetch_persona_message_analytics'",
      "description": "Function 'fetch_persona_message_analytics' on line 185 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 185,
      "code_snippet": "\n\ndef fetch_persona_message_analytics(\n    db_session: Session,\n    persona_id: int,\n    start: datetime.datetime,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_214_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'fetch_persona_unique_users'",
      "description": "Function 'fetch_persona_unique_users' on line 214 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 214,
      "code_snippet": "\n\ndef fetch_persona_unique_users(\n    db_session: Session,\n    persona_id: int,\n    start: datetime.datetime,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_243_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'fetch_assistant_message_analytics'",
      "description": "Function 'fetch_assistant_message_analytics' on line 243 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 243,
      "code_snippet": "\n\ndef fetch_assistant_message_analytics(\n    db_session: Session,\n    assistant_id: int,\n    start: datetime.datetime,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_274_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'fetch_assistant_unique_users'",
      "description": "Function 'fetch_assistant_unique_users' on line 274 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 274,
      "code_snippet": "\n\ndef fetch_assistant_unique_users(\n    db_session: Session,\n    assistant_id: int,\n    start: datetime.datetime,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_305_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'fetch_assistant_unique_users_total'",
      "description": "Function 'fetch_assistant_unique_users_total' on line 305 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 305,
      "code_snippet": "\n\ndef fetch_assistant_unique_users_total(\n    db_session: Session,\n    assistant_id: int,\n    start: datetime.datetime,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_58_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'fetch_per_user_query_analytics'",
      "description": "Function 'fetch_per_user_query_analytics' on line 58 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 58,
      "code_snippet": "\n\ndef fetch_per_user_query_analytics(\n    start: datetime.datetime,\n    end: datetime.datetime,\n    db_session: Session,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_29",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 29,
      "code_snippet": "    db_session: Session,\n) -> Sequence[tuple[int, int, int, datetime.date]]:\n    stmt = (\n        select(\n            func.count(ChatMessage.id),\n            func.sum(case((ChatMessageFeedback.is_positive, 1), else_=0)),\n            func.sum(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_64",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 64,
      "code_snippet": "    db_session: Session,\n) -> Sequence[tuple[int, int, int, datetime.date, UUID]]:\n    stmt = (\n        select(\n            func.count(ChatMessage.id),\n            func.sum(case((ChatMessageFeedback.is_positive, 1), else_=0)),\n            func.sum(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_111",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 111,
      "code_snippet": "    # along with the first Assistant message which is the response to the user question.\n    # Generally there should not be more than one AI message per chat session of this type\n    subquery_first_ai_response = (\n        db_session.query(\n            ChatMessage.chat_session_id.label(\"chat_session_id\"),\n            func.min(ChatMessage.id).label(\"chat_message_id\"),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_131",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 131,
      "code_snippet": "    # Get the chat message ids and most recent feedback for each of those chat messages,\n    # not including the messages that have no feedback\n    subquery_last_feedback = (\n        db_session.query(\n            ChatMessageFeedback.chat_message_id.label(\"chat_message_id\"),\n            func.max(ChatMessageFeedback.id).label(\"max_feedback_id\"),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_140",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 140,
      "code_snippet": "    )\n\n    results = (\n        db_session.query(\n            func.count(ChatSession.id).label(\"total_sessions\"),\n            # Need to explicitly specify this as False to handle the NULL case so the cases without\n            # feedback aren't counted against Onyxbot",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_193",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 193,
      "code_snippet": ") -> list[tuple[int, datetime.date]]:\n    \"\"\"Gets the daily message counts for a specific persona within the given time range.\"\"\"\n    query = (\n        select(\n            func.count(ChatMessage.id),\n            cast(ChatMessage.time_sent, Date),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_222",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 222,
      "code_snippet": ") -> list[tuple[int, datetime.date]]:\n    \"\"\"Gets the daily unique user counts for a specific persona within the given time range.\"\"\"\n    query = (\n        select(\n            func.count(func.distinct(ChatSession.user_id)),\n            cast(ChatMessage.time_sent, Date),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_253",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 253,
      "code_snippet": "    Gets the daily message counts for a specific assistant in the given time range.\n    \"\"\"\n    query = (\n        select(\n            func.count(ChatMessage.id),\n            cast(ChatMessage.time_sent, Date),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_284",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 284,
      "code_snippet": "    Gets the daily unique user counts for a specific assistant in the given time range.\n    \"\"\"\n    query = (\n        select(\n            func.count(func.distinct(ChatSession.user_id)),\n            cast(ChatMessage.time_sent, Date),\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_316",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 316,
      "code_snippet": "    the specified assistant in the given time range.\n    \"\"\"\n    query = (\n        select(func.count(func.distinct(ChatSession.user_id)))\n        .select_from(ChatMessage)\n        .join(\n            ChatSession,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_132",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 132,
      "code_snippet": "    # not including the messages that have no feedback\n    subquery_last_feedback = (\n        db_session.query(\n            ChatMessageFeedback.chat_message_id.label(\"chat_message_id\"),\n            func.max(ChatMessageFeedback.id).label(\"max_feedback_id\"),\n        )\n        .group_by(ChatMessageFeedback.chat_message_id)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py_112",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/db/analytics.py",
      "line_number": 112,
      "code_snippet": "    # Generally there should not be more than one AI message per chat session of this type\n    subquery_first_ai_response = (\n        db_session.query(\n            ChatMessage.chat_session_id.label(\"chat_session_id\"),\n            func.min(ChatMessage.id).label(\"chat_message_id\"),\n        )\n        .join(ChatSession, ChatSession.id == ChatMessage.chat_session_id)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/onyxbot/slack/handlers/handle_standard_answers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/onyxbot/slack/handlers/handle_standard_answers.py",
      "line_number": 1,
      "code_snippet": "from slack_sdk import WebClient\nfrom slack_sdk.models.blocks import ActionsBlock\nfrom slack_sdk.models.blocks import Block\nfrom slack_sdk.models.blocks import ButtonElement\nfrom slack_sdk.models.blocks import SectionBlock\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.db.standard_answer import fetch_standard_answer_categories_by_names\nfrom ee.onyx.db.standard_answer import find_matching_standard_answers\nfrom onyx.configs.constants import MessageType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/middleware/tenant_tracking.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/middleware/tenant_tracking.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom collections.abc import Awaitable\nfrom collections.abc import Callable\n\nfrom fastapi import FastAPI\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom fastapi import Response\n\nfrom ee.onyx.auth.users import decode_anonymous_user_jwt_token",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_api.py_32_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'generate_report'",
      "description": "API endpoint 'generate_report' on line 32 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_api.py",
      "line_number": 32,
      "code_snippet": "\n@router.post(\"/admin/usage-report\", status_code=204)\ndef generate_report(\n    params: GenerateUsageReportParams,\n    user: User = Depends(current_admin_user),\n) -> None:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_api.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\nfrom datetime import datetime\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Response\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_generation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/reporting/usage_export_generation.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport tempfile\nimport uuid\nimport zipfile\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\n\nfrom fastapi_users_db_sqlalchemy import UUID_ID\nfrom sqlalchemy import cast",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/store.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom io import BytesIO\nfrom typing import Any\nfrom typing import cast\nfrom typing import IO\n\nfrom fastapi import HTTPException\nfrom fastapi import UploadFile\n\nfrom ee.onyx.server.enterprise_settings.models import AnalyticsScriptUpload",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Any\nfrom typing import List\n\nfrom pydantic import BaseModel\nfrom pydantic import Field\n\n\nclass NavigationItem(BaseModel):\n    link: str",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/enterprise_settings/api.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom typing import Any\n\nimport httpx\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Response\nfrom fastapi import status",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/user_group/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/user_group/api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.db.user_group import add_users_to_user_group\nfrom ee.onyx.db.user_group import fetch_user_groups\nfrom ee.onyx.db.user_group import fetch_user_groups_for_user\nfrom ee.onyx.db.user_group import insert_user_group",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py_146_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network/admin operation without confirmation in 'confluence_oauth_callback'",
      "description": "Function 'confluence_oauth_callback' on line 146 performs high-risk delete/write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py",
      "line_number": 146,
      "code_snippet": "\n@router.post(\"/connector/confluence/callback\")\ndef confluence_oauth_callback(\n    code: str,\n    state: str,\n    user: User = Depends(current_admin_user),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py_146_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'confluence_oauth_callback'",
      "description": "API endpoint 'confluence_oauth_callback' on line 146 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py",
      "line_number": 146,
      "code_snippet": "\n@router.post(\"/connector/confluence/callback\")\ndef confluence_oauth_callback(\n    code: str,\n    state: str,\n    user: User = Depends(current_admin_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nImplement monitoring to detect extraction attempts:\n\n1. Log all model access:\n   - Request metadata (IP, user, timestamp)\n   - Query patterns and frequency\n   - Response characteristics\n\n2. Anomaly detection:\n   - Unusual query patterns\n   - High-frequency requests from single source\n   - Systematic probing of model boundaries\n\n3. Set up alerts:\n   - Threshold-based alerts (requests/hour)\n   - Pattern-based alerts (systematic extraction)\n   - Geographic anomalies\n\n4. Response analysis:\n   - Track what outputs are returned\n   - Detect if sensitive info is leaked\n   - Monitor for data exfiltration patterns\n\n5. Implement honeypots:\n   - Fake endpoints to detect scanners\n   - Canary tokens in responses\n   - Deception techniques\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py_252",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/confluence_cloud.py",
      "line_number": 252,
      "code_snippet": "        content={\n            \"success\": True,\n            \"message\": \"Confluence Cloud OAuth completed successfully.\",\n            \"finalize_url\": ConfluenceCloudOAuth.generate_finalize_url(credential.id),\n            \"redirect_on_success\": session.redirect_on_success,\n        }\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_24",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'prepare_authorization_request' on line 24 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 24,
      "code_snippet": "def prepare_authorization_request(\n    connector: DocumentSource,\n    redirect_on_success: str | None,\n    user: User = Depends(current_admin_user),\n    tenant_id: str | None = Depends(get_current_tenant_id),\n) -> JSONResponse:\n    \"\"\"Used by the frontend to generate the url for the user's browser during auth request.\n\n    Example: https://www.oauth.com/oauth2-servers/authorization/the-authorization-request/\n    \"\"\"\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_24_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network/admin operation without confirmation in 'prepare_authorization_request'",
      "description": "Function 'prepare_authorization_request' on line 24 performs high-risk write/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 24,
      "code_snippet": "\n@router.post(\"/prepare-authorization-request\")\ndef prepare_authorization_request(\n    connector: DocumentSource,\n    redirect_on_success: str | None,\n    user: User = Depends(current_admin_user),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_24_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'prepare_authorization_request'",
      "description": "Function 'prepare_authorization_request' on line 24 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 24,
      "code_snippet": "\n@router.post(\"/prepare-authorization-request\")\ndef prepare_authorization_request(\n    connector: DocumentSource,\n    redirect_on_success: str | None,\n    user: User = Depends(current_admin_user),",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_24_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'prepare_authorization_request'",
      "description": "API endpoint 'prepare_authorization_request' on line 24 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 24,
      "code_snippet": "\n@router.post(\"/prepare-authorization-request\")\ndef prepare_authorization_request(\n    connector: DocumentSource,\n    redirect_on_success: str | None,\n    user: User = Depends(current_admin_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_49",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 49,
      "code_snippet": "        if not DEV_MODE:\n            oauth_url = SlackOAuth.generate_oauth_url(oauth_state)\n        else:\n            oauth_url = SlackOAuth.generate_dev_oauth_url(oauth_state)\n\n        session = SlackOAuth.session_dump_json(\n            email=user.email, redirect_on_success=redirect_on_success",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_58",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 58,
      "code_snippet": "        if not DEV_MODE:\n            oauth_url = ConfluenceCloudOAuth.generate_oauth_url(oauth_state)\n        else:\n            oauth_url = ConfluenceCloudOAuth.generate_dev_oauth_url(oauth_state)\n        session = ConfluenceCloudOAuth.session_dump_json(\n            email=user.email, redirect_on_success=redirect_on_success\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py_66",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/api.py",
      "line_number": 66,
      "code_snippet": "        if not DEV_MODE:\n            oauth_url = GoogleDriveOAuth.generate_oauth_url(oauth_state)\n        else:\n            oauth_url = GoogleDriveOAuth.generate_dev_oauth_url(oauth_state)\n        session = GoogleDriveOAuth.session_dump_json(\n            email=user.email, redirect_on_success=redirect_on_success\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/google_drive.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/google_drive.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport json\nimport uuid\nfrom typing import Any\nfrom typing import cast\n\nimport requests\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi.responses import JSONResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/slack.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/oauth/slack.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport uuid\nfrom typing import cast\n\nimport requests\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\n\nfrom pydantic import BaseModel\n\nfrom onyx.server.settings.models import ApplicationStatus\n\n\nclass PlanType(str, Enum):\n    MONTHLY = \"monthly\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/api.py_96_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 96. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/api.py",
      "line_number": 96,
      "code_snippet": "        token = generate_data_plane_token()\n    except ValueError as e:\n        logger.error(f\"Failed to generate data plane token: {e}\")\n        raise HTTPException(\n            status_code=500, detail=\"Authentication configuration error\"",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/license/api.py",
      "line_number": 1,
      "code_snippet": "\"\"\"License API endpoints.\"\"\"\n\nimport requests\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import File\nfrom fastapi import HTTPException\nfrom fastapi import UploadFile\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/team_membership_api.py_54",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 54 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/team_membership_api.py",
      "line_number": 54,
      "code_snippet": "        except Exception as e:\n            logger.exception(\n                f\"Failed to delete user from control plane for tenant {tenant_id}: {e}\"\n            )\n            raise HTTPException(",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/team_membership_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/team_membership_api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.server.tenants.provisioning import delete_user_from_control_plane\nfrom ee.onyx.server.tenants.user_mapping import remove_all_users_from_tenant\nfrom ee.onyx.server.tenants.user_mapping import remove_users_from_tenant\nfrom onyx.auth.users import current_admin_user\nfrom onyx.auth.users import User",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/user_invitations_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/user_invitations_api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\n\nfrom ee.onyx.server.tenants.models import ApproveUserRequest\nfrom ee.onyx.server.tenants.models import PendingUserSnapshot\nfrom ee.onyx.server.tenants.models import RequestInviteRequest\nfrom ee.onyx.server.tenants.user_mapping import accept_user_invite\nfrom ee.onyx.server.tenants.user_mapping import approve_user_invite\nfrom ee.onyx.server.tenants.user_mapping import deny_user_invite",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/tenant_management_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/tenant_management_api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\n\nfrom ee.onyx.server.tenants.models import TenantByDomainResponse\nfrom ee.onyx.server.tenants.provisioning import get_tenant_by_domain_from_control_plane\nfrom onyx.auth.users import current_user\nfrom onyx.auth.users import User\nfrom onyx.utils.logger import setup_logger\nfrom shared_configs.contextvars import get_current_tenant_id\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/billing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/billing.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nimport requests\nimport stripe\n\nfrom ee.onyx.configs.app_configs import STRIPE_PRICE_ID\nfrom ee.onyx.configs.app_configs import STRIPE_SECRET_KEY\nfrom ee.onyx.server.tenants.access import generate_data_plane_token\nfrom ee.onyx.server.tenants.models import BillingInformation\nfrom ee.onyx.server.tenants.models import SubscriptionStatusResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/anonymous_users_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/anonymous_users_api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Response\nfrom sqlalchemy.exc import IntegrityError\n\nfrom ee.onyx.auth.users import generate_anonymous_user_jwt_token\nfrom ee.onyx.server.tenants.anonymous_user_path import get_anonymous_user_path\nfrom ee.onyx.server.tenants.anonymous_user_path import (\n    get_tenant_id_for_anonymous_user_path,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/access.py_52_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 52. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/access.py",
      "line_number": 52,
      "code_snippet": "        raise HTTPException(status_code=401, detail=\"Token has expired\")\n    except jwt.InvalidTokenError:\n        logger.warning(\"Invalid token\")\n        raise HTTPException(status_code=401, detail=\"Invalid token\")",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/access.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timedelta\n\nimport jwt\nfrom fastapi import HTTPException\nfrom fastapi import Request\n\nfrom onyx.configs.app_configs import DATA_PLANE_SECRET\nfrom onyx.configs.app_configs import EXPECTED_API_KEY\nfrom onyx.configs.app_configs import JWT_ALGORITHM",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/admin_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/admin_api.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom fastapi import Response\nfrom fastapi_users import exceptions\n\nfrom ee.onyx.auth.users import current_cloud_superuser\nfrom ee.onyx.server.tenants.models import ImpersonateRequest\nfrom ee.onyx.server.tenants.user_mapping import get_tenant_id_for_email\nfrom onyx.auth.users import auth_backend",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/user_mapping.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/user_mapping.py",
      "line_number": 1,
      "code_snippet": "from fastapi_users import exceptions\nfrom sqlalchemy import select\n\nfrom onyx.auth.invited_users import get_invited_users\nfrom onyx.auth.invited_users import get_pending_users\nfrom onyx.auth.invited_users import write_invited_users\nfrom onyx.auth.invited_users import write_pending_users\nfrom onyx.db.engine.sql_engine import get_session_with_shared_schema\nfrom onyx.db.engine.sql_engine import get_session_with_tenant\nfrom onyx.db.models import UserTenantMapping",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/schema_management.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/schema_management.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom types import SimpleNamespace\n\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.schema import CreateSchema\n\nfrom alembic import command\nfrom alembic.config import Config",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/provisioning.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/provisioning.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport uuid\n\nimport aiohttp  # Async HTTP client\nimport httpx\nimport requests\nfrom fastapi import HTTPException\nfrom fastapi import Request\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/billing_api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/billing_api.py",
      "line_number": 1,
      "code_snippet": "import stripe\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\n\nfrom ee.onyx.auth.users import current_admin_user\nfrom ee.onyx.configs.app_configs import STRIPE_SECRET_KEY\nfrom ee.onyx.server.tenants.access import control_plane_dep\nfrom ee.onyx.server.tenants.billing import fetch_billing_information\nfrom ee.onyx.server.tenants.billing import fetch_stripe_checkout_session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/product_gating.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/tenants/product_gating.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom ee.onyx.configs.app_configs import GATED_TENANTS_KEY\nfrom onyx.configs.constants import ONYX_CLOUD_TENANT_ID\nfrom onyx.redis.redis_pool import get_redis_client\nfrom onyx.redis.redis_pool import get_redis_replica_client\nfrom onyx.server.settings.models import ApplicationStatus\nfrom onyx.server.settings.store import load_settings\nfrom onyx.server.settings.store import store_settings\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/manage/standard_answer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/manage/standard_answer.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.db.standard_answer import fetch_standard_answer\nfrom ee.onyx.db.standard_answer import fetch_standard_answer_categories\nfrom ee.onyx.db.standard_answer import fetch_standard_answer_category\nfrom ee.onyx.db.standard_answer import fetch_standard_answers\nfrom ee.onyx.db.standard_answer import insert_standard_answer",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/documents/cc_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/documents/cc_pair.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom http import HTTPStatus\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.background.celery.tasks.doc_permission_syncing.tasks import (\n    try_creating_permissions_sync_task,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_and_chat/token_limit.py_72_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_fetch_user_usage'",
      "description": "Function '_fetch_user_usage' on line 72 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_and_chat/token_limit.py",
      "line_number": 72,
      "code_snippet": "\n\ndef _fetch_user_usage(\n    user_id: UUID, cutoff_time: datetime, db_session: Session\n) -> Sequence[tuple[datetime, int]]:\n    \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_and_chat/token_limit.py_160_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_fetch_user_group_usage'",
      "description": "Function '_fetch_user_group_usage' on line 160 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_and_chat/token_limit.py",
      "line_number": 160,
      "code_snippet": "\n\ndef _fetch_user_group_usage(\n    user_group_ids: list[int], cutoff_time: datetime, db_session: Session\n) -> dict[int, list[Tuple[datetime, int]]]:\n    \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\nfrom ee.onyx.background.task_name_builders import QUERY_HISTORY_TASK_NAME_PREFIX\nfrom onyx.auth.users import get_display_email\nfrom onyx.background.task_utils import extract_task_id_from_query_history_report_name\nfrom onyx.configs.constants import MessageType\nfrom onyx.configs.constants import QAFeedbackType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/api.py_271_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'list_all_query_history_exports'",
      "description": "API endpoint 'list_all_query_history_exports' on line 271 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/api.py",
      "line_number": 271,
      "code_snippet": "\n@router.get(\"/admin/query-history/list\")\ndef list_all_query_history_exports(\n    _: User | None = Depends(current_admin_user),\n    db_session: Session = Depends(get_session),\n) -> list[QueryHistoryExport]:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/query_history/api.py",
      "line_number": 1,
      "code_snippet": "import uuid\nfrom collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\nfrom http import HTTPStatus\nfrom uuid import UUID\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/token_rate_limits/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/token_rate_limits/api.py",
      "line_number": 1,
      "code_snippet": "from collections import defaultdict\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.db.token_limit import fetch_all_user_group_token_rate_limits_by_group\nfrom ee.onyx.db.token_limit import fetch_user_group_token_rate_limits_for_user\nfrom ee.onyx.db.token_limit import insert_user_group_token_rate_limit\nfrom onyx.auth.users import current_admin_user",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/analytics/api.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/server/analytics/api.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom collections import defaultdict\nfrom typing import List\n\nfrom fastapi import APIRouter\nfrom fastapi import Depends\nfrom fastapi import HTTPException\nfrom pydantic import BaseModel\nfrom sqlalchemy.orm import Session\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/jira/group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/jira/group_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\n\nfrom jira import JIRA\n\nfrom ee.onyx.db.external_perm import ExternalUserGroup\nfrom onyx.connectors.jira.utils import build_jira_client\nfrom onyx.db.models import ConnectorCredentialPair\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/jira/page_access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/jira/page_access.py",
      "line_number": 1,
      "code_snippet": "from collections import defaultdict\n\nfrom jira import JIRA\nfrom jira.resources import PermissionScheme\nfrom pydantic import ValidationError\n\nfrom ee.onyx.external_permissions.jira.models import Holder\nfrom ee.onyx.external_permissions.jira.models import Permission\nfrom ee.onyx.external_permissions.jira.models import User\nfrom onyx.access.models import ExternalAccess",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/group_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\n\nfrom github import Repository\n\nfrom ee.onyx.db.external_perm import ExternalUserGroup\nfrom ee.onyx.external_permissions.github.utils import get_external_user_group\nfrom onyx.connectors.github.connector import GithubConnector\nfrom onyx.db.models import ConnectorCredentialPair\nfrom onyx.utils.logger import setup_logger\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/doc_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/doc_sync.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom collections.abc import Generator\n\nfrom github import Github\nfrom github.Repository import Repository\n\nfrom ee.onyx.external_permissions.github.utils import fetch_repository_team_slugs\nfrom ee.onyx.external_permissions.github.utils import form_collaborators_group_id\nfrom ee.onyx.external_permissions.github.utils import form_organization_group_id\nfrom ee.onyx.external_permissions.github.utils import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/github/utils.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom enum import Enum\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import TypeVar\n\nfrom github import Github\nfrom github import RateLimitExceededException\nfrom github.GithubException import GithubException",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py_24_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 24. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py",
      "line_number": 24,
      "code_snippet": ") -> dict[str, set[str]]:\n    group_member_emails: dict[str, set[str]] = {}\n    for user in confluence_client.paginated_cql_user_retrieval():\n        logger.info(f\"Processing groups for user: {user}\")\n\n        email = user.email",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py_51_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 51. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py",
      "line_number": 51,
      "code_snippet": "\n        all_users_groups: set[str] = set()\n        for group in confluence_client.paginated_groups_by_user_retrieval(user.user_id):\n            # group name uniqueness is enforced by Confluence, so we can use it as a group ID\n            group_id = group[\"name\"]\n            group_member_emails.setdefault(group_id, set()).add(email)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py_104_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 104. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py",
      "line_number": 104,
      "code_snippet": "\n            all_users_groups: set[str] = set()\n            for group in confluence_client.paginated_groups_by_user_retrieval(user_key):\n                # group name uniqueness is enforced by Confluence, so we can use it as a group ID\n                group_id = group[\"name\"]\n                group_member_emails.setdefault(group_id, set()).add(email)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/group_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\n\nfrom ee.onyx.db.external_perm import ExternalUserGroup\nfrom ee.onyx.external_permissions.confluence.constants import ALL_CONF_EMAILS_GROUP_NAME\nfrom onyx.background.error_logging import emit_background_error\nfrom onyx.configs.app_configs import CONFLUENCE_USE_ONYX_USERS_FOR_GROUP_SYNC\nfrom onyx.connectors.confluence.onyx_confluence import (\n    get_user_email_from_username__server,\n)\nfrom onyx.connectors.confluence.onyx_confluence import OnyxConfluence",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/doc_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/doc_sync.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nRules defined here:\nhttps://confluence.atlassian.com/conf85/check-who-can-view-a-page-1283360557.html\n\"\"\"\n\nfrom collections.abc import Generator\n\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsFunction\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsIdsFunction\nfrom ee.onyx.external_permissions.utils import generic_doc_sync",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/page_access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/page_access.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom onyx.access.models import ExternalAccess\nfrom onyx.connectors.confluence.onyx_confluence import (\n    get_user_email_from_username__server,\n)\nfrom onyx.connectors.confluence.onyx_confluence import OnyxConfluence\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/space_access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/confluence/space_access.py",
      "line_number": 1,
      "code_snippet": "from ee.onyx.configs.app_configs import CONFLUENCE_ANONYMOUS_ACCESS_IS_PUBLIC\nfrom ee.onyx.external_permissions.confluence.constants import ALL_CONF_EMAILS_GROUP_NAME\nfrom ee.onyx.external_permissions.confluence.constants import REQUEST_PAGINATION_LIMIT\nfrom ee.onyx.external_permissions.confluence.constants import VIEWSPACE_PERMISSION_TYPE\nfrom onyx.access.models import ExternalAccess\nfrom onyx.connectors.confluence.onyx_confluence import (\n    get_user_email_from_username__server,\n)\nfrom onyx.connectors.confluence.onyx_confluence import OnyxConfluence\nfrom onyx.utils.logger import setup_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/sharepoint/group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/sharepoint/group_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\n\nfrom office365.sharepoint.client_context import ClientContext  # type: ignore[import-untyped]\n\nfrom ee.onyx.db.external_perm import ExternalUserGroup\nfrom ee.onyx.external_permissions.sharepoint.permission_utils import (\n    get_sharepoint_external_groups,\n)\nfrom onyx.connectors.sharepoint.connector import acquire_token_for_rest\nfrom onyx.connectors.sharepoint.connector import SharepointConnector",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/sharepoint/permission_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/sharepoint/permission_utils.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom collections import deque\nfrom typing import Any\nfrom urllib.parse import unquote\nfrom urllib.parse import urlparse\n\nfrom office365.graph_client import GraphClient  # type: ignore[import-untyped]\nfrom office365.onedrive.driveitems.driveItem import DriveItem  # type: ignore[import-untyped]\nfrom office365.runtime.client_request import ClientRequestException  # type: ignore\nfrom office365.sharepoint.client_context import ClientContext  # type: ignore[import-untyped]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py_43",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 43 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py",
      "line_number": 43,
      "code_snippet": "\ndef _query_salesforce_user_id(sf_client: Salesforce, user_email: str) -> str | None:\n    query = f\"SELECT Id FROM User WHERE Username = '{user_email}' AND IsActive = true\"\n    result = sf_client.query(query)\n    if len(result[\"records\"]) > 0:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py_49",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 49 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py",
      "line_number": 49,
      "code_snippet": "\n    # try emails\n    query = f\"SELECT Id FROM User WHERE Email = '{user_email}' AND IsActive = true\"\n    result = sf_client.query(query)\n    if len(result[\"records\"]) > 0:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py_123",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 123 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py",
      "line_number": 123,
      "code_snippet": "    truncated_record_ids = record_ids[:_MAX_RECORD_IDS_PER_QUERY]\n    record_ids_str = \"'\" + \"','\".join(truncated_record_ids) + \"'\"\n    access_query = f\"\"\"\n    SELECT RecordId, HasReadAccess\n    FROM UserRecordAccess",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/utils.py",
      "line_number": 1,
      "code_snippet": "from simple_salesforce import Salesforce\nfrom sqlalchemy.orm import Session\n\nfrom onyx.db.connector_credential_pair import get_connector_credential_pair_from_id\nfrom onyx.db.document import get_cc_pairs_for_document\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\n_ANY_SALESFORCE_CLIENT: Salesforce | None = None",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/postprocessing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/salesforce/postprocessing.py",
      "line_number": 1,
      "code_snippet": "import time\n\nfrom ee.onyx.db.external_perm import fetch_external_groups_for_user_email_and_group_ids\nfrom ee.onyx.external_permissions.salesforce.utils import (\n    get_any_salesforce_client_for_doc_id,\n)\nfrom ee.onyx.external_permissions.salesforce.utils import get_objects_access_for_user_id\nfrom ee.onyx.external_permissions.salesforce.utils import (\n    get_salesforce_user_id_from_email,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/channel_access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/channel_access.py",
      "line_number": 1,
      "code_snippet": "from slack_sdk import WebClient\n\nfrom onyx.access.models import ExternalAccess\nfrom onyx.connectors.models import BasicExpertInfo\nfrom onyx.connectors.slack.connector import ChannelType\nfrom onyx.connectors.slack.utils import expert_info_from_slack_id\nfrom onyx.connectors.slack.utils import make_paginated_slack_api_call\n\n\ndef get_channel_access(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/group_sync.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nTHIS IS NOT USEFUL OR USED FOR PERMISSION SYNCING\nWHEN USERGROUPS ARE ADDED TO A CHANNEL, IT JUST RESOLVES ALL THE USERS TO THAT CHANNEL\nSO WHEN CHECKING IF A USER CAN ACCESS A DOCUMENT, WE ONLY NEED TO CHECK THEIR EMAIL\nTHERE IS NO USERGROUP <-> DOCUMENT PERMISSION MAPPING\n\"\"\"\n\nfrom slack_sdk import WebClient\n\nfrom ee.onyx.db.external_perm import ExternalUserGroup",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/doc_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/slack/doc_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\n\nfrom slack_sdk import WebClient\n\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsFunction\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsIdsFunction\nfrom ee.onyx.external_permissions.slack.utils import fetch_user_id_to_email_map\nfrom onyx.access.models import DocExternalAccess\nfrom onyx.access.models import ExternalAccess\nfrom onyx.connectors.credentials_provider import OnyxDBCredentialsProvider",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/gmail/doc_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/gmail/doc_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsFunction\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsIdsFunction\nfrom onyx.access.models import DocExternalAccess\nfrom onyx.connectors.gmail.connector import GmailConnector\nfrom onyx.connectors.interfaces import GenerateSlimDocumentOutput\nfrom onyx.db.models import ConnectorCredentialPair",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/permission_retrieval.py_35_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 35. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/permission_retrieval.py",
      "line_number": 35,
      "code_snippet": "\n    # Fetch all permissions for the document\n    fetched_permissions = execute_paginated_retrieval(\n        retrieval_function=drive_service.permissions().list,\n        list_key=\"permissions\",\n        fileId=doc_id,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/permission_retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/permission_retrieval.py",
      "line_number": 1,
      "code_snippet": "from retry import retry\n\nfrom ee.onyx.external_permissions.google_drive.models import GoogleDrivePermission\nfrom onyx.connectors.google_utils.google_utils import execute_paginated_retrieval\nfrom onyx.connectors.google_utils.resources import GoogleDriveService\nfrom onyx.utils.logger import setup_logger\n\nlogger = setup_logger()\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\n\nclass PermissionType(str, Enum):\n    USER = \"user\"\n    GROUP = \"group\"\n    DOMAIN = \"domain\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py_200_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 200. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py",
      "line_number": 200,
      "code_snippet": "\n        try:\n            for permission in execute_paginated_retrieval(\n                drive_service.permissions().list,\n                list_key=\"permissions\",\n                fileId=drive_id,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py_260_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 260. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py",
      "line_number": 260,
      "code_snippet": "    \"\"\"\n    group_emails: set[str] = set()\n    for group in execute_paginated_retrieval(\n        admin_service.groups().list,\n        list_key=\"groups\",\n        domain=google_domain,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py_278_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 278. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py",
      "line_number": 278,
      "code_snippet": "    \"\"\"\n    group_member_emails: set[str] = set()\n    for member in execute_paginated_retrieval(\n        admin_service.members().list,\n        list_key=\"members\",\n        groupKey=group_email,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py_302_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 302. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py",
      "line_number": 302,
      "code_snippet": "    for group_email in group_emails:\n        group_member_emails: set[str] = set()\n        for member in execute_paginated_retrieval(\n            admin_service.members().list,\n            list_key=\"members\",\n            groupKey=group_email,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/group_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\n\nfrom googleapiclient.errors import HttpError  # type: ignore\nfrom pydantic import BaseModel\n\nfrom ee.onyx.db.external_perm import ExternalUserGroup\nfrom ee.onyx.external_permissions.google_drive.folder_retrieval import (\n    get_folder_permissions_by_ids,\n)\nfrom ee.onyx.external_permissions.google_drive.folder_retrieval import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/doc_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/doc_sync.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom ee.onyx.external_permissions.google_drive.models import GoogleDrivePermission\nfrom ee.onyx.external_permissions.google_drive.models import PermissionType\nfrom ee.onyx.external_permissions.google_drive.permission_retrieval import (\n    get_permissions_by_ids,\n)\nfrom ee.onyx.external_permissions.perm_sync_types import FetchAllDocumentsFunction",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/folder_retrieval.py_73_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 73. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/external_permissions/google_drive/folder_retrieval.py",
      "line_number": 73,
      "code_snippet": "\n    # Retrieve and yield folders\n    for folder in execute_paginated_retrieval(\n        retrieval_function=service.files().list,\n        list_key=\"files\",\n        continue_on_404_or_403=True,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/beat_schedule.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/beat_schedule.py",
      "line_number": 1,
      "code_snippet": "from datetime import timedelta\nfrom typing import Any\n\nfrom ee.onyx.configs.app_configs import CHECK_TTL_MANAGEMENT_TASK_FREQUENCY_IN_HOURS\nfrom onyx.background.celery.tasks.beat_schedule import (\n    beat_cloud_tasks as base_beat_system_tasks,\n)\nfrom onyx.background.celery.tasks.beat_schedule import BEAT_EXPIRES_DEFAULT\nfrom onyx.background.celery.tasks.beat_schedule import (\n    beat_task_templates as base_beat_task_templates,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/external_group_syncing/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/external_group_syncing/tasks.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom typing import Any\nfrom typing import cast\nfrom uuid import uuid4\n\nfrom celery import Celery\nfrom celery import shared_task",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/vespa/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/vespa/tasks.py",
      "line_number": 1,
      "code_snippet": "from typing import cast\n\nfrom redis import Redis\nfrom sqlalchemy.orm import Session\n\nfrom ee.onyx.db.user_group import delete_user_group\nfrom ee.onyx.db.user_group import fetch_user_group\nfrom ee.onyx.db.user_group import mark_user_group_as_synced\nfrom ee.onyx.db.user_group import prepare_user_group_for_deletion\nfrom onyx.background.celery.apps.app_base import task_logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/doc_permission_syncing/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/doc_permission_syncing/tasks.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom time import sleep\nfrom typing import Any\nfrom typing import cast\nfrom uuid import uuid4\n\nfrom celery import Celery",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/cleanup/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/cleanup/tasks.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timedelta\n\nfrom celery import shared_task\n\nfrom ee.onyx.db.query_history import get_all_query_history_export_tasks\nfrom onyx.configs.app_configs import JOB_TIMEOUT\nfrom onyx.configs.constants import OnyxCeleryTask\nfrom onyx.db.engine.sql_engine import get_session_with_tenant\nfrom onyx.db.enums import TaskStatus",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/cloud/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/cloud/tasks.py",
      "line_number": 1,
      "code_snippet": "import time\n\nfrom celery import shared_task\nfrom celery import Task\nfrom celery.exceptions import SoftTimeLimitExceeded\nfrom redis.lock import Lock as RedisLock\n\nfrom onyx.background.celery.apps.app_base import task_logger\nfrom onyx.background.celery.tasks.beat_schedule import BEAT_EXPIRES_DEFAULT\nfrom onyx.configs.constants import CELERY_GENERIC_BEAT_LOCK_TIMEOUT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/ttl_management/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/ttl_management/tasks.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom datetime import timezone\nfrom uuid import UUID\n\nfrom celery import shared_task\nfrom celery import Task\n\nfrom ee.onyx.background.celery_utils import should_perform_chat_ttl_check\nfrom ee.onyx.background.task_name_builders import name_chat_ttl_task\nfrom onyx.configs.app_configs import JOB_TIMEOUT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/tenant_provisioning/tasks.py_104_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'pre_provision_tenant'",
      "description": "Function 'pre_provision_tenant' on line 104 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/tenant_provisioning/tasks.py",
      "line_number": 104,
      "code_snippet": "\n\ndef pre_provision_tenant() -> None:\n    \"\"\"\n    Pre-provision a new tenant and store it in the NewAvailableTenant table.\n    This function fully sets up the tenant with all necessary configurations,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/tenant_provisioning/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/tenant_provisioning/tasks.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nPeriodic tasks for tenant pre-provisioning.\n\"\"\"\n\nimport asyncio\nimport datetime\nimport uuid\n\nfrom celery import shared_task\nfrom celery import Task",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/query_history/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/backend/ee/onyx/background/celery/tasks/query_history/tasks.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport io\nfrom datetime import datetime\n\nfrom celery import shared_task\nfrom celery import Task\n\nfrom ee.onyx.server.query_history.api import fetch_and_process_chat_session_history\nfrom ee.onyx.server.query_history.api import ONYX_ANONYMIZED_EMAIL\nfrom ee.onyx.server.query_history.models import QuestionAnswerPairSnapshot",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/hatch_build.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/hatch_build.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\nimport subprocess\nfrom typing import Any\n\nimport manygo\nfrom hatchling.builders.hooks.plugin.interface import BuildHookInterface\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py_139",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 139 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
      "line_number": 139,
      "code_snippet": "    try:\n        result = subprocess.run(cmd, check=False)  # noqa: S603\n\n        if result.returncode == 0:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py_100_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'generate_client'",
      "description": "Function 'generate_client' on line 100 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
      "line_number": 100,
      "code_snippet": "\n    return schema\n\n\ndef generate_client(\n    openapi_json_path: str, output_dir: str | None = None, strip_tags: bool = True\n) -> bool:\n    \"\"\"Generate Python client from OpenAPI schema using openapi-generator-cli.\n\n    Returns True on success, False on failure.",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py_100_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'generate_client'",
      "description": "Function 'generate_client' on line 100 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
      "line_number": 100,
      "code_snippet": "\n\ndef generate_client(\n    openapi_json_path: str, output_dir: str | None = None, strip_tags: bool = True\n) -> bool:\n    \"\"\"Generate Python client from OpenAPI schema using openapi-generator-cli.",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py_25_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'generate_schema'",
      "description": "API endpoint 'generate_schema' on line 25 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
      "line_number": 25,
      "code_snippet": "\n\ndef generate_schema(output_path: str, tagged_for_docs: str | None = None) -> bool:\n    \"\"\"Generate OpenAPI schema to the specified path.\n\n    By default outputs tag-stripped schema (for client generation).",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp6gr5nbjh/danswer/tools/ods/internal/openapi/openapi_schema.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Generate OpenAPI schema and Python client for Onyx API.\n\nThis script is bundled with the ods wheel and executed by the Go binary\nto generate the OpenAPI schema without starting the full API server.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 2083,
      "kept": 1647,
      "filtered": 436,
      "reduction_pct": 20.9,
      "avg_tp_probability": 0.577,
      "filter_reasons": {
        "high severity with context": 419,
        "test file": 291,
        "build tool subprocess": 103,
        "SQLAlchemy session.exec": 21,
        "executor pool pattern": 16,
        "asyncio.run pattern": 12,
        "callback handler pattern": 11,
        "example file": 7,
        "placeholder value": 3,
        "server runner": 2,
        "PyTorch model.eval": 2
      }
    }
  }
}