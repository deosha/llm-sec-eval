{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:15:05.175177Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin",
    "files_scanned": 1216,
    "overall_score": 6.38,
    "confidence": 0.66,
    "duration_seconds": 38.416,
    "findings_count": 1138,
    "severity_breakdown": {
      "CRITICAL": 678,
      "HIGH": 211,
      "MEDIUM": 146,
      "LOW": 6,
      "INFO": 97
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.45,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Sandboxing"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.45,
      "subscores": {
        "model_protection": 80,
        "extraction_defense": 57,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption at rest",
        "Encryption in transit",
        "OAuth",
        "Rate limiting",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": []
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.54,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "PII encryption",
        "PII masking",
        "Explicit consent",
        "Consent withdrawal",
        "Right to access",
        "Right to delete",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 22,
      "confidence": 0.57,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 50,
        "audit_logging": 0,
        "incident_response": 50,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.91,
      "subscores": {
        "LLM01": 72,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 0,
        "LLM07": 0,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 30
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 1 critical",
        "Insecure Output Handling: 59 critical, 1 high",
        "Model Denial of Service: 38 critical",
        "Supply Chain Vulnerabilities: 3 critical, 44 high, 46 medium, 6 low",
        "Sensitive Information Disclosure: 5 critical, 10 high, 8 medium",
        "Insecure Plugin Design: 30 critical, 9 high",
        "Excessive Agency: 23 critical, 121 high, 17 medium",
        "Overreliance: 24 critical, 18 high",
        "Model Theft: 4 high",
        "ML: 495 critical, 75 medium",
        "SQLI: 4 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/run_maintenance_tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/run_maintenance_tasks.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom datetime import datetime, timedelta, timezone\n\nfrom server.logger import logger\nfrom storage.database import session_maker\nfrom storage.maintenance_task import (\n    MaintenanceTask,\n    MaintenanceTaskStatus,\n)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/version.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/version.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom pathlib import Path\n\n__package_name__ = 'openhands_ai'\n\n\ndef get_version():\n    # Try getting the version from pyproject.toml\n    try:\n        root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/scripts/update_openapi.py_128_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'generate_openapi_spec'",
      "description": "API endpoint 'generate_openapi_spec' on line 128 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/scripts/update_openapi.py",
      "line_number": 128,
      "code_snippet": "\n\ndef generate_openapi_spec():\n    \"\"\"Generate the OpenAPI specification from the FastAPI app.\"\"\"\n    spec = app.openapi()\n",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/scripts/update_openapi.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/scripts/update_openapi.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nUpdate OpenHands OpenAPI documentation.\n\nGenerates the OpenAPI specification from the FastAPI application and writes it\nto docs/openapi.json.\n\nUsage:\n    python scripts/update_openapi.py\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_193",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'eval_output_path' flows to 'EvalMetadata' on line 193 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 193,
      "code_snippet": "\n    metadata = EvalMetadata(\n        agent_class=agent_class,\n        llm_config=llm_config,",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_699_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 699. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 699,
      "code_snippet": "\n\ndef get_default_sandbox_config_for_eval() -> SandboxConfig:\n    return SandboxConfig(\n        use_host_network=False,\n        # large enough timeout, since some testcases take very long to run",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_715_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 715. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 715,
      "code_snippet": "\n\ndef get_openhands_config_for_eval(\n    metadata: EvalMetadata | None = None,\n    sandbox_config: SandboxConfig | None = None,\n    runtime: str | None = None,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_734_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 734. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 734,
      "code_snippet": "    Args:\n        metadata: EvalMetadata containing agent class, max iterations, etc.\n        sandbox_config: Custom sandbox config. If None, uses get_default_sandbox_config_for_eval()\n        runtime: Runtime type. If None, uses environment RUNTIME or 'docker'\n        max_iterations: Max iterations for the agent. If None, uses metadata.max_iterations\n        default_agent: Agent class name. If None, uses metadata.agent_class",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_752_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 752. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 752,
      "code_snippet": "    # Use provided sandbox config or get default\n    if sandbox_config is None:\n        sandbox_config = get_default_sandbox_config_for_eval()\n\n    # Extract values from metadata if provided\n    if metadata is not None:",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_193_exec_taint",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output flows to code execution in 'make_metadata'",
      "description": "In function 'make_metadata', LLM output variable 'model_path' flows to 'EvalMetadata' on line 193 via single_hop flow. This grants excessive agency to the LLM, allowing it to execute arbitrary code without human oversight.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 193,
      "code_snippet": "    logger.info(f'Using evaluation output directory: {eval_output_path}')\n\n    metadata = EvalMetadata(\n        agent_class=agent_class,\n        llm_config=llm_config,\n        agent_config=agent_config,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_164_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'make_metadata'",
      "description": "Function 'make_metadata' on line 164 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 164,
      "code_snippet": "\n\ndef make_metadata(\n    llm_config: LLMConfig,\n    dataset_name: str,\n    agent_class: str,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/shared.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport multiprocessing as mp\nimport os\nimport pathlib\nimport signal\nimport subprocess\nimport time\nimport traceback\nfrom contextlib import contextmanager",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_hf.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_hf.py",
      "line_number": 1,
      "code_snippet": "# Copyright 2023 https://github.com/ShishirPatil/gorilla\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py_45_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 45. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
      "line_number": 45,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py_47_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 47. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
      "line_number": 47,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py_42_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 42 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
      "line_number": 42,
      "code_snippet": "    'CodeActAgent': 'When you think you have completed the request, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py_58_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 58 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
      "line_number": 58,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py_58_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 58 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/run_infer.py",
      "line_number": 58,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_th.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_th.py",
      "line_number": 1,
      "code_snippet": "# Copyright 2023 https://github.com/ShishirPatil/gorilla\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom functools import partial\n\nimport httpx\nimport pandas as pd\nfrom ast_eval_hf import ast_eval_hf, ast_parse\nfrom ast_eval_tf import ast_eval_tf\nfrom ast_eval_th import ast_eval_th\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_tf.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gorilla/ast_eval_tf.py",
      "line_number": 1,
      "code_snippet": "# Copyright 2023 https://github.com/ShishirPatil/gorilla\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_320",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 320 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 320,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_153_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 153. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 153,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_164_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 164. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 164,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        enable_browser=RUN_WITH_BROWSING,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_136_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 136 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 136,
      "code_snippet": "        return ('docker.io/luolin101/'.rstrip('/') + '/' + image_name).lower()\n    return (DOCKER_IMAGE_PREFIX.rstrip('/') + '/' + image_name).lower()\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of swe-bench eval\n    use_official_image = bool(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_186_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 186 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 186,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_320_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 320 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 320,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_440_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 440 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 440,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_186_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 186 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 186,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_440_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 440 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 440,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_320_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 320 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 320,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py_215",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visual_swe_bench/run_infer.py",
      "line_number": 215,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_50_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 50. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 50,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_52_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 52. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 52,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_148_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'token_gh' containing sensitive data is included in a prompt string on line 148. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 148,
      "code_snippet": "\n    token_gh = bench_config['token_gh']\n    commandf = f'export TOKEN_GH={token_gh}'\n    action = CmdRunAction(command=commandf)\n    logger.info(action, extra={'msg_type': 'ACTION'})",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_203_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'token_gh' containing sensitive data is included in a prompt string on line 203. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 203,
      "code_snippet": "\n    token_gh = bench_config['token_gh']\n    commandf = f'export TOKEN_GH={token_gh}'\n    action = CmdRunAction(command=commandf)\n    logger.info(action, extra={'msg_type': 'ACTION'})",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_47_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 47 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 47,
      "code_snippet": "from openhands.runtime.base import Runtime\nfrom openhands.utils.async_utils import call_async_from_sync\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_87_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 87 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 87,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_180_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 180 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 180,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_180_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 180 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 180,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_87_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 87 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 87,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_169",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 169,
      "code_snippet": "    commandf = 'cat branch_name.txt'\n    action = CmdRunAction(command=commandf)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    bench_config['user_branch_name'] = obs.content\n\n    # Navigate to the task's code path",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py_211",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/run_infer.py",
      "line_number": 211,
      "code_snippet": "    # Navigate to the lca-baseslines scripts path\n    action = CmdRunAction(command=f'cd {lca_ci_path}')\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    assert obs.exit_code == 0\n\n    commandf = f'poetry run python run_push_datapoint.py --id {instance[\"id\"]} --model-name {model_name} --user-branch-name {user_branch_name} > single_output.json'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py_28",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 28 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
      "line_number": 28,
      "code_snippet": "    print(f'Cloning lca-baselines repository from {repo_url} into {lca_path}')\n    result = subprocess.run(\n        ['git', 'clone', repo_url], cwd=lca_path, capture_output=True, text=True\n    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py_36",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 36 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
      "line_number": 36,
      "code_snippet": "    print('Switching branches')\n    result = subprocess.run(\n        ['git', 'switch', 'open-hands-integration'],\n        cwd=lca_ci_path,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py_10_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'setup'",
      "description": "Function 'setup' on line 10 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
      "line_number": 10,
      "code_snippet": "\nimport yaml\n\n\ndef setup():\n    # Read config.yaml\n    print('Reading config.yaml')\n    script_dir = os.path.dirname(\n        os.path.abspath(__file__)\n    )  # Get the absolute path of the script",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py_10_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup'",
      "description": "Function 'setup' on line 10 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
      "line_number": 10,
      "code_snippet": "\n\ndef setup():\n    # Read config.yaml\n    print('Reading config.yaml')\n    script_dir = os.path.dirname(",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/setup.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Installs LCA CI Build Repair benchmark with scripts for OH integration.\"\"\"\n\nimport os\nimport shutil\nimport subprocess\n\nimport yaml\n\n\ndef setup():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_39_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 39. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 39,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_41_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 41. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 41,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_68_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 68. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 68,
      "code_snippet": "\n\ndef run_eval(\n    runtime: Runtime,\n):\n    \"\"\"Run the evaluation and create report\"\"\"",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_206_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 206. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 206,
      "code_snippet": "\n    # get results\n    results_str = run_eval(runtime)\n    results_path = os.path.join(args.predictions_path, 'results.jsonl')\n    with open(results_path, 'w') as file:\n        file.write(results_str)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_115_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'token_gh' containing sensitive data is included in a prompt string on line 115. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 115,
      "code_snippet": "\n    token_gh = bench_config['token_gh']\n    commandf = f'export TOKEN_GH={token_gh}'\n    action = CmdRunAction(command=commandf)\n    logger.info(action, extra={'msg_type': 'ACTION'})",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_36_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 36 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 36,
      "code_snippet": "from openhands.runtime.base import Runtime\nfrom openhands.utils.async_utils import call_async_from_sync\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_68_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_eval'",
      "description": "Function 'run_eval' on line 68 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 68,
      "code_snippet": "\nbench_config = load_bench_config()\n\n\ndef run_eval(\n    runtime: Runtime,\n):\n    \"\"\"Run the evaluation and create report\"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Runtime Initialization Fn {\"-\" * 50}')\n    obs: CmdOutputObservation",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_68_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_eval'",
      "description": "Function 'run_eval' on line 68 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 68,
      "code_snippet": "\n\ndef run_eval(\n    runtime: Runtime,\n):\n    \"\"\"Run the evaluation and create report\"\"\"",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/lca_ci_build_repair/eval_infer.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Implements evaluation on JetBrains CI builds repair baselines\n\nPlease see https://github.com/JetBrains-Research/lca-baselines/tree/main/ci-builds-repair\nand https://huggingface.co/datasets/JetBrains-Research/lca-ci-builds-repair\n\nTODOs:\n- Add more flags\n\"\"\"\n\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_334",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'tests' flows to 'runtimes.append' on line 334 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 334,
      "code_snippet": "                status.append(tests[test_id]['outcome'])\n                runtimes.append(tests[test_id]['duration'])\n                no_runs += 1\n            else:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_483",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'instruction' flows to 'EvalOutput' on line 483 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 483,
      "code_snippet": "    # Save the output\n    output = EvalOutput(\n        instance_id=instance.instance_id,\n        instruction=instruction,",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_198",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 198 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 198,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_115_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 115. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 115,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n\n    config = get_openhands_config_for_eval(",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_118_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 118. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 118,
      "code_snippet": "    sandbox_config.base_container_image = base_container_image\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        sandbox_config=sandbox_config,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_138_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 138 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 138,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_198_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 198 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 198,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_380_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 380 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 380,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_198_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 198 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 198,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_380_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 380 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 380,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py_138_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 138 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/commit0/run_infer.py",
      "line_number": 138,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py_115_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'answerer'",
      "description": "Function 'answerer' on line 115 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py",
      "line_number": 115,
      "code_snippet": "        logger=LOGGER,\n    )\n    def answerer(self, question):\n        openai.api_base = self.user_api_base\n        client = OpenAI(api_key=openai.api_key)\n        user_messages = [",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py_175_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'answerer'",
      "description": "Function 'answerer' on line 175 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py",
      "line_number": 175,
      "code_snippet": "        logger=LOGGER,\n    )\n    def answerer(self, question):\n        openai.api_base = self.user_api_base\n        client = OpenAI(api_key=openai.api_key)\n        user_messages = [",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py_133",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py",
      "line_number": 133,
      "code_snippet": "            },\n        ]\n\n        response = client.chat.completions.create(\n            model=self.answerer_model,\n            messages=user_messages,\n            max_tokens=6,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py_193",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/game.py",
      "line_number": 193,
      "code_snippet": "            },\n        ]\n\n        response = client.chat.completions.create(\n            model=self.answerer_model,\n            messages=user_messages,\n            max_tokens=6,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py_65_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 65. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py",
      "line_number": 65,
      "code_snippet": ") -> OpenHandsConfig:\n    # Create config with EDA-specific container image\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py_61_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 61 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py",
      "line_number": 61,
      "code_snippet": "    'CodeActAgent': 'When you think you have solved the question, please first send your answer to user through message and then exit.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # Create config with EDA-specific container image\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py_79_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 79 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py",
      "line_number": 79,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py_79_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 79 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py",
      "line_number": 79,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py_43",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/EDA/run_infer.py",
      "line_number": 43,
      "code_snippet": "        model_guess = last_agent_message.content if last_agent_message else ''\n\n    assert game is not None, 'Game is not initialized.'\n    msg = game.generate_user_response(model_guess)\n    game.curr_turn += 1\n    logger.info(f'Model guess: {model_guess}')\n    logger.info(f'Answer response: {msg}')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_422",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'runtime.run_action' is used in 'commands.' on line 422 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 422,
      "code_snippet": "    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to remove git remotes: {str(obs)}')",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_465",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 465 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 465,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_261_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 261. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 261,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_248_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 248 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 248,
      "code_snippet": "    )  # to comply with docker image naming convention\n    return (docker_image_prefix.rstrip('/') + '/' + image_name).lower()\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    base_container_image = get_instance_docker_image(\n        instance['instance_id'],",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_304_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 304 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 304,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_465_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 465 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 465,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_634_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 634 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 634,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_304_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 304 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 304,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_634_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 634 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 634,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_465_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 465 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 465,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py_335",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/run_infer.py",
      "line_number": 335,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/format_conversion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/format_conversion.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom argparse import ArgumentParser\n\nparser = ArgumentParser()\nparser.add_argument('--input_path', type=str, help='Name of input path to JSON file.')\nparser.add_argument('--output_path', type=str, help='Name of output path to JSON file.')\nargs = parser.parse_args()\n\ninput_path = args.input_path",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/binary_patch_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/binary_patch_utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nUtilities for handling binary files and patch generation in SWE-Perf evaluation.\n\"\"\"\n\n\ndef remove_binary_diffs(patch_text):\n    \"\"\"\n    Remove binary file diffs from a git patch.\n\n    Args:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_87_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 87. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 87,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_89_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 89. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 89,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_84_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 84 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 84,
      "code_snippet": "    'CodeActAgent': 'When you think you have fixed the issue through code changes, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_104_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 104 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 104,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_146_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 146 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 146,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_146_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 146 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 146,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_194_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 194 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 194,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py_104_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 104 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/humanevalfix/run_infer.py",
      "line_number": 104,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/env.py",
      "line_number": 1,
      "code_snippet": "import re\nimport traceback\n\nfrom datatypes import ParseError, StepOutput, TaskState\nfrom tasks.base import Task\n\nfrom openhands.controller.state.state import State\n\n\nclass SimplifiedEnv:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py_114_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 114. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py",
      "line_number": 114,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py_105_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 105 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py",
      "line_number": 105,
      "code_snippet": "    ) as f:\n        return f.read()\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'xingyaoww/od-eval-mint:v1.0'\n    sandbox_config.runtime_extra_deps = (",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py_125_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 125 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py",
      "line_number": 125,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py_147_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 147 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py",
      "line_number": 147,
      "code_snippet": "\n\ndef process_instance(\n    instance: Any,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py_125_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 125 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/run_infer.py",
      "line_number": 125,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/datatypes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/datatypes.py",
      "line_number": 1,
      "code_snippet": "import enum\nfrom typing import Any\n\n\nclass TaskState:\n    def __init__(\n        self,\n        finished: bool = False,\n        success: bool = False,\n        agent_action_count: dict = None,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/utils.py_63_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 63. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/utils.py",
      "line_number": 63,
      "code_snippet": "                    # Once you have read this disclaimer and taken appropriate precautions,\n                    # uncomment the following line and proceed at your own risk:\n                    exec(check_program, exec_globals)\n            result.append('passed')\n        except TimeoutException:\n            result.append('timed out')",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/utils.py",
      "line_number": 1,
      "code_snippet": "import contextlib\nimport faulthandler\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport signal\nimport tempfile\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_83",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'initialize_runtime' on line 83 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 83,
      "code_snippet": "def initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    \"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Runtime Initialization Fn {\"-\" * 50}')\n    obs: CmdOutputObservation\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_64_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 64. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 64,
      "code_snippet": "    instance_id: str,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = (\n        'docker.io/xingyaoww/openhands-eval-scienceagentbench'\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_68_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 68. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 68,
      "code_snippet": "        'docker.io/xingyaoww/openhands-eval-scienceagentbench'\n    )\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime=os.environ.get('RUNTIME', 'docker'),\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_83_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 83 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 83,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_125_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 125 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 125,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_159_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 159 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 159,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_125_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 125 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 125,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py_83_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 83 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/run_infer.py",
      "line_number": 83,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/post_proc.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/scienceagentbench/post_proc.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom argparse import ArgumentParser\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument(\n        'log_fname',\n        type=str,\n    )\n    args = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_43_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 43. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 43,
      "code_snippet": "    agent_config: AgentConfig | None,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    # If the web services are running on the host machine, this must be set to True",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_48_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 48. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 48,
      "code_snippet": "    # If the web services are running on the host machine, this must be set to True\n    sandbox_config.use_host_network = True\n    config = get_openhands_config_for_eval(\n        max_iterations=100,\n        # we mount trajectories path so that trajectories, generated by OpenHands\n        # controller, can be accessible to the evaluator file in the runtime container",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_36_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 36 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 36,
      "code_snippet": "from openhands.runtime.base import Runtime\nfrom openhands.utils.async_utils import call_async_from_sync\n\n\ndef get_config(\n    base_container_image: str,\n    task_short_name: str,\n    mount_path_on_host: str,\n    llm_config: LLMConfig,\n    agent_config: AgentConfig | None,",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_126_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'run_solver' executes dangerous operations",
      "description": "Tool function 'run_solver' on line 126 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 126,
      "code_snippet": "            )\n    return msg\n\n\ndef run_solver(\n    runtime: Runtime,\n    task_name: str,\n    config: OpenHandsConfig,\n    dependencies: list[str],\n    save_final_state: bool,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_71_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'load_dependencies'",
      "description": "Function 'load_dependencies' on line 71 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 71,
      "code_snippet": "\n\ndef load_dependencies(runtime: Runtime) -> list[str]:\n    \"\"\"Every task has a dependencies.yml file, which lists all the services that the\n    task depends on. This function loads the file and returns all dependent service names.\n    \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_87_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'init_task_env'",
      "description": "Function 'init_task_env' on line 87 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 87,
      "code_snippet": "\n\ndef init_task_env(runtime: Runtime, hostname: str, env_llm_config: LLMConfig):\n    command = (\n        f'SERVER_HOSTNAME={hostname} '\n        f'LITELLM_API_KEY={env_llm_config.api_key.get_secret_value() if env_llm_config.api_key else None} '",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_126_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'run_solver'",
      "description": "Function 'run_solver' on line 126 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 126,
      "code_snippet": "\n\ndef run_solver(\n    runtime: Runtime,\n    task_name: str,\n    config: OpenHandsConfig,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_181_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'run_evaluator'",
      "description": "Function 'run_evaluator' on line 181 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 181,
      "code_snippet": "\n\ndef run_evaluator(\n    runtime: Runtime, env_llm_config: LLMConfig, trajectory_path: str, result_path: str\n):\n    command = (",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_126_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_solver'",
      "description": "Function 'run_solver' on line 126 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 126,
      "code_snippet": "\n\ndef run_solver(\n    runtime: Runtime,\n    task_name: str,\n    config: OpenHandsConfig,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py_181_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_evaluator'",
      "description": "Function 'run_evaluator' on line 181 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/run_infer.py",
      "line_number": 181,
      "code_snippet": "\n\ndef run_evaluator(\n    runtime: Runtime, env_llm_config: LLMConfig, trajectory_path: str, result_path: str\n):\n    command = (",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/browsing.py_168_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'pre_login'",
      "description": "Function 'pre_login' on line 168 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/browsing.py",
      "line_number": 168,
      "code_snippet": "\n\ndef pre_login(\n    runtime: Runtime,\n    services: list[str],\n    save_screenshots=True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/browsing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/browsing.py",
      "line_number": 1,
      "code_snippet": "##################################################################################################\n# Adapted from https://github.com/TheAgentCompany/TheAgentCompany/blob/main/evaluation/browsing.py\n##################################################################################################\n\nimport base64\nimport os\nimport re\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom typing import Union",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py_52_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 52. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py",
      "line_number": 52,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py_54_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 54. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py",
      "line_number": 54,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py_49_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 49 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py",
      "line_number": 49,
      "code_snippet": "    'CodeActAgent': 'When you think you have completed the request, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py_65_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 65 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/tau_bench/run_infer.py",
      "line_number": 65,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_52_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 52. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 52,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.11-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_54_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 54. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 54,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.11-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        sandbox_config=sandbox_config,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_49_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 49 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 49,
      "code_snippet": "    int(SKIP_NUM) if SKIP_NUM and SKIP_NUM.isdigit() and int(SKIP_NUM) >= 0 else None\n)\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.11-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_72_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 72 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 72,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_113_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 113 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 113,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_113_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 113 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 113,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py_72_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 72 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/run_infer.py",
      "line_number": 72,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/create_dataset.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/create_dataset.py",
      "line_number": 1,
      "code_snippet": "# This file was used to create the hugging face dataset from the exercism/python\n# github repo.\n# Refer to: https://github.com/exercism/python/tree/main/exercises/practice\n\nimport os\nfrom pathlib import Path\n\nfrom datasets import Dataset\n\ntests = sorted(os.listdir('practice/'))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_45_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 45. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 45,
      "code_snippet": ") -> OpenHandsConfig:\n    # Create config with agent_bench-specific container image\n    config = get_openhands_config_for_eval(metadata=metadata)\n\n    # Override the container image for agent_bench\n    config.sandbox.base_container_image = 'python:3.12-slim'",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_41_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 41 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 41,
      "code_snippet": "from openhands.runtime.base import Runtime\nfrom openhands.utils.async_utils import call_async_from_sync\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # Create config with agent_bench-specific container image\n    config = get_openhands_config_for_eval(metadata=metadata)\n",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_56_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network/admin operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 56 performs high-risk write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 56,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_100_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network/admin operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 100 performs high-risk write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 100,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_169_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 169 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 169,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_100_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 100 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 100,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py_56_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 56 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/run_infer.py",
      "line_number": 56,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/helper.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/helper.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nfrom functools import partial\n\nfrom evaluation.utils.shared import codeact_user_response\nfrom openhands.events.action import CmdRunAction, MessageAction\n\n\ndef try_parse_answer(act) -> str | None:\n    raw_ans = ''",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py_44_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 44. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py",
      "line_number": 44,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py_46_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 46. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py",
      "line_number": 46,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py_41_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 41 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bfcl/run_infer.py",
      "line_number": 41,
      "code_snippet": "    'CodeActAgent': 'When you think you have completed the request, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py_16",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 16 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py",
      "line_number": 16,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime and export the git patch for SWE-bench-Live.\"\"\"\n    logger.info('-' * 30)\n    logger.info('BEGIN Runtime Completion Fn')\n    logger.info('-' * 30)\n    obs: CmdOutputObservation\n    workspace_dir_name = instance.instance_id\n    action = CmdRunAction(command=f'cd /workspace/{workspace_dir_name}')",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py_16_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 16 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py",
      "line_number": 16,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py_16_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 16 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py",
      "line_number": 16,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/live_utils.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nimport pandas as pd\n\nfrom evaluation.utils.shared import assert_and_raise\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action import CmdRunAction\nfrom openhands.events.observation import (\n    CmdOutputObservation,\n    ErrorObservation,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_394",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'runtime.run_action' is used in 'commands.' on line 394 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 394,
      "code_snippet": "    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to remove git remotes: {str(obs)}')",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_271",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'initialize_runtime' on line 271 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 271,
      "code_snippet": "def initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,\n):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    \"\"\"\n    logger.info('-' * 30)\n    logger.info('BEGIN Runtime Initialization Fn')",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_440",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 440 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 440,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_223_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 223. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 223,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_234_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 234. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 234,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        enable_browser=RUN_WITH_BROWSING,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_206_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 206 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 206,
      "code_snippet": "        )  # to comply with docker image naming convention\n        return (docker_image_prefix.rstrip('/') + '/' + image_name).lower()\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of swe-bench eval\n    use_swebench_official_image = DATASET_TYPE != 'SWE-Gym'",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_271_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 271 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 271,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_440_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 440 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 440,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_609_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 609 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 609,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_271_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 271 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 271,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_609_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 609 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 609,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_440_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 440 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 440,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py_302",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer.py",
      "line_number": 302,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py_77",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
      "line_number": 77,
      "code_snippet": "    def generate_reply(self, question):\n        if self.turns > 3:\n            return 'Please continue working on the task. Do NOT ask for more help.'\n        self.chat_history.append({'role': 'user', 'content': question.content})\n\n        response = litellm_completion(\n            model=self.llm_config.model,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py_118_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 118 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
      "line_number": 118,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py_74_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_reply'",
      "description": "Function 'generate_reply' on line 74 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
      "line_number": 74,
      "code_snippet": "        )  # You can change 'fake_user' to any config name you want\n\n    def generate_reply(self, question):\n        if self.turns > 3:\n            return 'Please continue working on the task. Do NOT ask for more help.'\n        self.chat_history.append({'role': 'user', 'content': question.content})",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py_88",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
      "line_number": 88,
      "code_snippet": "        )\n\n        reply = response.choices[0].message.content\n        self.chat_history.append({'role': 'assistant', 'content': reply})\n        self.turns += 1\n        return reply\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py_103",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_infer_interact.py",
      "line_number": 103,
      "code_snippet": "        return 'Please continue working on the task.'\n    last_agent_message = state.get_last_agent_message()\n    if last_agent_message:\n        return fake_user.generate_reply(last_agent_message)\n    return 'Please continue working on the task.'\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/binary_patch_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/binary_patch_utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utilities for handling binary files and patch generation in SWE-bench evaluation.\"\"\"\n\n\ndef remove_binary_diffs(patch_text):\n    \"\"\"Remove binary file diffs from a git patch.\n\n    Args:\n        patch_text (str): The git patch text\n\n    Returns:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_226",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'initialize_runtime' on line 226 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 226,
      "code_snippet": "def initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    \"\"\"\n    logger.info('-' * 30)\n    logger.info('BEGIN Runtime Initialization Fn')\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_399",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 399 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 399,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_186_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 186. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 186,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_203_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 203. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 203,
      "code_snippet": "    }\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        enable_browser=RUN_WITH_BROWSING,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_169_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 169 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 169,
      "code_snippet": "        )  # to comply with docker image naming convention\n    return (docker_image_prefix.rstrip('/') + '/' + image_name).lower()\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of swe-bench eval\n    use_official_image = bool(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_226_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 226 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 226,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_399_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 399 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 399,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_519_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 519 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 519,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_519_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 519 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 519,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_226_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 226 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 226,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_399_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 399 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 399,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py_255",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/run_localize.py",
      "line_number": 255,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_342",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'apply_patch_output' flows to 'RuntimeError' on line 342 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 342,
      "code_snippet": "            )\n            raise RuntimeError(\n                instance_id,\n                f'Unexpected output when applying patch:\\n{apply_patch_output}',",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_260",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 260 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 260,
      "code_snippet": "                        break\n                    check_action = CmdRunAction(\n                        command=f'ps -p {pid} > /dev/null; echo $?'\n                    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_264",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 264 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 264,
      "code_snippet": "                    check_action.set_hard_timeout(300)\n                    check_obs = runtime.run_action(check_action)\n                    if (\n                        isinstance(check_obs, CmdOutputObservation)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_81_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 81. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 81,
      "code_snippet": "        f'Submit an issue on https://github.com/OpenHands/OpenHands if you run into any issues.'\n    )\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.remote_runtime_resource_factor = get_instance_resource_factor(\n        dataset_name=metadata.dataset,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_87_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 87. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 87,
      "code_snippet": "        instance_id=instance['instance_id'],\n    )\n    config = get_openhands_config_for_eval(\n        runtime=os.environ.get('RUNTIME', 'docker'),\n        sandbox_config=sandbox_config,\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_103_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network/admin operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 103 performs high-risk write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 103,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/eval_infer.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport json\nimport os\nimport subprocess\nimport tempfile\nimport time\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Callable\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/consistants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/consistants.py",
      "line_number": 1,
      "code_snippet": "DOCPATH_PATTERNS = [\n    r'docs/',\n    r'^CHANGES\\.rst$',\n    r'doc/',\n    r'ChangeLog',\n    r'^changelog/',\n    r'^CHANGES$',\n]\n\nMATPLOTLIB_CONFIG = {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_372",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 372 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 372,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_167_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 167. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 167,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_178_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 178. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 178,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime=os.environ.get('RUNTIME', 'docker'),\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_150_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 150 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 150,
      "code_snippet": "    else:\n        raise\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of NoCode-bench eval\n    use_swebench_official_image = True",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_221_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 221 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 221,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_372_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 372 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 372,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_541_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 541 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 541,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_221_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 221 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 221,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_541_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 541 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 541,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_372_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 372 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 372,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py_252",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/run_infer_nc.py",
      "line_number": 252,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/binary_patch_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/binary_patch_utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nUtilities for handling binary files and patch generation in SWE-bench evaluation.\n\"\"\"\n\n\ndef remove_binary_diffs(patch_text):\n    \"\"\"\n    Remove binary file diffs from a git patch.\n\n    Args:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/compute_skip_ids.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/compute_skip_ids.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport fnmatch\nimport json\nfrom collections import Counter\nfrom pathlib import Path\n\n\ndef find_final_reports(base_dir, pattern=None):\n    base_path = Path(base_dir)\n    if not base_path.exists():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_500",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 500 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 500,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_322_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 322. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 322,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_333_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 333. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 333,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        enable_browser=RUN_WITH_BROWSING,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_311_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 311 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 311,
      "code_snippet": "        container_tag = f'pr-{tag_suffix}'\n        return f'{DOCKER_IMAGE_PREFIX}/{container_name}:{container_tag}'\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    base_container_image = get_instance_docker_image(instance)\n    logger.info(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_355_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 355 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 355,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_500_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 500 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 500,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_623_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 623 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 623,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_355_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 355 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 355,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_623_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 623 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 623,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_500_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 500 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 500,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py_385",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/run_infer.py",
      "line_number": 385,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_312",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'apply_patch_output' flows to 'RuntimeError' on line 312 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 312,
      "code_snippet": "            )\n            raise RuntimeError(\n                instance_id,\n                f'Unexpected output when applying patch:\\n{apply_patch_output}',",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_236",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 236 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 236,
      "code_snippet": "                        break\n                    check_action = CmdRunAction(\n                        command=f'ps -p {pid} > /dev/null; echo $?'\n                    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_240",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 240 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 240,
      "code_snippet": "                    check_action.set_hard_timeout(300)\n                    check_obs = runtime.run_action(check_action)\n                    if (\n                        isinstance(check_obs, CmdOutputObservation)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_85_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 85. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 85,
      "code_snippet": "        f'Submit an issue on https://github.com/OpenHands/OpenHands if you run into any issues.'\n    )\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.remote_runtime_resource_factor = get_instance_resource_factor(\n        dataset_name=metadata.dataset,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_91_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 91. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 91,
      "code_snippet": "        instance_id=instance['instance_id'],\n    )\n    config = get_openhands_config_for_eval(\n        runtime=os.environ.get('RUNTIME', 'docker'),\n        sandbox_config=sandbox_config,\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_98_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network/admin operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 98 performs high-risk write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 98,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/eval_infer.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport subprocess\nimport tempfile\nimport time\nfrom functools import partial\n\nimport pandas as pd\nfrom swebench.harness.grading import get_eval_report\nfrom swebench.harness.run_evaluation import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_137",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'initialize_runtime' on line 137 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 137,
      "code_snippet": "def initialize_runtime(runtime: Runtime, data_files: list[str]):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    \"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Runtime Initialization Fn {\"-\" * 50}')\n    obs: CmdOutputObservation\n\n    action = CmdRunAction(command='mkdir -p /workspace')\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_67_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 67. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 67,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_69_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 69. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 69,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_64_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 64 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 64,
      "code_snippet": "    'CodeActAgent': 'When you think you have fixed the issue through code changes, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_137_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 137 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 137,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime, data_files: list[str]):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_223_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 223 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 223,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_223_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 223 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 223,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py_137_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 137 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/run_infer.py",
      "line_number": 137,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime, data_files: list[str]):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_150",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'runtime.run_action' is used in 'UPDATE' on line 150 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 150,
      "code_snippet": "    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    assert obs.exit_code == 0\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_156",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'runtime.run_action' is used in 'UPDATE' on line 156 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 156,
      "code_snippet": "    )\n    runtime.run_action(action)\n    logger.info(f'{\"-\" * 50} END Runtime Initialization Fn {\"-\" * 50}')\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_145",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'runtime.run_action' is used in 'UPDATE' on line 145 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 145,
      "code_snippet": "            logger.info(action, extra={'msg_type': 'ACTION'})\n            obs = runtime.run_action(action)\n            assert obs.exit_code == 0\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_68_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 68. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 68,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'nikolaik/python-nodejs:python3.12-nodejs22'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_70_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 70. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 70,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'nikolaik/python-nodejs:python3.12-nodejs22'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        sandbox_config=sandbox_config,\n        runtime='docker',",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_64_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 64 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 64,
      "code_snippet": "    'CodeActAgent': 'When you think you have solved the question, please use the finish tool and include your final answer in the message parameter of the finish tool. Your final answer MUST be encapsulated within <solution> and </solution>.\\n'\n}\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'nikolaik/python-nodejs:python3.12-nodejs22'",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_100_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 100 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 100,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_160_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 160 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 160,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_100_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 100 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 100,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py_160_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 160 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/run_infer.py",
      "line_number": 160,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/utils.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport io\n\nimport numpy as np\nfrom PIL import Image\n\n\ndef image_to_png_base64_url(\n    image: np.ndarray | Image.Image, add_data_prefix: bool = True\n):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/get_score.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/get_score.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Get agent's gaia score\")\n    parser.add_argument('--file', type=str, help=\"Path to the agent's output.jsonl\")\n    args = parser.parse_args()\n    this_log = args.file\n    outs = []",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/scorer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gaia/scorer.py",
      "line_number": 1,
      "code_snippet": "import re\nimport string\nimport warnings\n\n\ndef normalize_number_str(number_str: str) -> float:\n    # we replace these common units and commas to allow\n    # conversion to float\n    for char in ['$', '%', ',']:\n        number_str = number_str.replace(char, '')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_123",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 123 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 123,
      "code_snippet": "                logger.info('Start Downloading...')\n                subprocess.run(['wget', download_url, '-O', download_path])\n                logger.info('Download completed.')\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_76_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 76. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 76,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n\n    config = get_openhands_config_for_eval(",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_79_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 79. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 79,
      "code_snippet": "    sandbox_config.base_container_image = 'python:3.12-bookworm'\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_73_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 73 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 73,
      "code_snippet": "    'CodeActAgent': 'When you think you have fixed the issue through code changes, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_90_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'execute_sql' executes dangerous operations",
      "description": "Tool function 'execute_sql' on line 90 takes LLM output as a parameter and performs dangerous operations (sql_exec) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 90,
      "code_snippet": "    agent_config.enable_prompt_extensions = False\n    return config\n\n\ndef execute_sql(db_path, gen_sql, gold_sql):\n    \"\"\"Execute the generated SQL and the ground truth SQL and compare the results.\"\"\"\n    with sqlite3.connect(db_path) as conn:\n        cursor = conn.cursor()\n        cursor.execute(gen_sql)\n        predicted_res = cursor.fetchall()",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_107_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'load_bird'",
      "description": "Function 'load_bird' on line 107 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 107,
      "code_snippet": "\nLOCAL_DATASET_PATH = os.path.join(os.path.dirname(__file__), 'data')\n\n\ndef load_bird():\n    \"\"\"Main function to handle the flow of downloading, processing, and loading the bird dataset.\"\"\"\n\n    def _download_bird():\n        \"\"\"Downloads and extracts the bird dataset from a specified URL into a local directory.\"\"\"\n        devset_path = os.path.join(LOCAL_DATASET_PATH, 'dev')",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_110_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '_download_bird'",
      "description": "Function '_download_bird' on line 110 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 110,
      "code_snippet": "\ndef load_bird():\n    \"\"\"Main function to handle the flow of downloading, processing, and loading the bird dataset.\"\"\"\n\n    def _download_bird():\n        \"\"\"Downloads and extracts the bird dataset from a specified URL into a local directory.\"\"\"\n        devset_path = os.path.join(LOCAL_DATASET_PATH, 'dev')\n        if not os.path.exists(devset_path):\n            logger.info(\n                f'{LOCAL_DATASET_PATH} folder does not exist, starting download and extraction...'",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_245_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 245 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 245,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_276_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 276 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 276,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_347_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 347 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 347,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_107_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'load_bird'",
      "description": "Function 'load_bird' on line 107 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 107,
      "code_snippet": "\n\ndef load_bird():\n    \"\"\"Main function to handle the flow of downloading, processing, and loading the bird dataset.\"\"\"\n\n    def _download_bird():",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_347_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 347 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 347,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_110_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '_download_bird'",
      "description": "Function '_download_bird' on line 110 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 110,
      "code_snippet": "    \"\"\"Main function to handle the flow of downloading, processing, and loading the bird dataset.\"\"\"\n\n    def _download_bird():\n        \"\"\"Downloads and extracts the bird dataset from a specified URL into a local directory.\"\"\"\n        devset_path = os.path.join(LOCAL_DATASET_PATH, 'dev')\n        if not os.path.exists(devset_path):",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_276_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 276 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 276,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_245_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 245 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 245,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py_162",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 162 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/bird/run_infer.py",
      "line_number": 162,
      "code_snippet": "\n            table_info_query = f'PRAGMA table_info(`{table_name}`);'\n            top_k_row_query = f'SELECT * FROM {table_name} LIMIT {limit_value};'\n            try:\n                headers = [",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_analysis.py_69_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'classify_error'",
      "description": "Function 'classify_error' on line 69 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_analysis.py",
      "line_number": 69,
      "code_snippet": "\n\ndef classify_error(llm: LLM, failed_case: dict) -> str:\n    prompt = f\"\"\"\n    Please classify the error for the following failed case based on the history and eval_output:\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_82_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 82. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 82,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'public.ecr.aws/i5g0m1f6/ml-bench'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_84_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 84. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 84,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'public.ecr.aws/i5g0m1f6/ml-bench'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_79_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 79 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 79,
      "code_snippet": "    15: 'GSA2_DS',\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'public.ecr.aws/i5g0m1f6/ml-bench'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_203_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'process_instance'",
      "description": "Function 'process_instance' on line 203 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 203,
      "code_snippet": "    logger.info(f'{\"-\" * 50} END Runtime Completion Fn {\"-\" * 50}')\n    return outputs\n\n\ndef process_instance(instance: Any, metadata: EvalMetadata, reset_logger: bool = True):\n    config = get_config(metadata)\n\n    # Setup the logger properly, so you can run multi-processing to parallelize the evaluation\n    if reset_logger:\n        log_dir = os.path.join(metadata.eval_output_dir, 'infer_logs')",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_95_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network/admin operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 95 performs high-risk execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 95,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_140_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 140 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 140,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_140_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 140 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 140,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_203_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 203 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 203,
      "code_snippet": "\n\ndef process_instance(instance: Any, metadata: EvalMetadata, reset_logger: bool = True):\n    config = get_config(metadata)\n\n    # Setup the logger properly, so you can run multi-processing to parallelize the evaluation",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py_95_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 95 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/run_infer.py",
      "line_number": 95,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/get_avg_reward.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/get_avg_reward.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\nimport browsergym.miniwob  # noqa F401 register miniwob tasks as gym environments\nimport gymnasium as gym\n\nparser = argparse.ArgumentParser(description='Calculate average reward.')\nparser.add_argument('output_path', type=str, help='path to output.jsonl')\n\nargs = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_60_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 60. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 60,
      "code_snippet": "    env_id: str,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'xingyaoww/od-eval-miniwob:v1.0'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_62_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 62. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 62,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'xingyaoww/od-eval-miniwob:v1.0'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime=os.environ.get('RUNTIME', 'docker'),\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_75_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 75 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 75,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n) -> tuple[str, BrowserOutputObservation]:\n    \"\"\"Initialize the runtime for the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_107_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 107 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 107,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_130_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 130 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 130,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_107_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 107 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 107,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_130_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 130 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 130,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py_75_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 75 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/miniwob/run_infer.py",
      "line_number": 75,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n) -> tuple[str, BrowserOutputObservation]:\n    \"\"\"Initialize the runtime for the agent.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_288",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to code_execution sink",
      "description": "LLM output variable 'full_output' flows to '_parse_evaluation_output' on line 288 via direct flow. This creates a code_execution vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 288,
      "code_snippet": "        if obs.exit_code == 0:\n            parsed_data = _parse_evaluation_output(full_output)\n            test_result = {\n                'instance_id': f'{task_name}_test',",
      "recommendation": "Mitigations for Code Execution:\n1. Never pass LLM output to eval() or exec()\n2. Use safe alternatives (ast.literal_eval for data)\n3. Implement sandboxing if code execution is required"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_76_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 76. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 76,
      "code_snippet": "    \"\"\"Configure OpenHands for algorithm optimization evaluation.\"\"\"\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.timeout = 600  # Set execution timeout to 10 minutes\n    sandbox_config.remote_runtime_api_timeout = 600\n    sandbox_config.base_container_image = 'linhaowei1/algotune-openhands:v0.0.2'",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_71_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 71 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 71,
      "code_snippet": "\n    return base_msg\n\n\ndef get_config(\n    metadata: EvalMetadata, workspace_id: str = None, enable_volumes: bool = True\n) -> OpenHandsConfig:\n    \"\"\"Configure OpenHands for algorithm optimization evaluation.\"\"\"\n\n    sandbox_config = get_default_sandbox_config_for_eval()",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_71_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_config'",
      "description": "Function 'get_config' on line 71 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 71,
      "code_snippet": "\n    return base_msg\n\n\ndef get_config(\n    metadata: EvalMetadata, workspace_id: str = None, enable_volumes: bool = True\n) -> OpenHandsConfig:\n    \"\"\"Configure OpenHands for algorithm optimization evaluation.\"\"\"\n\n    sandbox_config = get_default_sandbox_config_for_eval()",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_152_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 152 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 152,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime, task_name: str):\n    \"\"\"Initialize the runtime for algorithm optimization training.\"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Runtime Initialization {\"-\" * 50}')\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_177_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_backup_solver_code'",
      "description": "Function '_backup_solver_code' on line 177 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 177,
      "code_snippet": "\n\ndef _backup_solver_code(runtime: Runtime) -> str:\n    \"\"\"Reads the content of the solver.py file from the runtime.\"\"\"\n    try:\n        # Use run_action to cat the file content",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_266_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'evaluate_test_cases'",
      "description": "Function 'evaluate_test_cases' on line 266 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 266,
      "code_snippet": "\n\ndef evaluate_test_cases(runtime: Any, task_name: str) -> dict[str, Any]:\n    \"\"\"Evaluate the final solution on test instances using the evaluator.py script.\"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Test Instance Evaluation {\"-\" * 50}')\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_379_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_training_and_testing'",
      "description": "Function 'process_training_and_testing' on line 379 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 379,
      "code_snippet": "\n\ndef process_training_and_testing(\n    metadata: EvalMetadata,\n    task_name: str,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_71_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_config'",
      "description": "Function 'get_config' on line 71 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 71,
      "code_snippet": "\n\ndef get_config(\n    metadata: EvalMetadata, workspace_id: str = None, enable_volumes: bool = True\n) -> OpenHandsConfig:\n    \"\"\"Configure OpenHands for algorithm optimization evaluation.\"\"\"",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_266_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'evaluate_test_cases'",
      "description": "Function 'evaluate_test_cases' on line 266 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 266,
      "code_snippet": "\n\ndef evaluate_test_cases(runtime: Any, task_name: str) -> dict[str, Any]:\n    \"\"\"Evaluate the final solution on test instances using the evaluator.py script.\"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Test Instance Evaluation {\"-\" * 50}')\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py_152_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 152 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/run_infer.py",
      "line_number": 152,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime, task_name: str):\n    \"\"\"Initialize the runtime for algorithm optimization training.\"\"\"\n    logger.info(f'{\"-\" * 50} BEGIN Runtime Initialization {\"-\" * 50}')\n",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_55_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 55. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 55,
      "code_snippet": "    assert openai_api_key is not None, 'OPENAI_API_KEY must be set'\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    sandbox_config.browsergym_eval_env = env_id\n    sandbox_config.runtime_startup_env_vars = {",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_69_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 69. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 69,
      "code_snippet": "        'HOMEPAGE': f'{base_url}:4399',\n    }\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_46_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 46 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 46,
      "code_snippet": "\nSUPPORTED_AGENT_CLS = {'BrowsingAgent'}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n    env_id: str,\n) -> OpenHandsConfig:\n    base_url = os.environ.get('WEBARENA_BASE_URL', None)\n    openai_api_key = os.environ.get('OPENAI_API_KEY', None)",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_80_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 80 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 80,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n) -> dict:\n    \"\"\"Initialize the runtime for the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_106_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 106 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 106,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_129_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 129 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 129,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_106_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 106 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 106,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_129_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 129 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 129,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py_80_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 80 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/run_infer.py",
      "line_number": 80,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n) -> dict:\n    \"\"\"Initialize the runtime for the agent.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/get_success_rate.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/webarena/get_success_rate.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\nimport browsergym.webarena  # noqa F401 register webarena tasks as gym environments\nimport gymnasium as gym\n\nparser = argparse.ArgumentParser(description='Calculate average reward.')\nparser.add_argument('output_path', type=str, help='path to output.jsonl')\n\nargs = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py_42_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 42. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py",
      "line_number": 42,
      "code_snippet": "        'max_iterations must be 1 for browsing delegation evaluation.'\n    )\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata, runtime='docker', sandbox_config=sandbox_config",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py_44_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 44. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py",
      "line_number": 44,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata, runtime='docker', sandbox_config=sandbox_config\n    )\n    config.set_llm_config(metadata.llm_config)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py_36_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 36 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py",
      "line_number": 36,
      "code_snippet": "# Only CodeActAgent can delegate to BrowsingAgent\nSUPPORTED_AGENT_CLS = {'CodeActAgent'}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    assert metadata.max_iterations == 1, (\n        'max_iterations must be 1 for browsing delegation evaluation.'\n    )",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py_53_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 53 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py",
      "line_number": 53,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py_53_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 53 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/browsing_delegation/run_infer.py",
      "line_number": 53,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_146",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'sandbox_config.runtime_startup_env_vars.update' is used in 'UPDATE' on line 146 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 146,
      "code_snippet": "    sandbox_config.remote_runtime_resource_factor = 4.0\n    sandbox_config.runtime_startup_env_vars.update(\n        {\n            'NO_CHANGE_TIMEOUT_SECONDS': '900',  # 15 minutes",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_326",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'complete_runtime' on line 326 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 326,
      "code_snippet": "def complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    logger.info('-' * 30)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_134_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 134. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 134,
      "code_snippet": "    )\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = base_container_image\n    sandbox_config.enable_auto_lint = True\n    sandbox_config.use_host_network = False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_119_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 119 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 119,
      "code_snippet": ") -> str:\n    return f'ghcr.io/swefficiency/swefficiency-images:{instance_id}'\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    cpu_group: list[int] | None = None,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of swe-bench eval",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_119_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'get_config'",
      "description": "Function 'get_config' on line 119 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 119,
      "code_snippet": ") -> str:\n    return f'ghcr.io/swefficiency/swefficiency-images:{instance_id}'\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    cpu_group: list[int] | None = None,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of swe-bench eval",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_190_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 190 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 190,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_326_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 326 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 326,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_532_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 532 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 532,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_119_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_config'",
      "description": "Function 'get_config' on line 119 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 119,
      "code_snippet": "\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    cpu_group: list[int] | None = None,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_190_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 190 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 190,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n    metadata: EvalMetadata,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_532_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 532 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 532,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_326_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 326 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 326,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py_221",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/run_infer.py",
      "line_number": 221,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/binary_patch_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swefficiency/binary_patch_utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nUtilities for handling binary files and patch generation in SWE-bench evaluation.\n\"\"\"\n\n\ndef remove_binary_diffs(patch_text):\n    \"\"\"\n    Remove binary file diffs from a git patch.\n\n    Args:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/metrics.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/metrics.py",
      "line_number": 1,
      "code_snippet": "import sys\nfrom typing import Callable, Optional, Sequence, TypeVar, Union\n\nimport nltk\nimport numpy as np\nfrom fuzzywuzzy import fuzz\nfrom rouge import Rouge\n\n# increase recursion depth to ensure ROUGE can be calculated for long sentences\nif sys.getrecursionlimit() < 10_000:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/report_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/report_utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport re\n\n\ndef check_coverage(coverage_output, code_file):\n    json_cov = json.loads(coverage_output)\n    if code_file in json_cov['files'].keys():\n        file_data = json_cov['files'][code_file]\n        return True, file_data['summary']['percent_covered']\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/compute_readability.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/compute_readability.py",
      "line_number": 1,
      "code_snippet": "import math\n\n\ndef total_byte_entropy_stats(python_code):\n    # Count the occurrence of each byte (character for simplicity)\n    byte_counts = {}\n    for byte in python_code.encode('utf-8'):\n        byte_counts[byte] = byte_counts.get(byte, 0) + 1\n\n    total_bytes = sum(byte_counts.values())",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/constants.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom pathlib import Path\nfrom typing import Any, TypedDict\n\n# Constants - Evaluation Log Directories\nBASE_IMAGE_BUILD_DIR = Path('logs/build_images/base')\nENV_IMAGE_BUILD_DIR = Path('logs/build_images/env')\nINSTANCE_IMAGE_BUILD_DIR = Path('logs/build_images/instances')\nRUN_EVALUATION_LOG_DIR = Path('logs/run_evaluation')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_146_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 146. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 146,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        sandbox_config=sandbox_config,\n        runtime=os.environ.get('RUNTIME', 'docker'),",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_118_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 118 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 118,
      "code_snippet": "    )  # to comply with docker image naming convention\n    return DOCKER_IMAGE_PREFIX.rstrip('/') + '/' + image_name\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    # We use a different instance image for the each instance of TestGenEval\n    base_container_image = get_instance_docker_image(instance['instance_id_swebench'])",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_167_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 167 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 167,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_323_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 323 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 323,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_382_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 382 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 382,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_167_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 167 makes critical data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 167,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_323_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 323 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 323,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_382_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 382 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 382,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py_198",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/run_infer.py",
      "line_number": 198,
      "code_snippet": "    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/pygments_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/pygments_utils.py",
      "line_number": 1,
      "code_snippet": "import re\n\nfrom pygments.lexers.python import PythonLexer\n\n\ndef tokenize_code(code):\n    lexer = PythonLexer()\n    tokens = process_pygments_tokens(lexer.get_tokens(code))\n    return tokens\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/utils.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom pathlib import Path\nfrom typing import cast\n\nfrom datasets import Dataset, load_dataset\n\nfrom evaluation.benchmarks.testgeneval.constants import (\n    KEY_INSTANCE_ID,\n    TestGenEvalInstance,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_135",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 135 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 135,
      "code_snippet": "\n        check_action = CmdRunAction(command=f'ps -p {pid} > /dev/null; echo $?')\n        check_obs = runtime.run_action(check_action)\n        if (",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_136",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 136 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 136,
      "code_snippet": "        check_action = CmdRunAction(command=f'ps -p {pid} > /dev/null; echo $?')\n        check_obs = runtime.run_action(check_action)\n        if (\n            isinstance(check_obs, CmdOutputObservation)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_173",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'pid' flows to 'CmdRunAction' on line 173 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 173,
      "code_snippet": "\n        check_action = CmdRunAction(command=f'ps -p {pid} > /dev/null; echo $?')\n        check_obs = runtime.run_action(check_action)\n        if (",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_174",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'check_action' flows to 'runtime.run_action' on line 174 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 174,
      "code_snippet": "        check_action = CmdRunAction(command=f'ps -p {pid} > /dev/null; echo $?')\n        check_obs = runtime.run_action(check_action)\n        if (\n            isinstance(check_obs, CmdOutputObservation)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_74_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 74. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 74,
      "code_snippet": "    )\n\n    return get_openhands_config_for_eval(\n        sandbox_config=sandbox_config,\n        runtime=os.environ.get('RUNTIME', 'docker'),  # Different default runtime\n    )",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_107_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'run_command'",
      "description": "Function 'run_command' on line 107 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 107,
      "code_snippet": "\n\ndef run_command(runtime, command, timeout=600):\n    action = CmdRunAction(command=command)\n    action.set_hard_timeout(timeout)\n    logger.info(action, extra={'msg_type': 'ACTION'})",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_107_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_command'",
      "description": "Function 'run_command' on line 107 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 107,
      "code_snippet": "\n\ndef run_command(runtime, command, timeout=600):\n    action = CmdRunAction(command=command)\n    action.set_hard_timeout(timeout)\n    logger.info(action, extra={'msg_type': 'ACTION'})",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/eval_infer.py",
      "line_number": 1,
      "code_snippet": "import os\nimport tempfile\nimport time\nfrom functools import partial\n\nimport pandas as pd\nfrom report_utils import (\n    check_coverage,\n    check_mutation,\n    count_methods,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py_66_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 66. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py",
      "line_number": 66,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py_68_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 68. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py",
      "line_number": 68,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py_63_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 63 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py",
      "line_number": 63,
      "code_snippet": "||FINAL_ANSWER>>\n\"\"\".strip()\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py_170_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 170 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/gpqa/run_infer.py",
      "line_number": 170,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/logic_inference.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/logic_inference.py",
      "line_number": 1,
      "code_snippet": "import os\nimport random\nimport re\nimport shutil\n\nfrom pyke import knowledge_engine\n\n\nclass PykeProgram:\n    def __init__(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py_56_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 56. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
      "line_number": 56,
      "code_snippet": "    )\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py_47_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 47 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
      "line_number": 47,
      "code_snippet": "    'CodeActAgent': 'When you think you have solved the question, please first send your answer to user through message and then exit.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'xingyaoww/od-eval-logic-reasoning:v1.0'\n    sandbox_config.runtime_extra_deps = (",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py_133_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 133 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
      "line_number": 133,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py_182_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 182 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
      "line_number": 182,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py_182_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 182 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
      "line_number": 182,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py_133_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 133 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/logic_reasoning/run_infer.py",
      "line_number": 133,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_46_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 46. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 46,
      "code_snippet": "    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_48_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 48. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 48,
      "code_snippet": "    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_43_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 43 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 43,
      "code_snippet": "    'CodeActAgent': 'When you think you have completed the request, please finish the interaction using the \"finish\" tool.\\n'\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    config = get_openhands_config_for_eval(",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_59_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 59 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 59,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_83_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 83 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 83,
      "code_snippet": "\n\ndef process_instance(instance: Any, metadata: EvalMetadata, reset_logger: bool = True):\n    config = get_config(metadata)\n\n    qid = instance.qid",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_83_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 83 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 83,
      "code_snippet": "\n\ndef process_instance(instance: Any, metadata: EvalMetadata, reset_logger: bool = True):\n    config = get_config(metadata)\n\n    qid = instance.qid",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py_59_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 59 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/run_infer.py",
      "line_number": 59,
      "code_snippet": "\n\ndef initialize_runtime(runtime: Runtime):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py_16_assignment",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Hardcoded Generic API Key detected in assignment",
      "description": "Hardcoded Generic API Key found in assignment on line 16. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version control history, or compiled binaries.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py",
      "line_number": 16,
      "code_snippet": "    if os.path.exists(data_path):\n        return data_path\n    url = 'https://drive.google.com/uc?id=1zRbHzPW2x4dDcfmphBWlan8cxUCRNmqk'\n    zip_path = os.path.join(dir, 'data.zip')\n    gdown.download(url, zip_path, quiet=False)",
      "recommendation": "Remove hardcoded secrets immediately:\n1. Use environment variables: os.getenv('API_KEY')\n2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault\n3. Use configuration files (never commit to git): config.ini, .env\n4. Rotate the exposed secret immediately\n5. Scan git history for leaked secrets: git-secrets, truffleHog\n6. Add secret scanning to CI/CD pipeline"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py_27_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'download_tools' executes dangerous operations",
      "description": "Tool function 'download_tools' on line 27 takes LLM output as a parameter and performs dangerous operations (http_request, file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py",
      "line_number": 27,
      "code_snippet": "    print(f'Data saved to {data_path}')\n    return data_path\n\n\ndef download_tools(dir, wolfram_alpha_appid='YOUR_WOLFRAMALPHA_APPID'):\n    tool_path = os.path.join(dir, 'tools')\n    if os.path.exists(tool_path):\n        return tool_path\n    os.mkdir(tool_path)\n    tools = [",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/toolqa/utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport re\nimport string\nimport zipfile\n\nimport httpx\n\n\ndef download_data(dir):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_62_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 62. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 62,
      "code_snippet": ") -> OpenHandsConfig:\n    BIOCODER_BENCH_CONTAINER_IMAGE = 'public.ecr.aws/i5g0m1f6/eval_biocoder:v1.0'\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = BIOCODER_BENCH_CONTAINER_IMAGE\n\n    config = get_openhands_config_for_eval(",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_65_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 65. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 65,
      "code_snippet": "    sandbox_config.base_container_image = BIOCODER_BENCH_CONTAINER_IMAGE\n\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_58_dynamic",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe dynamic plugin loading in 'get_config'",
      "description": "Function 'get_config' on line 58 uses dynamic loading (__import__, eval, exec) to load plugins without validation. This creates a critical security risk where attackers can inject malicious code through plugin loading mechanisms, leading to arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 58,
      "code_snippet": "    'typescript': 'ts',\n}\n\n\ndef get_config(\n    metadata: EvalMetadata,\n) -> OpenHandsConfig:\n    BIOCODER_BENCH_CONTAINER_IMAGE = 'public.ecr.aws/i5g0m1f6/eval_biocoder:v1.0'\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = BIOCODER_BENCH_CONTAINER_IMAGE",
      "recommendation": "Safe Plugin Loading:\n1. NEVER use eval(), exec(), or __import__() for plugins\n2. Use importlib with strict module path validation\n3. Implement plugin allowlists (approved plugins only)\n4. Verify plugin checksums/signatures before loading\n5. Load plugins from trusted directories only\n6. Use separate Python processes for plugin isolation\n7. Implement plugin capability restrictions\n8. Validate plugin code with static analysis before loading\n9. Use virtual environments for plugin execution\n10. Monitor plugin behavior for anomalies"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_76_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network/admin operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 76 performs high-risk delete/write/execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 76,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: BiocoderData,  # this argument is not required\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_163_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 163 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 163,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_76_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 76 makes critical financial, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 76,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: BiocoderData,  # this argument is not required\n):",
      "recommendation": "Critical financial, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py_163_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 163 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/run_infer.py",
      "line_number": 163,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_61_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 61. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 61,
      "code_snippet": "    assert openai_base_url is not None, 'OPENAI_BASE_URL must be set'\n\n    sandbox_config = get_default_sandbox_config_for_eval()\n    sandbox_config.base_container_image = 'python:3.12-bookworm'\n    sandbox_config.browsergym_eval_env = env_id\n    sandbox_config.runtime_startup_env_vars = {",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_77_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 77. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 77,
      "code_snippet": "        'VWA_HOMEPAGE': f'{base_url}:4399',\n    }\n    config = get_openhands_config_for_eval(\n        metadata=metadata,\n        runtime='docker',\n        sandbox_config=sandbox_config,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_69",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 69. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 69,
      "code_snippet": "        'OPENAI_BASE_URL': openai_base_url,\n        'VWA_CLASSIFIEDS': f'{base_url}:9980',\n        'VWA_CLASSIFIEDS_RESET_TOKEN': '4b61655535e7ed388f0d40a93600254c',\n        'VWA_SHOPPING': f'{base_url}:7770',\n        'VWA_SHOPPING_ADMIN': f'{base_url}:7780/admin',",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_92_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 92 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 92,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n) -> tuple[str, list]:\n    \"\"\"Initialize the runtime for the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_119_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 119 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 119,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_142_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'process_instance'",
      "description": "Function 'process_instance' on line 142 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 142,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_119_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete_runtime'",
      "description": "Function 'complete_runtime' on line 119 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 119,
      "code_snippet": "\n\ndef complete_runtime(\n    runtime: Runtime,\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_142_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'process_instance'",
      "description": "Function 'process_instance' on line 142 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 142,
      "code_snippet": "\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py_92_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 92 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/run_infer.py",
      "line_number": 92,
      "code_snippet": "\n\ndef initialize_runtime(\n    runtime: Runtime,\n) -> tuple[str, list]:\n    \"\"\"Initialize the runtime for the agent.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/get_success_rate.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/visualwebarena/get_success_rate.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\nimport browsergym.visualwebarena  # noqa F401 register visualwebarena tasks as gym environments\nimport gymnasium as gym\n\nparser = argparse.ArgumentParser(description='Calculate average reward.')\nparser.add_argument('output_path', type=str, help='path to output.jsonl')\n\nargs = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/scripts/setup/remove_code.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/scripts/setup/remove_code.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\nimport re\nfrom collections import defaultdict\n\n\ndef get_likely_indent_size(array_of_tabs) -> int:\n    sizes = defaultdict(int)\n\n    for i in range(len(array_of_tabs) - 1):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/scripts/setup/copy_changed_code.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/biocoder/scripts/setup/copy_changed_code.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\n\ndef get_changed_code(target_filepath, line_start, include_signature=False):\n    # copies changed code into /testing_files/\n    # Note that this does NOT copy the function signature\n    selected_lines = []\n    offset = 1 if include_signature else 0\n\n    with open('/testing_files/first_line_after_removed.txt', 'r') as f:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py_32",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
      "line_number": 32,
      "code_snippet": "\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],\n        frequency_penalty=0.1,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py_32_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model_name' is used without version pinning on line 32. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
      "line_number": 32,
      "code_snippet": "\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],\n        frequency_penalty=0.1,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py_21_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'tokens' containing sensitive data is being logged on line 21. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
      "line_number": 21,
      "code_snippet": "\n    tokens = encoding.encode(text, disallowed_special=disallowed_special)\n    print(len(tokens))\n\n    if len(tokens) > max_tokens:",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py_31_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'predict'",
      "description": "Function 'predict' on line 31 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
      "line_number": 31,
      "code_snippet": "\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py_31_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'predict'",
      "description": "Function 'predict' on line 31 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
      "line_number": 31,
      "code_snippet": "\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py_31",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_code_migration.py",
      "line_number": 31,
      "code_snippet": "    truncated_text = encoding.decode(tokens)\n\n    return truncated_text\n\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],\n        frequency_penalty=0.1,\n        max_tokens=128,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py_32",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py",
      "line_number": 32,
      "code_snippet": "\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],\n        frequency_penalty=0.1,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py_31_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Critical decision without oversight in 'predict'",
      "description": "Function 'predict' on line 31 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py",
      "line_number": 31,
      "code_snippet": "\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py_31",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/inference_utils/api_test_block_completion.py",
      "line_number": 31,
      "code_snippet": "    truncated_text = encoding.decode(tokens)\n\n    return truncated_text\n\n\ndef predict(content, model_name):\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=[{'role': 'user', 'content': content}],\n        frequency_penalty=0.1,\n        max_tokens=128,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/clear_ans.py_18_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 18. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/clear_ans.py",
      "line_number": 18,
      "code_snippet": "for data in data_list:\n    temp_list = []\n    model_output_list = eval(data['model_output'])\n    for output in model_output_list:\n        if '<start>' in output and '<end>' in output:\n            start_index = output.find('<start>') + len('<start>')",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/clear_ans.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/clear_ans.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Clear the<start>and<end>generated by the model in inference\"\"\"\n\nimport json\n\nmodel_name = ''\ntask = 'block_completion'\n\nresult_path = f'../data/result_data/{task}/{model_name}/VersiCode_block_completion.json'  # Modify the file according to the task format\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_block_versicode.py_87_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 87. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_block_versicode.py",
      "line_number": 87,
      "code_snippet": "            core_line_in_output_list = []\n\n            for entry in eval(model_output_clear):\n                _, core_line_in_output = process_line_mask(entry, core_token)\n                if core_line_in_output:\n                    core_line_in_output_list.append(core_line_in_output)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_block_versicode.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_block_versicode.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Find the line of code generated by the model using the block in the version code\"\"\"\n\nimport json\nimport os\nimport random\nimport re\n\n\ndef process_line_mask(code_snippet, core_token):\n    if not core_token:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_migration_versicode.py_90_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 90. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_migration_versicode.py",
      "line_number": 90,
      "code_snippet": "\n            core_token = item['new_name']\n            for entry in eval(model_output_clear):\n                _, core_line_in_output = process_line_mask(entry, core_token)\n                if core_line_in_output:\n                    core_line_in_output_list.append(core_line_in_output)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_migration_versicode.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/output_processing/choose_core_line_from_migration_versicode.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Find the line of code generated by the model using the block in the version code\"\"\"\n\nimport json\nimport os\nimport random\nimport re\n\n\ndef process_line_mask(code_snippet, core_token):\n    if not core_token:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py_174_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 174. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py",
      "line_number": 174,
      "code_snippet": "        for data in data_list:\n            answer = data['core_token']\n            model_output = eval(data['model_output_clear'])\n            model_filled_code = [\n                data['masked_code'].replace('<mask>', i) for i in model_output\n            ]",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py_188_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 188. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py",
      "line_number": 188,
      "code_snippet": "        for data in data_list:\n            answer = data['core_token']\n            model_output = eval(data['model_output_clear'])\n            model_filled_code = eval(data['model_output_clear'])\n            core_line = data['core_line']\n            core_line_in_output_clear = data['core_line_in_output_clear']",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py_189_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 189. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py",
      "line_number": 189,
      "code_snippet": "            answer = data['core_token']\n            model_output = eval(data['model_output_clear'])\n            model_filled_code = eval(data['model_output_clear'])\n            core_line = data['core_line']\n            core_line_in_output_clear = data['core_line_in_output_clear']\n            score_list.append(",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py_13_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "compile() for execution on line 13. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py",
      "line_number": 13,
      "code_snippet": "def is_code_valid(code):\n    try:\n        compile(code, '<string>', 'exec')\n        return True\n    except Exception:\n        return False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_em_score.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Calculate the cdc score for line and block\"\"\"\n\nimport json\nimport math\nimport os\nimport re\n\n# warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py_283_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 283. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py",
      "line_number": 283,
      "code_snippet": "\n    for data in data_list:\n        # model_output_list = eval(data['model_output'])\n        model_output_list = eval(data['model_output_clear'])[:1]\n        temp_list = []\n        for o in model_output_list:",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py_284_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 284. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py",
      "line_number": 284,
      "code_snippet": "    for data in data_list:\n        # model_output_list = eval(data['model_output'])\n        model_output_list = eval(data['model_output_clear'])[:1]\n        temp_list = []\n        for o in model_output_list:\n            temp_out = o.replace('```python', '')",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py_17_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "compile() for execution on line 17. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py",
      "line_number": 17,
      "code_snippet": "def is_code_valid(code):\n    try:\n        compile(code, '<string>', 'exec')\n        return True\n    except Exception:\n        return False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_ism_pm_score.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\u8bc4\u6d4bblock\u7684\u9884\u6d4b\u80fd\u529b\n1\u3001\u5224\u65ad\u662f\u5426\u5305\u542b\u6b63\u786e\u7684\u51fd\u6570\u540d\n2\u3001\u5224\u65ad\u662f\u5426\u5408\u6cd5\n3\u3001\u8ba1\u7b97ISM\uff0c\u548cPM\n\"\"\"\n\nimport io\nimport json\nimport math\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_migration_cdc_score.py_130_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "compile() for execution on line 130. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_migration_cdc_score.py",
      "line_number": 130,
      "code_snippet": "def is_code_valid(code):\n    try:\n        compile(code, '<string>', 'exec')\n        return True\n    except Exception:\n        return False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_migration_cdc_score.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_migration_cdc_score.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Calculate the cdc score for migration\"\"\"\n\nimport json\nimport math\nimport os\nimport re\n\n# warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py_190_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 190. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py",
      "line_number": 190,
      "code_snippet": "        for data in data_list:\n            answer = data['core_token']\n            model_output = eval(data['model_output_clear'])\n            model_filled_code = [\n                data['masked_code'].replace('<mask>', i) for i in model_output\n            ]",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py_204_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 204. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py",
      "line_number": 204,
      "code_snippet": "        for data in data_list:\n            answer = data['core_token']\n            model_output = eval(data['model_output_clear'])\n            model_filled_code = eval(data['model_output_clear'])\n            core_line = data['core_line']\n            core_line_in_output_clear = data['core_line_in_output_clear']",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py_205_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 205. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py",
      "line_number": 205,
      "code_snippet": "            answer = data['core_token']\n            model_output = eval(data['model_output_clear'])\n            model_filled_code = eval(data['model_output_clear'])\n            core_line = data['core_line']\n            core_line_in_output_clear = data['core_line_in_output_clear']\n            score_list.append(",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py_13_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "compile() for execution on line 13. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py",
      "line_number": 13,
      "code_snippet": "def is_code_valid(code):\n    try:\n        compile(code, '<string>', 'exec')\n        return True\n    except Exception:\n        return False",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/versicode/metric/compute_versicode_cdc_score.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Calculate the cdc score for line and block\"\"\"\n\nimport json\nimport math\nimport os\nimport re\n\n# warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/compare_txt_files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/compare_txt_files.py",
      "line_number": 1,
      "code_snippet": "def print_diff_ignore_order(file1, file2):\n    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n        file1_lines = set(f1.readlines())\n        file2_lines = set(f2.readlines())\n\n    only_in_file1 = file1_lines - file2_lines\n    only_in_file2 = file2_lines - file1_lines\n\n    if only_in_file1:\n        print(f'Lines in {file1} but not in {file2}:')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py_11",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 11 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
      "line_number": 11,
      "code_snippet": "    try:\n        subprocess.run(command, check=True, shell=True)\n    except subprocess.CalledProcessError as e:\n        print(f'An error occurred: {e}')",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py_9_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_command'",
      "description": "Function 'run_command' on line 9 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
      "line_number": 9,
      "code_snippet": "from datasets import load_dataset\n\n\n# Function to run shell commands\ndef run_command(command):\n    try:\n        subprocess.run(command, check=True, shell=True)\n    except subprocess.CalledProcessError as e:\n        print(f'An error occurred: {e}')\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py_9_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_command'",
      "description": "Function 'run_command' on line 9 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
      "line_number": 9,
      "code_snippet": "\n# Function to run shell commands\ndef run_command(command):\n    try:\n        subprocess.run(command, check=True, shell=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/docker/add_testing_dependencies.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\nimport subprocess\n\nfrom datasets import load_dataset\n\n\n# Function to run shell commands\ndef run_command(command):\n    try:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/convert_oh_output_to_md.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/convert_oh_output_to_md.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"Convert OpenHands output to a readable markdown format for visualization.\"\"\"\n\nimport argparse\nimport json\nimport os\n\nimport pandas as pd\nfrom tqdm import tqdm\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/build_outputs_ablation.py_80_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 80. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/build_outputs_ablation.py",
      "line_number": 80,
      "code_snippet": "                                if tool_calls is not None:\n                                    for tool_call in tool_calls:\n                                        tool_call_dict = eval(\n                                            tool_call['function']['arguments']\n                                        )\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/build_outputs_ablation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/build_outputs_ablation.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport copy\nimport difflib\nimport json\nimport os\nimport traceback\n\n\ndef insert_line_in_string(input_string, new_str, insert_line):\n    \"\"\"Inserts a new line into a string at the specified line number.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/compare_outputs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/compare_outputs.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\nimport argparse\n\nimport pandas as pd\n\nparser = argparse.ArgumentParser(\n    description='Compare two TestGenEval output JSONL files and print the resolved diff'\n)\nparser.add_argument('input_file_1', type=str)\nparser.add_argument('input_file_2', type=str)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/summarize_outputs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/summarize_outputs.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\nimport argparse\nimport json\nfrom collections import Counter\n\nfrom openhands.events.serialization import event_from_dict\nfrom openhands.events.utils import get_pairs_from_events\n\nERROR_KEYWORDS = [\n    'Agent encountered an error while processing the last action',",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/convert_oh_output_to_swe_json.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/convert_oh_output_to_swe_json.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\n\nimport pandas as pd\n\nfrom evaluation.swe_bench.eval_infer import process_git_patch\n\nparser = argparse.ArgumentParser()\nparser.add_argument('oh_output_file', type=str)\nargs = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/download_gold_test_suites.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/testgeneval/scripts/eval/download_gold_test_suites.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\nimport pandas as pd\nfrom datasets import load_dataset\n\nparser = argparse.ArgumentParser()\nparser.add_argument('output_filepath', type=str, help='Path to save the output file')\nparser.add_argument(\n    '--dataset_name',\n    type=str,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/adapter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/adapter.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport shutil\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import ClassVar\n\nfrom utils import (\n    AlgoTuneData,\n    extract_function_source,\n    get_instruction,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_27",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "User input 'repo_url' embedded in LLM prompt",
      "description": "User input parameter 'repo_url' is directly passed to LLM API call 'subprocess.run'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 27,
      "code_snippet": "        # Using --depth 1 for a shallow clone to save time and space\n        subprocess.run(\n            ['git', 'clone', '--depth', '1', repo_url, str(repo_path)],",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_27",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 27 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 27,
      "code_snippet": "        # Using --depth 1 for a shallow clone to save time and space\n        subprocess.run(\n            ['git', 'clone', '--depth', '1', repo_url, str(repo_path)],\n            check=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_92",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'adapter.generate_task' is used in 'subprocess.' on line 92 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 92,
      "code_snippet": "                if task_py_path.exists():\n                    adapter.generate_task(\n                        task_name=task_name, task_py_path=task_py_path\n                    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_40",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'main' on line 40 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 40,
      "code_snippet": "def main():\n    parser = argparse.ArgumentParser(\n        description='Convert AlgoTune tasks into T-Bench format using the adapter.',\n        formatter_class=argparse.RawTextHelpFormatter,\n    )\n    parser.add_argument(\n        '--repo-url',\n        type=str,\n        default='git@github.com:oripress/AlgoTune.git',\n        help='The URL of the Git repository containing AlgoTune tasks.',\n    )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_20_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'clone_repo'",
      "description": "Function 'clone_repo' on line 20 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 20,
      "code_snippet": "logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s: %(message)s')\nlogger = logging.getLogger(__name__)\n\n\ndef clone_repo(repo_url: str, temp_dir: Path) -> Path:\n    \"\"\"Clones the specified repository to a temporary directory.\"\"\"\n    repo_path = temp_dir / 'algotune_repo'\n    logger.info(f'Cloning AlgoTune repository from {repo_url}...')\n\n    try:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_40_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'main'",
      "description": "Function 'main' on line 40 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 40,
      "code_snippet": "        logger.error(f'Failed to clone repository: {e.stderr.strip()}')\n        raise\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Convert AlgoTune tasks into T-Bench format using the adapter.',\n        formatter_class=argparse.RawTextHelpFormatter,\n    )\n    parser.add_argument(",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_20_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'clone_repo'",
      "description": "Function 'clone_repo' on line 20 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 20,
      "code_snippet": "\n\ndef clone_repo(repo_url: str, temp_dir: Path) -> Path:\n    \"\"\"Clones the specified repository to a temporary directory.\"\"\"\n    repo_path = temp_dir / 'algotune_repo'\n    logger.info(f'Cloning AlgoTune repository from {repo_url}...')",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/run_adapter.py",
      "line_number": 1,
      "code_snippet": "# run_adapter.py\n\nimport argparse\nimport logging\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\n\nfrom adapter import AlgoTuneAdapter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/algotune/adapter/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nUtilities for the Algotune Adapter\n\"\"\"\n\nimport ast\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/scripts/summarise_results.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/ml_bench/scripts/summarise_results.py",
      "line_number": 1,
      "code_snippet": "import json\nimport pprint\nimport sys\n\n\ndef extract_test_results(res_file_path: str) -> tuple[list[str], list[str]]:\n    passed = []\n    failed = []\n    costs = []\n    instance_ids = set()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/eval_w_subhypo_gen.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/eval_w_subhypo_gen.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\n\nfrom openai import OpenAI\n\nfrom .lm_utils import run_chatgpt_query_multi_turn\nfrom .openai_helpers import get_response\n\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py_93",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
      "line_number": 93,
      "code_snippet": "def get_response(client, prompt, max_retry=5, model='gpt-3.5-turbo', verbose=False):\n    n_try = 0\n    while n_try < max_retry:\n        response = client.chat.completions.create(\n            model=model, messages=create_prompt(prompt), **OPENAI_GEN_HYP\n        )\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py_90",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_response' on line 90 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
      "line_number": 90,
      "code_snippet": "def get_response(client, prompt, max_retry=5, model='gpt-3.5-turbo', verbose=False):\n    n_try = 0\n    while n_try < max_retry:\n        response = client.chat.completions.create(\n            model=model, messages=create_prompt(prompt), **OPENAI_GEN_HYP\n        )\n\n        # COMMENT: changed from\n        # response.choices[0].message.content.strip().strip('```json').strip('```')\n        content = response.choices[0].message.content\n        cleaned_content = content.split('```json')[1].split('```')[0].strip()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py_93_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 93. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
      "line_number": 93,
      "code_snippet": "    n_try = 0\n    while n_try < max_retry:\n        response = client.chat.completions.create(\n            model=model, messages=create_prompt(prompt), **OPENAI_GEN_HYP\n        )\n",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py_90_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_response'",
      "description": "Function 'get_response' on line 90 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
      "line_number": 90,
      "code_snippet": "\n\ndef get_response(client, prompt, max_retry=5, model='gpt-3.5-turbo', verbose=False):\n    n_try = 0\n    while n_try < max_retry:\n        response = client.chat.completions.create(",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py_90",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_helpers.py",
      "line_number": 90,
      "code_snippet": "        },\n        {'role': 'user', 'content': usr_msg},\n    ]\n\n\ndef get_response(client, prompt, max_retry=5, model='gpt-3.5-turbo', verbose=False):\n    n_try = 0\n    while n_try < max_retry:\n        response = client.chat.completions.create(\n            model=model, messages=create_prompt(prompt), **OPENAI_GEN_HYP\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/response_parser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/response_parser.py",
      "line_number": 1,
      "code_snippet": "workflow_summary_markers = [\n    'WORKFLOW SUMMARY',\n    'WORKFLOW_SUMMARY',\n    'WORKFLOW-SUMMARY',\n    'Workflow Summary',\n]\n\nfinal_answer_markers = [\n    'FINAL ANSWER',\n    'FINAL_ANSWER',",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_47",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 47,
      "code_snippet": "            client = OpenAI()\n\n            if json_response:\n                response = client.chat.completions.create(\n                    model=model_name,\n                    response_format={'type': 'json_object'},\n                    messages=messages,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_54",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 54,
      "code_snippet": "                    **OPENAI_GEN_HYP,\n                )\n            else:\n                response = client.chat.completions.create(\n                    model=model_name, messages=messages, **OPENAI_GEN_HYP\n                )\n            break",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_31",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'run_chatgpt_query_multi_turn' on line 31 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 31,
      "code_snippet": "def run_chatgpt_query_multi_turn(\n    messages,\n    model_name='gpt-4-turbo',  # pass \"gpt4\" for more recent model output\n    max_tokens=256,\n    temperature=0.0,\n    json_response=False,\n):\n    response = None\n    num_retries = 3\n    retry = 0\n    while retry < num_retries:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_47_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model_name' is used without version pinning on line 47. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 47,
      "code_snippet": "\n            if json_response:\n                response = client.chat.completions.create(\n                    model=model_name,\n                    response_format={'type': 'json_object'},\n                    messages=messages,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_54_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model_name' is used without version pinning on line 54. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 54,
      "code_snippet": "                )\n            else:\n                response = client.chat.completions.create(\n                    model=model_name, messages=messages, **OPENAI_GEN_HYP\n                )\n            break",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_31_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'run_chatgpt_query_multi_turn'",
      "description": "Function 'run_chatgpt_query_multi_turn' on line 31 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 31,
      "code_snippet": "\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef run_chatgpt_query_multi_turn(\n    messages,\n    model_name='gpt-4-turbo',  # pass \"gpt4\" for more recent model output\n    max_tokens=256,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py_31_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_chatgpt_query_multi_turn'",
      "description": "Function 'run_chatgpt_query_multi_turn' on line 31 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/lm_utils.py",
      "line_number": 31,
      "code_snippet": "\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef run_chatgpt_query_multi_turn(\n    messages,\n    model_name='gpt-4-turbo',  # pass \"gpt4\" for more recent model output\n    max_tokens=256,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_semantic_gen_prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/discoverybench/eval_utils/openai_semantic_gen_prompts.py",
      "line_number": 1,
      "code_snippet": "common_hypothesis_features = [\n    '1-2 sentences',\n    'surprising finding',\n    'includes numeric concepts',\n    'includes categorical concepts',\n    'includes binary concepts',\n]\nhypothesis_features = [\n    ['requires within-cluster analysis'],\n    ['requires across-cluster analysis'],",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/resource/mapping.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/resource/mapping.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Mapping instance_id to resource_factor.\n\nDifferent instances may have different resource requirements.\ne.g., some instances may require more memory/CPU to run inference.\nThis file tracks the resource requirements of different instances.\n\"\"\"\n\nimport json\nimport os\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/setup/compare_patch_filename.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/setup/compare_patch_filename.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This script compares gold patches with OpenHands-generated patches and check whether\nOpenHands found the right (set of) files to modify.\n\"\"\"\n\nimport argparse\nimport json\nimport re\n\n\ndef extract_modified_files(patch):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_output_with_eval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_output_with_eval.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport os\nfrom collections import defaultdict\n\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument('input_file', type=str)\nparser.add_argument(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/combine_final_completions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/combine_final_completions.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport gzip\nimport json\nimport os\nfrom glob import glob\n\nfrom tqdm import tqdm\n\ntqdm.pandas()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/convert.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/convert.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport re\n\n\ndef main(input_file, output_file):\n    with open(input_file, 'r') as fin:\n        with open(output_file, 'w') as fout:\n            for line in fin:\n                data = json.loads(line)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py_12",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 12 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
      "line_number": 12,
      "code_snippet": "    # Run the conversion script\n    subprocess.run(\n        [\n            'python3',",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py_7_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'update_multi_swe_config'",
      "description": "Function 'update_multi_swe_config' on line 7 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
      "line_number": 7,
      "code_snippet": "import os\nimport subprocess\n\n\ndef update_multi_swe_config(output_jsonl_path, config_path, dataset):\n    path_to_parent = os.path.dirname(os.path.abspath(output_jsonl_path))\n    converted_path = os.path.join(path_to_parent, 'output_converted.jsonl')\n\n    # Run the conversion script\n    subprocess.run(",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py_7_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'update_multi_swe_config'",
      "description": "Function 'update_multi_swe_config' on line 7 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
      "line_number": 7,
      "code_snippet": "\n\ndef update_multi_swe_config(output_jsonl_path, config_path, dataset):\n    path_to_parent = os.path.dirname(os.path.abspath(output_jsonl_path))\n    converted_path = os.path.join(path_to_parent, 'output_converted.jsonl')\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/eval/update_multi_swe_bench_config.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport os\nimport subprocess\n\n\ndef update_multi_swe_config(output_jsonl_path, config_path, dataset):\n    path_to_parent = os.path.dirname(os.path.abspath(output_jsonl_path))\n    converted_path = os.path.join(path_to_parent, 'output_converted.jsonl')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/data/data_change.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/multi_swe_bench/scripts/data/data_change.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\n\ndef main(input_file, output_file):\n    with (\n        open(input_file, 'r', encoding='utf-8') as fin,\n        open(output_file, 'w', encoding='utf-8') as fout,\n    ):\n        for line in fin:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/resource/mapping.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/resource/mapping.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Mapping instance_id to resource_factor.\n\nDifferent instances may have different resource requirements.\ne.g., some instances may require more memory/CPU to run inference.\nThis file tracks the resource requirements of different instances.\n\"\"\"\n\nimport json\nimport os\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/setup/compare_patch_filename.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/setup/compare_patch_filename.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This script compares gold patches with OpenHands-generated patches and check whether\nOpenHands found the right (set of) files to modify.\n\"\"\"\n\nimport argparse\nimport json\nimport re\n\n\ndef extract_modified_files(patch):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/utils/evaluation_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/utils/evaluation_utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport multiprocessing as mp\nfrom typing import Awaitable, Callable, TextIO\n\nimport numpy as np\nimport pandas as pd\nfrom pydantic import SecretStr\nfrom tqdm import tqdm\n\nfrom evaluation.utils.shared import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/eval/verify_costs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/eval/verify_costs.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\nimport pandas as pd\n\nfrom openhands.core.logger import openhands_logger as logger\n\n\ndef verify_instance_costs(row: pd.Series) -> float:\n    \"\"\"\n    Verifies that the accumulated_cost matches the sum of individual costs in metrics.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/eval/convert.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/nocode_bench/scripts/eval/convert.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\n\ndef main(output_jsonl: str):\n    with open(output_jsonl, 'r') as f:\n        for line in f:\n            try:\n                output = json.loads(line)\n                pred = {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/resource/mapping.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/resource/mapping.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Mapping instance_id to resource_factor.\n\nDifferent instances may have different resource requirements.\ne.g., some instances may require more memory/CPU to run inference.\nThis file tracks the resource requirements of different instances.\n\"\"\"\n\nimport json\nimport os\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_utils.py_621_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_analyze_source_code_with_runtime'",
      "description": "Function '_analyze_source_code_with_runtime' on line 621 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_utils.py",
      "line_number": 621,
      "code_snippet": "        )\n\n    def _analyze_source_code_with_runtime(\n        self, runtime: Runtime, file_path: str, affected_lines: list[int]\n    ) -> tuple[list[str], list[str], dict[int, str], dict[int, str]]:\n        \"\"\"Analyze source code using OpenHands runtime to find functions and classes.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_utils.py_621_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_analyze_source_code_with_runtime'",
      "description": "Function '_analyze_source_code_with_runtime' on line 621 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_utils.py",
      "line_number": 621,
      "code_snippet": "        )\n\n    def _analyze_source_code_with_runtime(\n        self, runtime: Runtime, file_path: str, affected_lines: list[int]\n    ) -> tuple[list[str], list[str], dict[int, str], dict[int, str]]:\n        \"\"\"Analyze source code using OpenHands runtime to find functions and classes.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_utils.py",
      "line_number": 1,
      "code_snippet": "import ast\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom typing import Any, Union\n\nimport pandas as pd\nfrom datasets import load_dataset\n\nfrom openhands.runtime.base import Runtime",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py_755_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 755. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py",
      "line_number": 755,
      "code_snippet": "        ]\n\n    def instance_loc_eval(\n        self,\n        instance: pd.Series = None,\n        repo_root: str = None,",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py_985_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 985. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py",
      "line_number": 985,
      "code_snippet": "        curr_trajectory = infer_instance['history']\n        curr_cost = infer_instance['metrics']['accumulated_cost']\n        loc_evaluator.instance_loc_eval(\n            swe_instance, repo_root, curr_trajectory, curr_cost\n        )\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/loc_eval/loc_evaluator.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport ast\nimport json\nimport os\nimport re\n\nimport pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/docker/push_docker_instance_images.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/docker/push_docker_instance_images.py",
      "line_number": 1,
      "code_snippet": "\"\"\"You should first perform the following steps:\n\n1. Build the docker images. Install SWE-Bench first (https://github.com/princeton-nlp/SWE-bench). Then run:\n```bash\nexport DATASET_NAME=princeton-nlp/SWE-bench_Lite\nexport SPLIT=test\nexport MAX_WORKERS=4\nexport RUN_ID=some-random-ID\npython -m swebench.harness.run_evaluation \\\n    --dataset_name $DATASET_NAME \\",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/docker/get_docker_image_names.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/docker/get_docker_image_names.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Get official docker image names for SWE-bench instances.\"\"\"\n\nimport argparse\n\nfrom datasets import load_dataset\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataset', type=str, default='princeton-nlp/SWE-bench')\nparser.add_argument('--split', type=str, default='test')\nparser.add_argument('--output', type=str, default='swebench_images.txt')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/setup/compare_patch_filename.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/setup/compare_patch_filename.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This script compares gold patches with OpenHands-generated patches and check whether\nOpenHands found the right (set of) files to modify.\n\"\"\"\n\nimport argparse\nimport json\nimport re\n\n\ndef extract_modified_files(patch):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/swtbench/convert.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/swtbench/convert.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport logging\n\nimport unidiff\n\nfrom evaluation.benchmarks.swe_bench.resource.swt_bench_constants import (\n    MAP_VERSION_TO_INSTALL,\n)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/live/convert.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/live/convert.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\n\n\ndef main(output_jsonl: str):\n    with open(output_jsonl, 'r') as f:\n        for line in f:\n            try:\n                output = json.loads(line)\n                pred = {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/update_output_with_eval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/update_output_with_eval.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport os\nfrom collections import defaultdict\n\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument('input_file', type=str)\nargs = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/convert_oh_output_to_md.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/convert_oh_output_to_md.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"Convert OpenHands output to a readable markdown format for visualization.\"\"\"\n\nimport argparse\nimport json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom tqdm import tqdm",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/verify_costs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/verify_costs.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\nimport pandas as pd\n\nfrom openhands.core.logger import openhands_logger as logger\n\n\ndef verify_instance_costs(row: pd.Series) -> float:\n    \"\"\"Verifies that the accumulated_cost matches the sum of individual costs in metrics.\n    Also checks for duplicate consecutive costs which might indicate buggy counting.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/combine_final_completions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/combine_final_completions.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport gzip\nimport json\nimport os\nfrom glob import glob\n\nfrom tqdm import tqdm\n\ntqdm.pandas()\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py_47",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 47 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
      "line_number": 47,
      "code_snippet": "    print('=' * 80)\n    subprocess.run(['python', summarize_script, file_path], check=True)\n    print('=' * 80)\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py_41_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'summarize_file'",
      "description": "Function 'summarize_file' on line 41 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
      "line_number": 41,
      "code_snippet": "    )\n\n\n# Add summarization step for each input file\ndef summarize_file(file_path):\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    summarize_script = os.path.join(script_dir, 'summarize_outputs.py')\n\n    print(f'\\nSummary for {file_path}:')\n    print('=' * 80)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py_41_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'summarize_file'",
      "description": "Function 'summarize_file' on line 41 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
      "line_number": 41,
      "code_snippet": "\n# Add summarization step for each input file\ndef summarize_file(file_path):\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    summarize_script = os.path.join(script_dir, 'summarize_outputs.py')\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/compare_outputs.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\nimport argparse\nimport os\nimport subprocess\n\nimport pandas as pd\nfrom termcolor import colored\n\nparser = argparse.ArgumentParser(\n    description='Compare two swe_bench output JSONL files and print the resolved diff'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/summarize_outputs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/summarize_outputs.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\nimport argparse\nimport glob\nimport json\nimport os\nimport random\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/convert_oh_output_to_swe_json.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/convert_oh_output_to_swe_json.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\n\nimport pandas as pd\n\nfrom evaluation.benchmarks.swe_bench.eval_infer import process_git_patch\n\nparser = argparse.ArgumentParser()\nparser.add_argument('oh_output_file', type=str)\nargs = parser.parse_args()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/download_gold_patch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_bench/scripts/eval/download_gold_patch.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\nimport pandas as pd\nfrom datasets import load_dataset\n\nparser = argparse.ArgumentParser()\nparser.add_argument('output_filepath', type=str, help='Path to save the output file')\nparser.add_argument(\n    '--dataset_name',\n    type=str,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/scripts/summarise_results.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/agent_bench/scripts/summarise_results.py",
      "line_number": 1,
      "code_snippet": "import json\nimport sys\n\n\ndef extract_test_results(res_file_path: str) -> tuple[list[str], list[str]]:\n    passed = []\n    failed = []\n    with open(res_file_path, 'r') as file:\n        for line in file:\n            data = json.loads(line.strip())",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/scripts/summarize_results.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/aider_bench/scripts/summarize_results.py",
      "line_number": 1,
      "code_snippet": "import argparse\n\nimport numpy as np\nimport pandas as pd\n\n\ndef extract_test_results(df: pd.DataFrame) -> tuple[list[str], list[str]]:\n    passed = []\n    failed = []\n    for _, row in df.iterrows():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_13_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'calculate_cost'",
      "description": "Function 'calculate_cost' on line 13 makes critical financial, security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 13,
      "code_snippet": "\n\ndef calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:\n    \"\"\"Calculate the cost of the model call.\"\"\"\n    if 'claude-3-5-sonnet' in model.lower():\n        # https://www.anthropic.com/pricing#anthropic-api, accessed 12/11/2024",
      "recommendation": "Critical financial, security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_15",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 15,
      "code_snippet": "\ndef calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:\n    \"\"\"Calculate the cost of the model call.\"\"\"\n    if 'claude-3-5-sonnet' in model.lower():\n        # https://www.anthropic.com/pricing#anthropic-api, accessed 12/11/2024\n        return 0.000003 * prompt_tokens + 0.000015 * completion_tokens\n    elif 'gpt-4o' in model.lower():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_18",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 18,
      "code_snippet": "    if 'claude-3-5-sonnet' in model.lower():\n        # https://www.anthropic.com/pricing#anthropic-api, accessed 12/11/2024\n        return 0.000003 * prompt_tokens + 0.000015 * completion_tokens\n    elif 'gpt-4o' in model.lower():\n        # https://openai.com/api/pricing/, accessed 12/11/2024\n        return 0.0000025 * prompt_tokens + 0.00001 * completion_tokens\n    elif 'gemini-1.5-pro' in model.lower():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_21",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 21,
      "code_snippet": "    elif 'gpt-4o' in model.lower():\n        # https://openai.com/api/pricing/, accessed 12/11/2024\n        return 0.0000025 * prompt_tokens + 0.00001 * completion_tokens\n    elif 'gemini-1.5-pro' in model.lower():\n        # https://ai.google.dev/pricing#1_5pro, accessed 12/11/2024\n        # assuming prompts up to 128k tokens\n        cost = 0.00000125 * prompt_tokens + 0.000005 * completion_tokens",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_28",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 28,
      "code_snippet": "        if prompt_tokens > 128000:\n            cost *= 2\n        return cost\n    elif 'gemini-2.0-flash-exp' in model.lower():\n        # price unknown for gemini-2.0-flash-exp, assuming same price as gemini-1.5-flash\n        cost = 0.000000075 * prompt_tokens + 0.0000003 * completion_tokens\n        if prompt_tokens > 128000:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_34",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 34,
      "code_snippet": "        if prompt_tokens > 128000:\n            cost *= 2\n        return cost\n    elif 'qwen2-72b' in model.lower():\n        # assuming hosted on Together\n        # https://www.together.ai/pricing, accessed 12/11/2024\n        return 0.0000009 * (prompt_tokens + completion_tokens)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_38",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 38,
      "code_snippet": "        # assuming hosted on Together\n        # https://www.together.ai/pricing, accessed 12/11/2024\n        return 0.0000009 * (prompt_tokens + completion_tokens)\n    elif 'qwen2p5-72b' in model.lower():\n        # assuming hosted on Together\n        # https://www.together.ai/pricing, accessed 12/14/2024\n        return 0.0000012 * (prompt_tokens + completion_tokens)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_42",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 42,
      "code_snippet": "        # assuming hosted on Together\n        # https://www.together.ai/pricing, accessed 12/14/2024\n        return 0.0000012 * (prompt_tokens + completion_tokens)\n    elif 'llama-v3p1-405b-instruct' in model.lower():\n        # assuming hosted on Fireworks AI\n        # https://fireworks.ai/pricing, accessed 12/11/2024\n        return 0.000003 * (prompt_tokens + completion_tokens)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_46",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 46,
      "code_snippet": "        # assuming hosted on Fireworks AI\n        # https://fireworks.ai/pricing, accessed 12/11/2024\n        return 0.000003 * (prompt_tokens + completion_tokens)\n    elif 'llama-v3p1-70b-instruct' in model.lower():\n        # assuming hosted on Fireworks AI\n        return 0.0000009 * (prompt_tokens + completion_tokens)\n    elif 'llama-v3p3-70b-instruct' in model.lower():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_49",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 49,
      "code_snippet": "    elif 'llama-v3p1-70b-instruct' in model.lower():\n        # assuming hosted on Fireworks AI\n        return 0.0000009 * (prompt_tokens + completion_tokens)\n    elif 'llama-v3p3-70b-instruct' in model.lower():\n        # assuming hosted on Fireworks AI\n        return 0.0000009 * (prompt_tokens + completion_tokens)\n    elif 'amazon.nova-pro-v1:0' in model.lower():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py_52",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/the_agent_company/scripts/summarise_results.py",
      "line_number": 52,
      "code_snippet": "    elif 'llama-v3p3-70b-instruct' in model.lower():\n        # assuming hosted on Fireworks AI\n        return 0.0000009 * (prompt_tokens + completion_tokens)\n    elif 'amazon.nova-pro-v1:0' in model.lower():\n        # assuming hosted on Amazon Bedrock\n        # https://aws.amazon.com/bedrock/pricing/, accessed 12/11/2024\n        return 0.0000008 * prompt_tokens + 0.0000032 * completion_tokens",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/reasoning.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/reasoning.py",
      "line_number": 1,
      "code_snippet": "import ast\nimport logging\nimport re\nimport traceback\nfrom typing import Any\n\nimport numpy as np\nfrom sympy import Rational\n\nfrom tasks.base import Task",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/codegen.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/codegen.py",
      "line_number": 1,
      "code_snippet": "import logging\n\nfrom utils import check_correctness\n\nfrom evaluation.benchmarks.mint.tasks.base import Task\n\nLOGGER = logging.getLogger('MINT')\n\n\nclass CodeGenTask(Task):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/mint/tasks/base.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nfrom abc import ABC, abstractmethod\n\nfrom utils import load_file\n\nLOGGER = logging.getLogger('MINT')\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/resource/mapping.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/resource/mapping.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Mapping instance_id to resource_factor.\n\nDifferent instances may have different resource requirements.\ne.g., some instances may require more memory/CPU to run inference.\nThis file tracks the resource requirements of different instances.\n\"\"\"\n\nimport json\nimport os\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/scripts/setup/compare_patch_filename.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/benchmarks/swe_perf/scripts/setup/compare_patch_filename.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This script compares gold patches with OpenHands-generated patches and check whether\nOpenHands found the right (set of) files to modify.\n\"\"\"\n\nimport argparse\nimport json\nimport re\n\n\ndef extract_modified_files(patch):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/scripts/aggregate_token_usage.py_120_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'cached_tokens' containing sensitive data is being logged on line 120. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/scripts/aggregate_token_usage.py",
      "line_number": 120,
      "code_snippet": "    print(f'  Input tokens (non-cached):             {totals[\"input_tokens\"]:,}')\n    print(f'  Output tokens:                         {totals[\"output_tokens\"]:,}')\n    print(f'  Cached tokens:                         {totals[\"cached_tokens\"]:,}')\n    print(f'  Total tokens:                          {totals[\"total_tokens\"]:,}')\n    print(f'  Total costs (based on returned value): ${totals[\"cost\"]:.6f}')",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/scripts/aggregate_token_usage.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/evaluation/utils/scripts/aggregate_token_usage.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nScript to aggregate token usage metrics from LLM completion files.\n\nUsage:\n    python aggregate_token_usage.py <directory_path> [--input-cost <cost>] [--output-cost <cost>] [--cached-cost <cost>]\n\nArguments:\n    directory_path: Path to the directory containing completion files\n    --input-cost: Cost per input token (default: 0.0)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py_85",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'devbox' flows to 'self.runloop_api_client.devboxes.retrieve' on line 85 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
      "line_number": 85,
      "code_snippet": "\n        devbox = self.runloop_api_client.devboxes.retrieve(id=devbox.id)\n        if devbox.status != \"running\":\n            raise ConnectionRefusedError(\"Devbox is not running\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py_197_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'token' containing sensitive data is included in a prompt string on line 197. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
      "line_number": 197,
      "code_snippet": "                port=self._vscode_port,\n            ).url\n            + f\"/?tkn={token}&folder={self.config.workspace_mount_path_in_sandbox}\"\n        )\n",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py_80_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_wait_for_devbox'",
      "description": "Function '_wait_for_devbox' on line 80 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
      "line_number": 80,
      "code_snippet": "        wait=tenacity.wait_fixed(1),\n    )\n    def _wait_for_devbox(self, devbox: DevboxView) -> DevboxView:\n        \"\"\"Pull devbox status until it is running\"\"\"\n        if devbox == \"running\":\n            return devbox",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py_174_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'close'",
      "description": "Function 'close' on line 174 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
      "line_number": 174,
      "code_snippet": "        super().check_if_alive()\n\n    def close(self, rm_all_containers: bool | None = True):\n        super().close()\n\n        if self.attach_to_existing:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py_184_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'vscode_url'",
      "description": "Function 'vscode_url' on line 184 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
      "line_number": 184,
      "code_snippet": "\n    @property\n    def vscode_url(self) -> str | None:\n        if self._vscode_url is not None:  # cached value\n            return self._vscode_url\n        token = super().get_vscode_token()",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/runloop/runloop_runtime.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom typing import Callable\n\nimport tenacity\nfrom runloop_api_client import Runloop\nfrom runloop_api_client.types import DevboxView\nfrom runloop_api_client.types.shared_params import LaunchParameters\n\nfrom openhands.core.config import OpenHandsConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/daytona/daytona_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/daytona/daytona_runtime.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Callable\n\nimport httpx\nimport tenacity\nfrom daytona import (\n    CreateSandboxFromSnapshotParams,\n    Daytona,\n    DaytonaConfig,\n    Sandbox,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/modal/modal_runtime.py_181_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_get_image_definition'",
      "description": "Function '_get_image_definition' on line 181 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/modal/modal_runtime.py",
      "line_number": 181,
      "code_snippet": "        self.check_if_alive()\n\n    def _get_image_definition(\n        self,\n        base_container_image_id: str | None,\n        runtime_container_image_id: str | None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/modal/modal_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/modal/modal_runtime.py",
      "line_number": 1,
      "code_snippet": "import os\nimport tempfile\nfrom time import sleep\nfrom pathlib import Path\nfrom typing import Callable\n\nimport httpx\nimport modal\nimport tenacity\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py_195_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_ipython'",
      "description": "Function 'run_ipython' on line 195 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py",
      "line_number": 195,
      "code_snippet": "            return ErrorObservation(f\"Failed to execute command: {e}\")\n    \n    def run_ipython(self, action: IPythonRunCellAction) -> Observation:\n        \"\"\"Execute IPython code using E2B's code interpreter.\"\"\"\n        if self.sandbox is None:\n            return ErrorObservation(\"E2B sandbox not initialized\")",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py_195_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_ipython'",
      "description": "Function 'run_ipython' on line 195 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py",
      "line_number": 195,
      "code_snippet": "            return ErrorObservation(f\"Failed to execute command: {e}\")\n    \n    def run_ipython(self, action: IPythonRunCellAction) -> Observation:\n        \"\"\"Execute IPython code using E2B's code interpreter.\"\"\"\n        if self.sandbox is None:\n            return ErrorObservation(\"E2B sandbox not initialized\")",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/e2b_runtime.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Callable\n\nfrom openhands.core.config import OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action import (\n    BrowseURLAction,\n    BrowseInteractiveAction,\n    CmdRunAction,\n    FileEditAction,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py_97",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.sandbox.commands.run' is used in 'run(' on line 97 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py",
      "line_number": 97,
      "code_snippet": "        try:\n            result = self.sandbox.commands.run(cmd)\n            output = \"\"\n            if hasattr(result, 'stdout') and result.stdout:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py_92_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'execute'",
      "description": "Function 'execute' on line 92 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py",
      "line_number": 92,
      "code_snippet": "        return tar_filename\n\n    def execute(self, cmd: str, timeout: int | None = None) -> tuple[int, str]:\n        timeout = timeout if timeout is not None else self.config.timeout\n        \n        # E2B code-interpreter uses commands.run()",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py_92_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'execute'",
      "description": "Function 'execute' on line 92 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py",
      "line_number": 92,
      "code_snippet": "        return tar_filename\n\n    def execute(self, cmd: str, timeout: int | None = None) -> tuple[int, str]:\n        timeout = timeout if timeout is not None else self.config.timeout\n        \n        # E2B code-interpreter uses commands.run()",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/third_party/runtime/impl/e2b/sandbox.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport os\nimport tarfile\nfrom glob import glob\n\nfrom e2b_code_interpreter import Sandbox\nfrom e2b.exceptions import TimeoutException\n\nfrom openhands.core.config import SandboxConfig\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/metrics.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/metrics.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport copy\nimport time\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/async_llm.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/async_llm.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport asyncio\nfrom functools import partial\nfrom typing import Any, Callable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/streaming_llm.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/streaming_llm.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport asyncio\nfrom functools import partial\nfrom typing import Any, Callable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/debug_mixin.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/debug_mixin.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom logging import DEBUG\nfrom typing import Any\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/retry_mixin.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/retry_mixin.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom typing import Any, Callable\n\nfrom tenacity import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/tool_names.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/tool_names.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n\"\"\"Constants for tool names used in function calling.\"\"\"\n\nEXECUTE_BASH_TOOL_NAME = 'execute_bash'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/fn_call_converter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/fn_call_converter.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n\"\"\"Convert function calling messages to non-function calling messages and vice versa.\n\nThis will inject prompts so that models that doesn't support function calling",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_utils.py_18_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/network operation without confirmation in 'check_tools'",
      "description": "Function 'check_tools' on line 18 performs high-risk delete/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_utils.py",
      "line_number": 18,
      "code_snippet": "\n\ndef check_tools(\n    tools: list['ChatCompletionToolParam'], llm_config: LLMConfig\n) -> list['ChatCompletionToolParam']:\n    \"\"\"Checks and modifies tools for compatibility with the current LLM.\"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py_75",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'llm.completion' is used in 'UPDATE' on line 75 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
      "line_number": 75,
      "code_snippet": "        llm = self.service_to_llm[service_id]\n        response = llm.completion(messages=messages)\n        return response.choices[0].message.content.strip()\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py_65",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'request_extraneous_completion' on line 65 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
      "line_number": 65,
      "code_snippet": "    def request_extraneous_completion(\n        self, service_id: str, llm_config: LLMConfig, messages: list[dict[str, str]]\n    ) -> str:\n        logger.info(f'extraneous completion: {service_id}')\n        if service_id not in self.service_to_llm:\n            self._create_new_llm(\n                config=llm_config, service_id=service_id, with_listener=False\n            )\n\n        llm = self.service_to_llm[service_id]\n        response = llm.completion(messages=messages)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py_65_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'request_extraneous_completion'",
      "description": "Function 'request_extraneous_completion' on line 65 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
      "line_number": 65,
      "code_snippet": "        return llm\n\n    def request_extraneous_completion(\n        self, service_id: str, llm_config: LLMConfig, messages: list[dict[str, str]]\n    ) -> str:\n        logger.info(f'extraneous completion: {service_id}')",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py_75",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/llm_registry.py",
      "line_number": 75,
      "code_snippet": "            )\n\n        llm = self.service_to_llm[service_id]\n        response = llm.completion(messages=messages)\n        return response.choices[0].message.content.strip()\n\n    def get_llm_from_agent_config(self, service_id: str, agent_config: AgentConfig):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/model_features.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/model_features.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/message_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/message_utils.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom openhands.events.event import Event\nfrom openhands.llm.metrics import Metrics, TokenUsage\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/message.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/message.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom enum import Enum\nfrom typing import Any, Literal\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/logger.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/logger.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport copy\nimport logging\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/setup.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport hashlib\nimport os\nimport uuid",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/exceptions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/exceptions.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# ============================================\n# Agent Exceptions\n# ============================================",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py_390",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
      "line_number": 390,
      "code_snippet": "            try:\n                if self.loop is None:\n                    self.loop = asyncio.get_running_loop()\n                asyncio.run_coroutine_threadsafe(\n                    self._set_runtime_status('error', status, message), self.loop\n                )\n            except (RuntimeError, KeyError) as e:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py_384",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'set_runtime_status' on line 384 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
      "line_number": 384,
      "code_snippet": "    def set_runtime_status(self, status: RuntimeStatus, message: str):\n        \"\"\"Sends an error message if the callback function was provided.\"\"\"\n        if self.status_callback:\n            try:\n                if self.loop is None:\n                    self.loop = asyncio.get_running_loop()\n                asyncio.run_coroutine_threadsafe(\n                    self._set_runtime_status('error', status, message), self.loop\n                )\n            except (RuntimeError, KeyError) as e:\n                logger.error(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py_86_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'on_event'",
      "description": "Function 'on_event' on line 86 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
      "line_number": 86,
      "code_snippet": "        self._load_user_microagents()\n\n    def on_event(self, event: Event):\n        \"\"\"Handle an event from the event stream.\"\"\"\n        asyncio.get_event_loop().run_until_complete(self._on_event(event))\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py_384_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'set_runtime_status'",
      "description": "Function 'set_runtime_status' on line 384 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
      "line_number": 384,
      "code_snippet": "        )\n\n    def set_runtime_status(self, status: RuntimeStatus, message: str):\n        \"\"\"Sends an error message if the callback function was provided.\"\"\"\n        if self.status_callback:\n            try:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py_384_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'set_runtime_status'",
      "description": "Function 'set_runtime_status' on line 384 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
      "line_number": 384,
      "code_snippet": "        )\n\n    def set_runtime_status(self, status: RuntimeStatus, message: str):\n        \"\"\"Sends an error message if the callback function was provided.\"\"\"\n        if self.status_callback:\n            try:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py_384",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/memory.py",
      "line_number": 384,
      "code_snippet": "        \"\"\"\n        self.conversation_instructions = ConversationInstructions(\n            content=conversation_instructions or ''\n        )\n\n    def set_runtime_status(self, status: RuntimeStatus, message: str):\n        \"\"\"Sends an error message if the callback function was provided.\"\"\"\n        if self.status_callback:\n            try:\n                if self.loop is None:\n                    self.loop = asyncio.get_running_loop()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/view.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom typing import overload\n\nfrom pydantic import BaseModel\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action.agent import CondensationAction, CondensationRequestAction\nfrom openhands.events.event import Event\nfrom openhands.events.observation.agent import AgentCondensationObservation",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py_192_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_process_action' executes dangerous operations",
      "description": "Tool function '_process_action' on line 192 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py",
      "line_number": 192,
      "code_snippet": "            formatted_messages.append(msg)\n            prev_role = msg.role  # Update prev_role after processing each message\n        return formatted_messages\n\n    def _process_action(\n        self,\n        action: Action,\n        pending_tool_call_action_messages: dict[str, Message],\n        vision_is_active: bool = False,\n    ) -> list[Message]:",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py_759_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_filter_unmatched_tool_calls' executes dangerous operations",
      "description": "Tool function '_filter_unmatched_tool_calls' on line 759 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py",
      "line_number": 759,
      "code_snippet": "                    return True\n        return False\n\n    @staticmethod\n    def _filter_unmatched_tool_calls(\n        messages: list[Message],\n    ) -> Generator[Message, None, None]:\n        \"\"\"Filter out tool calls that don't have matching tool responses and vice versa.\n\n        This ensures that every tool_call_id in a tool message has a corresponding tool_calls[].id",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/conversation_memory.py",
      "line_number": 1,
      "code_snippet": "from typing import Generator\n\nfrom litellm import ModelResponse\n\nfrom openhands.core.config.agent_config import AgentConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.core.message import ImageContent, Message, TextContent\nfrom openhands.core.schema import ActionType\nfrom openhands.events.action import (\n    Action,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/config.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Configuration for the OpenHands App Server.\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import AsyncContextManager\n\nimport httpx\nfrom fastapi import Depends, Request\nfrom pydantic import Field, SecretStr\nfrom sqlalchemy.ext.asyncio import AsyncSession",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/experiments/experiment_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/experiments/experiment_manager.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.sdk import Agent\nfrom openhands.server.session.conversation_init_data import ConversationInitData\nfrom openhands.server.shared import file_store",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/io/io.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/io/io.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport sys\n\n\ndef read_input(cli_multiline_input: bool = False) -> str:\n    \"\"\"Read input from user based on config settings.\"\"\"\n    if cli_multiline_input:\n        print('Enter your message (enter \"/exit\" on a new line to finish):')\n        lines = []\n        while True:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/io/json.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/io/json.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom datetime import datetime\n\nfrom json_repair import repair_json\nfrom litellm.types.utils import ModelResponse\n\nfrom openhands.core.exceptions import LLMResponseError\nfrom openhands.events.event import Event\nfrom openhands.events.observation import CmdOutputMetadata\nfrom openhands.events.serialization import event_to_dict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/file_viewer_server.py_16_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'create_app'",
      "description": "API endpoint 'create_app' on line 16 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/file_viewer_server.py",
      "line_number": 16,
      "code_snippet": "\n\ndef create_app() -> FastAPI:\n    \"\"\"Create the FastAPI application.\"\"\"\n    app = FastAPI(\n        title='File Viewer Server', openapi_url=None, docs_url=None, redoc_url=None",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/file_viewer_server.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/file_viewer_server.py",
      "line_number": 1,
      "code_snippet": "\"\"\"A tiny, isolated server that provides only the /view endpoint from the action execution server.\nThis server has no authentication and only listens to localhost traffic.\n\"\"\"\n\nimport os\nimport threading\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom uvicorn import Config, Server",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/__init__.py",
      "line_number": 1,
      "code_snippet": "import importlib\n\nfrom openhands.runtime.base import Runtime\nfrom openhands.runtime.impl.cli.cli_runtime import CLIRuntime\nfrom openhands.runtime.impl.docker.docker_runtime import (\n    DockerRuntime,\n)\nfrom openhands.runtime.impl.kubernetes.kubernetes_runtime import KubernetesRuntime\nfrom openhands.runtime.impl.local.local_runtime import LocalRuntime\nfrom openhands.runtime.impl.remote.remote_runtime import RemoteRuntime",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/runtime_status.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/runtime_status.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass RuntimeStatus(Enum):\n    STOPPED = 'STATUS$STOPPED'\n    BUILDING_RUNTIME = 'STATUS$BUILDING_RUNTIME'\n    STARTING_RUNTIME = 'STATUS$STARTING_RUNTIME'\n    RUNTIME_STARTED = 'STATUS$RUNTIME_STARTED'\n    SETTING_UP_WORKSPACE = 'STATUS$SETTING_UP_WORKSPACE'\n    SETTING_UP_GIT_HOOKS = 'STATUS$SETTING_UP_GIT_HOOKS'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/action_execution_server.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/action_execution_server.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This is the main file for the runtime client.\nIt is responsible for executing actions received from OpenHands backend and producing observations.\n\nNOTE: this will be executed inside the docker sandbox.\n\"\"\"\n\nimport argparse\nimport asyncio\nimport base64\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_287",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 287 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 287,
      "code_snippet": "            if not isinstance(obs, CmdOutputObservation) or obs.exit_code != 0:\n                raise RuntimeError(\n                    f'Failed to add env vars [{env_vars.keys()}] to environment: {obs.content}'\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_313",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 313 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 313,
      "code_snippet": "            if not isinstance(obs, CmdOutputObservation) or obs.exit_code != 0:\n                raise RuntimeError(\n                    f'Failed to add env vars [{env_vars.keys()}] to environment: {obs.content}'\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_322",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 322 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 322,
      "code_snippet": "            if not isinstance(obs, CmdOutputObservation) or obs.exit_code != 0:\n                raise RuntimeError(\n                    f'Failed to add env vars [{env_vars.keys()}] to .bashrc: {obs.content}'\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_1072_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '_setup_git_config'",
      "description": "Function '_setup_git_config' on line 1072 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 1072,
      "code_snippet": "        pass\n\n    def _setup_git_config(self) -> None:\n        \"\"\"Configure git user settings during initial environment setup.\n\n        This method is called automatically during setup_initial_env() to ensure",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_1199_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_execute_shell_fn_git_handler'",
      "description": "Function '_execute_shell_fn_git_handler' on line 1199 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 1199,
      "code_snippet": "    # ====================================================================\n\n    def _execute_shell_fn_git_handler(\n        self, command: str, cwd: str | None\n    ) -> CommandResult:\n        \"\"\"This function is used by the GitHandler to execute shell commands.\"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_1199_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_execute_shell_fn_git_handler'",
      "description": "Function '_execute_shell_fn_git_handler' on line 1199 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 1199,
      "code_snippet": "    # ====================================================================\n\n    def _execute_shell_fn_git_handler(\n        self, command: str, cwd: str | None\n    ) -> CommandResult:\n        \"\"\"This function is used by the GitHandler to execute shell commands.\"\"\"",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/base.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport atexit\nimport copy\nimport json\nimport os\nimport random\nimport shlex\nimport shutil\nimport string\nimport tempfile",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/shared.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/shared.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport os\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/utils.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport uuid\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/app.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/app.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport contextlib\nimport warnings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/listen_socket.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/listen_socket.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport asyncio\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/file_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/file_config.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport os\nimport re",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/__main__.py_17_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'main'",
      "description": "Function 'main' on line 17 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/__main__.py",
      "line_number": 17,
      "code_snippet": "\n\ndef main():\n    # Suppress SyntaxWarnings from pydub.utils about invalid escape sequences\n    warnings.filterwarnings('ignore', category=SyntaxWarning, module=r'pydub\\.utils')\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/__main__.py_17_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'main'",
      "description": "Function 'main' on line 17 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/__main__.py",
      "line_number": 17,
      "code_snippet": "\n\ndef main():\n    # Suppress SyntaxWarnings from pydub.utils about invalid escape sequences\n    warnings.filterwarnings('ignore', category=SyntaxWarning, module=r'pydub\\.utils')\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/middleware.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/middleware.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport asyncio\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/shutdown_listener.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/shutdown_listener.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This module monitors the app for shutdown signals. This exists because the atexit module\ndoes not play nocely with stareltte / uvicorn shutdown signals.\n\"\"\"\n\nimport asyncio\nimport signal\nimport threading\nimport time\nfrom types import FrameType\nfrom typing import Callable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py_21_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'call_async_from_sync'",
      "description": "Function 'call_async_from_sync' on line 21 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py",
      "line_number": 21,
      "code_snippet": "\n\ndef call_async_from_sync(\n    corofn: Callable, timeout: float = GENERAL_TIMEOUT, *args, **kwargs\n):\n    \"\"\"Shorthand for running a coroutine in the default background thread pool executor",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py_115_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_run_in_loop'",
      "description": "Function '_run_in_loop' on line 115 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py",
      "line_number": 115,
      "code_snippet": "\n\ndef _run_in_loop(coro: Coroutine, loop: asyncio.AbstractEventLoop, timeout: float):\n    future = asyncio.run_coroutine_threadsafe(coro, loop)\n    result = future.result(timeout=timeout)\n    return result",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py_37_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'run'",
      "description": "Function 'run' on line 37 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py",
      "line_number": 37,
      "code_snippet": "        return result\n\n    def run():\n        loop_for_thread = asyncio.new_event_loop()\n        try:\n            asyncio.set_event_loop(loop_for_thread)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py_115_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in '_run_in_loop'",
      "description": "Function '_run_in_loop' on line 115 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py",
      "line_number": 115,
      "code_snippet": "\n\ndef _run_in_loop(coro: Coroutine, loop: asyncio.AbstractEventLoop, timeout: float):\n    future = asyncio.run_coroutine_threadsafe(coro, loop)\n    result = future.result(timeout=timeout)\n    return result",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py_37_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run'",
      "description": "Function 'run' on line 37 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/async_utils.py",
      "line_number": 37,
      "code_snippet": "        return result\n\n    def run():\n        loop_for_thread = asyncio.new_event_loop()\n        try:\n            asyncio.set_event_loop(loop_for_thread)",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py_14",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_supported_llm_models' on line 14 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py",
      "line_number": 14,
      "code_snippet": "def get_supported_llm_models(config: OpenHandsConfig) -> list[str]:\n    \"\"\"Get all models supported by LiteLLM.\n\n    This function combines models from litellm and Bedrock, removing any\n    error-prone Bedrock models.\n\n    Returns:\n        list[str]: A sorted list of unique model names.\n    \"\"\"\n    litellm_model_list = litellm.model_list + list(litellm.model_cost.keys())\n    litellm_model_list_without_bedrock = bedrock.remove_error_modelId(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py_14_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'get_supported_llm_models'",
      "description": "Function 'get_supported_llm_models' on line 14 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py",
      "line_number": 14,
      "code_snippet": "\n\ndef get_supported_llm_models(config: OpenHandsConfig) -> list[str]:\n    \"\"\"Get all models supported by LiteLLM.\n\n    This function combines models from litellm and Bedrock, removing any",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/llm.py",
      "line_number": 1,
      "code_snippet": "import warnings\n\nimport httpx\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    import litellm\n\nfrom openhands.core.config import LLMConfig, OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/utils.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom copy import deepcopy\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.llm.llm_registry import LLMRegistry\nfrom openhands.server.services.conversation_stats import ConversationStats\nfrom openhands.storage import get_file_store\nfrom openhands.storage.data_models.settings import Settings\nfrom openhands.utils.environment import get_effective_llm_base_url\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/conversation_summary.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/conversation_summary.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utility functions for generating conversation summaries.\"\"\"\n\nfrom typing import Optional\n\nfrom openhands.core.config import LLMConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action.message import MessageAction\nfrom openhands.events.event import EventSource\nfrom openhands.events.event_store import EventStore\nfrom openhands.llm.llm_registry import LLMRegistry",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/term_color.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/term_color.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\nfrom termcolor import colored\n\n\nclass TermColor(Enum):\n    \"\"\"Terminal color codes.\"\"\"\n\n    WARNING = 'yellow'\n    SUCCESS = 'green'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/environment.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/environment.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\nfrom functools import lru_cache\nfrom pathlib import Path\n\nLEMONADE_DOCKER_BASE_URL = 'http://host.docker.internal:8000/api/v1/'\n_LEMONADE_PROVIDER_NAME = 'lemonade'\n_LEMONADE_MODEL_PREFIX = 'lemonade/'\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/chunk_localizer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/chunk_localizer.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Chunk localizer to help localize the most relevant chunks in a file.\n\nThis is primarily used to localize the most relevant chunks in a file\nfor a given query (e.g. edit draft produced by the agent).\n\"\"\"\n\nfrom pydantic import BaseModel\nfrom rapidfuzz.distance import LCSseq\nfrom tree_sitter_language_pack import get_parser\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/prompt.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom dataclasses import dataclass, field\nfrom itertools import islice\n\nfrom jinja2 import Environment, FileSystemLoader, Template\n\nfrom openhands.controller.state.state import State\nfrom openhands.core.message import Message, TextContent\nfrom openhands.events.observation.agent import MicroagentKnowledge\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/http_session.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/http_session.py",
      "line_number": 1,
      "code_snippet": "import ssl\nfrom dataclasses import dataclass, field\nfrom threading import Lock\nfrom typing import MutableMapping\n\nimport httpx\n\nfrom openhands.core.logger import openhands_logger as logger\n\n_client_lock = Lock()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/import_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/import_utils.py",
      "line_number": 1,
      "code_snippet": "import importlib\nfrom functools import lru_cache\nfrom typing import TypeVar\n\nT = TypeVar('T')\n\n\ndef import_from(qual_name: str):\n    \"\"\"Import a value from its fully qualified name.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/search_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/utils/search_utils.py",
      "line_number": 1,
      "code_snippet": "import base64\nfrom typing import AsyncIterator, Callable\n\n\ndef offset_to_page_id(offset: int, has_next: bool) -> str | None:\n    if not has_next:\n        return None\n    next_page_id = base64.b64encode(str(offset).encode()).decode()\n    return next_page_id\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/mcp/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/mcp/client.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom typing import Optional\n\nfrom fastmcp import Client\nfrom fastmcp.client.transports import (\n    SSETransport,\n    StdioTransport,\n    StreamableHttpTransport,\n)\nfrom mcp import McpError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/mcp/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/mcp/utils.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport json\nimport shutil\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from openhands.controller.agent import Agent\n    from openhands.memory.memory import Memory\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/google_cloud.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/google_cloud.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom google.api_core.exceptions import NotFound\nfrom google.cloud import storage\nfrom google.cloud.storage.blob import Blob\nfrom google.cloud.storage.bucket import Bucket\nfrom google.cloud.storage.client import Client\n\nfrom openhands.storage.files import FileStore\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/batched_web_hook.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/batched_web_hook.py",
      "line_number": 1,
      "code_snippet": "import threading\nfrom typing import Optional\n\nimport httpx\nimport tenacity\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.storage.files import FileStore\nfrom openhands.utils.async_utils import EXECUTOR\nfrom openhands.utils.http_session import httpx_verify_option",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/memory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/memory.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.storage.files import FileStore\n\n\nclass InMemoryFileStore(FileStore):\n    files: dict[str, str]\n\n    def __init__(self, files: dict[str, str] | None = None) -> None:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/local.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/local.py",
      "line_number": 1,
      "code_snippet": "import os\nimport shutil\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.storage.files import FileStore\n\n\nclass LocalFileStore(FileStore):\n    root: str\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/web_hook.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/web_hook.py",
      "line_number": 1,
      "code_snippet": "import httpx\nimport tenacity\n\nfrom openhands.storage.files import FileStore\nfrom openhands.utils.async_utils import EXECUTOR\nfrom openhands.utils.http_session import httpx_verify_option\n\n\nclass WebHookFileStore(FileStore):\n    \"\"\"File store which includes a web hook to be invoked after any changes occur.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/__init__.py",
      "line_number": 1,
      "code_snippet": "import os\n\nimport httpx\n\nfrom openhands.storage.batched_web_hook import BatchedWebHookFileStore\nfrom openhands.storage.files import FileStore\nfrom openhands.storage.google_cloud import GoogleCloudFileStore\nfrom openhands.storage.local import LocalFileStore\nfrom openhands.storage.memory import InMemoryFileStore\nfrom openhands.storage.s3 import S3FileStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py_149",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 149 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py",
      "line_number": 149,
      "code_snippet": "            else:\n                raise FileNotFoundError(\n                    f\"Error: Failed to delete key '{path}' from bucket '{self.bucket}': {e}\"\n                )\n        except Exception as e:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py_153",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 153 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py",
      "line_number": 153,
      "code_snippet": "        except Exception as e:\n            raise FileNotFoundError(\n                f\"Error: Failed to delete key '{path}' from bucket '{self.bucket}: {e}\"\n            )\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/s3.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any, TypedDict\n\nimport boto3\nimport botocore\n\nfrom openhands.storage.files import FileStore\n\n\nclass S3ObjectDict(TypedDict):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent_controller.py_793_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'end_delegate'",
      "description": "Function 'end_delegate' on line 793 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent_controller.py",
      "line_number": 793,
      "code_snippet": "        )\n\n    def end_delegate(self) -> None:\n        \"\"\"Ends the currently active delegate (e.g., if it is finished or errored).\n\n        so that this controller can resume normal operation.",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent_controller.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent_controller.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# V1 replacement for this module lives in the Software Agent SDK.\nfrom __future__ import annotations\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/replay.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/replay.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/stuck.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/stuck.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom dataclasses import dataclass\nfrom typing import Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/agent.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# V1 replacement for this module lives in the Software Agent SDK.\nfrom __future__ import annotations\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/provider.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\nfrom collections.abc import Mapping\nfrom types import MappingProxyType\nfrom typing import Any, Coroutine, Literal, cast, overload\nfrom urllib.parse import quote\n\nimport httpx\nfrom pydantic import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/utils.py",
      "line_number": 1,
      "code_snippet": "from pydantic import SecretStr\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.azure_devops.azure_devops_service import (\n    AzureDevOpsServiceImpl as AzureDevOpsService,\n)\nfrom openhands.integrations.bitbucket.bitbucket_service import BitBucketService\nfrom openhands.integrations.forgejo.forgejo_service import ForgejoService\nfrom openhands.integrations.github.github_service import GitHubService\nfrom openhands.integrations.gitlab.gitlab_service import GitLabService",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/service_types.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/service_types.py",
      "line_number": 1,
      "code_snippet": "from abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Protocol\n\nfrom jinja2 import Environment, FileSystemLoader\nfrom pydantic import BaseModel, SecretStr\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/event.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/event.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom enum import Enum\n\nfrom openhands.events.tool import ToolCallMetadata\nfrom openhands.llm.metrics import Metrics\n\n\nclass EventSource(str, Enum):\n    AGENT = 'agent'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/event_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/event_store.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom dataclasses import dataclass\nfrom typing import Iterable\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.event import Event, EventSource\nfrom openhands.events.event_filter import EventFilter\nfrom openhands.events.event_store_abc import EventStoreABC\nfrom openhands.events.serialization.event import event_from_dict\nfrom openhands.storage.files import FileStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/nested_event_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/nested_event_store.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom typing import Iterable\nfrom urllib.parse import urlencode\n\nimport httpx  # type: ignore\nfrom fastapi import status\n\nfrom openhands.events.event import Event\nfrom openhands.events.event_filter import EventFilter\nfrom openhands.events.event_store_abc import EventStoreABC",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/microagent/microagent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/microagent/microagent.py",
      "line_number": 1,
      "code_snippet": "import io\nimport re\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import ClassVar, Union\n\nimport frontmatter\nfrom pydantic import BaseModel\n\nfrom openhands.core.exceptions import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/visualize_resolver_output.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/visualize_resolver_output.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\n\nfrom openhands.resolver.io_utils import load_single_resolver_output\n\n\ndef visualize_resolver_output(\n    issue_number: int, output_dir: str, vis_method: str\n) -> None:\n    output_jsonl = os.path.join(output_dir, 'output.jsonl')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_237",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on line 237 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 237,
      "code_snippet": "    if result.returncode != 0:\n        raise RuntimeError(f'Failed to commit changes: {result}')\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_547",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.95,
      "title": "LLM output flows to xss sink",
      "description": "LLM output variable 'comment_message' flows to 'template.render' on line 547 via direct flow. This creates a xss vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 547,
      "code_snippet": "                        template = jinja2.Template(f.read())\n                    prompt = template.render(comment_message=comment_message)\n                    response = llm.completion(\n                        messages=[{'role': 'user', 'content': prompt}],",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_169",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'make_commit' on line 169 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 169,
      "code_snippet": "def make_commit(\n    repo_dir: str,\n    issue: Issue,\n    issue_type: str,\n    git_user_name: str = 'openhands',\n    git_user_email: str = 'openhands@all-hands.dev',\n) -> None:\n    \"\"\"Make a commit with the changes to the repository.\n\n    Args:\n        repo_dir: The directory containing the repository",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_240",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'send_pull_request' on line 240 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 240,
      "code_snippet": "def send_pull_request(\n    issue: Issue,\n    token: str,\n    username: str | None,\n    platform: ProviderType,\n    patch_dir: str,\n    pr_type: str,\n    fork_owner: str | None = None,\n    additional_message: str | None = None,\n    target_branch: str | None = None,\n    reviewer: str | None = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_438",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'update_existing_pull_request' on line 438 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 438,
      "code_snippet": "def update_existing_pull_request(\n    issue: Issue,\n    token: str,\n    username: str | None,\n    platform: ProviderType,\n    patch_dir: str,\n    llm_config: LLMConfig,\n    comment_message: str | None = None,\n    additional_message: str | None = None,\n    base_domain: str | None = None,\n) -> str:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_131_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'initialize_repo'",
      "description": "Function 'initialize_repo' on line 131 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 131,
      "code_snippet": "\n    logger.info('Patch applied successfully')\n\n\ndef initialize_repo(\n    output_dir: str, issue_number: int, issue_type: str, base_commit: str | None = None\n) -> str:\n    \"\"\"Initialize the repository.\n\n    Args:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_169_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'make_commit'",
      "description": "Function 'make_commit' on line 169 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 169,
      "code_snippet": "\n    return dest_dir\n\n\ndef make_commit(\n    repo_dir: str,\n    issue: Issue,\n    issue_type: str,\n    git_user_name: str = 'openhands',\n    git_user_email: str = 'openhands@all-hands.dev',",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_240_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'send_pull_request'",
      "description": "Function 'send_pull_request' on line 240 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 240,
      "code_snippet": "    if result.returncode != 0:\n        raise RuntimeError(f'Failed to commit changes: {result}')\n\n\ndef send_pull_request(\n    issue: Issue,\n    token: str,\n    username: str | None,\n    platform: ProviderType,\n    patch_dir: str,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_438_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'update_existing_pull_request'",
      "description": "Function 'update_existing_pull_request' on line 438 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 438,
      "code_snippet": "\n    return url\n\n\ndef update_existing_pull_request(\n    issue: Issue,\n    token: str,\n    username: str | None,\n    platform: ProviderType,\n    patch_dir: str,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_131_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'initialize_repo'",
      "description": "Function 'initialize_repo' on line 131 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 131,
      "code_snippet": "\n\ndef initialize_repo(\n    output_dir: str, issue_number: int, issue_type: str, base_commit: str | None = None\n) -> str:\n    \"\"\"Initialize the repository.",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_169_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'make_commit'",
      "description": "Function 'make_commit' on line 169 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 169,
      "code_snippet": "\n\ndef make_commit(\n    repo_dir: str,\n    issue: Issue,\n    issue_type: str,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_240_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'send_pull_request'",
      "description": "Function 'send_pull_request' on line 240 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 240,
      "code_snippet": "\n\ndef send_pull_request(\n    issue: Issue,\n    token: str,\n    username: str | None,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py_438_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'update_existing_pull_request'",
      "description": "Function 'update_existing_pull_request' on line 438 makes critical security, data_modification decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/send_pull_request.py",
      "line_number": 438,
      "code_snippet": "\n\ndef update_existing_pull_request(\n    issue: Issue,\n    token: str,\n    username: str | None,",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nCritical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/resolve_issue.py_8_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'main'",
      "description": "Function 'main' on line 8 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/resolve_issue.py",
      "line_number": 8,
      "code_snippet": "\n\ndef main() -> None:\n    import argparse\n\n    def int_or_none(value: str) -> int | None:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py_291",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 291 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py",
      "line_number": 291,
      "code_snippet": "        if not isinstance(obs, CmdOutputObservation) or obs.exit_code != 0:\n            raise RuntimeError(f'Failed to change directory to /workspace.\\n{obs}')\n\n        if self.platform == ProviderType.GITLAB and self.GITLAB_CI:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py_304",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'obs' flows to 'RuntimeError' on line 304 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py",
      "line_number": 304,
      "code_snippet": "        if not isinstance(obs, CmdOutputObservation) or obs.exit_code != 0:\n            raise RuntimeError(f'Failed to set git config.\\n{obs}')\n\n        # Run setup script if it exists",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py_272_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network/admin operation without confirmation in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 272 performs high-risk execute/network/admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py",
      "line_number": 272,
      "code_snippet": "        openhands_config.sandbox.user_id = sandbox_config.user_id\n\n    def initialize_runtime(\n        self,\n        runtime: Runtime,\n    ) -> None:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py_272_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'initialize_runtime'",
      "description": "Function 'initialize_runtime' on line 272 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_resolver.py",
      "line_number": 272,
      "code_snippet": "        openhands_config.sandbox.user_id = sandbox_config.user_id\n\n    def initialize_runtime(\n        self,\n        runtime: Runtime,\n    ) -> None:",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/utils.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport multiprocessing as mp\nimport os\nimport re\nfrom typing import Callable\n\nfrom pydantic import SecretStr\n\nfrom openhands.controller.state.state import State\nfrom openhands.core.logger import get_console_handler",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_handler_factory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/issue_handler_factory.py",
      "line_number": 1,
      "code_snippet": "from openhands.core.config import LLMConfig\nfrom openhands.integrations.provider import ProviderType\nfrom openhands.resolver.interfaces.azure_devops import AzureDevOpsIssueHandler\nfrom openhands.resolver.interfaces.bitbucket import (\n    BitbucketIssueHandler,\n    BitbucketPRHandler,\n)\nfrom openhands.resolver.interfaces.forgejo import (\n    ForgejoIssueHandler,\n    ForgejoPRHandler,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/bitbucket.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/bitbucket.py",
      "line_number": 1,
      "code_snippet": "import base64\nfrom typing import Any\n\nimport httpx\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.resolver.interfaces.issue import (\n    Issue,\n    IssueHandlerInterface,\n    ReviewThread,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/forgejo.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/forgejo.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom typing import Any\nfrom urllib.parse import quote\n\nimport httpx\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.resolver.interfaces.issue import (\n    Issue,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/gitlab.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/gitlab.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\nfrom urllib.parse import quote\n\nimport httpx\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.resolver.interfaces.issue import (\n    Issue,\n    IssueHandlerInterface,\n    ReviewThread,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py_175",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py",
      "line_number": 175,
      "code_snippet": "\n    def _check_feedback_with_llm(self, prompt: str) -> tuple[bool, str]:\n        \"\"\"Helper function to check feedback with LLM and parse response.\"\"\"\n        response = self.llm.completion(messages=[{'role': 'user', 'content': prompt}])\n\n        answer = response.choices[0].message.content.strip()\n        pattern = r'--- success\\n*(true|false)\\n*--- explanation*\\n((?:.|\\n)*)'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py_402",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py",
      "line_number": 402,
      "code_snippet": "            git_patch=git_patch or self.default_git_patch,\n        )\n\n        response = self.llm.completion(messages=[{'role': 'user', 'content': prompt}])\n\n        answer = response.choices[0].message.content.strip()\n        pattern = r'--- success\\n*(true|false)\\n*--- explanation*\\n((?:.|\\n)*)'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py_173",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/issue_definitions.py",
      "line_number": 173,
      "code_snippet": "            issues=issues_str, repo_instruction=repo_instruction\n        )\n\n        return user_instruction, conversation_instructions, images\n\n    def _check_feedback_with_llm(self, prompt: str) -> tuple[bool, str]:\n        \"\"\"Helper function to check feedback with LLM and parse response.\"\"\"\n        response = self.llm.completion(messages=[{'role': 'user', 'content': prompt}])\n\n        answer = response.choices[0].message.content.strip()\n        pattern = r'--- success\\n*(true|false)\\n*--- explanation*\\n((?:.|\\n)*)'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/azure_devops.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/azure_devops.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport re\nfrom typing import Any\n\nimport httpx\n\nfrom openhands.resolver.interfaces.issue import (\n    Issue,\n    IssueHandlerInterface,\n    ReviewThread,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/github.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/interfaces/github.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nimport httpx\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.resolver.interfaces.issue import (\n    Issue,\n    IssueHandlerInterface,\n    ReviewThread,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/patch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/patch.py",
      "line_number": 1,
      "code_snippet": "# -*- coding: utf-8 -*-\nimport base64\nimport re\nimport zlib\nfrom collections import namedtuple\nfrom typing import Iterable\n\nfrom . import exceptions\nfrom .snippets import findall_regex, split_by_regex\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/snippets.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/snippets.py",
      "line_number": 1,
      "code_snippet": "# -*- coding: utf-8 -*-\n\nimport os\nimport re\nfrom shutil import rmtree\n\n\ndef remove(path: str) -> None:\n    if os.path.exists(path):\n        if os.path.isdir(path):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/apply.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/resolver/patching/apply.py",
      "line_number": 1,
      "code_snippet": "# -*- coding: utf-8 -*-\n\nimport os.path\nimport subprocess\nimport tempfile\n\nfrom .exceptions import HunkApplyException, SubprocessException\nfrom .patch import Change, diffobj\nfrom .snippets import remove, which\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/observation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/observation.py",
      "line_number": 1,
      "code_snippet": "import copy\nfrom typing import Any\n\nfrom openhands.events.event import RecallType\nfrom openhands.events.observation.agent import (\n    AgentCondensationObservation,\n    AgentStateChangedObservation,\n    AgentThinkObservation,\n    MicroagentKnowledge,\n    RecallObservation,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/event.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/event.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import asdict\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom openhands.events import Event, EventSource\nfrom openhands.events.serialization.action import action_from_dict\nfrom openhands.events.serialization.observation import observation_from_dict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/action.py_84_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 84. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/action.py",
      "line_number": 84,
      "code_snippet": "                # Extract the dictionary string between the prefix and the closing parentheses\n                dict_str = code[len(file_editor_prefix) : -2]  # Remove prefix and '))'\n                file_args = ast.literal_eval(dict_str)\n\n                # Update args with the extracted file editor arguments\n                args.update(file_args)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/action.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/serialization/action.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom openhands.core.exceptions import LLMMalformedActionError\nfrom openhands.events.action.action import Action, ActionSecurityRisk\nfrom openhands.events.action.agent import (\n    AgentDelegateAction,\n    AgentFinishAction,\n    AgentRejectAction,\n    AgentThinkAction,\n    ChangeAgentStateAction,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/action/action.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/action/action.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom enum import Enum\nfrom typing import ClassVar\n\nfrom openhands.events.event import Event\n\n\nclass ActionConfirmationStatus(str, Enum):\n    CONFIRMED = 'confirmed'\n    REJECTED = 'rejected'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/action/agent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/action/agent.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass, field\nfrom typing import Any\n\nfrom openhands.core.schema import ActionType\nfrom openhands.events.action.action import Action\nfrom openhands.events.event import RecallType\n\n\n@dataclass\nclass ChangeAgentStateAction(Action):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/observation/files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/observation/files.py",
      "line_number": 1,
      "code_snippet": "\"\"\"File-related observation classes for tracking file operations.\"\"\"\n\nfrom dataclasses import dataclass\nfrom difflib import SequenceMatcher\n\nfrom openhands.core.schema import ObservationType\nfrom openhands.events.event import FileEditSource, FileReadSource\nfrom openhands.events.observation.observation import Observation\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/observation/commands.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/events/observation/commands.py",
      "line_number": 1,
      "code_snippet": "import json\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Any, Self\n\nfrom pydantic import BaseModel\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.core.schema import ObservationType\nfrom openhands.events.observation.observation import Observation",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/gitlab_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/gitlab_service.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.gitlab.service import (\n    GitLabBranchesMixin,\n    GitLabFeaturesMixin,\n    GitLabPRsMixin,\n    GitLabReposMixin,\n    GitLabResolverMixin,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/github_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/github_service.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.github.service import (\n    GitHubBranchesMixin,\n    GitHubFeaturesMixin,\n    GitHubPRsMixin,\n    GitHubReposMixin,\n    GitHubResolverMixin,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/queries.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/queries.py",
      "line_number": 1,
      "code_snippet": "suggested_task_pr_graphql_query = \"\"\"\n    query GetUserPRs($login: String!) {\n        user(login: $login) {\n        pullRequests(first: 50, states: [OPEN], orderBy: {field: UPDATED_AT, direction: DESC}) {\n            nodes {\n            number\n            title\n            repository {\n                nameWithOwner\n            }",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/azure_devops_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/azure_devops_service.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any\n\nimport httpx\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.azure_devops.service.branches import (\n    AzureDevOpsBranchesMixin,\n)\nfrom openhands.integrations.azure_devops.service.features import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/branches.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/branches.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom openhands.integrations.forgejo.service.base import ForgejoMixinBase\nfrom openhands.integrations.service_types import Branch, PaginatedBranchesResponse\n\n\nclass ForgejoBranchesMixin(ForgejoMixinBase):\n    \"\"\"Branch-related operations for Forgejo.\"\"\"\n\n    async def get_branches(self, repository: str) -> list[Branch]:  # type: ignore[override]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/features.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/features.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport base64\nfrom typing import Any\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.forgejo.service.base import ForgejoMixinBase\nfrom openhands.integrations.service_types import (\n    MicroagentContentResponse,\n    MicroagentResponse,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/repos.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/repos.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom openhands.integrations.forgejo.service.base import ForgejoMixinBase\nfrom openhands.integrations.service_types import Repository\nfrom openhands.server.types import AppMode\n\n\nclass ForgejoReposMixin(ForgejoMixinBase):\n    \"\"\"Repository operations for Forgejo.\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/resolver.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/resolver.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom typing import cast\n\nfrom openhands.integrations.forgejo.service.base import ForgejoMixinBase\nfrom openhands.integrations.service_types import Comment\nfrom openhands.resolver.interfaces.issue import ReviewThread\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/prs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/prs.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.forgejo.service.base import ForgejoMixinBase\nfrom openhands.integrations.service_types import RequestMethod, UnknownException\n\n\nclass ForgejoPRsMixin(ForgejoMixinBase):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/forgejo/service/base.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\nfrom typing import Any\nfrom urllib.parse import urlparse\n\nimport httpx\nfrom pydantic import SecretStr\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/work_items.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/work_items.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Work item operations for Azure DevOps integration.\"\"\"\n\nfrom datetime import datetime\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.azure_devops.service.base import AzureDevOpsMixinBase\nfrom openhands.integrations.service_types import Comment, RequestMethod\n\n\nclass AzureDevOpsWorkItemsMixin(AzureDevOpsMixinBase):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/branches.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/branches.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Branch operations for Azure DevOps integration.\"\"\"\n\nfrom openhands.integrations.azure_devops.service.base import AzureDevOpsMixinBase\nfrom openhands.integrations.service_types import Branch, PaginatedBranchesResponse\n\n\nclass AzureDevOpsBranchesMixin(AzureDevOpsMixinBase):\n    \"\"\"Mixin for Azure DevOps branch operations.\"\"\"\n\n    async def get_branches(self, repository: str) -> list[Branch]:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/features.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/features.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Feature operations for Azure DevOps integration (microagents, suggested tasks, user).\"\"\"\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.azure_devops.service.base import AzureDevOpsMixinBase\nfrom openhands.integrations.service_types import (\n    MicroagentContentResponse,\n    ProviderType,\n    RequestMethod,\n    SuggestedTask,\n    TaskType,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/repos.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/repos.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Repository operations for Azure DevOps integration.\"\"\"\n\nfrom openhands.integrations.azure_devops.service.base import AzureDevOpsMixinBase\nfrom openhands.integrations.service_types import ProviderType, Repository\nfrom openhands.server.types import AppMode\n\n\nclass AzureDevOpsReposMixin(AzureDevOpsMixinBase):\n    \"\"\"Mixin for Azure DevOps repository operations.\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/resolver.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/resolver.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.azure_devops.service.base import AzureDevOpsMixinBase\nfrom openhands.integrations.service_types import Comment\n\n\nclass AzureDevOpsResolverMixin(AzureDevOpsMixinBase):\n    \"\"\"Helper methods used for the Azure DevOps Resolver.\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/prs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/azure_devops/service/prs.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Pull request operations for Azure DevOps integration.\"\"\"\n\nfrom datetime import datetime\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.azure_devops.service.base import AzureDevOpsMixinBase\nfrom openhands.integrations.service_types import Comment, RequestMethod\n\n\nclass AzureDevOpsPRsMixin(AzureDevOpsMixinBase):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/branches.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/branches.py",
      "line_number": 1,
      "code_snippet": "from openhands.integrations.bitbucket.service.base import BitBucketMixinBase\nfrom openhands.integrations.service_types import Branch, PaginatedBranchesResponse\n\n\nclass BitBucketBranchesMixin(BitBucketMixinBase):\n    \"\"\"\n    Mixin for BitBucket branch-related operations\n    \"\"\"\n\n    async def get_branches(self, repository: str) -> list[Branch]:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/repos.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/repos.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom typing import Any\nfrom urllib.parse import urlparse\n\nfrom openhands.integrations.bitbucket.service.base import BitBucketMixinBase\nfrom openhands.integrations.service_types import Repository, SuggestedTask\nfrom openhands.server.types import AppMode\n\n\nclass BitBucketReposMixin(BitBucketMixinBase):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/prs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/prs.py",
      "line_number": 1,
      "code_snippet": "from openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.bitbucket.service.base import BitBucketMixinBase\nfrom openhands.integrations.service_types import RequestMethod\n\n\nclass BitBucketPRsMixin(BitBucketMixinBase):\n    \"\"\"\n    Mixin for BitBucket pull request operations\n    \"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/bitbucket/service/base.py",
      "line_number": 1,
      "code_snippet": "import base64\nfrom typing import Any\n\nimport httpx\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.protocols.http_client import HTTPClient\nfrom openhands.integrations.service_types import (\n    BaseGitService,\n    OwnerType,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/branches_prs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/branches_prs.py",
      "line_number": 1,
      "code_snippet": "from openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.queries import (\n    search_branches_graphql_query,\n)\nfrom openhands.integrations.github.service.base import GitHubMixinBase\nfrom openhands.integrations.service_types import Branch, PaginatedBranchesResponse\n\n\nclass GitHubBranchesMixin(GitHubMixinBase):\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/features.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/features.py",
      "line_number": 1,
      "code_snippet": "import base64\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.queries import (\n    suggested_task_issue_graphql_query,\n    suggested_task_pr_graphql_query,\n)\nfrom openhands.integrations.github.service.base import GitHubMixinBase\nfrom openhands.integrations.service_types import (\n    MicroagentContentResponse,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/repos.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/repos.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.service.base import GitHubMixinBase\nfrom openhands.integrations.service_types import OwnerType, ProviderType, Repository\nfrom openhands.server.types import AppMode\n\n\nclass GitHubReposMixin(GitHubMixinBase):\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/resolver.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/resolver.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import Any\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.queries import (\n    get_review_threads_graphql_query,\n    get_thread_comments_graphql_query,\n    get_thread_from_comment_graphql_query,\n)\nfrom openhands.integrations.github.service.base import GitHubMixinBase",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/prs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/prs.py",
      "line_number": 1,
      "code_snippet": "from openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.service.base import GitHubMixinBase\nfrom openhands.integrations.service_types import RequestMethod\n\n\nclass GitHubPRsMixin(GitHubMixinBase):\n    \"\"\"\n    Methods for interacting with GitHub PRs\n    \"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/github/service/base.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any, cast\n\nimport httpx\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.protocols.http_client import HTTPClient\nfrom openhands.integrations.service_types import (\n    BaseGitService,\n    RequestMethod,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/branches.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/branches.py",
      "line_number": 1,
      "code_snippet": "from openhands.integrations.gitlab.service.base import GitLabMixinBase\nfrom openhands.integrations.service_types import Branch, PaginatedBranchesResponse\n\n\nclass GitLabBranchesMixin(GitLabMixinBase):\n    \"\"\"\n    Methods for interacting with GitLab branches\n    \"\"\"\n\n    async def get_branches(self, repository: str) -> list[Branch]:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/features.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/features.py",
      "line_number": 1,
      "code_snippet": "from openhands.integrations.gitlab.service.base import GitLabMixinBase\nfrom openhands.integrations.service_types import (\n    MicroagentContentResponse,\n    ProviderType,\n    RequestMethod,\n    SuggestedTask,\n    TaskType,\n)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/repos.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/repos.py",
      "line_number": 1,
      "code_snippet": "from openhands.integrations.gitlab.service.base import GitLabMixinBase\nfrom openhands.integrations.service_types import OwnerType, ProviderType, Repository\nfrom openhands.server.types import AppMode\n\n\nclass GitLabReposMixin(GitLabMixinBase):\n    \"\"\"\n    Methods for interacting with GitLab repositories\n    \"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/resolver.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/resolver.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nfrom openhands.integrations.gitlab.service.base import GitLabMixinBase\nfrom openhands.integrations.service_types import Comment\n\n\nclass GitLabResolverMixin(GitLabMixinBase):\n    \"\"\"\n    Helper methods used for the GitLab Resolver\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/prs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/prs.py",
      "line_number": 1,
      "code_snippet": "from openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.gitlab.service.base import GitLabMixinBase\nfrom openhands.integrations.service_types import RequestMethod\n\n\nclass GitLabPRsMixin(GitLabMixinBase):\n    \"\"\"\n    Methods for interacting with GitLab merge requests (PRs)\n    \"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/integrations/gitlab/service/base.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nimport httpx\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.protocols.http_client import HTTPClient\nfrom openhands.integrations.service_types import (\n    BaseGitService,\n    RequestMethod,\n    UnknownException,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state_tracker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state_tracker.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom openhands.controller.state.control_flags import (\n    BudgetControlFlag,\n    IterationControlFlag,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py_12_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 12. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py",
      "line_number": 12,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py_129_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in 'save_to_session'",
      "description": "Function 'save_to_session' on line 129 exposes sklearn model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py",
      "line_number": 129,
      "code_snippet": "    metrics: Metrics = field(default_factory=Metrics)\n\n    def save_to_session(\n        self, sid: str, file_store: FileStore, user_id: str | None\n    ) -> None:\n        conversation_stats = self.conversation_stats",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets with signed URLs\n   - Never store in /static or /public directories\n   - Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model encryption\n   - Implement model quantization\n   - Remove unnecessary metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor access:\n   - Track who downloads models\n   - Alert on unauthorized access\n   - Maintain audit logs"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/controller/state/state.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nimport base64",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_metadata.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_metadata.py",
      "line_number": 12,
      "code_snippet": "    GUI = 'gui'\n    SUGGESTED_TASK = 'suggested_task'\n    REMOTE_API_KEY = 'openhands_api'\n    SLACK = 'slack'\n    MICROAGENT_MANAGEMENT = 'microagent_management'",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_metadata.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_metadata.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom enum import Enum\n\nfrom openhands.integrations.service_types import ProviderType\n\n\nclass ConversationTrigger(Enum):\n    RESOLVER = 'resolver'\n    GUI = 'gui'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_status.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/conversation_status.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nThis class is similar to the RuntimeStatus defined in the runtime api. (When this class was defined\na RuntimeStatus class already existed in OpenHands which serves a completely different purpose) Some of\nthe status definitions do not match up:\n\nSTOPPED/paused - the runtime is not running but may be restarted\nARCHIVED/stopped - the runtime is not running and will not restart due to deleted files.\n\"\"\"\n\nfrom enum import Enum",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/secrets.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/secrets.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Mapping\nfrom types import MappingProxyType\nfrom typing import Any\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SerializationInfo,\n    field_serializer,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/data_models/settings.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    SerializationInfo,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/settings/file_settings_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/settings/file_settings_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.storage import get_file_store\nfrom openhands.storage.data_models.settings import Settings\nfrom openhands.storage.files import FileStore\nfrom openhands.storage.settings.settings_store import SettingsStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/secrets/file_secrets_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/secrets/file_secrets_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.storage import get_file_store\nfrom openhands.storage.data_models.secrets import Secrets\nfrom openhands.storage.files import FileStore\nfrom openhands.storage.secrets.secrets_store import SecretsStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/conversation/conversation_validator.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/conversation/conversation_validator.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom datetime import datetime, timezone\n\nfrom openhands.core.config.utils import load_openhands_config\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.server.config.server_config import ServerConfig\nfrom openhands.storage.conversation.conversation_store import ConversationStore\nfrom openhands.storage.data_models.conversation_metadata import ConversationMetadata\nfrom openhands.utils.conversation_summary import get_default_conversation_title\nfrom openhands.utils.import_utils import get_impl",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/conversation/file_conversation_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/storage/conversation/file_conversation_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom pydantic import TypeAdapter\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/conversation_manager/standalone_conversation_manager.py_97",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/conversation_manager/standalone_conversation_manager.py",
      "line_number": 97,
      "code_snippet": "        if self._cleanup_task:\n            self._cleanup_task.cancel()\n            self._cleanup_task = None\n        get_runtime_cls(self.config.runtime).teardown(self.config)\n\n    async def attach_to_conversation(\n        self, sid: str, user_id: str | None = None",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/conversation_manager/docker_nested_conversation_manager.py_554",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/conversation_manager/docker_nested_conversation_manager.py",
      "line_number": 554,
      "code_snippet": "        # This session is created here only because it is the easiest way to get a runtime, which\n        # is the easiest way to create the needed docker container\n\n        config: OpenHandsConfig = ExperimentManagerImpl.run_config_variant_test(\n            user_id, sid, self.config\n        )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/__init__.py",
      "line_number": 1,
      "code_snippet": "from fastapi import Request\nfrom pydantic import SecretStr\n\nfrom openhands.integrations.provider import PROVIDER_TOKEN_TYPE\nfrom openhands.server.settings import Settings\nfrom openhands.server.user_auth.user_auth import AuthType, get_user_auth\nfrom openhands.storage.data_models.secrets import Secrets\nfrom openhands.storage.secrets.secrets_store import SecretsStore\nfrom openhands.storage.settings.settings_store import SettingsStore\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/user_auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/user_auth.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nfrom __future__ import annotations\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/default_user_auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/user_auth/default_user_auth.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nfrom dataclasses import dataclass\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/config/server_config.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Base64 High Entropy String",
      "description": "detect-secrets found a potential Base64 High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/config/server_config.py",
      "line_number": 19,
      "code_snippet": "    config_cls = os.environ.get('OPENHANDS_CONFIG_CLS', None)\n    app_mode = AppMode.OPENHANDS\n    posthog_client_key = 'phc_3ESMmY9SgqEAGBB6sMGK5ayYHkeUuknH2vP6FmWH9RA'\n    github_client_id = os.environ.get('GITHUB_APP_CLIENT_ID', '')\n    enable_billing = os.environ.get('ENABLE_BILLING', 'false') == 'true'",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/config/server_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/config/server_config.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport os\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/files.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport os\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/git.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/git.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nfrom types import MappingProxyType\nfrom typing import Annotated, cast",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/conversation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/conversation.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport uuid\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/feedback.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/feedback.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nfrom fastapi import APIRouter, Depends, Request, status\nfrom fastapi.responses import JSONResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/manage_conversations.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/manage_conversations.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport asyncio\nimport base64",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/mcp.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/mcp.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nfrom typing import Annotated\n\nfrom fastmcp import FastMCP\nfrom fastmcp.exceptions import ToolError\nfrom fastmcp.server.dependencies import get_http_request\nfrom pydantic import Field\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/secrets.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/secrets.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nfrom fastapi import APIRouter, Depends, status\nfrom fastapi.responses import JSONResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/routes/settings.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nfrom fastapi import APIRouter, Depends, status\nfrom fastapi.responses import JSONResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_service.py_145",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_service.py",
      "line_number": 145,
      "code_snippet": "\n    conversation_init_data = ConversationInitData(**session_init_args)\n\n    conversation_init_data = ExperimentManagerImpl.run_conversation_variant_test(\n        user_id, conversation_id, conversation_init_data\n    )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_service.py_286",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_service.py",
      "line_number": 286,
      "code_snippet": "\n    conversation_init_data = ConversationInitData(**session_init_args)\n    # We should recreate the same experiment conditions when restarting a conversation\n    return ExperimentManagerImpl.run_conversation_variant_test(\n        user_id, conversation_id, conversation_init_data\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py_10_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 10. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py",
      "line_number": 10,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py_42_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in 'save_metrics'",
      "description": "Function 'save_metrics' on line 42 exposes sklearn model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py",
      "line_number": 42,
      "code_snippet": "        self.maybe_restore_metrics()\n\n    def save_metrics(self):\n        if not self.file_store:\n            return\n",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets with signed URLs\n   - Never store in /static or /public directories\n   - Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model encryption\n   - Implement model quantization\n   - Remove unnecessary metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor access:\n   - Track who downloads models\n   - Alert on unauthorized access\n   - Maintain audit logs"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/services/conversation_stats.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport base64\nimport pickle",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/agent_session.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/agent_session.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n# This module belongs to the old V0 web server. The V1 application server lives under openhands/app_server/.\nimport asyncio\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_470",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 470,
      "code_snippet": "        self, msg_type: str, runtime_status: RuntimeStatus, message: str\n    ) -> None:\n        \"\"\"Queues a status message to be sent asynchronously.\"\"\"\n        asyncio.run_coroutine_threadsafe(\n            self._send_status_message(msg_type, runtime_status, message), self.loop\n        )\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_82",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '__init__' on line 82 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 82,
      "code_snippet": "    def __init__(\n        self,\n        sid: str,\n        config: OpenHandsConfig,\n        llm_registry: LLMRegistry,\n        conversation_stats: ConversationStats,\n        file_store: FileStore,\n        sio: socketio.AsyncServer | None,\n        user_id: str | None = None,\n    ):\n        self.sid = sid",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_466",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'queue_status_message' on line 466 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 466,
      "code_snippet": "    def queue_status_message(\n        self, msg_type: str, runtime_status: RuntimeStatus, message: str\n    ) -> None:\n        \"\"\"Queues a status message to be sent asynchronously.\"\"\"\n        asyncio.run_coroutine_threadsafe(\n            self._send_status_message(msg_type, runtime_status, message), self.loop\n        )\n\n\n# Backward-compatible alias for external imports that still reference\n# openhands.server.session.session import Session",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_82_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in '__init__'",
      "description": "Function '__init__' on line 82 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 82,
      "code_snippet": "    logger: LoggerAdapter\n\n    def __init__(\n        self,\n        sid: str,\n        config: OpenHandsConfig,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_321_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'on_event'",
      "description": "Function 'on_event' on line 321 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 321,
      "code_snippet": "        )\n\n    def on_event(self, event: Event) -> None:\n        asyncio.get_event_loop().run_until_complete(self._on_event(event))\n\n    async def _on_event(self, event: Event) -> None:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_466_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'queue_status_message'",
      "description": "Function 'queue_status_message' on line 466 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 466,
      "code_snippet": "        )\n\n    def queue_status_message(\n        self, msg_type: str, runtime_status: RuntimeStatus, message: str\n    ) -> None:\n        \"\"\"Queues a status message to be sent asynchronously.\"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_115",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 115,
      "code_snippet": "        # Lazy import to avoid circular dependency\n        from openhands.experiments.experiment_manager import ExperimentManagerImpl\n\n        self.config = ExperimentManagerImpl.run_config_variant_test(\n            user_id, sid, self.config\n        )\n        self.loop = asyncio.get_event_loop()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py_466",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/server/session/session.py",
      "line_number": 466,
      "code_snippet": "                'id': runtime_status.value,\n                'message': message,\n            }\n        )\n\n    def queue_status_message(\n        self, msg_type: str, runtime_status: RuntimeStatus, message: str\n    ) -> None:\n        \"\"\"Queues a status message to be sent asynchronously.\"\"\"\n        asyncio.run_coroutine_threadsafe(\n            self._send_status_message(msg_type, runtime_status, message), self.loop",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/log_capture.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/log_capture.py",
      "line_number": 1,
      "code_snippet": "import io\nimport logging\nfrom contextlib import asynccontextmanager\n\n\n@asynccontextmanager\nasync def capture_logs(logger_name, level=logging.ERROR):\n    logger = logging.getLogger(logger_name)\n\n    # Store original handlers and level",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/files.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom pathlib import Path\n\nfrom openhands.events.observation import (\n    ErrorObservation,\n    FileReadObservation,\n    FileWriteObservation,\n    Observation,\n)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/system.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/system.py",
      "line_number": 1,
      "code_snippet": "import random\nimport socket\nimport time\n\n\ndef check_port_available(port: int) -> bool:\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.bind(('0.0.0.0', port))\n        return True",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/command.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/command.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom openhands.core.config import OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.runtime.plugins import PluginRequirement\n\nDEFAULT_PYTHON_PREFIX = [\n    '/openhands/micromamba/bin/micromamba',\n    'run',\n    '-n',",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py_32",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on line 32 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
      "line_number": 32,
      "code_snippet": "    if result.returncode != 0:\n        raise RuntimeError(\n            f'error_running_cmd:{result.returncode}:{byte_content.decode()}'\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py_25_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run'",
      "description": "Function 'run' on line 25 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
      "line_number": 25,
      "code_snippet": "        if path.parent == path:\n            return None\n\n\ndef run(cmd: str, cwd: str) -> str:\n    result = subprocess.run(\n        args=cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd\n    )\n    byte_content = result.stderr or result.stdout or b''\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py_25_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run'",
      "description": "Function 'run' on line 25 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
      "line_number": 25,
      "code_snippet": "\n\ndef run(cmd: str, cwd: str) -> str:\n    result = subprocess.run(\n        args=cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd\n    )",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_diff.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"Get git diff in a single git file for the closest git repo in the file system\nNOTE: Since this is run as a script, there should be no imports from project files!\n\"\"\"\n\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_91",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to 'RuntimeError' on line 91 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 91,
      "code_snippet": "            if output.returncode != 0:\n                raise RuntimeError(f'Failed to add sudoer: {output.stderr.decode()}')\n            logger.debug(\n                f'Added sudoer successfully. Output: [{output.stdout.decode()}]'",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_106",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'output' flows to 'RuntimeError' on line 106 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 106,
      "code_snippet": "            else:\n                raise RuntimeError(\n                    f'Failed to create user `{username}` with UID {user_id}. Output: [{output.stderr.decode()}]'\n                )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_8",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'init_user_and_working_directory' on line 8 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 8,
      "code_snippet": "def init_user_and_working_directory(\n    username: str, user_id: int, initial_cwd: str\n) -> int | None:\n    \"\"\"Create working directory and user if not exists.\n    It performs the following steps effectively:\n    * Creates the Working Directory:\n        - Uses mkdir -p to create the directory.\n        - Sets ownership to username:group (respects SANDBOX_GROUP_ID if set).\n        - Adjusts permissions to be readable and writable by group and others.\n    * User Verification and Creation:\n        - Checks if the user exists using id -u.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_8_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'init_user_and_working_directory'",
      "description": "Function 'init_user_and_working_directory' on line 8 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 8,
      "code_snippet": "\nfrom openhands.core.logger import openhands_logger as logger\n\n\ndef init_user_and_working_directory(\n    username: str, user_id: int, initial_cwd: str\n) -> int | None:\n    \"\"\"Create working directory and user if not exists.\n    It performs the following steps effectively:\n    * Creates the Working Directory:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_8_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'init_user_and_working_directory'",
      "description": "Function 'init_user_and_working_directory' on line 8 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 8,
      "code_snippet": "\n\ndef init_user_and_working_directory(\n    username: str, user_id: int, initial_cwd: str\n) -> int | None:\n    \"\"\"Create working directory and user if not exists.",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_113",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 113,
      "code_snippet": "    # First create the working directory, independent of the user\n    logger.debug(f'Client working directory: {initial_cwd}')\n    command = f'umask 002; mkdir -p {initial_cwd}'\n    output = subprocess.run(command, shell=True, capture_output=True)\n    out_str = output.stdout.decode()\n\n    # Get group ID from environment variable, default to 'root' for backward compatibility",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_119",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 119,
      "code_snippet": "    # Get group ID from environment variable, default to 'root' for backward compatibility\n    group_id = os.getenv('SANDBOX_GROUP_ID', 'root')\n    command = f'chown -R {username}:{group_id} {initial_cwd}'\n    output = subprocess.run(command, shell=True, capture_output=True)\n    out_str += output.stdout.decode()\n\n    command = f'chmod g+rw {initial_cwd}'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_59",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 59,
      "code_snippet": "        logger.debug(f'Attempting to create user `{username}` with UID {user_id}.')\n        setup_user = True\n        try:\n            result = subprocess.run(\n                f'id -u {username}', shell=True, check=True, capture_output=True\n            )\n            existing_user_id = int(result.stdout.decode().strip())",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_89",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 89,
      "code_snippet": "        if setup_user:\n            # Add sudoer\n            sudoer_line = r\"echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\"\n            output = subprocess.run(sudoer_line, shell=True, capture_output=True)\n            if output.returncode != 0:\n                raise RuntimeError(f'Failed to add sudoer: {output.stderr.decode()}')\n            logger.debug(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py_100",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_init.py",
      "line_number": 100,
      "code_snippet": "                f'useradd -rm -d /home/{username} -s /bin/bash '\n                f'-g root -G sudo -u {user_id} {username}'\n            )\n            output = subprocess.run(command, shell=True, capture_output=True)\n            if output.returncode == 0:\n                logger.debug(\n                    f'Added user `{username}` successfully with UID {user_id}. Output: [{output.stdout.decode()}]'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/bash.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/bash.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nimport time\nimport uuid\nfrom enum import Enum\nfrom typing import Any\n\nimport bashlex\nimport libtmux\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/request.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/request.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Any\n\nimport httpx\nfrom tenacity import retry, retry_if_exception, stop_after_attempt, wait_exponential\n\nfrom openhands.utils.http_session import HttpSession\nfrom openhands.utils.tenacity_stop import stop_if_should_exit\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_103",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'resp' flows to '_extract_code' on line 103 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 103,
      "code_snippet": "        resp = llm.completion(messages=messages)\n        new_contents = _extract_code(resp['choices'][0]['message']['content'])\n        if new_contents is not None:\n            return new_contents",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_430",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to 'codeact_function_calling.response_to_actions' on line 430 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 430,
      "code_snippet": "            response = self.draft_editor_llm.completion(**params)\n            actions = codeact_function_calling.response_to_actions(response)\n            if len(actions) != 1:\n                return error_obs",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_90",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_new_file_contents' on line 90 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 90,
      "code_snippet": "def get_new_file_contents(\n    llm: LLM, old_contents: str, draft_changes: str, num_retries: int = 3\n) -> str | None:\n    while num_retries > 0:\n        messages = [\n            {\n                'role': 'user',\n                'content': USER_MSG.format(\n                    old_contents=old_contents, draft_changes=draft_changes\n                ),\n            },",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_407_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'correct_edit'",
      "description": "Function 'correct_edit' on line 407 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 407,
      "code_snippet": "        return correct_num < retry_num\n\n    def correct_edit(\n        self, file_content: str, error_obs: ErrorObservation, retry_num: int = 0\n    ) -> Observation:\n        import openhands.agenthub.codeact_agent.function_calling as codeact_function_calling",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_90_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_new_file_contents'",
      "description": "Function 'get_new_file_contents' on line 90 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 90,
      "code_snippet": "\n\ndef get_new_file_contents(\n    llm: LLM, old_contents: str, draft_changes: str, num_retries: int = 3\n) -> str | None:\n    while num_retries > 0:",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_102",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 102,
      "code_snippet": "                ),\n            },\n        ]\n        resp = llm.completion(messages=messages)\n        new_contents = _extract_code(resp['choices'][0]['message']['content'])\n        if new_contents is not None:\n            return new_contents",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py_429",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/edit.py",
      "line_number": 429,
      "code_snippet": "        ]\n        params: dict = {'messages': messages, 'tools': tools}\n        try:\n            response = self.draft_editor_llm.completion(**params)\n            actions = codeact_function_calling.response_to_actions(response)\n            if len(actions) != 1:\n                return error_obs",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/port_lock.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/port_lock.py",
      "line_number": 1,
      "code_snippet": "\"\"\"File-based port locking system for preventing race conditions in port allocation.\"\"\"\n\nimport os\nimport random\nimport socket\nimport tempfile\nimport time\nfrom typing import Optional\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_build.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/runtime_build.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport hashlib\nimport os\nimport shutil\nimport string\nimport tempfile\nfrom enum import Enum\nfrom pathlib import Path\n\nimport docker",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py_20",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'result' flows to 'RuntimeError' on line 20 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
      "line_number": 20,
      "code_snippet": "    if result.returncode != 0:\n        raise RuntimeError(\n            f'error_running_cmd:{result.returncode}:{byte_content.decode()}'\n        )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py_13_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run'",
      "description": "Function 'run' on line 13 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
      "line_number": 13,
      "code_snippet": "import subprocess\nfrom pathlib import Path\n\n\ndef run(cmd: str, cwd: str) -> str:\n    result = subprocess.run(\n        args=cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd\n    )\n    byte_content = result.stderr or result.stdout or b''\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py_13_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run'",
      "description": "Function 'run' on line 13 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
      "line_number": 13,
      "code_snippet": "\n\ndef run(cmd: str, cwd: str) -> str:\n    result = subprocess.run(\n        args=cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd\n    )",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_changes.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"Get git changes in the current working directory relative to the remote origin if possible.\nNOTE: Since this is run as a script, there should be no imports from project files!\n\"\"\"\n\nimport glob\nimport json\nimport os\nimport subprocess\nfrom pathlib import Path",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_77",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 77 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 77,
      "code_snippet": "            try:\n                completed = subprocess.run(\n                    [str(pwsh_path), '--version'],\n                    capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_102",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 102 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 102,
      "code_snippet": "            try:\n                completed = subprocess.run(\n                    [str(exe_path), '--version'],\n                    capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_51_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'find_latest_pwsh_sdk_path'",
      "description": "Function 'find_latest_pwsh_sdk_path' on line 51 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 51,
      "code_snippet": "    logger.error(f'{error_msg} Details: {details}')\n    raise DotNetMissingError(error_msg, details)\n\n\ndef find_latest_pwsh_sdk_path(\n    executable_name='pwsh.exe',\n    dll_name='System.Management.Automation.dll',\n    min_version=(7, 0, 0),\n    env_var='PWSH_DIR',\n):",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_187_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in '__init__'",
      "description": "Function '__init__' on line 187 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 187,
      "code_snippet": "    \"\"\"\n\n    def __init__(\n        self,\n        work_dir: str,\n        username: str | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_1433_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/execute/network operation without confirmation in 'close'",
      "description": "Function 'close' on line 1433 performs high-risk delete/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 1433,
      "code_snippet": "        )\n\n    def close(self) -> None:\n        \"\"\"Closes the PowerShell runspace and releases resources, stopping any active job.\"\"\"\n        if self._closed:\n            return",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_51_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'find_latest_pwsh_sdk_path'",
      "description": "Function 'find_latest_pwsh_sdk_path' on line 51 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 51,
      "code_snippet": "\n\ndef find_latest_pwsh_sdk_path(\n    executable_name='pwsh.exe',\n    dll_name='System.Management.Automation.dll',\n    min_version=(7, 0, 0),",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/windows_bash.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This module provides a Windows-specific implementation for running commands\nin a PowerShell session using the pythonnet library to interact with the .NET\nPowerShell SDK directly. This aims to provide a more robust and integrated\nway to manage PowerShell processes compared to using temporary script files.\n\"\"\"\n\nimport os\nimport re\nimport subprocess\nimport time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_handler.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/git_handler.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.runtime.utils import git_changes, git_diff\n\nGIT_CHANGES_CMD = 'python3 /openhands/code/openhands/runtime/utils/git_changes.py'\nGIT_DIFF_CMD = (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/system_stats.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/system_stats.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utilities for getting system resource statistics.\"\"\"\n\nimport time\n\nimport psutil\n\n_start_time = time.time()\n_last_execution_time = time.time()\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/file_viewer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/utils/file_viewer.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utility module for generating file viewer HTML content.\"\"\"\n\nimport base64\nimport mimetypes\nimport os\n\n\ndef generate_file_viewer_html(file_path: str) -> str:\n    \"\"\"Generate HTML content for viewing different file types.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/base64.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/base64.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport io\n\nimport numpy as np\nfrom PIL import Image\n\n\ndef image_to_png_base64_url(\n    image: np.ndarray | Image.Image, add_data_prefix: bool = False\n) -> str:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/utils.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport datetime\nimport os\nfrom pathlib import Path\nfrom typing import Any\n\nfrom browsergym.utils.obs import flatten_axtree_to_str\nfrom PIL import Image\n\nfrom openhands.core.exceptions import BrowserUnavailableException",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/browser_env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/browser/browser_env.py",
      "line_number": 1,
      "code_snippet": "import atexit\nimport json\nimport multiprocessing\nimport os\nimport time\nimport uuid\nfrom pathlib import Path\n\nimport browsergym.core  # noqa F401 (we register the openended task as a gym environment)\nimport gymnasium as gym",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/remote.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/remote.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport io\nimport tarfile\nimport time\n\nimport httpx\n\nfrom openhands.core.exceptions import AgentRuntimeBuildError\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.runtime.builder import RuntimeBuilder",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_45",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 45 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 45,
      "code_snippet": "        try:\n            result = subprocess.run(\n                ['docker' if not is_podman else 'podman', 'buildx', 'version'],\n                capture_output=True,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_122",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'subprocess.' on line 122 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 122,
      "code_snippet": "                try:\n                    subprocess.run(\n                        cmd, shell=True, check=True, stdout=subprocess.DEVNULL\n                    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_54",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'build' on line 54 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 54,
      "code_snippet": "    def build(\n        self,\n        path: str,\n        tags: list[str],\n        platform: str | None = None,\n        extra_build_args: list[str] | None = None,\n        use_local_cache: bool = False,\n    ) -> str:\n        \"\"\"Builds a Docker image using BuildKit and handles the build logs appropriately.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_42_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'check_buildx'",
      "description": "Function 'check_buildx' on line 42 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 42,
      "code_snippet": "\n        self.rolling_logger = RollingLogger(max_lines=10)\n\n    @staticmethod\n    def check_buildx(is_podman: bool = False) -> bool:\n        \"\"\"Check if Docker Buildx is available.\"\"\"\n        try:\n            result = subprocess.run(\n                ['docker' if not is_podman else 'podman', 'buildx', 'version'],\n                capture_output=True,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_54_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'build'",
      "description": "Function 'build' on line 54 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 54,
      "code_snippet": "            return result.returncode == 0\n        except FileNotFoundError:\n            return False\n\n    def build(\n        self,\n        path: str,\n        tags: list[str],\n        platform: str | None = None,\n        extra_build_args: list[str] | None = None,",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_42_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'check_buildx'",
      "description": "Function 'check_buildx' on line 42 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 42,
      "code_snippet": "\n    @staticmethod\n    def check_buildx(is_podman: bool = False) -> bool:\n        \"\"\"Check if Docker Buildx is available.\"\"\"\n        try:\n            result = subprocess.run(",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_54_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'build'",
      "description": "Function 'build' on line 54 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 54,
      "code_snippet": "            return False\n\n    def build(\n        self,\n        path: str,\n        tags: list[str],",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/builder/docker.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport os\nimport subprocess\nimport time\n\nimport docker\n\nfrom openhands.core.exceptions import AgentRuntimeBuildError\nfrom openhands.core.logger import RollingLogger\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/mcp/proxy/manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/mcp/proxy/manager.py",
      "line_number": 1,
      "code_snippet": "\"\"\"MCP Proxy Manager for OpenHands.\n\nThis module provides a manager class for handling FastMCP proxy instances,\nincluding initialization, configuration, and mounting to FastAPI applications.\n\"\"\"\n\nimport logging\nfrom typing import Any, Optional\n\nfrom anyio import get_cancelled_exc_class",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/vscode/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/vscode/__init__.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\nimport shutil\nimport sys\nimport uuid\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional\nfrom urllib.parse import urlparse\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/jupyter/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/jupyter/__init__.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\nimport subprocess\nimport sys\nimport time\nfrom dataclasses import dataclass\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action import Action, IPythonRunCellAction\nfrom openhands.events.observation import IPythonRunCellObservation",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/jupyter/execute_server.py_292_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'make_app'",
      "description": "Function 'make_app' on line 292 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/jupyter/execute_server.py",
      "line_number": 292,
      "code_snippet": "\n\ndef make_app() -> tornado.web.Application:\n    jupyter_kernel = JupyterKernel(\n        f'localhost:{os.environ.get(\"JUPYTER_GATEWAY_PORT\", \"8888\")}',\n        os.environ.get('JUPYTER_GATEWAY_KERNEL_ID', 'default'),",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/jupyter/execute_server.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/jupyter/execute_server.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\nimport asyncio\nimport logging\nimport os\nimport re\nfrom uuid import uuid4\n\nimport tornado\nimport tornado.websocket",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/agentskills.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/agentskills.py",
      "line_number": 1,
      "code_snippet": "from inspect import signature\n\nfrom openhands.runtime.plugins.agent_skills import file_ops, file_reader\nfrom openhands.runtime.plugins.agent_skills.utils.dependency import import_functions\n\nimport_functions(\n    module=file_ops, function_names=file_ops.__all__, target_globals=globals()\n)\nimport_functions(\n    module=file_reader, function_names=file_reader.__all__, target_globals=globals()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_ops/file_ops.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_ops/file_ops.py",
      "line_number": 1,
      "code_snippet": "\"\"\"File operations module for OpenHands agent.\n\nThis module provides a collection of file manipulation skills that enable the OpenHands\nagent to perform various file operations such as opening, searching, and navigating\nthrough files and directories.\n\nFunctions:\n- open_file(path: str, line_number: int | None = 1, context_lines: int = 100): Opens a file and optionally moves to a specific line.\n- goto_line(line_number: int): Moves the window to show the specified line number.\n- scroll_down(): Moves the window down by the number of lines specified in WINDOW.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_155_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model '_get_openai_model()' is used without version pinning on line 155. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 155,
      "code_snippet": "    try:\n        base64_image = _base64_img(file_path)\n        response = _get_openai_client().chat.completions.create(\n            model=_get_openai_model(),\n            messages=_prepare_image_messages(task, base64_image),\n            max_tokens=_get_max_token(),",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_200_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model '_get_openai_model()' is used without version pinning on line 200. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 200,
      "code_snippet": "        # TODO: record the COST of the API call\n        try:\n            response = _get_openai_client().chat.completions.create(\n                model=_get_openai_model(),\n                messages=_prepare_image_messages(task, base64_frame),\n                max_tokens=_get_max_token(),",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_142_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'parse_image'",
      "description": "Function 'parse_image' on line 142 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 142,
      "code_snippet": "\n\ndef parse_image(\n    file_path: str, task: str = 'Describe this image as detail as possible.'\n) -> None:\n    \"\"\"Parses the content of an image file and prints the description.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_167_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'parse_video'",
      "description": "Function 'parse_video' on line 167 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 167,
      "code_snippet": "\n\ndef parse_video(\n    file_path: str,\n    task: str = 'Describe this image as detail as possible.',\n    frame_interval: int = 30,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_142_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'parse_image'",
      "description": "Function 'parse_image' on line 142 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 142,
      "code_snippet": "\n\ndef parse_image(\n    file_path: str, task: str = 'Describe this image as detail as possible.'\n) -> None:\n    \"\"\"Parses the content of an image file and prints the description.",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_167_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'parse_video'",
      "description": "Function 'parse_video' on line 167 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 167,
      "code_snippet": "\n\ndef parse_video(\n    file_path: str,\n    task: str = 'Describe this image as detail as possible.',\n    frame_interval: int = 30,",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_155",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 155,
      "code_snippet": "    # TODO: record the COST of the API call\n    try:\n        base64_image = _base64_img(file_path)\n        response = _get_openai_client().chat.completions.create(\n            model=_get_openai_model(),\n            messages=_prepare_image_messages(task, base64_image),\n            max_tokens=_get_max_token(),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py_200",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/plugins/agent_skills/file_reader/file_readers.py",
      "line_number": 200,
      "code_snippet": "        print(f'Process the {file_path}, current No. {idx * frame_interval} frame...')\n        # TODO: record the COST of the API call\n        try:\n            response = _get_openai_client().chat.completions.create(\n                model=_get_openai_model(),\n                messages=_prepare_image_messages(task, base64_frame),\n                max_tokens=_get_max_token(),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py_110_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_send_action_server_request' executes dangerous operations",
      "description": "Tool function '_send_action_server_request' on line 110 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py",
      "line_number": 110,
      "code_snippet": "        retry=retry_if_exception(_is_retryable_error),\n        stop=stop_after_attempt(5) | stop_if_should_exit(),\n        wait=wait_exponential(multiplier=1, min=4, max=15),\n    )\n    def _send_action_server_request(\n        self,\n        method: str,\n        url: str,\n        **kwargs,\n    ) -> httpx.Response:",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py_276_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Insecure tool function 'send_action_for_execution' executes dangerous operations",
      "description": "Tool function 'send_action_for_execution' on line 276 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py",
      "line_number": 276,
      "code_snippet": "            return response_json['token']\n        else:\n            return ''\n\n    def send_action_for_execution(self, action: Action) -> Observation:\n        if (\n            isinstance(action, FileEditAction)\n            and action.impl_source == FileEditSource.LLM_BASED_EDIT\n        ):\n            return self.llm_based_edit(action)",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/action_execution/action_execution_client.py",
      "line_number": 1,
      "code_snippet": "import os\nimport tempfile\nimport threading\nfrom pathlib import Path\nfrom typing import Any\nfrom zipfile import ZipFile\n\nimport httpcore\nimport httpx\nfrom tenacity import retry, retry_if_exception, stop_after_attempt, wait_exponential",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py_521",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.docker_client.containers.run' is used in 'run(' on line 521 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py",
      "line_number": 521,
      "code_snippet": "\n            self.container = self.docker_client.containers.run(\n                self.runtime_container_image,\n                # Use Docker's tini init process to ensure proper signal handling and reaping of",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py_391_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'init_container'",
      "description": "Function 'init_container' on line 391 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py",
      "line_number": 391,
      "code_snippet": "        return overlay_mounts\n\n    def init_container(self) -> None:\n        self.log('debug', 'Preparing to start container...')\n        self.set_runtime_status(RuntimeStatus.STARTING_RUNTIME)\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/docker/docker_runtime.py",
      "line_number": 1,
      "code_snippet": "import os\nimport platform\nimport typing\nfrom functools import lru_cache\nfrom typing import Callable\nfrom uuid import UUID\n\nimport docker\nimport httpx\nimport tenacity",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/cli/cli_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/cli/cli_runtime.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This runtime runs commands locally using subprocess and performs file operations using Python's standard library.\nIt does not implement browser functionality.\n\"\"\"\n\nimport asyncio\nimport os\nimport select\nimport shutil\nimport signal\nimport subprocess",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/local/local_runtime.py_1_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [781]) and executes code (lines [1, 30, 31]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/local/local_runtime.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This runtime runs the action_execution_server directly on the local machine without Docker.\"\"\"\n\nimport os\nimport shutil\nimport subprocess\nimport sys",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/local/local_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/local/local_runtime.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This runtime runs the action_execution_server directly on the local machine without Docker.\"\"\"\n\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Callable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/kubernetes/kubernetes_runtime.py_454_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in '_get_runtime_pod_manifest'",
      "description": "Function '_get_runtime_pod_manifest' on line 454 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/kubernetes/kubernetes_runtime.py",
      "line_number": 454,
      "code_snippet": "        return service\n\n    def _get_runtime_pod_manifest(self):\n        \"\"\"Create a pod manifest for the runtime sandbox.\"\"\"\n        # Prepare environment variables\n        environment = [",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/kubernetes/kubernetes_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/kubernetes/kubernetes_runtime.py",
      "line_number": 1,
      "code_snippet": "from functools import lru_cache\nfrom typing import Callable\nfrom uuid import UUID\n\nimport tenacity\nimport yaml\nfrom kubernetes import client, config\nfrom kubernetes.client.models import (\n    V1Container,\n    V1ContainerPort,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py_531_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_send_action_server_request' executes dangerous operations",
      "description": "Tool function '_send_action_server_request' on line 531 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py",
      "line_number": 531,
      "code_snippet": "                f'No response received within the timeout period for url: {url}',\n            )\n            raise\n\n    def _send_action_server_request(\n        self, method: str, url: str, **kwargs: Any\n    ) -> httpx.Response:\n        if not self.config.sandbox.remote_runtime_enable_retries:\n            return self._send_action_server_request_impl(method, url, **kwargs)\n",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py_549_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function '_send_action_server_request_impl' executes dangerous operations",
      "description": "Tool function '_send_action_server_request_impl' on line 549 takes LLM output as a parameter and performs dangerous operations (http_request) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py",
      "line_number": 549,
      "code_snippet": "        return retry_decorator(self._send_action_server_request_impl)(\n            method, url, **kwargs\n        )\n\n    def _send_action_server_request_impl(\n        self, method: str, url: str, **kwargs: Any\n    ) -> httpx.Response:\n        try:\n            return super()._send_action_server_request(method, url, **kwargs)\n        except httpx.TimeoutException:",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/runtime/impl/remote/remote_runtime.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nfrom typing import Any, Callable\nfrom urllib.parse import urlparse\n\nimport httpx\nimport tenacity\nfrom tenacity import RetryCallState\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/client.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Any\n\nimport httpx\n\n\nclass InvariantClient:\n    timeout: int = 120\n\n    def __init__(self, server_url: str, session_id: str | None = None) -> None:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py_57",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'self.docker_client.containers.run' is used in 'run(' on line 57 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py",
      "line_number": 57,
      "code_snippet": "                self.api_port = find_available_tcp_port()\n                self.container = self.docker_client.containers.run(\n                    self.image_name,\n                    name=self.container_name,",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py_25_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in '__init__'",
      "description": "Function '__init__' on line 25 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py",
      "line_number": 25,
      "code_snippet": "    timeout: int = 180\n\n    def __init__(\n        self,\n        policy: str | None = None,\n        sid: str | None = None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/analyzer.py",
      "line_number": 1,
      "code_snippet": "import re\nimport uuid\nfrom typing import Any\n\nimport docker\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action.action import Action, ActionSecurityRisk\nfrom openhands.runtime.utils import find_available_tcp_port\nfrom openhands.security.analyzer import SecurityAnalyzer",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/parser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/invariant/parser.py",
      "line_number": 1,
      "code_snippet": "from pydantic import BaseModel, Field\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action import (\n    Action,\n    ChangeAgentStateAction,\n    MessageAction,\n    NullAction,\n)\nfrom openhands.events.event import EventSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/analyzer.py_58",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 58. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/analyzer.py",
      "line_number": 58,
      "code_snippet": "        if not self.policy_id:\n            self.policy_id = (\n                '689ca4885af3538a39b2ba04'  # GraySwan default coding agent policy\n            )\n            logger.info(f'Using default GraySwan policy ID: {self.policy_id}')",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/analyzer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/analyzer.py",
      "line_number": 1,
      "code_snippet": "\"\"\"GraySwan security analyzer for OpenHands.\"\"\"\n\nimport asyncio\nimport os\nfrom typing import Any\n\nimport aiohttp\nfrom fastapi import Request\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/security/grayswan/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utility for converting OpenHands events to OpenAI message format.\"\"\"\n\nfrom typing import Any\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action.message import MessageAction, SystemMessageAction\nfrom openhands.events.event import EventSource\nfrom openhands.events.observation.browse import BrowserOutputObservation\nfrom openhands.events.observation.commands import (\n    CmdOutputObservation,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/webhook_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/webhook_router.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Event Callback router for OpenHands App Server.\"\"\"\n\nimport asyncio\nimport importlib\nimport logging\nimport pkgutil\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException, Response, status\nfrom fastapi.security import APIKeyHeader",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/event_callback_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/event_callback_models.py",
      "line_number": 1,
      "code_snippet": "# pyright: reportIncompatibleMethodOverride=false\nfrom __future__ import annotations\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Literal\nfrom uuid import UUID, uuid4\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/event_callback_result_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/event_callback_result_models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\nfrom openhands.agent_server.utils import OpenHandsUUID, utc_now\nfrom openhands.sdk.event.types import EventID\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/sql_event_callback_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/sql_event_callback_service.py",
      "line_number": 1,
      "code_snippet": "# pyright: reportArgumentType=false\n\"\"\"SQL implementation of EventCallbackService.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nfrom dataclasses import dataclass\nfrom typing import AsyncGenerator\nfrom uuid import UUID",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/set_title_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event_callback/set_title_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom uuid import UUID\n\nfrom openhands.app_server.app_conversation.app_conversation_models import (\n    AppConversationInfo,\n)\nfrom openhands.app_server.event_callback.event_callback_models import (\n    EventCallback,\n    EventCallbackProcessor,\n    EventCallbackStatus,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/encryption_key.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/encryption_key.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any\n\nimport base62\nfrom pydantic import BaseModel, Field, SecretStr, TypeAdapter, field_serializer\n\nfrom openhands.agent_server.utils import utc_now",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/sql_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/sql_utils.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\nfrom enum import Enum\nfrom typing import TypeVar\n\nfrom pydantic import SecretStr, TypeAdapter\nfrom sqlalchemy import JSON, DateTime, String, TypeDecorator\nfrom sqlalchemy.orm import declarative_base\n\nBase = declarative_base()\nT = TypeVar('T', bound=Enum)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/import_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/utils/import_utils.py",
      "line_number": 1,
      "code_snippet": "import importlib\nfrom functools import lru_cache\nfrom typing import TypeVar\n\nT = TypeVar('T')\n\n\ndef import_from(qual_name: str):\n    \"\"\"Import a value from its fully qualified name.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/user/auth_user_context.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/user/auth_user_context.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom typing import Any, AsyncGenerator\n\nfrom fastapi import Request\nfrom pydantic import PrivateAttr\n\nfrom openhands.app_server.errors import AuthError\nfrom openhands.app_server.services.injector import InjectorState\nfrom openhands.app_server.user.specifiy_user_context import USER_CONTEXT_ATTR\nfrom openhands.app_server.user.user_context import UserContext, UserContextInjector",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_spec_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_spec_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\nfrom abc import ABC, abstractmethod\n\nfrom openhands.agent_server import env_parser\nfrom openhands.app_server.errors import SandboxError\nfrom openhands.app_server.sandbox.sandbox_spec_models import (\n    SandboxSpecInfo,\n    SandboxSpecInfoPage,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport time\nfrom abc import ABC, abstractmethod\n\nimport httpx\n\nfrom openhands.app_server.errors import SandboxError\nfrom openhands.app_server.sandbox.sandbox_models import (\n    AGENT_SERVER,\n    SandboxInfo,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\n\nfrom pydantic import BaseModel, Field\n\nfrom openhands.agent_server.utils import utc_now\n\n\nclass SandboxStatus(Enum):\n    STARTING = 'STARTING'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/preset_sandbox_spec_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/preset_sandbox_spec_service.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\n\nfrom openhands.app_server.sandbox.sandbox_spec_models import (\n    SandboxSpecInfo,\n    SandboxSpecInfoPage,\n)\nfrom openhands.app_server.sandbox.sandbox_spec_service import (\n    SandboxSpecService,\n)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/sandbox_router.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Runtime Containers router for OpenHands App Server.\"\"\"\n\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, HTTPException, Query, status\n\nfrom openhands.agent_server.models import Success\nfrom openhands.app_server.config import depends_sandbox_service\nfrom openhands.app_server.sandbox.sandbox_models import SandboxInfo, SandboxPage\nfrom openhands.app_server.sandbox.sandbox_service import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/process_sandbox_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/process_sandbox_service.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Process-based sandbox service implementation.\n\nThis service creates sandboxes by spawning separate agent server processes,\neach running within a dedicated directory.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport socket",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_service.py_40",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 40. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_service.py",
      "line_number": 40,
      "code_snippet": "\n_logger = logging.getLogger(__name__)\nSESSION_API_KEY_VARIABLE = 'OH_SESSION_API_KEYS_0'\nWEBHOOK_CALLBACK_VARIABLE = 'OH_WEBHOOKS_0_BASE_URL'\nSTARTUP_GRACE_SECONDS = 15",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport logging\nimport os\nimport socket\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom typing import AsyncGenerator\n\nimport base62\nimport docker",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_spec_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/docker_sandbox_spec_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport logging\nfrom typing import AsyncGenerator\n\nimport docker\nfrom fastapi import Request\nfrom pydantic import Field\n\nfrom openhands.app_server.errors import SandboxError\nfrom openhands.app_server.sandbox.preset_sandbox_spec_service import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/remote_sandbox_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/sandbox/remote_sandbox_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport logging\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, AsyncGenerator, Union\nfrom uuid import UUID\n\nimport base62\nimport httpx\nfrom fastapi import Request",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/oss_app_lifespan_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/oss_app_lifespan_service.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\nfrom alembic import command\nfrom alembic.config import Config\n\nfrom openhands.app_server.app_lifespan.app_lifespan_service import AppLifespanService\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Literal\nfrom uuid import UUID, uuid4\n\nfrom pydantic import BaseModel, Field\n\nfrom openhands.agent_server.models import OpenHandsModel, SendMessageRequest\nfrom openhands.agent_server.utils import OpenHandsUUID, utc_now\nfrom openhands.app_server.event_callback.event_callback_models import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/sql_app_conversation_info_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/sql_app_conversation_info_service.py",
      "line_number": 1,
      "code_snippet": "\"\"\"SQL implementation of AppConversationService.\n\nThis implementation provides CRUD operations for sandboxed conversations focused purely\non SQL operations:\n- Direct database access without permission checks\n- Batch operations for efficient data retrieval\n- Integration with SandboxService for sandbox information\n- HTTP client integration for agent status retrieval\n- Full async/await support using SQL async db_sessions\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/sql_app_conversation_start_task_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/sql_app_conversation_start_task_service.py",
      "line_number": 1,
      "code_snippet": "# pyright: reportArgumentType=false, reportAttributeAccessIssue=false, reportOptionalMemberAccess=false\n\"\"\"SQL implementation of AppConversationStartTaskService.\n\nThis implementation provides CRUD operations for conversation start tasks focused purely\non SQL operations:\n- Direct database access without permission checks\n- Batch operations for efficient data retrieval\n- Full async/await support using SQL async sessions\n\nSecurity and permission checks are handled by wrapper services.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py_385",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'llm.model_copy' is used in 'UPDATE' on line 385 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py",
      "line_number": 385,
      "code_snippet": "        condenser_kwargs = {\n            'llm': llm.model_copy(\n                update={\n                    'usage_id': (",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py_367_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_create_condenser'",
      "description": "Function '_create_condenser' on line 367 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py",
      "line_number": 367,
      "code_snippet": "        _logger.info('Git pre-commit hook installed successfully')\n\n    def _create_condenser(\n        self,\n        llm: LLM,\n        agent_type: AgentType,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py_367_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_create_condenser'",
      "description": "Function '_create_condenser' on line 367 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py",
      "line_number": 367,
      "code_snippet": "        _logger.info('Git pre-commit hook installed successfully')\n\n    def _create_condenser(\n        self,\n        llm: LLM,\n        agent_type: AgentType,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_service_base.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport tempfile\nfrom abc import ABC\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, AsyncGenerator\nfrom uuid import UUID\n\nif TYPE_CHECKING:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/app_conversation_router.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Sandboxed Conversation router for OpenHands App Server.\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport sys\nimport tempfile\nfrom datetime import datetime\nfrom typing import Annotated, AsyncGenerator, Literal\nfrom uuid import UUID",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/skill_loader.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/skill_loader.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Utilities for loading skills for V1 conversations.\n\nThis module provides functions to load skills from various sources:\n- Global skills from OpenHands/skills/\n- User skills from ~/.openhands/skills/\n- Repository-level skills from the workspace\n\nAll skills are used in V1 conversations.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_934",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'agent.llm.model_copy' is used in 'UPDATE' on line 934 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 934,
      "code_snippet": "            )\n            updated_llm = agent.llm.model_copy(\n                update={'litellm_extra_body': {'metadata': llm_metadata}}\n            )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_949",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'condenser_llm.model_copy' is used in 'UPDATE' on line 949 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 949,
      "code_snippet": "                )\n                updated_condenser_llm = condenser_llm.model_copy(\n                    update={'litellm_extra_body': {'metadata': condenser_metadata}}\n                )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_624",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_configure_llm' on line 624 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 624,
      "code_snippet": "    def _configure_llm(self, user: UserInfo, llm_model: str | None) -> LLM:\n        \"\"\"Configure LLM settings.\n\n        Args:\n            user: User information containing LLM preferences\n            llm_model: Optional specific model to use, falls back to user default\n\n        Returns:\n            Configured LLM instance\n        \"\"\"\n        model = llm_model or user.llm_model",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_905",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_update_agent_with_llm_metadata' on line 905 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 905,
      "code_snippet": "    def _update_agent_with_llm_metadata(\n        self,\n        agent: Agent,\n        conversation_id: UUID,\n        user_id: str | None,\n    ) -> Agent:\n        \"\"\"Update agent's LLM and condenser LLM with litellm_extra_body metadata.\n\n        This adds tracing metadata (conversation_id, user_id, etc.) to the LLM\n        for analytics and debugging purposes. Only applies to openhands/ models.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_905_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_update_agent_with_llm_metadata'",
      "description": "Function '_update_agent_with_llm_metadata' on line 905 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 905,
      "code_snippet": "        return agent\n\n    def _update_agent_with_llm_metadata(\n        self,\n        agent: Agent,\n        conversation_id: UUID,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_905_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_update_agent_with_llm_metadata'",
      "description": "Function '_update_agent_with_llm_metadata' on line 905 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 905,
      "code_snippet": "        return agent\n\n    def _update_agent_with_llm_metadata(\n        self,\n        agent: Agent,\n        conversation_id: UUID,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_1255",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 1255 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 1255,
      "code_snippet": "        except Exception as e:\n            _logger.warning(\n                f'Failed to delete conversation from agent server: {e}',\n                extra={'conversation_id': str(conversation_id)},\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_996",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 996,
      "code_snippet": "        conversation_id = conversation_id or uuid4()\n\n        # Apply experiment variants\n        agent = ExperimentManagerImpl.run_agent_variant_tests__v1(\n            user.id, conversation_id, agent\n        )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_636",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 636,
      "code_snippet": "        \"\"\"\n        model = llm_model or user.llm_model\n        base_url = user.llm_base_url\n        if model and model.startswith('openhands/'):\n            base_url = user.llm_base_url or self.openhands_provider_base_url\n\n        return LLM(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_934",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 934,
      "code_snippet": "                conversation_id=conversation_id,\n                user_id=user_id,\n            )\n            updated_llm = agent.llm.model_copy(\n                update={'litellm_extra_body': {'metadata': llm_metadata}}\n            )\n            updates['llm'] = updated_llm",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py_949",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_conversation/live_status_app_conversation_service.py",
      "line_number": 949,
      "code_snippet": "                    conversation_id=conversation_id,\n                    user_id=user_id,\n                )\n                updated_condenser_llm = condenser_llm.model_copy(\n                    update={'litellm_extra_body': {'metadata': condenser_metadata}}\n                )\n                updated_condenser = agent.condenser.model_copy(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/event_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/event_router.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Event router for OpenHands App Server.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Annotated\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Query\n\nfrom openhands.agent_server.models import EventPage, EventSortOrder\nfrom openhands.app_server.config import depends_event_service",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/filesystem_event_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/filesystem_event_service.py",
      "line_number": 1,
      "code_snippet": "import glob\nimport logging\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import AsyncGenerator\n\nfrom fastapi import Request\n\nfrom openhands.app_server.event.event_service import EventService, EventServiceInjector\nfrom openhands.app_server.event.event_service_base import EventServiceBase",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/google_cloud_event_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/google_cloud_event_service.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Google Cloud Storage-based EventService implementation.\"\"\"\n\nimport json\nimport logging\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import AsyncGenerator, Iterator\n\nfrom fastapi import Request\nfrom google.api_core.exceptions import NotFound",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/event_service_base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/event/event_service_base.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom uuid import UUID\n\nfrom openhands.agent_server.models import EventPage, EventSortOrder\nfrom openhands.agent_server.sockets import page_iterator\nfrom openhands.app_server.app_conversation.app_conversation_info_service import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/httpx_client_injector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/httpx_client_injector.py",
      "line_number": 1,
      "code_snippet": "from typing import AsyncGenerator\n\nimport httpx\nfrom fastapi import Request\nfrom pydantic import BaseModel, Field\n\nfrom openhands.app_server.services.injector import Injector, InjectorState\n\nHTTPX_CLIENT_ATTR = 'httpx_client'\nHTTPX_CLIENT_KEEP_OPEN_ATTR = 'httpx_client_keep_open'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/db_session_injector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/db_session_injector.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Database configuration and session management for OpenHands App Server.\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Any, AsyncGenerator\n\nimport asyncpg\nfrom fastapi import Request",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/jwt_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/services/jwt_service.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport json\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom typing import Any, AsyncGenerator\n\nimport jwt\nfrom fastapi import Request\nfrom jose import jwe\nfrom jose.constants import ALGORITHMS",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py_78_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'password_value' containing sensitive data is included in a prompt string on line 78. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
      "line_number": 78,
      "code_snippet": "            db_session.password.get_secret_value() if db_session.password else ''\n        )\n        url = f'postgresql://{db_session.user}:{password_value}@{db_session.host}:{db_session.port}/{db_session.name}'\n    else:\n        url = f'sqlite:///{db_session.persistence_dir}/openhands.db'",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py_57_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 57 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
      "line_number": 57,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py_93_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 93 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
      "line_number": 93,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py_57_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 57 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
      "line_number": 57,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py_93_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 93 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
      "line_number": 93,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/env.py",
      "line_number": 1,
      "code_snippet": "import sys\nfrom logging.config import fileConfig\nfrom pathlib import Path\n\nfrom alembic import context\n\n# Add the project root to the Python path so we can import OpenHands modules\n# From alembic/env.py, we need to go up 5 levels to reach the OpenHands project root\nproject_root = Path(__file__).absolute().parent.parent.parent.parent.parent\nsys.path.insert(0, str(project_root))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/versions/002.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/app_server/app_lifespan/alembic/versions/002.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Sync DB with Models\n\nRevision ID: 001\nRevises:\nCreate Date: 2025-10-05 11:28:41.772294\n\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Sequence, Union",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/function_calling.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/function_calling.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n\"\"\"This file contains the function calling implementation for different actions.\n\nThis is similar to the functionality of `CodeActResponseParser`.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py_260",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_get_messages' on line 260 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py",
      "line_number": 260,
      "code_snippet": "    def _get_messages(\n        self,\n        events: list[Event],\n        initial_user_message: MessageAction,\n        forgotten_event_ids: set[int],\n    ) -> list[Message]:\n        \"\"\"Constructs the message history for the LLM conversation.\n\n        This method builds a structured conversation history by processing events from the state\n        and formatting them into messages that the LLM can understand. It handles both regular\n        message flow and function-calling scenarios.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py_308",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py",
      "line_number": 308,
      "code_snippet": "            vision_is_active=self.llm.vision_is_active(),\n        )\n\n        if self.llm.is_caching_prompt_active():\n            self.conversation_memory.apply_prompt_caching(messages)\n\n        return messages",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py_305",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/codeact_agent.py",
      "line_number": 305,
      "code_snippet": "            initial_user_action=initial_user_message,\n            forgotten_event_ids=forgotten_event_ids,\n            max_message_chars=self.llm.config.max_message_chars,\n            vision_is_active=self.llm.vision_is_active(),\n        )\n\n        if self.llm.is_caching_prompt_active():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/utils.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport collections\nimport re\nfrom warnings import warn",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/browsing_agent.py_140_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'step'",
      "description": "Function 'step' on line 140 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/browsing_agent.py",
      "line_number": 140,
      "code_snippet": "        self.error_accumulator = 0\n\n    def step(self, state: State) -> Action:\n        \"\"\"Performs one step using the Browsing Agent.\n        This includes gathering information on previous steps and prompting the model to make a browsing command to execute.\n",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/response_parser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/browsing_agent/response_parser.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport ast\nimport re\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/visualbrowsing_agent/visualbrowsing_agent.py_187_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'step'",
      "description": "Function 'step' on line 187 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/visualbrowsing_agent/visualbrowsing_agent.py",
      "line_number": 187,
      "code_snippet": "        self.error_accumulator = 0\n\n    def step(self, state: State) -> Action:\n        \"\"\"Performs one step using the VisualBrowsingAgent.\n\n        This includes gathering information on previous steps and prompting the model to make a browsing command to execute.",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/function_calling.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/function_calling.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n\"\"\"This file contains the function calling implementation for different actions.\n\nThis is similar to the functionality of `CodeActResponseParser`.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/dummy_agent/agent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/dummy_agent/agent.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom typing import TypedDict\n\nfrom openhands.controller.agent import Agent",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/readonly_agent/function_calling.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/readonly_agent/function_calling.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n\"\"\"This file contains the function calling implementation for different actions.\n\nThis is similar to the functionality of `CodeActResponseParser`.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/tools/search_content.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/tools/search_content.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom litellm import (\n    ChatCompletionToolParam,\n    ChatCompletionToolParamFunctionChunk,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/tools/explore_structure.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/loc_agent/tools/explore_structure.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom litellm import (\n    ChatCompletionToolParam,\n    ChatCompletionToolParamFunctionChunk,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/bash.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/bash.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom litellm import ChatCompletionToolParam, ChatCompletionToolParamFunctionChunk\n\nfrom openhands.agenthub.codeact_agent.tools.prompt import refine_prompt",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/browser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/browser.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom browsergym.core.action.highlevel import HighLevelActionSet\nfrom litellm import ChatCompletionToolParam, ChatCompletionToolParamFunctionChunk\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/ipython.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/ipython.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom litellm import ChatCompletionToolParam, ChatCompletionToolParamFunctionChunk\n\nfrom openhands.agenthub.codeact_agent.tools.security_utils import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/prompt.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/prompt.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport re\nimport sys\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/str_replace_editor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/str_replace_editor.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport os\n\nfrom litellm import ChatCompletionToolParam, ChatCompletionToolParamFunctionChunk",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/task_tracker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/agenthub/codeact_agent/tools/task_tracker.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom litellm import ChatCompletionToolParam, ChatCompletionToolParamFunctionChunk\n\nfrom openhands.llm.tool_names import TASK_TRACKER_TOOL_NAME",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/condenser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/condenser.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom contextlib import contextmanager\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom openhands.controller.state.state import State\nfrom openhands.core.config.condenser_config import CondenserConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/conversation_window_condenser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/conversation_window_condenser.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom openhands.core.config.condenser_config import ConversationWindowCondenserConfig\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.events.action.agent import (\n    CondensationAction,\n    RecallAction,\n)\nfrom openhands.events.action.message import MessageAction, SystemMessageAction\nfrom openhands.events.event import EventSource",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/amortized_forgetting_condenser.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/amortized_forgetting_condenser.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom openhands.core.config.condenser_config import AmortizedForgettingCondenserConfig\nfrom openhands.events.action.agent import CondensationAction\nfrom openhands.llm.llm_registry import LLMRegistry\nfrom openhands.memory.condenser.condenser import (\n    Condensation,\n    RollingCondenser,\n    View,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py_87",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to 'ImportantEventSelection.model_validate_json' on line 87 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py",
      "line_number": 87,
      "code_snippet": "\n        response_ids = ImportantEventSelection.model_validate_json(\n            response.choices[0].message.content\n        ).ids",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py_67",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py",
      "line_number": 67,
      "code_snippet": "        contents of the item are for the next step of the coding agent's task, from most important to least\n        important.\"\"\"\n\n        response = self.llm.completion(\n            messages=[\n                {'content': message, 'role': 'user'},\n                *[",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py_91",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_attention_condenser.py",
      "line_number": 91,
      "code_snippet": "            response.choices[0].message.content\n        ).ids\n\n        self.add_metadata('metrics', self.llm.metrics.get())\n\n        # Filter out any IDs from the head and trim the results down\n        response_ids = [",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py_52",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_condensation' on line 52 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
      "line_number": 52,
      "code_snippet": "    def get_condensation(self, view: View) -> Condensation:\n        head = view[: self.keep_first]\n        target_size = self.max_size // 2\n        # Number of events to keep from the tail -- target size, minus however many\n        # prefix events from the head, minus one for the summarization event\n        events_from_tail = target_size - len(head) - 1\n\n        summary_event = (\n            view[self.keep_first]\n            if isinstance(view[self.keep_first], AgentCondensationObservation)\n            else AgentCondensationObservation('No events summarized')",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py_52_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_condensation'",
      "description": "Function 'get_condensation' on line 52 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
      "line_number": 52,
      "code_snippet": "        return truncate_content(content, max_chars=self.max_event_length)\n\n    def get_condensation(self, view: View) -> Condensation:\n        head = view[: self.keep_first]\n        target_size = self.max_size // 2\n        # Number of events to keep from the tail -- target size, minus however many",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py_142",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
      "line_number": 142,
      "code_snippet": "\n        messages = [Message(role='user', content=[TextContent(text=prompt)])]\n\n        response = self.llm.completion(\n            messages=self.llm.format_messages_for_llm(messages),\n            extra_body={'metadata': self.llm_metadata},\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py_149",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
      "line_number": 149,
      "code_snippet": "        summary = response.choices[0].message.content\n\n        self.add_metadata('response', response.model_dump())\n        self.add_metadata('metrics', self.llm.metrics.get())\n\n        return Condensation(\n            action=CondensationAction(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py_143",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/llm_summarizing_condenser.py",
      "line_number": 143,
      "code_snippet": "        messages = [Message(role='user', content=[TextContent(text=prompt)])]\n\n        response = self.llm.completion(\n            messages=self.llm.format_messages_for_llm(messages),\n            extra_body={'metadata': self.llm_metadata},\n        )\n        summary = response.choices[0].message.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py_199",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_condensation' on line 199 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py",
      "line_number": 199,
      "code_snippet": "    def get_condensation(self, view: View) -> Condensation:\n        head = view[: self.keep_first]\n        target_size = self.max_size // 2\n        # Number of events to keep from the tail -- target size, minus however many\n        # prefix events from the head, minus one for the summarization event\n        events_from_tail = target_size - len(head) - 1\n\n        summary_event = (\n            view[self.keep_first]\n            if isinstance(view[self.keep_first], AgentCondensationObservation)\n            else AgentCondensationObservation('No events summarized')",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py_255_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'get_condensation'",
      "description": "Function 'get_condensation' on line 199 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py",
      "line_number": 255,
      "code_snippet": "        messages = [Message(role='user', content=[TextContent(text=prompt)])]\n\n        response = self.llm.completion(\n            messages=self.llm.format_messages_for_llm(messages),\n            tools=[StateSummary.tool_description()],\n            tool_choice={",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py_255",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py",
      "line_number": 255,
      "code_snippet": "\n        messages = [Message(role='user', content=[TextContent(text=prompt)])]\n\n        response = self.llm.completion(\n            messages=self.llm.format_messages_for_llm(messages),\n            tools=[StateSummary.tool_description()],\n            tool_choice={",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py_256",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/memory/condenser/impl/structured_summary_condenser.py",
      "line_number": 256,
      "code_snippet": "        messages = [Message(role='user', content=[TextContent(text=prompt)])]\n\n        response = self.llm.completion(\n            messages=self.llm.format_messages_for_llm(messages),\n            tools=[StateSummary.tool_description()],\n            tool_choice={\n                'type': 'function',",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/condenser_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/condenser_config.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nfrom typing import Literal, cast",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/agent_config.py_91",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'from_toml_section' on line 91 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/agent_config.py",
      "line_number": 91,
      "code_snippet": "    def from_toml_section(cls, data: dict) -> dict[str, AgentConfig]:\n        \"\"\"Create a mapping of AgentConfig instances from a toml dictionary representing the [agent] section.\n\n        The default configuration is built from all non-dict keys in data.\n        Then, each key with a dict value is treated as a custom agent configuration, and its values override\n        the default configuration.\n\n        Example:\n        Apply generic agent config with custom agent overrides, e.g.\n            [agent]\n            enable_prompt_extensions = false",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/agent_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/agent_config.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nfrom pydantic import BaseModel, ConfigDict, Field, ValidationError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py_465",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'cfg.runtime.lower' is used in 'SELECT' on line 465 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py",
      "line_number": 465,
      "code_snippet": "    # Assuming 'cli' is the identifier for CLIRuntime\n    if cfg.runtime and cfg.runtime.lower() == 'cli':\n        for age_nt_name, agent_config in cfg.agents.items():\n            if agent_config.enable_jupyter:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py_386_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'finalize_config'",
      "description": "Function 'finalize_config' on line 386 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py",
      "line_number": 386,
      "code_snippet": "\n\ndef finalize_config(cfg: OpenHandsConfig) -> None:\n    \"\"\"More tweaks to the config after it's been loaded.\"\"\"\n    # Handle the sandbox.volumes parameter\n    if cfg.sandbox.volumes is not None:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/utils.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport argparse\nimport os\nimport pathlib",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/arg_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/arg_utils.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\n\"\"\"Centralized command line argument configuration for OpenHands CLI and headless modes.\"\"\"\n\nimport argparse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/mcp_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/mcp_config.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py_176_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'model_post_init'",
      "description": "Function 'model_post_init' on line 176 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py",
      "line_number": 176,
      "code_snippet": "        return llm_mapping\n\n    def model_post_init(self, __context: Any) -> None:\n        \"\"\"Post-initialization hook to assign OpenRouter-related variables to environment variables.\n\n        This ensures that these values are accessible to litellm at runtime.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py_176_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'model_post_init'",
      "description": "Function 'model_post_init' on line 176 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py",
      "line_number": 176,
      "code_snippet": "        return llm_mapping\n\n    def model_post_init(self, __context: Any) -> None:\n        \"\"\"Post-initialization hook to assign OpenRouter-related variables to environment variables.\n\n        This ensures that these values are accessible to litellm at runtime.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/llm_config.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom __future__ import annotations\n\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/config_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/config/config_utils.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom types import UnionType\nfrom typing import Any, get_args, get_origin\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/observation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/observation.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom enum import Enum\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/action.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/action.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom enum import Enum\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/agent.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/core/schema/agent.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nfrom enum import Enum\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py_140",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'selected_llm.completion' is used in 'SELECT' on line 140 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
      "line_number": 140,
      "code_snippet": "            # Delegate to selected LLM\n            return selected_llm.completion(*args, **kwargs)\n\n        return router_completion",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py_113_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'completion'",
      "description": "Function 'completion' on line 113 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
      "line_number": 113,
      "code_snippet": "\n    @property\n    def completion(self) -> Callable:\n        \"\"\"\n        Override completion to route to appropriate LLM.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py_121_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'router_completion'",
      "description": "Function 'router_completion' on line 121 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
      "line_number": 121,
      "code_snippet": "        \"\"\"\n\n        def router_completion(*args: Any, **kwargs: Any) -> Any:\n            # Extract messages for routing decision\n            messages = kwargs.get('messages', [])\n            if args and not messages:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py_113_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'completion'",
      "description": "Function 'completion' on line 113 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
      "line_number": 113,
      "code_snippet": "\n    @property\n    def completion(self) -> Callable:\n        \"\"\"\n        Override completion to route to appropriate LLM.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py_121_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'router_completion'",
      "description": "Function 'router_completion' on line 121 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
      "line_number": 121,
      "code_snippet": "        \"\"\"\n\n        def router_completion(*args: Any, **kwargs: Any) -> Any:\n            # Extract messages for routing decision\n            messages = kwargs.get('messages', [])\n            if args and not messages:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/base.py",
      "line_number": 1,
      "code_snippet": "# IMPORTANT: LEGACY V0 CODE\n# This file is part of the legacy (V0) implementation of OpenHands and will be removed soon as we complete the migration to V1.\n# OpenHands V1 uses the Software Agent SDK for the agentic core and runs a new application server. Please refer to:\n#   - V1 agentic core (SDK): https://github.com/OpenHands/software-agent-sdk\n#   - V1 application server (in this repo): openhands/app_server/\n# Unless you are working on deprecation, please avoid extending this legacy file and consult the V1 codepaths above.\n# Tag: Legacy-V0\nimport copy\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any, Callable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/rule_based/impl.py_33_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_select_llm'",
      "description": "Function '_select_llm' on line 33 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/rule_based/impl.py",
      "line_number": 33,
      "code_snippet": "        self.max_token_exceeded = False\n\n    def _select_llm(self, messages: list[Message]) -> str:\n        \"\"\"Select LLM based on multimodal content and token limits.\"\"\"\n        route_to_primary = False\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/rule_based/impl.py_53",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/rule_based/impl.py",
      "line_number": 53,
      "code_snippet": "        secondary_llm = self.available_llms.get(self.SECONDARY_MODEL_CONFIG_NAME)\n        if secondary_llm and (\n            secondary_llm.config.max_input_tokens\n            and secondary_llm.get_token_count(messages)\n            > secondary_llm.config.max_input_tokens\n        ):\n            logger.warning(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/rule_based/impl.py_57",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/openhands/llm/router/rule_based/impl.py",
      "line_number": 57,
      "code_snippet": "            > secondary_llm.config.max_input_tokens\n        ):\n            logger.warning(\n                f\"Messages having {secondary_llm.get_token_count(messages)} tokens, exceed secondary model's max input tokens ({secondary_llm.config.max_input_tokens} tokens). \"\n                'Routing to the primary model.'\n            )\n            self.max_token_exceeded = True",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py_69_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 69 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
      "line_number": 69,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py_92_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 92 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
      "line_number": 92,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py_69_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 69 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
      "line_number": 69,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py_92_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 92 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
      "line_number": 92,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/env.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom logging.config import fileConfig\n\nfrom alembic import context\nfrom google.cloud.sql.connector import Connector\nfrom sqlalchemy import create_engine\nfrom storage.base import Base\n\ntarget_metadata = Base.metadata\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/enterprise_local/convert_to_env.py_119",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Base64 High Entropy String",
      "description": "detect-secrets found a potential Base64 High Entropy String on line 119. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/enterprise_local/convert_to_env.py",
      "line_number": 119,
      "code_snippet": "lines.append('MAX_CONCURRENT_CONVERSATIONS=10')\nlines.append('LITE_LLM_API_URL=https://llm-proxy.eval.all-hands.dev')\nlines.append('LITELLM_DEFAULT_MODEL=litellm_proxy/claude-opus-4-5-20251101')\nlines.append(f'LITE_LLM_API_KEY={lite_llm_api_key}')\nlines.append('LOCAL_DEPLOYMENT=true')",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/enterprise_local/convert_to_env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/enterprise_local/convert_to_env.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport os\nimport sys\n\nimport yaml\n\n\ndef convert_yaml_to_env(yaml_file, target_parameters, output_env_file, prefix):\n    \"\"\"Converts a YAML file into .env file format for specified target parameters under 'stringData' and 'data'.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/constants.py_17_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'api_key' containing sensitive data is being logged on line 17. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/constants.py",
      "line_number": 17,
      "code_snippet": "else:\n    masked_key = 'not_set_or_too_short'\nlogger.info('posthog_configuration', extra={'posthog_api_key_masked': masked_key})\n\n# Global toggle for the experiment manager",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/constants.py",
      "line_number": 1,
      "code_snippet": "import os\n\nimport posthog\n\nfrom openhands.core.logger import openhands_logger as logger\n\n# Initialize PostHog\nposthog.api_key = os.environ.get('POSTHOG_CLIENT_KEY', 'phc_placeholder')\nposthog.host = os.environ.get('POSTHOG_HOST', 'https://us.i.posthog.com')\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/config.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport hmac\nimport os\nimport time\nimport typing\n\nimport jwt\nimport requests  # type: ignore\nfrom fastapi import HTTPException\nfrom server.auth.constants import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/constants.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\n\n# Get the host from environment variable\nHOST = os.getenv('WEB_HOST', 'app.all-hands.dev').strip()\n\n# Check if this is a feature environment\n# Feature environments have a host format like {some-text}.staging.all-hands.dev\n# Just staging.all-hands.dev doesn't count as a feature environment\nIS_STAGING_ENV = bool(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/saas_nested_conversation_manager.py_98_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 98. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/saas_nested_conversation_manager.py",
      "line_number": 98,
      "code_snippet": "\n\nclass EventRetrieval(Enum):\n    \"\"\"Determine mode for getting events out of the nested runtime back into the main app.\"\"\"\n\n    WEBHOOK_PUSH = 'WEBHOOK_PUSH'",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/saas_nested_conversation_manager.py_409",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/saas_nested_conversation_manager.py",
      "line_number": 409,
      "code_snippet": "            ExperimentManagerImpl,\n        )\n\n        config: OpenHandsConfig = ExperimentManagerImpl.run_config_variant_test(\n            user_id, sid, self.config\n        )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/logger.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/logger.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import TextIO\n\nfrom pythonjsonlogger.json import JsonFormatter\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/rate_limit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/rate_limit.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nUsage:\n\nCall setup_rate_limit_handler on your FastAPI app to add the exception handler\n\nCreate a rate limiter like:\n    `rate_limiter = create_redis_rate_limiter(\"10/second; 100/minute\")`\n\nCall hit() with some key and allow the RateLimitException to propagate:\n    `rate_limiter.hit('some action', user_id)`",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/clustered_conversation_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/clustered_conversation_manager.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport json\nimport time\nfrom dataclasses import dataclass, field\nfrom uuid import uuid4\n\nimport socketio\nfrom server.logger import logger\nfrom server.utils.conversation_callback_utils import invoke_conversation_callbacks\nfrom storage.database import session_maker",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/middleware.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/middleware.py",
      "line_number": 1,
      "code_snippet": "from typing import Callable\n\nimport jwt\nfrom fastapi import Request, Response, status\nfrom fastapi.responses import JSONResponse\nfrom pydantic import SecretStr\nfrom server.auth.auth_error import (\n    AuthError,\n    EmailNotVerifiedError,\n    NoCredentialsError,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_settings.py",
      "line_number": 1,
      "code_snippet": "from server.constants import DEFAULT_BILLING_MARGIN\nfrom sqlalchemy import JSON, Boolean, Column, DateTime, Float, Identity, Integer, String\nfrom storage.base import Base\n\n\nclass UserSettings(Base):  # type: ignore\n    __tablename__ = 'user_settings'\n    id = Column(Integer, Identity(), primary_key=True)\n    keycloak_user_id = Column(String, nullable=True, index=True)\n    language = Column(String, nullable=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/subscription_access.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/subscription_access.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\n\nfrom sqlalchemy import DECIMAL, Column, DateTime, Enum, Integer, String\nfrom storage.base import Base\n\n\nclass SubscriptionAccess(Base):  # type: ignore\n    \"\"\"\n    Represents a user's subscription access record.\n    Tracks subscription status, duration, payment information, and cancellation status.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py_291",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'user_model.split' is used in 'UPDATE' on line 291 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
      "line_number": 291,
      "code_snippet": "            old_default_base = USER_SETTINGS_VERSION_TO_MODEL[old_user_version]\n            user_model_base = user_model.split('/')[-1]\n            if user_model_base == old_default_base:\n                return False  # Matches old default",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py_250",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_has_custom_settings' on line 250 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
      "line_number": 250,
      "code_snippet": "    def _has_custom_settings(\n        self, settings: Settings, old_user_version: int | None\n    ) -> bool:\n        \"\"\"\n        Check if user has custom LLM settings that should be preserved.\n        Returns True if user customized either model or base_url.\n\n        Args:\n            settings: The user's current settings\n            old_user_version: The user's old settings version, if any\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py_266",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
      "line_number": 266,
      "code_snippet": "        \"\"\"\n        # Normalize values\n        user_model = (\n            settings.llm_model.strip()\n            if settings.llm_model and settings.llm_model.strip()\n            else None\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py_267",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
      "line_number": 267,
      "code_snippet": "        # Normalize values\n        user_model = (\n            settings.llm_model.strip()\n            if settings.llm_model and settings.llm_model.strip()\n            else None\n        )\n        user_base_url = (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py_291",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_settings_store.py",
      "line_number": 291,
      "code_snippet": "            and old_user_version in USER_SETTINGS_VERSION_TO_MODEL\n        ):\n            old_default_base = USER_SETTINGS_VERSION_TO_MODEL[old_user_version]\n            user_model_base = user_model.split('/')[-1]\n            if user_model_base == old_default_base:\n                return False  # Matches old default\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/conversation_callback.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/conversation_callback.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Type\n\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import Column, DateTime, ForeignKey, Integer, String, Text, text\nfrom sqlalchemy import Enum as SQLEnum",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/auth_tokens.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/auth_tokens.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import BigInteger, Column, Identity, Index, Integer, String\nfrom storage.base import Base\n\n\nclass AuthTokens(Base):  # type: ignore\n    __tablename__ = 'auth_tokens'\n    id = Column(Integer, Identity(), primary_key=True)\n    keycloak_user_id = Column(String, nullable=False, index=True)\n    identity_provider = Column(String, nullable=False)\n    access_token = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_repo_map_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_repo_map_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport sqlalchemy\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.database import session_maker\nfrom storage.user_repo_map import UserRepositoryMap\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_conversation_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_conversation_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport dataclasses\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import UTC\n\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.database import session_maker\nfrom storage.stored_conversation_metadata import StoredConversationMetadata",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/conversation_work.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/conversation_work.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\n\nfrom sqlalchemy import Column, Float, Index, Integer, String\nfrom storage.base import Base\n\n\nclass ConversationWork(Base):  # type: ignore\n    __tablename__ = 'conversation_work'\n\n    id = Column(Integer, primary_key=True, autoincrement=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/gitlab_webhook.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/gitlab_webhook.py",
      "line_number": 1,
      "code_snippet": "import sys\nfrom enum import IntEnum\n\nfrom sqlalchemy import ARRAY, Boolean, Column, DateTime, Integer, String, Text, text\nfrom storage.base import Base\n\n\nclass WebhookStatus(IntEnum):\n    PENDING = 0  # Conditions for installation webhook need checking\n    VERIFIED = 1  # Conditions are met for installing webhook",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stripe_customer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stripe_customer.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass StripeCustomer(Base):  # type: ignore\n    \"\"\"\n    Represents a stripe customer. We can't simply use the stripe API for this because:\n    \"Don\u2019t use search in read-after-write flows where strict consistency is necessary.\n    Under normal operating conditions, data is searchable in less than a minute.\n    Occasionally, propagation of new or updated data can be up to an hour behind during outages\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_workspace.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_workspace.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass LinearWorkspace(Base):  # type: ignore\n    __tablename__ = 'linear_workspaces'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    name = Column(String, nullable=False)\n    linear_org_id = Column(String, nullable=False)\n    admin_user_id = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_workspace.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_workspace.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass JiraWorkspace(Base):  # type: ignore\n    __tablename__ = 'jira_workspaces'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    name = Column(String, nullable=False)\n    jira_cloud_id = Column(String, nullable=False)\n    admin_user_id = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/experiment_assignment.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/experiment_assignment.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nDatabase model for experiment assignments.\n\nThis model tracks which experiments a conversation is assigned to and what variant\nthey received from PostHog feature flags.\n\"\"\"\n\nimport uuid\nfrom datetime import UTC, datetime\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_user.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass LinearUser(Base):  # type: ignore\n    __tablename__ = 'linear_users'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    keycloak_user_id = Column(String, nullable=False, index=True)\n    linear_user_id = Column(String, nullable=False, index=True)\n    linear_workspace_id = Column(Integer, nullable=False, index=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_user.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Identity, Integer, String, text\nfrom storage.base import Base\n\n\nclass SlackUser(Base):  # type: ignore\n    __tablename__ = 'slack_users'\n    id = Column(Integer, Identity(), primary_key=True)\n    keycloak_user_id = Column(String, nullable=False, index=True)\n    slack_user_id = Column(String, nullable=False, index=True)\n    slack_display_name = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_conversation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_conversation.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Boolean, Column, Identity, Integer, String\nfrom storage.base import Base\n\n\nclass SlackConversation(Base):  # type: ignore\n    __tablename__ = 'slack_conversation'\n    id = Column(Integer, Identity(), primary_key=True)\n    conversation_id = Column(String, nullable=False, index=True)\n    channel_id = Column(String, nullable=False)\n    keycloak_user_id = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/auth_token_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/auth_token_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import Awaitable, Callable, Dict\n\nfrom sqlalchemy import select, update\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.auth_tokens import AuthTokens\nfrom storage.database import a_session_maker",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/feedback.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/feedback.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import JSON, Column, DateTime, Enum, Integer, String, Text\nfrom sqlalchemy.sql import func\nfrom storage.base import Base\n\n\nclass Feedback(Base):  # type: ignore\n    __tablename__ = 'feedback'\n\n    id = Column(String, primary_key=True)\n    version = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_conversation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_conversation.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass LinearConversation(Base):  # type: ignore\n    __tablename__ = 'linear_conversations'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    conversation_id = Column(String, nullable=False, index=True)\n    issue_id = Column(String, nullable=False, index=True)\n    issue_key = Column(String, nullable=False, index=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_custom_secrets.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_custom_secrets.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, Identity, Integer, String\nfrom storage.base import Base\n\n\nclass StoredCustomSecrets(Base):  # type: ignore\n    __tablename__ = 'custom_secrets'\n    id = Column(Integer, Identity(), primary_key=True)\n    keycloak_user_id = Column(String, nullable=True, index=True)\n    secret_name = Column(String, nullable=False)\n    secret_value = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/maintenance_task.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/maintenance_task.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Type\n\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import Column, DateTime, Integer, String, Text, text\nfrom sqlalchemy import Enum as SQLEnum",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/database.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/database.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.pool import NullPool\nfrom sqlalchemy.util import await_only\n\nDB_HOST = os.environ.get('DB_HOST', 'localhost')  # for non-GCP environments",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_team.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/slack_team.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Identity, Integer, String, text\nfrom storage.base import Base\n\n\nclass SlackTeam(Base):  # type: ignore\n    __tablename__ = 'slack_teams'\n    id = Column(Integer, Identity(), primary_key=True)\n    team_id = Column(String, nullable=False, index=True, unique=True)\n    bot_access_token = Column(String, nullable=False)\n    created_at = Column(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_integration_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_integration_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom storage.database import session_maker\nfrom storage.jira_conversation import JiraConversation\nfrom storage.jira_user import JiraUser\nfrom storage.jira_workspace import JiraWorkspace\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_integration_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_integration_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom storage.database import session_maker\nfrom storage.jira_dc_conversation import JiraDcConversation\nfrom storage.jira_dc_user import JiraDcUser\nfrom storage.jira_dc_workspace import JiraDcWorkspace\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/offline_token_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/offline_token_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.database import session_maker\nfrom storage.stored_offline_token import StoredOfflineToken\n\nfrom openhands.core.config.openhands_config import OpenHandsConfig\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_integration_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/linear_integration_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom storage.database import session_maker\nfrom storage.linear_conversation import LinearConversation\nfrom storage.linear_user import LinearUser\nfrom storage.linear_workspace import LinearWorkspace\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_user.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass JiraUser(Base):  # type: ignore\n    __tablename__ = 'jira_users'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    keycloak_user_id = Column(String, nullable=False, index=True)\n    jira_user_id = Column(String, nullable=False, index=True)\n    jira_workspace_id = Column(Integer, nullable=False, index=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/gitlab_webhook_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/gitlab_webhook_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom integrations.types import GitLabResourceType\nfrom sqlalchemy import and_, asc, select, text, update\nfrom sqlalchemy.dialects.postgresql import insert\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.database import a_session_maker\nfrom storage.gitlab_webhook import GitlabWebhook",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/telemetry_identity.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/telemetry_identity.py",
      "line_number": 1,
      "code_snippet": "\"\"\"SQLAlchemy model for telemetry identity.\n\nThis model stores persistent identity information that must survive container restarts\nfor the OpenHands Enterprise Telemetry Service.\n\"\"\"\n\nfrom datetime import UTC, datetime\nfrom typing import Optional\n\nfrom sqlalchemy import CheckConstraint, Column, DateTime, Integer, String",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass ApiKey(Base):\n    \"\"\"\n    Represents an API key for a user.\n    \"\"\"\n\n    __tablename__ = 'api_keys'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py_28",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'create_device_code' on line 28 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
      "line_number": 28,
      "code_snippet": "    def create_device_code(\n        self,\n        expires_in: int = 600,  # 10 minutes default\n        max_attempts: int = 10,\n    ) -> DeviceCode:\n        \"\"\"Create a new device code entry.\n\n        Uses database constraints to ensure uniqueness, avoiding TOCTOU race conditions.\n        Retries on constraint violations until unique codes are generated.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Base64 High Entropy String",
      "description": "detect-secrets found a potential Base64 High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
      "line_number": 20,
      "code_snippet": "        \"\"\"Generate a human-readable user code (8 characters, uppercase letters and digits).\"\"\"\n        # Use a mix of uppercase letters and digits, avoiding confusing characters\n        alphabet = 'ABCDEFGHJKLMNPQRSTUVWXYZ23456789'  # No I, O, 0, 1\n        return ''.join(secrets.choice(alphabet) for _ in range(8))\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py_28_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'create_device_code'",
      "description": "Function 'create_device_code' on line 28 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
      "line_number": 28,
      "code_snippet": "        return ''.join(secrets.choice(alphabet) for _ in range(128))\n\n    def create_device_code(\n        self,\n        expires_in: int = 600,  # 10 minutes default\n        max_attempts: int = 10,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py_28_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_device_code'",
      "description": "Function 'create_device_code' on line 28 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
      "line_number": 28,
      "code_snippet": "        return ''.join(secrets.choice(alphabet) for _ in range(128))\n\n    def create_device_code(\n        self,\n        expires_in: int = 600,  # 10 minutes default\n        max_attempts: int = 10,",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py_49",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
      "line_number": 49,
      "code_snippet": "            RuntimeError: If unable to generate unique codes after max_attempts\n        \"\"\"\n        for attempt in range(max_attempts):\n            user_code = self.generate_user_code()\n            device_code = self.generate_device_code()\n            expires_at = datetime.now(timezone.utc) + timedelta(seconds=expires_in)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py_50",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code_store.py",
      "line_number": 50,
      "code_snippet": "        \"\"\"\n        for attempt in range(max_attempts):\n            user_code = self.generate_user_code()\n            device_code = self.generate_device_code()\n            expires_at = datetime.now(timezone.utc) + timedelta(seconds=expires_in)\n\n            device_code_entry = DeviceCode(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_conversation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_conversation.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass JiraDcConversation(Base):  # type: ignore\n    __tablename__ = 'jira_dc_conversations'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    conversation_id = Column(String, nullable=False, index=True)\n    issue_id = Column(String, nullable=False, index=True)\n    issue_key = Column(String, nullable=False, index=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/blocked_email_domain.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/blocked_email_domain.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\n\nfrom sqlalchemy import Column, DateTime, Identity, Integer, String\nfrom storage.base import Base\n\n\nclass BlockedEmailDomain(Base):  # type: ignore\n    \"\"\"Stores blocked email domain patterns.\n\n    Supports blocking:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/redis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/redis.py",
      "line_number": 1,
      "code_snippet": "import os\n\nimport redis\n\n# Redis configuration\nREDIS_HOST = os.environ.get('REDIS_HOST', 'localhost')\nREDIS_PORT = int(os.environ.get('REDIS_PORT', '6379'))\nREDIS_PASSWORD = os.environ.get('REDIS_PASSWORD', '')\nREDIS_DB = int(os.environ.get('REDIS_DB', '0'))\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/proactive_convos.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/proactive_convos.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\n\nfrom sqlalchemy import JSON, Boolean, Column, DateTime, Integer, String\nfrom storage.base import Base\n\n\nclass ProactiveConversation(Base):\n    __tablename__ = 'proactive_conversation_table'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    repo_id = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_conversation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_conversation.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass JiraConversation(Base):  # type: ignore\n    __tablename__ = 'jira_conversations'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    conversation_id = Column(String, nullable=False, index=True)\n    issue_id = Column(String, nullable=False, index=True)\n    issue_key = Column(String, nullable=False, index=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_offline_token.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_offline_token.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, String, text\nfrom storage.base import Base\n\n\nclass StoredOfflineToken(Base):\n    __tablename__ = 'offline_tokens'\n\n    user_id = Column(String(255), primary_key=True)\n    offline_token = Column(String, nullable=False)\n    created_at = Column(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/proactive_conversation_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/proactive_conversation_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom datetime import UTC, datetime, timedelta\nfrom typing import Callable\n\nfrom integrations.github.github_types import (\n    WorkflowRun,\n    WorkflowRunGroup,\n    WorkflowRunStatus,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_repo_map.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/user_repo_map.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Boolean, Column, Integer, String\nfrom storage.base import Base\n\n\nclass UserRepositoryMap(Base):\n    \"\"\"\n    Represents a map between user id and repo ids\n    \"\"\"\n\n    __tablename__ = 'user-repos'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_secrets_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_secrets_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport hashlib\nfrom base64 import b64decode, b64encode\nfrom dataclasses import dataclass\n\nfrom cryptography.fernet import Fernet\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.database import session_maker\nfrom storage.stored_custom_secrets import StoredCustomSecrets",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/device_code.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Device code storage model for OAuth 2.0 Device Flow.\"\"\"\n\nfrom datetime import datetime, timezone\nfrom enum import Enum\n\nfrom sqlalchemy import Column, DateTime, Integer, String\nfrom storage.base import Base\n\n\nclass DeviceCodeStatus(Enum):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_user.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass JiraDcUser(Base):  # type: ignore\n    __tablename__ = 'jira_dc_users'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    keycloak_user_id = Column(String, nullable=False, index=True)\n    jira_dc_user_id = Column(String, nullable=False, index=True)\n    jira_dc_workspace_id = Column(Integer, nullable=False, index=True)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/billing_session.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/billing_session.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\n\nfrom sqlalchemy import DECIMAL, Column, DateTime, Enum, String\nfrom storage.base import Base\n\n\nclass BillingSession(Base):  # type: ignore\n    \"\"\"\n    Represents a Stripe billing session for credit purchases.\n    Tracks the status of payment transactions and associated user information.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/openhands_pr.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/openhands_pr.py",
      "line_number": 1,
      "code_snippet": "from integrations.types import PRStatus\nfrom sqlalchemy import (\n    Boolean,\n    Column,\n    DateTime,\n    Enum,\n    Identity,\n    Integer,\n    String,\n    text,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py_28",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'create_api_key' on line 28 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
      "line_number": 28,
      "code_snippet": "    def create_api_key(\n        self, user_id: str, name: str | None = None, expires_at: datetime | None = None\n    ) -> str:\n        \"\"\"Create a new API key for a user.\n\n        Args:\n            user_id: The ID of the user to create the key for\n            name: Optional name for the key\n            expires_at: Optional expiration date for the key\n\n        Returns:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
      "line_number": 20,
      "code_snippet": "    session_maker: sessionmaker\n\n    API_KEY_PREFIX = 'sk-oh-'\n\n    def generate_api_key(self, length: int = 32) -> str:",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py_167_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'api_key' containing sensitive data is being logged on line 167. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
      "line_number": 167,
      "code_snippet": "    def get_instance(cls) -> ApiKeyStore:\n        \"\"\"Get an instance of the ApiKeyStore.\"\"\"\n        logger.debug('api_key_store.get_instance')\n        return ApiKeyStore(session_maker)",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py_28_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'create_api_key'",
      "description": "Function 'create_api_key' on line 28 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
      "line_number": 28,
      "code_snippet": "        return f'{self.API_KEY_PREFIX}{random_part}'\n\n    def create_api_key(\n        self, user_id: str, name: str | None = None, expires_at: datetime | None = None\n    ) -> str:\n        \"\"\"Create a new API key for a user.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py_28_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'create_api_key'",
      "description": "Function 'create_api_key' on line 28 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
      "line_number": 28,
      "code_snippet": "        return f'{self.API_KEY_PREFIX}{random_part}'\n\n    def create_api_key(\n        self, user_id: str, name: str | None = None, expires_at: datetime | None = None\n    ) -> str:\n        \"\"\"Create a new API key for a user.",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/api_key_store.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport secrets\nimport string\nfrom dataclasses import dataclass\nfrom datetime import UTC, datetime\n\nfrom sqlalchemy import update\nfrom sqlalchemy.orm import sessionmaker\nfrom storage.api_key import ApiKey",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_workspace.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/jira_dc_workspace.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass JiraDcWorkspace(Base):  # type: ignore\n    __tablename__ = 'jira_dc_workspaces'\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    name = Column(String, nullable=False)\n    admin_user_id = Column(String, nullable=False)\n    webhook_secret = Column(String, nullable=False)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_repository.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/stored_repository.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Boolean, Column, Integer, String\nfrom storage.base import Base\n\n\nclass StoredRepository(Base):  # type: ignore\n    \"\"\"\n    Represents a repositories fetched from git providers.\n    \"\"\"\n\n    __tablename__ = 'repos'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_conversation_validator.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/saas_conversation_validator.py",
      "line_number": 1,
      "code_snippet": "from server.auth.auth_error import AuthError, ExpiredError\nfrom server.auth.saas_user_auth import saas_user_auth_from_signed_token\nfrom server.auth.token_manager import TokenManager\nfrom socketio.exceptions import ConnectionRefusedError\nfrom storage.api_key_store import ApiKeyStore\n\nfrom openhands.core.config import load_openhands_config\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.server.shared import ConversationStoreImpl\nfrom openhands.storage.conversation.conversation_validator import ConversationValidator",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/github_app_installation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/github_app_installation.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import Column, DateTime, Integer, String, text\nfrom storage.base import Base\n\n\nclass GithubAppInstallation(Base):  # type: ignore\n    \"\"\"\n    Represents a Github App Installation with associated token.\n    \"\"\"\n\n    __tablename__ = 'github_app_installations'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/telemetry_metrics.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/storage/telemetry_metrics.py",
      "line_number": 1,
      "code_snippet": "\"\"\"SQLAlchemy model for telemetry metrics data.\n\nThis model stores individual metric collection records with upload tracking\nand retry logic for the OpenHands Enterprise Telemetry Service.\n\"\"\"\n\nimport uuid\nfrom datetime import UTC, datetime\nfrom typing import Any, Dict, Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/stripe_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/stripe_service.py",
      "line_number": 1,
      "code_snippet": "import stripe\nfrom server.auth.token_manager import TokenManager\nfrom server.constants import STRIPE_API_KEY\nfrom server.logger import logger\nfrom storage.database import session_maker\nfrom storage.stripe_customer import StripeCustomer\n\nstripe.api_key = STRIPE_API_KEY\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\nfrom pydantic import BaseModel\n\nfrom openhands.core.schema import AgentState\n\n\nclass SourceType(str, Enum):\n    GITHUB = 'github'\n    GITLAB = 'gitlab'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/types.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/types.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom enum import Enum\n\nfrom jinja2 import Environment\nfrom pydantic import BaseModel\n\n\nclass GitLabResourceType(Enum):\n    GROUP = 'group'\n    SUBGROUP = 'subgroup'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/utils.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport json\nimport os\nimport re\nfrom typing import TYPE_CHECKING\n\nfrom jinja2 import Environment, FileSystemLoader\nfrom server.config import get_config\nfrom server.constants import WEB_HOST",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/resolver_context.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/resolver_context.py",
      "line_number": 1,
      "code_snippet": "from openhands.app_server.user.user_context import UserContext\nfrom openhands.app_server.user.user_models import UserInfo\nfrom openhands.integrations.provider import PROVIDER_TOKEN_TYPE\nfrom openhands.integrations.service_types import ProviderType\nfrom openhands.sdk.secret import SecretSource, StaticSecret\nfrom openhands.server.user_auth.user_auth import UserAuth\n\n\nclass ResolverUserContext(UserContext):\n    \"\"\"User context for resolver operations that inherits from UserContext.\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/v1_utils.py_14_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'offline_token' containing sensitive data is being logged on line 14. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/v1_utils.py",
      "line_number": 14,
      "code_snippet": "    offline_token = await token_manager.load_offline_token(keycloak_user_id)\n    if offline_token is None:\n        logger.info('no_offline_token_found')\n\n    user_auth = SaasUserAuth(",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/install_gitlab_webhooks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/install_gitlab_webhooks.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom typing import cast\n\nfrom integrations.gitlab.webhook_installation import (\n    BreakLoopException,\n    install_webhook_on_resource,\n    verify_webhook_conditions,\n)\nfrom integrations.types import GitLabResourceType\nfrom integrations.utils import GITLAB_WEBHOOK_URL",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/common_room_sync.py_549_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'sync_recent_conversations_to_common_room'",
      "description": "Function 'sync_recent_conversations_to_common_room' on line 549 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/common_room_sync.py",
      "line_number": 549,
      "code_snippet": "\n\ndef sync_recent_conversations_to_common_room(minutes: int = 60):\n    \"\"\"Main function to sync recent conversations to Common Room.\n\n    Args:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/common_room_sync.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/common_room_sync.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"\nCommon Room Sync\n\nThis script queries the database to count conversations created by each user,\nthen creates or updates a signal in Common Room for each user with their\nconversation count.\n\"\"\"\n\nimport asyncio",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/resend_keycloak.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/resend_keycloak.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\"\"\"Sync script to add Keycloak users to Resend.com audience.\n\nThis script uses the Keycloak admin client to fetch users and adds them to a\nResend.com audience. It handles rate limiting and retries with exponential\nbackoff for adding contacts. When a user is newly added to the mailing list, a welcome email is sent.\n\nRequired environment variables:\n- KEYCLOAK_SERVER_URL: URL of the Keycloak server\n- KEYCLOAK_REALM_NAME: Keycloak realm name",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/enrich_user_interaction_data.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/sync/enrich_user_interaction_data.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.github.data_collector import GitHubDataCollector\nfrom storage.openhands_pr import OpenhandsPR\nfrom storage.openhands_pr_store import OpenhandsPRStore\n\nfrom openhands.core.logger import openhands_logger as logger\n\nPROCESS_AMOUNT = 50\nMAX_RETRIES = 3",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.types import GitLabResourceType\nfrom integrations.utils import store_repositories_in_db\nfrom pydantic import SecretStr\nfrom server.auth.token_manager import TokenManager\nfrom storage.gitlab_webhook import GitlabWebhook, WebhookStatus\nfrom storage.gitlab_webhook_store import GitlabWebhookStore\n\nfrom openhands.core.logger import openhands_logger as logger",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_view.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\n\nfrom integrations.models import Message\nfrom integrations.types import ResolverViewInterface, UserData\nfrom integrations.utils import HOST, get_oh_labels, has_exact_mention\nfrom jinja2 import Environment\nfrom server.auth.token_manager import TokenManager\nfrom server.config import get_config\nfrom storage.database import session_maker\nfrom storage.saas_secrets_store import SaasSecretsStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/gitlab_manager.py",
      "line_number": 1,
      "code_snippet": "from types import MappingProxyType\n\nfrom integrations.gitlab.gitlab_view import (\n    GitlabFactory,\n    GitlabInlineMRComment,\n    GitlabIssue,\n    GitlabIssueComment,\n    GitlabMRComment,\n    GitlabViewType,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/webhook_installation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/gitlab/webhook_installation.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Shared utilities for GitLab webhook installation.\n\nThis module contains reusable functions and classes for installing GitLab webhooks\nthat can be used by both the cron job and API routes.\n\"\"\"\n\nfrom typing import cast\nfrom uuid import uuid4\n\nfrom integrations.types import GitLabResourceType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/linear/linear_view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/linear/linear_view.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\n\nfrom integrations.linear.linear_types import LinearViewInterface, StartingConvoException\nfrom integrations.models import JobContext\nfrom integrations.utils import CONVERSATION_URL, get_final_agent_observation\nfrom jinja2 import Environment\nfrom storage.linear_conversation import LinearConversation\nfrom storage.linear_integration_store import LinearIntegrationStore\nfrom storage.linear_user import LinearUser\nfrom storage.linear_workspace import LinearWorkspace",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/linear/linear_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/linear/linear_manager.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport hmac\nfrom typing import Dict, Optional, Tuple\n\nimport httpx\nfrom fastapi import Request\nfrom integrations.linear.linear_types import LinearViewInterface\nfrom integrations.linear.linear_view import (\n    LinearExistingConversationView,\n    LinearFactory,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira/jira_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira/jira_manager.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport hmac\nfrom typing import Dict, Optional, Tuple\nfrom urllib.parse import urlparse\n\nimport httpx\nfrom fastapi import Request\nfrom integrations.jira.jira_types import JiraViewInterface\nfrom integrations.jira.jira_view import (\n    JiraExistingConversationView,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira/jira_view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira/jira_view.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\n\nfrom integrations.jira.jira_types import JiraViewInterface, StartingConvoException\nfrom integrations.models import JobContext\nfrom integrations.utils import CONVERSATION_URL, get_final_agent_observation\nfrom jinja2 import Environment\nfrom storage.jira_conversation import JiraConversation\nfrom storage.jira_integration_store import JiraIntegrationStore\nfrom storage.jira_user import JiraUser\nfrom storage.jira_workspace import JiraWorkspace",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_service.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.utils import store_repositories_in_db\nfrom pydantic import SecretStr\nfrom server.auth.token_manager import TokenManager\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.github_service import GitHubService\nfrom openhands.integrations.service_types import ProviderType, Repository\nfrom openhands.server.types import AppMode",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/data_collector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/data_collector.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport json\nimport os\nimport re\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\n\nfrom github import Auth, Github, GithubIntegration\nfrom integrations.github.github_view import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_v1_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_v1_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Any\nfrom uuid import UUID\n\nimport httpx\nfrom github import Auth, Github, GithubIntegration\nfrom integrations.utils import CONVERSATION_URL, get_summary_instruction\nfrom pydantic import Field\nfrom server.auth.constants import GITHUB_APP_CLIENT_ID, GITHUB_APP_PRIVATE_KEY\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_types.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_types.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass WorkflowRunStatus(Enum):\n    FAILURE = 'failure'\n    COMPLETED = 'completed'\n    PENDING = 'pending'\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_view.py_532_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'leave_requesting_comment'",
      "description": "Function 'leave_requesting_comment' on line 532 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_view.py",
      "line_number": 532,
      "code_snippet": "\n    @staticmethod\n    def leave_requesting_comment(pr: Issue, failed_runs: WorkflowRunGroup):\n        failed_jobs: dict = {'actions': [], 'merge conflict': []}\n\n        pr_obj = pr.as_pull_request()",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_view.py_532_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'leave_requesting_comment'",
      "description": "Function 'leave_requesting_comment' on line 532 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_view.py",
      "line_number": 532,
      "code_snippet": "\n    @staticmethod\n    def leave_requesting_comment(pr: Issue, failed_runs: WorkflowRunGroup):\n        failed_jobs: dict = {'actions': [], 'merge conflict': []}\n\n        pr_obj = pr.as_pull_request()",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_view.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom uuid import UUID, uuid4\n\nfrom github import Auth, Github, GithubIntegration\nfrom github.Issue import Issue\nfrom integrations.github.github_types import (\n    WorkflowRun,\n    WorkflowRunGroup,\n    WorkflowRunStatus,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_solvability.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_solvability.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport time\n\nfrom github import Auth, Github\nfrom integrations.github.github_view import (\n    GithubInlinePRComment,\n    GithubIssueComment,\n    GithubPRComment,\n    GithubViewType,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/github/github_manager.py",
      "line_number": 1,
      "code_snippet": "from types import MappingProxyType\n\nfrom github import Auth, Github, GithubIntegration\nfrom integrations.github.data_collector import GitHubDataCollector\nfrom integrations.github.github_solvability import summarize_issue_solvability\nfrom integrations.github.github_view import (\n    GithubFactory,\n    GithubFailingAction,\n    GithubInlinePRComment,\n    GithubIssue,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira_dc/jira_dc_view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira_dc/jira_dc_view.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\n\nfrom integrations.jira_dc.jira_dc_types import (\n    JiraDcViewInterface,\n    StartingConvoException,\n)\nfrom integrations.models import JobContext\nfrom integrations.utils import CONVERSATION_URL, get_final_agent_observation\nfrom jinja2 import Environment\nfrom storage.jira_dc_conversation import JiraDcConversation",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira_dc/jira_dc_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/jira_dc/jira_dc_manager.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport hmac\nfrom typing import Dict, Optional, Tuple\nfrom urllib.parse import urlparse\n\nimport httpx\nfrom fastapi import Request\nfrom integrations.jira_dc.jira_dc_types import (\n    JiraDcViewInterface,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py_201",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py",
      "line_number": 201,
      "code_snippet": "            state = jwt.encode(\n                message.message, jwt_secret.get_secret_value(), algorithm='HS256'\n            )\n            link = authorize_url_generator.generate(state)\n            msg = self.login_link.format(link)\n\n            logger.info('slack_not_yet_authenticated')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py_218",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py",
      "line_number": 218,
      "code_snippet": "    async def send_message(self, message: Message, slack_view: SlackViewInterface):\n        client = AsyncWebClient(token=slack_view.bot_access_token)\n        if message.ephemeral and isinstance(message.message, str):\n            await client.chat_postEphemeral(\n                channel=slack_view.channel_id,\n                markdown_text=message.message,\n                user=slack_view.slack_user_id,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py_225",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py",
      "line_number": 225,
      "code_snippet": "                thread_ts=slack_view.thread_ts,\n            )\n        elif message.ephemeral and isinstance(message.message, dict):\n            await client.chat_postEphemeral(\n                channel=slack_view.channel_id,\n                user=slack_view.slack_user_id,\n                thread_ts=slack_view.thread_ts,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py_233",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_manager.py",
      "line_number": 233,
      "code_snippet": "                blocks=message.message['blocks'],\n            )\n        else:\n            await client.chat_postMessage(\n                channel=slack_view.channel_id,\n                markdown_text=message.message,\n                thread_ts=slack_view.message_ts,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_v1_callback_processor.py_114",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_v1_callback_processor.py",
      "line_number": 114,
      "code_snippet": "\n        try:\n            # Post the summary as a threaded reply\n            response = client.chat_postMessage(\n                channel=channel_id,\n                text=summary,\n                thread_ts=thread_ts,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_view.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/slack/slack_view.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom uuid import UUID, uuid4\n\nfrom integrations.models import Message\nfrom integrations.resolver_context import ResolverUserContext\nfrom integrations.slack.slack_types import SlackViewInterface, StartingConvoException\nfrom integrations.slack.slack_v1_callback_processor import SlackV1CallbackProcessor\nfrom integrations.utils import (\n    CONVERSATION_URL,\n    ENABLE_V1_SLACK_RESOLVER,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/bitbucket/bitbucket_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/bitbucket/bitbucket_service.py",
      "line_number": 1,
      "code_snippet": "from pydantic import SecretStr\nfrom server.auth.token_manager import TokenManager\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.bitbucket.bitbucket_service import BitBucketService\nfrom openhands.integrations.service_types import ProviderType\n\n\nclass SaaSBitBucketService(BitBucketService):\n    def __init__(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/difficulty_level.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/difficulty_level.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom enum import Enum\n\n\nclass DifficultyLevel(Enum):\n    \"\"\"Enum representing the difficulty level based on solvability score.\"\"\"\n\n    EASY = ('EASY', 0.7, '\ud83d\udfe2')\n    MEDIUM = ('MEDIUM', 0.4, '\ud83d\udfe1')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py_255",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'embed' on line 255 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py",
      "line_number": 255,
      "code_snippet": "    def embed(\n        self,\n        issue_description: str,\n        llm_config: LLMConfig,\n        temperature: float = 1.0,\n        samples: int = 10,\n    ) -> FeatureEmbedding:\n        \"\"\"\n        Generate a feature embedding for a single issue description.\n\n        Makes multiple LLM calls to collect samples and reduce variance in feature evaluations.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py_288_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'embed'",
      "description": "Function 'embed' on line 255 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py",
      "line_number": 288,
      "code_snippet": "        for _ in range(samples):\n            start_time = time.time()\n            response = llm.completion(\n                messages=[\n                    self.system_message(),\n                    self.user_message(issue_description, set_cache=(samples > 1)),",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py_288",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/featurizer.py",
      "line_number": 288,
      "code_snippet": "        # Generate multiple samples to account for LLM variability\n        for _ in range(samples):\n            start_time = time.time()\n            response = llm.completion(\n                messages=[\n                    self.system_message(),\n                    self.user_message(issue_description, set_cache=(samples > 1)),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_258",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.predict_proba' is used in 'WHERE' on line 258 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 258,
      "code_snippet": "        \"\"\"\n        probabilities = self.predict_proba(issues, llm_config=llm_config)\n        # Apply 0.5 threshold to convert probabilities to binary predictions\n        labels = probabilities[:, 1] >= 0.5",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_205",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.classifier.predict_proba' is used in 'WHERE' on line 205 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 205,
      "code_snippet": "        self._classifier_attrs['feature_importances_'] = self._importance(\n            features, self.classifier.predict_proba(features), labels\n        )\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_4_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 4. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 4,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_183_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'fit'",
      "description": "Function 'fit' on line 183 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 183,
      "code_snippet": "        return self.features_\n\n    def fit(\n        self, issues: pd.Series, labels: pd.Series, llm_config: LLMConfig\n    ) -> SolvabilityClassifier:\n        \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_210_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'predict_proba'",
      "description": "Function 'predict_proba' on line 210 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 210,
      "code_snippet": "        return self\n\n    def predict_proba(self, issues: pd.Series, llm_config: LLMConfig) -> np.ndarray:\n        \"\"\"\n        Predict the solvability probabilities for the input issues.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_245_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'predict'",
      "description": "Function 'predict' on line 245 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 245,
      "code_snippet": "        return scores  # type: ignore[no-any-return]\n\n    def predict(self, issues: pd.Series, llm_config: LLMConfig) -> np.ndarray:\n        \"\"\"\n        Predict the solvability of the input issues by returning binary labels.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_383_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'solvability_report'",
      "description": "Function 'solvability_report' on line 383 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 383,
      "code_snippet": "        )\n\n    def solvability_report(\n        self, issue: str, llm_config: LLMConfig, **kwargs: Any\n    ) -> SolvabilityReport:\n        \"\"\"",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/classifier.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport base64\nimport pickle\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nimport shap\nfrom integrations.solvability.models.featurizer import Feature, Featurizer",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/summary.py_127_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'from_report'",
      "description": "Function 'from_report' on line 122 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/summary.py",
      "line_number": 127,
      "code_snippet": "\n        start_time = time.time()\n        response = llm.completion(\n            messages=[\n                SolvabilitySummary.system_message(),\n                SolvabilitySummary.user_message(report),",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/summary.py_127",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/integrations/solvability/models/summary.py",
      "line_number": 127,
      "code_snippet": "        import time\n\n        start_time = time.time()\n        response = llm.completion(\n            messages=[\n                SolvabilitySummary.system_message(),\n                SolvabilitySummary.user_message(report),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/sql_shared_conversation_info_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/sql_shared_conversation_info_service.py",
      "line_number": 1,
      "code_snippet": "\"\"\"SQL implementation of SharedConversationInfoService.\n\nThis implementation provides read-only access to shared conversations:\n- Direct database access without user permission checks\n- Filters only conversations marked as shared (currently public)\n- Full async/await support using SQL async db_sessions\n\"\"\"\n\nfrom __future__ import annotations\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_conversation_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_conversation_router.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Shared Conversation router for OpenHands Server.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Annotated\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom server.sharing.shared_conversation_info_service import (\n    SharedConversationInfoService,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_conversation_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_conversation_models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\n\n# Simplified imports to avoid dependency chain issues\n# from openhands.integrations.service_types import ProviderType\n# from openhands.sdk.llm import MetricsSnapshot\n# from openhands.storage.data_models.conversation_metadata import ConversationTrigger\n# For now, use Any to avoid import issues\nfrom typing import Any\nfrom uuid import uuid4",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/google_cloud_shared_event_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/google_cloud_shared_event_service.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Implementation of SharedEventService.\n\nThis implementation provides read-only access to events from shared conversations:\n- Validates that the conversation is shared before returning events\n- Uses existing EventService for actual event retrieval\n- Uses SharedConversationInfoService for shared conversation validation\n\"\"\"\n\nfrom __future__ import annotations\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_event_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/sharing/shared_event_router.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Shared Event router for OpenHands Server.\"\"\"\n\nfrom datetime import datetime\nfrom typing import Annotated\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom server.sharing.google_cloud_shared_event_service import (\n    GoogleCloudSharedEventServiceInjector,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/domain_blocker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/domain_blocker.py",
      "line_number": 1,
      "code_snippet": "from storage.blocked_email_domain_store import BlockedEmailDomainStore\nfrom storage.database import session_maker\n\nfrom openhands.core.logger import openhands_logger as logger\n\n\nclass DomainBlocker:\n    def __init__(self, store: BlockedEmailDomainStore) -> None:\n        logger.debug('Initializing DomainBlocker')\n        self.store = store",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/token_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/token_manager.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport base64\nimport hashlib\nimport json\nimport time\nfrom base64 import b64encode\nfrom urllib.parse import parse_qs\n\nimport httpx\nimport jwt",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/email_validation.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/email_validation.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Email validation utilities for preventing duplicate signups with + modifier.\"\"\"\n\nimport re\n\n\ndef extract_base_email(email: str) -> str | None:\n    \"\"\"Extract base email from an email address.\n\n    For emails with + modifier, extracts the base email (local part before + and @, plus domain).\n    For emails without + modifier, returns the email as-is.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/constants.py",
      "line_number": 1,
      "code_snippet": "import os\n\nGITHUB_APP_CLIENT_ID = os.getenv('GITHUB_APP_CLIENT_ID', '').strip()\nGITHUB_APP_CLIENT_SECRET = os.getenv('GITHUB_APP_CLIENT_SECRET', '').strip()\nGITHUB_APP_WEBHOOK_SECRET = os.getenv('GITHUB_APP_WEBHOOK_SECRET', '')\nGITHUB_APP_PRIVATE_KEY = os.getenv('GITHUB_APP_PRIVATE_KEY', '').replace('\\\\n', '\\n')\nKEYCLOAK_SERVER_URL = os.getenv('KEYCLOAK_SERVER_URL', '').rstrip('/')\nKEYCLOAK_REALM_NAME = os.getenv('KEYCLOAK_REALM_NAME', '')\nKEYCLOAK_PROVIDER_NAME = os.getenv('KEYCLOAK_PROVIDER_NAME', '')\nKEYCLOAK_CLIENT_ID = os.getenv('KEYCLOAK_CLIENT_ID', '')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/auth_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/auth_utils.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom server.auth.sheets_client import GoogleSheetsClient\n\nfrom openhands.core.logger import openhands_logger as logger\n\n\nclass UserVerifier:\n    def __init__(self) -> None:\n        logger.debug('Initializing UserVerifier')",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/github_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/github_utils.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom integrations.github.github_service import SaaSGitHubService\nfrom pydantic import SecretStr\nfrom server.auth.sheets_client import GoogleSheetsClient\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.integrations.github.github_types import GitHubUser\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/recaptcha_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/recaptcha_service.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport hmac\nfrom dataclasses import dataclass\n\nfrom google.cloud import recaptchaenterprise_v1\nfrom server.auth.constants import (\n    RECAPTCHA_BLOCK_THRESHOLD,\n    RECAPTCHA_HMAC_SECRET,\n    RECAPTCHA_PROJECT_ID,\n    RECAPTCHA_SITE_KEY,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/saas_user_auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/saas_user_auth.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom dataclasses import dataclass\nfrom types import MappingProxyType\n\nimport jwt\nfrom fastapi import Request\nfrom keycloak.exceptions import KeycloakError\nfrom pydantic import SecretStr\nfrom server.auth.auth_error import (\n    AuthError,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/sheets_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/auth/sheets_client.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Tuple\n\nimport gspread\nfrom google.auth import default\n\nfrom openhands.core.logger import openhands_logger as logger\n\n\nclass GoogleSheetsClient:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/rate_limit_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/rate_limit_utils.py",
      "line_number": 1,
      "code_snippet": "from fastapi import HTTPException, Request, status\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.server.shared import sio\n\n# Rate limiting constants\nRATE_LIMIT_USER_SECONDS = 120  # 2 minutes per user_id\nRATE_LIMIT_IP_SECONDS = 300  # 5 minutes per IP address\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/conversation_callback_utils.py_3_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 3. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/conversation_callback_utils.py",
      "line_number": 3,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/conversation_callback_utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/utils/conversation_callback_utils.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport json\nimport pickle\nfrom datetime import datetime\n\nfrom server.logger import logger\nfrom storage.conversation_callback import (\n    CallbackStatus,\n    ConversationCallback,\n    ConversationCallbackProcessor,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/github_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/github_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom datetime import datetime\n\nfrom integrations.github.github_manager import GithubManager\nfrom integrations.github.github_view import GithubViewType\nfrom integrations.models import Message, SourceType\nfrom integrations.utils import (\n    extract_summary_from_conversation_manager,\n    get_summary_instruction,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/jira_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/jira_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.jira.jira_manager import JiraManager\nfrom integrations.utils import (\n    extract_summary_from_conversation_manager,\n    get_last_user_msg_from_conversation_manager,\n    get_summary_instruction,\n    markdown_to_jira_markup,\n)\nfrom server.auth.token_manager import TokenManager",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/linear_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/linear_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.linear.linear_manager import LinearManager\nfrom integrations.utils import (\n    extract_summary_from_conversation_manager,\n    get_last_user_msg_from_conversation_manager,\n    get_summary_instruction,\n)\nfrom server.auth.token_manager import TokenManager\nfrom storage.conversation_callback import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/jira_dc_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/jira_dc_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.jira_dc.jira_dc_manager import JiraDcManager\nfrom integrations.utils import (\n    extract_summary_from_conversation_manager,\n    get_last_user_msg_from_conversation_manager,\n    get_summary_instruction,\n    markdown_to_jira_markup,\n)\nfrom server.auth.token_manager import TokenManager",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/gitlab_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/gitlab_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom datetime import datetime\n\nfrom integrations.gitlab.gitlab_manager import GitlabManager\nfrom integrations.gitlab.gitlab_view import GitlabViewType\nfrom integrations.models import Message, SourceType\nfrom integrations.utils import (\n    extract_summary_from_conversation_manager,\n    get_summary_instruction,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/slack_callback_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/conversation_callback_processor/slack_callback_processor.py",
      "line_number": 1,
      "code_snippet": "import asyncio\n\nfrom integrations.models import Message, SourceType\nfrom integrations.slack.slack_manager import SlackManager\nfrom integrations.slack.slack_view import SlackFactory\nfrom integrations.utils import (\n    extract_summary_from_conversation_manager,\n    get_last_user_msg_from_conversation_manager,\n    get_summary_instruction,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py_162_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'user_info' containing sensitive data is being logged on line 162. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py",
      "line_number": 162,
      "code_snippet": "\n    user_info = await token_manager.get_user_info(keycloak_access_token)\n    logger.debug(f'user_info: {user_info}')\n    if ROLE_CHECK_ENABLED and 'roles' not in user_info:\n        return JSONResponse(",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py_411_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'user_info' containing sensitive data is being logged on line 411. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py",
      "line_number": 411,
      "code_snippet": "\n    user_info = await token_manager.get_user_info(keycloak_access_token)\n    logger.debug(f'user_info: {user_info}')\n    if 'sub' not in user_info:\n        return JSONResponse(",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py_558_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 558. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py",
      "line_number": 558,
      "code_snippet": "        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail='Forbidden')\n\n    logger.info(f'Refreshing token for conversation {sid}')\n    provider_handler = ProviderHandler(\n        create_provider_tokens_object([provider]), external_auth_id=user_id",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/auth.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport json\nimport warnings\nfrom datetime import datetime, timezone\nfrom typing import Annotated, Literal, Optional\nfrom urllib.parse import quote\n\nimport posthog\nfrom fastapi import APIRouter, Header, HTTPException, Request, Response, status\nfrom fastapi.responses import JSONResponse, RedirectResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/user.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/user.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nfrom fastapi import APIRouter, Depends, Query, status\nfrom fastapi.responses import JSONResponse\nfrom pydantic import SecretStr\nfrom server.auth.token_manager import TokenManager\n\nfrom openhands.integrations.provider import (\n    PROVIDER_TOKEN_TYPE,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/oauth_device.py_23",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 23. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/oauth_device.py",
      "line_number": 23,
      "code_snippet": "DEVICE_TOKEN_POLL_INTERVAL = 5  # seconds\n\nAPI_KEY_NAME = 'Device Link Access Key'\nKEY_EXPIRATION_TIME = timedelta(days=1)  # Key expires in 24 hours\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/oauth_device.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/oauth_device.py",
      "line_number": 1,
      "code_snippet": "\"\"\"OAuth 2.0 Device Flow endpoints for CLI authentication.\"\"\"\n\nfrom datetime import UTC, datetime, timedelta\nfrom typing import Optional\n\nfrom fastapi import APIRouter, Depends, Form, HTTPException, Request, status\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom storage.api_key_store import ApiKeyStore\nfrom storage.database import session_maker",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/billing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/billing.py",
      "line_number": 1,
      "code_snippet": "# billing.py - Handles all billing-related operations including credit management and Stripe integration\nimport typing\nfrom datetime import UTC, datetime\nfrom decimal import Decimal\nfrom enum import Enum\n\nimport httpx\nimport stripe\nfrom dateutil.relativedelta import relativedelta  # type: ignore\nfrom fastapi import APIRouter, Depends, HTTPException, Request, status",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/event_webhook.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/event_webhook.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport json\nfrom enum import Enum\nfrom typing import Annotated, Tuple\n\nfrom fastapi import (\n    APIRouter,\n    BackgroundTasks,\n    Header,\n    HTTPException,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/feedback.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/feedback.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, List, Optional\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field\nfrom sqlalchemy.future import select\nfrom storage.database import session_maker\nfrom storage.feedback import ConversationFeedback\nfrom storage.stored_conversation_metadata import StoredConversationMetadata\n\nfrom openhands.events.event_store import EventStore",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/debugging.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/debugging.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os\nimport time\nfrom threading import Thread\n\nfrom fastapi import APIRouter, FastAPI\nfrom sqlalchemy import func, select\nfrom storage.database import a_session_maker, engine, session_maker\nfrom storage.user_settings import UserSettings\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/email.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/email.py",
      "line_number": 1,
      "code_snippet": "import re\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request, status\nfrom fastapi.responses import JSONResponse, RedirectResponse\nfrom pydantic import BaseModel, field_validator\nfrom server.auth.constants import KEYCLOAK_CLIENT_ID\nfrom server.auth.keycloak_manager import get_keycloak_admin\nfrom server.auth.saas_user_auth import SaasUserAuth\nfrom server.routes.auth import set_response_cookie\nfrom server.utils.rate_limit_utils import check_rate_limit_by_user_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/github_proxy.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/github_proxy.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport json\nimport os\nfrom base64 import b64decode, b64encode\nfrom urllib.parse import parse_qs, urlencode, urlparse\n\nimport httpx\nfrom cryptography.fernet import Fernet\nfrom fastapi import FastAPI, Request, Response\nfrom fastapi.responses import RedirectResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/mcp_patch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/mcp_patch.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom fastmcp import Client, FastMCP\nfrom fastmcp.client.transports import NpxStdioTransport\n\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.server.routes.mcp import mcp_server\n\nENABLE_MCP_SEARCH_ENGINE = (\n    os.getenv('ENABLE_MCP_SEARCH_ENGINE', 'false').lower() == 'true'",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/api_keys.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/api_keys.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\n\nimport httpx\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, field_validator\nfrom server.config import get_config\nfrom server.constants import (\n    BYOR_KEY_VERIFICATION_TIMEOUT,\n    LITE_LLM_API_KEY,\n    LITE_LLM_API_URL,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/maintenance_task_processor/user_version_upgrade_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/maintenance_task_processor/user_version_upgrade_processor.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nfrom typing import List\n\nfrom server.constants import CURRENT_USER_SETTINGS_VERSION\nfrom server.logger import logger\nfrom storage.database import session_maker\nfrom storage.maintenance_task import MaintenanceTask, MaintenanceTaskProcessor\nfrom storage.saas_settings_store import SaasSettingsStore\nfrom storage.user_settings import UserSettings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/gitlab.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/gitlab.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport hashlib\nimport json\n\nfrom fastapi import APIRouter, Depends, Header, HTTPException, Request, status\nfrom fastapi.responses import JSONResponse\nfrom integrations.gitlab.gitlab_manager import GitlabManager\nfrom integrations.gitlab.gitlab_service import SaaSGitLabService\nfrom integrations.gitlab.webhook_installation import (\n    BreakLoopException,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/linear.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/linear.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport re\nimport uuid\n\nimport requests\nfrom fastapi import APIRouter, BackgroundTasks, HTTPException, Request, status\nfrom fastapi.responses import JSONResponse, RedirectResponse\nfrom integrations.linear.linear_manager import LinearManager\nfrom integrations.models import Message, SourceType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/jira_dc.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/jira_dc.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport re\nimport uuid\nfrom urllib.parse import urlparse\n\nimport requests\nfrom fastapi import (\n    APIRouter,\n    BackgroundTasks,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/jira.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/jira.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport re\nimport uuid\nfrom urllib.parse import urlparse\n\nimport requests\nfrom fastapi import APIRouter, BackgroundTasks, HTTPException, Request, status\nfrom fastapi.responses import JSONResponse, RedirectResponse\nfrom integrations.jira.jira_manager import JiraManager",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/github.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/github.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport hashlib\nimport hmac\nimport os\n\nfrom fastapi import APIRouter, Header, HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom integrations.github.data_collector import GitHubDataCollector\nfrom integrations.github.github_manager import GithubManager\nfrom integrations.models import Message, SourceType",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/slack.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/server/routes/integration/slack.py",
      "line_number": 1,
      "code_snippet": "import html\nimport json\nfrom urllib.parse import quote\n\nimport jwt\nfrom fastapi import APIRouter, BackgroundTasks, HTTPException, Request\nfrom fastapi.responses import (\n    HTMLResponse,\n    JSONResponse,\n    PlainTextResponse,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_003_llm_claude4_vs_gpt5_experiment.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_003_llm_claude4_vs_gpt5_experiment.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nLiteLLM model experiment handler.\n\nThis module contains the handler for the LiteLLM model experiment.\n\"\"\"\n\nimport posthog\nfrom experiments.constants import EXPERIMENT_CLAUDE4_VS_GPT5\nfrom server.constants import (\n    IS_FEATURE_ENV,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py_227",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'agent.llm.model_copy' is used in 'UPDATE' on line 227 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
      "line_number": 227,
      "code_snippet": "\n    condenser_llm = agent.llm.model_copy(update={'usage_id': 'condenser'})\n    condenser = LLMSummarizingCondenser(\n        llm=condenser_llm, max_size=condenser_max_size, keep_first=4",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py_201",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'handle_condenser_max_step_experiment__v1' on line 201 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
      "line_number": 201,
      "code_snippet": "def handle_condenser_max_step_experiment__v1(\n    user_id: str | None,\n    conversation_id: UUID,\n    agent: Agent,\n) -> Agent:\n    enabled_variant = _get_condenser_max_step_variant(user_id, str(conversation_id))\n\n    if enabled_variant is None:\n        return agent\n\n    if enabled_variant == 'control':",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py_201_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'handle_condenser_max_step_experiment__v1'",
      "description": "Function 'handle_condenser_max_step_experiment__v1' on line 201 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
      "line_number": 201,
      "code_snippet": "\n\ndef handle_condenser_max_step_experiment__v1(\n    user_id: str | None,\n    conversation_id: UUID,\n    agent: Agent,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py_201_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'handle_condenser_max_step_experiment__v1'",
      "description": "Function 'handle_condenser_max_step_experiment__v1' on line 201 makes critical medical, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
      "line_number": 201,
      "code_snippet": "\n\ndef handle_condenser_max_step_experiment__v1(\n    user_id: str | None,\n    conversation_id: UUID,\n    agent: Agent,",
      "recommendation": "Critical medical, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_004_condenser_max_step_experiment.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nCondenser max step experiment handler.\n\nThis module contains the handler for the condenser max step experiment that tests\ndifferent max_size values for the condenser configuration.\n\"\"\"\n\nfrom uuid import UUID\n\nimport posthog",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_002_system_prompt_experiment.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/experiments/experiment_versions/_002_system_prompt_experiment.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nSystem prompt experiment handler.\n\nThis module contains the handler for the system prompt experiment that uses\nthe PostHog variant as the system prompt filename.\n\"\"\"\n\nimport copy\n\nimport posthog",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/022_create_api_keys_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/022_create_api_keys_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create API keys table\n\nRevision ID: 022\nRevises: 021\nCreate Date: 2025-04-03\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/047_create_conversation_feedback_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/047_create_conversation_feedback_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create conversation feedback table\n\nRevision ID: 046\nRevises: 045\nCreate Date: 2025-06-10\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/061_create_experiment_assignments_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/061_create_experiment_assignments_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create experiment assignments table\n\nRevision ID: 061\nRevises: 060\nCreate Date: 2025-07-29\n\nThis migration creates a table to track experiment assignments for conversations.\nEach row represents one experiment assignment with experiment_name and variant columns.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/060_create_user_version_upgrade_tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/060_create_user_version_upgrade_tasks.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create user version upgrade tasks\n\nRevision ID: 060\nRevises: 059\nCreate Date: 2025-07-21\n\nThis migration creates maintenance tasks for upgrading user versions\nto replace the removed admin maintenance endpoint.\n\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/019_remove_duplicates_from_stripe.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/019_remove_duplicates_from_stripe.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Remove duplicates from stripe. This is a non standard alembic migration for non sql resources.\n\nRevision ID: 019\nRevises: 018\nCreate Date: 2025-03-20 16:30:00.000\n\n\"\"\"\n\nimport json\nimport os",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/044_add_llm_model_to_conversation_metadata.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/044_add_llm_model_to_conversation_metadata.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add llm_model to conversation_metadata\n\nRevision ID: 044\nRevises: 043\nCreate Date: 2025-05-30\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/062_add_git_user_fields_to_user_settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/062_add_git_user_fields_to_user_settings.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add git_user_name and git_user_email to user_settings\n\nRevision ID: 062\nRevises: 061\nCreate Date: 2025-08-06\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/051_update_conversation_callbacks_fk_to_cascade.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/051_update_conversation_callbacks_fk_to_cascade.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update conversation_callbacks foreign key to cascade deletes\n\nRevision ID: 051\nRevises: 050\nCreate Date: 2025-06-24\n\n\"\"\"\n\nfrom alembic import op\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/049_create_conversation_callbacks_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/049_create_conversation_callbacks_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create conversation callbacks table\n\nRevision ID: 049\nRevises: 048\nCreate Date: 2025-06-19\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/030_add_proactive_conversation_starters_column.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/030_add_proactive_conversation_starters_column.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add proactive conversation starters column\n\nRevision ID: 030\nRevises: 029\nCreate Date: 2025-04-30\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/059_create_maintenance_tasks_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/059_create_maintenance_tasks_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create maintenance tasks table\n\nRevision ID: 059\nRevises: 058\nCreate Date: 2025-07-19\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/023_add_cost_and_token_metrics_columns.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/023_add_cost_and_token_metrics_columns.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add cost and token metrics columns to conversation_metadata table.\n\nRevision ID: 023\nRevises: 022\nCreate Date: 2025-04-07\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/084_create_device_codes_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/084_create_device_codes_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Create device_codes table for OAuth 2.0 Device Flow\n\nRevision ID: 084\nRevises: 083\nCreate Date: 2024-12-10 12:00:00.000000\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/080_add_status_and_updated_at_to_callback.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/080_add_status_and_updated_at_to_callback.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add status and updated_at to callback\n\nRevision ID: 080\nRevises: 079\nCreate Date: 2025-11-05 00:00:00.000000\n\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Sequence, Union",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/042_add_git_provider_to_conversation_metadata.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpc74yp9z5/opendevin/enterprise/migrations/versions/042_add_git_provider_to_conversation_metadata.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add git_provider to conversation_metadata\n\nRevision ID: 042\nRevises: 041\nCreate Date: 2025-05-29\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 1709,
      "kept": 1138,
      "filtered": 571,
      "reduction_pct": 33.4,
      "avg_tp_probability": 0.499,
      "filter_reasons": {
        "high severity with context": 550,
        "test file": 308,
        "build tool subprocess": 152,
        "asyncio.run pattern": 115,
        "callback handler pattern": 5,
        "executor pool pattern": 5,
        "placeholder value": 3,
        "server runner": 2,
        "env variable reference": 2
      }
    }
  }
}