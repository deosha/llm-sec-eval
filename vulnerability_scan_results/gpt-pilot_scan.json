{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:16:40.454519Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot",
    "files_scanned": 151,
    "overall_score": 3.18,
    "confidence": 0.66,
    "duration_seconds": 3.692,
    "findings_count": 98,
    "severity_breakdown": {
      "CRITICAL": 68,
      "HIGH": 12,
      "MEDIUM": 15,
      "LOW": 0,
      "INFO": 3
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.45,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "ML-based input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Token Limiting"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.42,
      "subscores": {
        "model_protection": 46,
        "extraction_defense": 47,
        "supply_chain_security": 12,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "OAuth",
        "Rate limiting",
        "Checksum verification"
      ],
      "gaps": [
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.4,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.35,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 0,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.94,
      "subscores": {
        "LLM01": 100,
        "LLM02": 72,
        "LLM03": 100,
        "LLM04": 100,
        "LLM05": 100,
        "LLM06": 0,
        "LLM07": 100,
        "LLM08": 65,
        "LLM09": 62,
        "LLM10": 100
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)",
        "Model Denial of Service (no vulnerabilities found)",
        "Supply Chain Vulnerabilities (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Insecure Output Handling: 2 high",
        "Sensitive Information Disclosure: 3 critical, 8 high",
        "Excessive Agency: 2 high, 1 medium",
        "Overreliance: 1 critical",
        "ML: 64 critical, 14 medium"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/main.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python\nimport os.path\nimport sys\n\ntry:\n    from core.cli.main import run_pythagora\nexcept ImportError as err:\n    pythagora_root = os.path.dirname(__file__)\n    venv_path = os.path.join(pythagora_root, \"venv\")\n    requirements_path = os.path.join(pythagora_root, \"requirements.txt\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/proc/process_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/proc/process_manager.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport signal\nimport sys\nimport time\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom os import environ\nfrom os.path import abspath, join\nfrom typing import Callable, Optional\nfrom uuid import UUID, uuid4",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/ipc_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/ipc_client.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport json\nfrom enum import Enum\nfrom typing import Optional, Union\n\nfrom pydantic import BaseModel, ValidationError\n\nfrom core.config import LocalIPCConfig\nfrom core.log import get_logger\nfrom core.ui.base import UIBase, UIClosedError, UISource, UserInput, UserInterruptError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/console.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/console.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nfrom prompt_toolkit.shortcuts import PromptSession\n\nfrom core.log import get_logger\nfrom core.ui.base import UIBase, UIClosedError, UISource, UserInput, UserInterruptError\n\nlog = get_logger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py_345",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py",
      "line_number": 345,
      "code_snippet": "                return conversation, await llm(conversation)\n\n        try:\n            convo, response = await ChatAgent(state_manager=self.state_manager, ui=VirtualUI(inputs=[])).run()\n            await self._add_to_chat_history(project_state_id, convo, message, str(response))\n\n            # Send final message",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py_320",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py",
      "line_number": 320,
      "code_snippet": "\n            async def generate_convo(self, convo_id: uuid.UUID, chat_history: list) -> Convo:\n                # chat_history = await self.state_manager.get_chat_history(convo_id)\n                conversation = await self.generate_prompt(\n                    convo_id=convo_id, user_msg=message.content.get(\"message\", \"\")\n                )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py_341",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/api_server.py",
      "line_number": 341,
      "code_snippet": "\n                convo_id = uuid.UUID(message.content.get(\"convoId\", \"\"))\n                chat_history = message.content.get(\"chatHistory\", [])\n                conversation = await self.generate_convo(convo_id, chat_history)\n                return conversation, await llm(conversation)\n\n        try:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/ui/base.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass ProjectStage(str, Enum):\n    PROJECT_NAME = \"project_name\"\n    PROJECT_DESCRIPTION = \"project_description\"\n    SPECS_STARTED = \"specs_started\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/convo.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/convo.py",
      "line_number": 1,
      "code_snippet": "from copy import deepcopy\nfrom typing import Any, Iterator, Optional\n\n\nclass Convo:\n    \"\"\"\n    A conversation between a user and a Large Language Model (LLM) assistant.\n\n    Holds messages and an optional metadata log (list of dicts with\n    prompt information).",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/azure_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/azure_client.py",
      "line_number": 1,
      "code_snippet": "from httpx import Timeout\nfrom openai import AsyncAzureOpenAI\n\nfrom core.config import LLMProvider\nfrom core.llm.openai_client import OpenAIClient\nfrom core.log import get_logger\n\nlog = get_logger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/parser.py_156_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '__call__'",
      "description": "Function '__call__' on line 156 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/parser.py",
      "line_number": 156,
      "code_snippet": "        return \"\\n\".join(error_txt)\n\n    def __call__(self, text: str) -> Union[BaseModel, dict, None]:\n        self.original_response = text.strip()  # Store the original text\n        text = self.original_response\n        if text.startswith(\"```\"):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/parser.py_156",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/parser.py",
      "line_number": 156,
      "code_snippet": "            etype = error[\"type\"]\n            msg = error[\"msg\"]\n            error_txt.append(f\"- `{loc}`: {etype} ({msg})\")\n        return \"\\n\".join(error_txt)\n\n    def __call__(self, text: str) -> Union[BaseModel, dict, None]:\n        self.original_response = text.strip()  # Store the original text\n        text = self.original_response\n        if text.startswith(\"```\"):\n            try:\n                text = CodeBlockParser()(text)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/groq_client.py_46",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/groq_client.py",
      "line_number": 46,
      "code_snippet": "        if json_mode:\n            completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        stream = await self.client.chat.completions.create(**completion_kwargs)\n        response = []\n        prompt_tokens = 0\n        completion_tokens = 0",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/openai_client.py_51",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/openai_client.py",
      "line_number": 51,
      "code_snippet": "        if json_mode:\n            completion_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        stream = await self.client.chat.completions.create(**completion_kwargs)\n        response = []\n        prompt_tokens = 0\n        completion_tokens = 0",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/anthropic_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/anthropic_client.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport datetime\nimport zoneinfo\nfrom typing import Optional, Tuple\n\nfrom anthropic import AsyncAnthropic, RateLimitError\nfrom httpx import Timeout\n\nfrom core.config import LLMProvider\nfrom core.llm.convo import Convo",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/base.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport datetime\nimport json\nimport sys\nfrom enum import Enum\nfrom time import time\nfrom typing import Any, Callable, Optional, Tuple\n\nimport httpx\nimport tiktoken",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/relace_client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/llm/relace_client.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nimport httpx\nfrom httpx import AsyncClient\n\nfrom core.config import LLMProvider\nfrom core.llm.base import BaseLLMClient\nfrom core.llm.convo import Convo\nfrom core.log import get_logger\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/user_settings.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/user_settings.py",
      "line_number": 1,
      "code_snippet": "import sys\nfrom os import getenv, makedirs\nfrom pathlib import Path\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field, PrivateAttr\n\nfrom core.log import get_logger\n\nlog = get_logger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/version.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/version.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom os.path import abspath, basename, dirname, isdir, isfile, join\nfrom typing import Optional\n\nGIT_DIR_PATH = abspath(join(dirname(__file__), \"..\", \"..\", \".git\"))\n\n\ndef get_git_commit() -> Optional[str]:\n    \"\"\"\n    Return the current git commit (if running from a repo).",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/actions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/actions.py",
      "line_number": 1,
      "code_snippet": "BH_START_BUG_HUNT = \"Start bug hunt for task #{}\"\nBH_WAIT_BUG_REP_INSTRUCTIONS = \"Awaiting bug reproduction instructions for task #{}\"\nBH_START_USER_TEST = \"Start user testing for task #{}\"\nBH_STARTING_PAIR_PROGRAMMING = \"Start pair programming for task #{}\"\n\nCM_UPDATE_FILES = \"Updating files\"\n\n\nDEV_WAIT_TEST = \"Awaiting user test\"\nDEV_TASK_START = \"Task #{} start\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/__init__.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom os.path import abspath, dirname, isdir, join\nfrom typing import Any, Literal, Optional, Union\n\nfrom pydantic import BaseModel, ConfigDict, Field, field_validator\nfrom typing_extensions import Annotated\n\nfrom core.config.constants import LOGS_LINE_LIMIT\n\nROOT_DIR = abspath(join(dirname(__file__), \"..\", \"..\"))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/magic_words.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/magic_words.py",
      "line_number": 1,
      "code_snippet": "PROBLEM_IDENTIFIED = \"PROBLEM_IDENTIFIED\"\nADD_LOGS = \"ADD_LOGS\"\nALWAYS_RELEVANT_FILES = [\n    \"client/src/App.tsx\",\n]\nGITIGNORE_CONTENT = \"\"\"# Logs\nlogs\n*.log\nnpm-debug.log*\nyarn-debug.log*",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/env_importer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/config/env_importer.py",
      "line_number": 1,
      "code_snippet": "from os.path import dirname, exists, join\n\nfrom dotenv import dotenv_values\n\nfrom core.config import Config, LLMProvider, ProviderConfig, loader\n\n\ndef import_from_dotenv(new_config_path: str) -> bool:\n    \"\"\"\n    Import configuration from old gpt-pilot .env file and save it to a new format.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/mixins.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/mixins.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport json\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom core.agents.convo import AgentConvo\nfrom core.agents.response import AgentResponse\nfrom core.cli.helpers import get_line_changes\nfrom core.config import GET_RELEVANT_FILES_AGENT_NAME, TASK_BREAKDOWN_AGENT_NAME, TROUBLESHOOTER_BUG_REPORT",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/git.py_163",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/git.py",
      "line_number": 163,
      "code_snippet": "            commit_message = user_message.text\n\n        # Create commit\n        status_code, _, stderr = await self.process_manager.run_command(\n            f'git commit -m \"{commit_message}\"', cwd=workspace_path\n        )\n        if status_code != 0:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/importer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/importer.py",
      "line_number": 1,
      "code_snippet": "from uuid import uuid4\n\nfrom core.agents.base import BaseAgent\nfrom core.agents.convo import AgentConvo\nfrom core.agents.response import AgentResponse, ResponseType\nfrom core.db.models import Complexity\nfrom core.llm.parser import JSONParser\nfrom core.log import get_logger\nfrom core.telemetry import telemetry\nfrom core.templates.example_project import EXAMPLE_PROJECT_DESCRIPTION",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/convo.py_112_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.75,
      "title": "Critical decision without oversight in 'require_schema'",
      "description": "Function 'require_schema' on line 112 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/convo.py",
      "line_number": 112,
      "code_snippet": "        return self\n\n    def require_schema(self, model: BaseModel) -> \"AgentConvo\":\n        def remove_defs(d):\n            if isinstance(d, dict):\n                return {k: remove_defs(v) for k, v in d.items() if k != \"$defs\"}",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/convo.py_123",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/convo.py",
      "line_number": 123,
      "code_snippet": "\n        # We want to make the schema as simple as possible to avoid confusing the LLM,\n        # so we remove (dereference) all the refs we can and show the \"final\" schema version.\n        schema_txt = json.dumps(remove_defs(jsonref.loads(json.dumps(model.model_json_schema()))))\n        self.user(\n            f\"IMPORTANT: Your response MUST conform to this JSON schema:\\n```\\n{schema_txt}\\n```.\"\n            f\"YOU MUST NEVER add any additional fields to your response, and NEVER add additional preamble like 'Here is your JSON'.\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py_415",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 415. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py",
      "line_number": 415,
      "code_snippet": "                            \"method\": \"POST\",\n                            \"endpoint\": \"/api/auth/register\",\n                            \"request_body\": {\"email\": \"string\", \"password\": \"string\"},\n                            \"response_body\": {\n                                \"id\": \"integer\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py_425",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 425. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py",
      "line_number": 425,
      "code_snippet": "                            \"method\": \"POST\",\n                            \"endpoint\": \"/api/auth/login\",\n                            \"request_body\": {\"username\": \"string\", \"password\": \"string\"},\n                            \"response_body\": {\"token\": \"string\"},\n                        },",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_lead.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\nfrom core.agents.base import BaseAgent\nfrom core.agents.convo import AgentConvo\nfrom core.agents.mixins import RelevantFilesMixin\nfrom core.agents.response import AgentResponse\nfrom core.config import TECH_LEAD_EPIC_BREAKDOWN, TECH_LEAD_PLANNING",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py_246",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py",
      "line_number": 246,
      "code_snippet": "            self.current_state.tasks.index(self.current_state.current_task) + 1\n        )\n        llm = self.get_llm(stream_output=True)\n        convo = self.generate_iteration_convo_so_far(True)\n        if len(convo.messages) > 1:\n            convo.remove_last_x_messages(1)\n        convo = convo.template(\"problem_explanation\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py_189",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py",
      "line_number": 189,
      "code_snippet": "            # self.next_state.current_iteration[\"bug_hunting_cycles\"][-1][\"fix_attempted\"] = True\n\n            if user_feedback.button == \"yes\":\n                self.next_state.complete_iteration()\n                return AgentResponse.done(self)\n            elif user_feedback.button == \"start_pair_programming\":\n                self.next_state.current_iteration[\"status\"] = IterationStatus.START_PAIR_PROGRAMMING",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py_218",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py",
      "line_number": 218,
      "code_snippet": "            )\n\n            if user_feedback.button == \"done\":\n                self.next_state.complete_iteration()\n                return AgentResponse.done(self)\n            elif user_feedback.button == \"start_pair_programming\":\n                self.next_state.current_iteration[\"status\"] = IterationStatus.START_PAIR_PROGRAMMING",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py_313",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/bug_hunter.py",
      "line_number": 313,
      "code_snippet": "\n            # TODO: in the future improve with a separate conversation that parses the user info and goes into an appropriate if statement\n            if next_step.button == \"done\":\n                self.next_state.complete_iteration()\n                break\n            elif next_step.button == \"question\":\n                user_response = await self.ask_question(\"Oh, cool, what would you like to know?\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/problem_solver.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/problem_solver.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom core.agents.base import BaseAgent\nfrom core.agents.convo import AgentConvo\nfrom core.agents.response import AgentResponse\nfrom core.agents.troubleshooter import IterationPromptMixin\nfrom core.db.models.project_state import IterationStatus\nfrom core.llm.parser import JSONParser",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/frontend.py_331",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/frontend.py",
      "line_number": 331,
      "code_snippet": "            # TODO Add question if user app is fully finished\n            self.next_state.action = FE_ITERATION_DONE\n\n            self.next_state.complete_epic()\n            await telemetry.trace_code_event(\n                \"frontend-finished\",\n                {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/frontend.py_445",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/frontend.py",
      "line_number": 445,
      "code_snippet": "                                    continue\n\n                        await self.send_message(f\"Running command: `{command}`...\")\n                        await self.process_manager.run_command(command)\n            else:\n                log.info(f\"Unknown block description: {description}\")\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/response.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/response.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import TYPE_CHECKING, Optional\n\nfrom core.log import get_logger\n\nif TYPE_CHECKING:\n    from core.agents.base import BaseAgent\n    from core.agents.error_handler import ErrorHandler\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_writer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/tech_writer.py",
      "line_number": 1,
      "code_snippet": "from core.agents.base import BaseAgent\nfrom core.agents.convo import AgentConvo\nfrom core.agents.response import AgentResponse\nfrom core.config.actions import TW_WRITE\nfrom core.db.models.project_state import TaskStatus\nfrom core.log import get_logger\nfrom core.ui.base import success_source\n\nlog = get_logger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/human_input.py_26",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/human_input.py",
      "line_number": 26,
      "code_snippet": "            default=\"continue\",\n            buttons_only=True,\n        )\n        self.next_state.complete_step(\"human_intervention\")\n        return AgentResponse.done(self)\n\n    async def input_required(self, files: list[dict]) -> AgentResponse:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/architect.py_250",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/architect.py",
      "line_number": 250,
      "code_snippet": "\n        for dep in deps:\n            await self.send_message(f\"Checking if {dep['name']} is available ...\")\n            status_code, _, _ = await self.process_manager.run_command(dep[\"test\"])\n            dep[\"installed\"] = bool(status_code == 0)\n            if status_code != 0:\n                if dep[\"required_locally\"]:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/task_completer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/task_completer.py",
      "line_number": 1,
      "code_snippet": "from core.agents.base import BaseAgent\nfrom core.agents.git import GitMixin\nfrom core.agents.response import AgentResponse\nfrom core.config.actions import TC_TASK_DONE\nfrom core.log import get_logger\nfrom core.telemetry import telemetry\n\nlog = get_logger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/wizard.py_65",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 65. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/wizard.py",
      "line_number": 65,
      "code_snippet": "                    buttons={\n                        \"none\": \"No authentication\",\n                        \"api_key\": \"API Key\",\n                        \"bearer\": \"HTTP Bearer (coming soon)\",\n                        \"open_id_connect\": \"OpenID Connect (coming soon)\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/wizard.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/wizard.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom urllib.parse import urljoin\nfrom uuid import uuid4\n\nimport httpx\nfrom sqlalchemy import inspect\n\nfrom core.agents.base import BaseAgent\nfrom core.agents.response import AgentResponse\nfrom core.cli.helpers import capture_exception",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/external_docs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/external_docs.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom pydantic import BaseModel\n\nfrom core.agents.base import BaseAgent\nfrom core.agents.convo import AgentConvo\nfrom core.agents.response import AgentResponse\nfrom core.config import EXTERNAL_DOCUMENTATION_API",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/error_handler.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/error_handler.py",
      "line_number": 1,
      "code_snippet": "from uuid import uuid4\n\nfrom core.agents.base import BaseAgent\nfrom core.agents.convo import AgentConvo\nfrom core.agents.response import AgentResponse\nfrom core.db.models.project_state import IterationStatus\nfrom core.log import get_logger\n\nlog = get_logger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/troubleshooter.py_107",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/troubleshooter.py",
      "line_number": 107,
      "code_snippet": "\n        if not should_iterate:\n            # User tested and reported no problems, we're done with the task\n            return await self.complete_task()\n\n        user_feedback = bug_report or change_description\n        user_feedback_qa = None  # await self.generate_bug_report(run_command, user_instructions, user_feedback)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/troubleshooter.py_71",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/troubleshooter.py",
      "line_number": 71,
      "code_snippet": "            user_instructions = await self.get_user_instructions()\n            if user_instructions is None:\n                # LLM decided we don't need to test anything, so we're done with the task\n                return await self.complete_task()\n\n            # Save the user instructions for future iterations and rerun\n            self.next_state.current_task[\"test_instructions\"] = user_instructions",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/developer.py_181",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/developer.py",
      "line_number": 181,
      "code_snippet": "                iteration[\"status\"] in (IterationStatus.AWAITING_USER_TEST, IterationStatus.AWAITING_BUG_REPRODUCTION)\n            ):\n                # This is just a support for old iterations that don't have status\n                self.next_state.complete_iteration()\n                self.next_state.action = DEV_TROUBLESHOOT.format(len(self.current_state.iterations))\n            elif iteration[\"status\"] == IterationStatus.IMPLEMENT_SOLUTION:\n                # If the user requested a change, then, we'll implement it and go straight back to testing",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/developer.py_185",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/developer.py",
      "line_number": 185,
      "code_snippet": "                self.next_state.action = DEV_TROUBLESHOOT.format(len(self.current_state.iterations))\n            elif iteration[\"status\"] == IterationStatus.IMPLEMENT_SOLUTION:\n                # If the user requested a change, then, we'll implement it and go straight back to testing\n                self.next_state.complete_iteration()\n                self.next_state.action = DEV_TROUBLESHOOT.format(len(self.current_state.iterations))\n            elif iteration[\"status\"] == IterationStatus.AWAITING_BUG_FIX:\n                # If bug fixing is done, ask user to test again",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/base.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Callable, Optional\n\nfrom core.agents.response import AgentResponse\nfrom core.config import get_config\nfrom core.db.models import ProjectState\nfrom core.llm.base import BaseLLMClient, LLMError\nfrom core.log import get_logger\nfrom core.proc.process_manager import ProcessManager\nfrom core.state.state_manager import StateManager\nfrom core.ui.base import AgentSource, UIBase, UserInput, pythagora_source",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/code_monkey.py_223",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/code_monkey.py",
      "line_number": 223,
      "code_snippet": "\n        await self.state_manager.save_file(file_path, new_content)\n        self.step[\"save_file\"][\"content\"] = new_content\n        self.next_state.complete_step(\"save_file\")\n\n        input_required = self.state_manager.get_input_required(new_content, file_path)\n        if input_required:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/spec_writer.py_320",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/spec_writer.py",
      "line_number": 320,
      "code_snippet": "        convo = AgentConvo(self).template(\"add_new_feature\", feature_description=feature_description)\n        llm_response: str = await llm(convo, temperature=0, parser=StringParser())\n        updated_spec = llm_response.strip()\n        await self.ui.generate_diff(\n            \"project_specification\", self.current_state.specification.description, updated_spec, source=self.ui_source\n        )\n        user_response = await self.ask_question(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/executor.py_169_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 169 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/executor.py",
      "line_number": 169,
      "code_snippet": "        return await llm(convo, parser=JSONParser(spec=CommandResult), temperature=0)\n\n    def complete(self):\n        \"\"\"\n        Mark the step as complete.\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/executor.py_97",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/agents/executor.py",
      "line_number": 97,
      "code_snippet": "        if confirm.button == \"no\":\n            log.info(f\"Skipping command execution of `{cmd}` (requested by user)\")\n            await self.send_message(f\"Skipping command {cmd}\")\n            self.complete()\n            self.next_state.action = EX_SKIP_COMMAND.format(cmd_name)\n            return AgentResponse.done(self)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/utils/text.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/utils/text.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nText processing utility functions.\n\"\"\"\n\n\ndef trim_logs(logs: str) -> str:\n    \"\"\"\n    Trim logs by removing everything after specific marker phrases.\n\n    This function cuts off the string at the first occurrence of",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/cli/helpers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/cli/helpers.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport os.path\nimport sys\nfrom argparse import ArgumentParser, ArgumentTypeError, Namespace\nfrom difflib import unified_diff\nfrom typing import Optional\nfrom urllib.parse import urlparse\nfrom uuid import UUID\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/state/state_manager.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/state/state_manager.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport os.path\nimport re\nimport traceback\nfrom contextlib import asynccontextmanager\nfrom typing import TYPE_CHECKING, Optional\nfrom urllib.parse import urljoin\nfrom uuid import UUID, uuid4\n\nimport httpx",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/fix_migrations.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/fix_migrations.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python\nimport os\nimport sqlite3\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom alembic import command\nfrom alembic.config import Config\nfrom alembic.script import ScriptDirectory\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/setup.py",
      "line_number": 1,
      "code_snippet": "from os.path import dirname, join\n\nfrom alembic import command\nfrom alembic.config import Config\n\nfrom core.config import DBConfig\nfrom core.log import get_logger\n\nlog = get_logger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/v0importer.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/v0importer.py",
      "line_number": 1,
      "code_snippet": "from json import loads\nfrom os.path import exists\nfrom pathlib import Path\nfrom uuid import UUID, uuid4\n\nimport aiosqlite\n\nfrom core.db.models import Branch, Project, ProjectState\nfrom core.db.models.project_state import TaskStatus\nfrom core.db.session import SessionManager",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/example_project.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/example_project.py",
      "line_number": 1,
      "code_snippet": "from core.db.models import Complexity\n\nEXAMPLE_PROJECT_DESCRIPTION = \"\"\"\nThe application is a simple ToDo app built using React. Its primary function is to allow users to manage a list of tasks (todos). Each task has a description and a state (open or completed, with the default state being open). The application is frontend-only, with no user sign-up or authentication process. The goal is to provide a straightforward and user-friendly interface for task management.\n\nFeatures:\n1. Display of Todos: A list that displays all todo items. Each item shows its description and a checkbox to indicate its state (open or completed).\n2. Add New Todo: A button to add a new todo item. Clicking this button will prompt the user to enter a description for the new todo.\n3. Toggle State: Each todo item includes a checkbox. Checking/unchecking this box toggles the todo's state between open and completed.\n4. Local Storage: The application will use the browser's local storage to persist todos between sessions, ensuring that users do not lose their data upon reloading the application.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/render.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/render.py",
      "line_number": 1,
      "code_snippet": "from __future__ import annotations\n\nimport os\nfrom os import walk\nfrom os.path import join, relpath\nfrom pathlib import Path\nfrom typing import Any, Callable\n\nfrom jinja2 import Environment, FileSystemLoader\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/vite_react.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/vite_react.py",
      "line_number": 1,
      "code_snippet": "from .base import BaseProjectTemplate, NoOptions\n\n\nclass ViteReactProjectTemplate(BaseProjectTemplate):\n    stack = \"backend\"\n    name = \"vite_react\"\n    path = \"vite_react\"\n    description = \"Vite + React + Tailwind CSS + TypeScript + Shadcn + Nodejs + Mongo\"\n    file_descriptions = {\n        \"client/components.json\": \"Configuration file for UI component library, defining schema, styling options, and resource paths.\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/javascript_react.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/javascript_react.py",
      "line_number": 1,
      "code_snippet": "from .base import BaseProjectTemplate, NoOptions\n\n\nclass JavascriptReactProjectTemplate(BaseProjectTemplate):\n    stack = \"frontend\"\n    name = \"javascript_react\"\n    path = \"javascript_react\"\n    description = \"React web app using Vite devserver/bundler\"\n    file_descriptions = {\n        \"vite.config.js\": \"Configuration file for Vite, a fast developer-friendly Javascript bundler/devserver.\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/node_express_mongoose.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/node_express_mongoose.py",
      "line_number": 1,
      "code_snippet": "from .base import BaseProjectTemplate, NoOptions\n\n\nclass NodeExpressMongooseProjectTemplate(BaseProjectTemplate):\n    stack = \"backend\"\n    name = \"node_express_mongoose\"\n    path = \"node_express_mongoose\"\n    description = \"Node + Express + MongoDB web app with session-based authentication, EJS views and Bootstrap 5\"\n    file_descriptions = {\n        \".env.example\": \"The .env.example file serves as a template for setting up environment variables used in the application. It provides placeholders for values such as the port number, MongoDB database URL, and session secret string.\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/vite_react_swagger.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/vite_react_swagger.py",
      "line_number": 1,
      "code_snippet": "from .base import BaseProjectTemplate, NoOptions\n\n\nclass ViteReactSwaggerProjectTemplate(BaseProjectTemplate):\n    stack = \"backend\"\n    name = \"vite_react_swagger\"\n    path = \"vite_react_swagger\"\n    description = \"Vite + React + Tailwind CSS + TypeScript + Shadcn\"\n    file_descriptions = {\n        \"client/components.json\": \"Configuration file for UI component library, defining schema, styling options, and resource paths.\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/base.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom json import loads\nfrom os.path import dirname, join\nfrom typing import TYPE_CHECKING, Any, Optional, Type\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel\n\nfrom core.log import get_logger\nfrom core.templates.render import Renderer",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/react_express.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/react_express.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom core.log import get_logger\n\nfrom .base import BaseProjectTemplate\n\nlog = get_logger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/log/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/log/__init__.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom collections import deque\nfrom logging import FileHandler, Formatter, Logger, StreamHandler, getLogger\n\nfrom core.config import LogConfig\nfrom core.config.constants import LOGS_LINE_LIMIT\n\n\nclass LineCountLimitedFileHandler(FileHandler):\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/disk/vfs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/disk/vfs.py",
      "line_number": 1,
      "code_snippet": "import os\nimport os.path\nfrom hashlib import sha1\nfrom pathlib import Path\n\nfrom core.disk.ignore import IgnoreMatcher\nfrom core.log import get_logger\n\nlog = get_logger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/telemetry/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/telemetry/__init__.py",
      "line_number": 1,
      "code_snippet": "import sys\nimport time\nimport traceback\nfrom copy import deepcopy\nfrom os import getenv\nfrom pathlib import Path\nfrom typing import Any\n\nimport httpx\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/tree/add_raw_tags.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/templates/tree/add_raw_tags.py",
      "line_number": 1,
      "code_snippet": "import os\nimport sys\n\n\ndef add_raw_tags_to_file(file_path):\n    \"\"\"Add {% raw %} at the beginning and {% endraw %} at the end of the file, if not already present.\"\"\"\n    try:\n        # Open the file and read the contents\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_52",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'context.run_migrations' is used in 'render' on line 52 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 52,
      "code_snippet": "    with context.begin_transaction():\n        context.run_migrations()\n\n",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_77",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'context.run_migrations' is used in 'render' on line 77 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 77,
      "code_snippet": "        with context.begin_transaction():\n            context.run_migrations()\n\n",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_30_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 30 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 30,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_55_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 55 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 55,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_30_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 30 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 30,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_55_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 55 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 55,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/env.py",
      "line_number": 1,
      "code_snippet": "from logging.config import fileConfig\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\n\nfrom core.db.models import Base\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/user_input.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/user_input.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import TYPE_CHECKING, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import ForeignKey, and_, delete, inspect, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\nfrom sqlalchemy.sql import func\n\nfrom core.db.models import Base",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/project_state.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/project_state.py",
      "line_number": 1,
      "code_snippet": "from copy import deepcopy\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING, Optional, Union\nfrom uuid import UUID, uuid4\n\nfrom sqlalchemy import ForeignKey, UniqueConstraint, and_, delete, inspect, or_, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, load_only, mapped_column, relationship\nfrom sqlalchemy.orm.attributes import flag_modified\nfrom sqlalchemy.sql import func",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/branch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/branch.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import TYPE_CHECKING, Optional, Union\nfrom uuid import UUID, uuid4\n\nfrom sqlalchemy import ForeignKey, inspect, select\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\nfrom sqlalchemy.sql import func\n\nfrom core.db.models import Base\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/chat_convo.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/chat_convo.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom typing import TYPE_CHECKING, Optional\nfrom uuid import UUID, uuid4\n\nfrom sqlalchemy import ForeignKey, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\nfrom sqlalchemy.sql import func\n\nfrom core.db.models import Base",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/file.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom typing import TYPE_CHECKING, Optional\nfrom uuid import UUID\n\nfrom sqlalchemy import ForeignKey, UniqueConstraint, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\n\nfrom core.db.models import Base\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/file_content.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/file_content.py",
      "line_number": 1,
      "code_snippet": "from typing import TYPE_CHECKING\n\nfrom sqlalchemy import delete, distinct, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\n\nfrom core.db.models import Base\n\nif TYPE_CHECKING:\n    from core.db.models import File",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/specification.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/specification.py",
      "line_number": 1,
      "code_snippet": "from copy import deepcopy\nfrom typing import TYPE_CHECKING, Optional\n\nfrom sqlalchemy import delete, distinct, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\n\nfrom core.db.models import Base\n\nif TYPE_CHECKING:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/project.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/models/project.py",
      "line_number": 1,
      "code_snippet": "import re\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING, Optional, Union\nfrom unicodedata import normalize\nfrom uuid import UUID, uuid4\n\nfrom sqlalchemy import Row, and_, delete, inspect, select\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship, selectinload\nfrom sqlalchemy.sql import func",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/0173e14719aa_vacuum_database.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/0173e14719aa_vacuum_database.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"0173e14719ab\"\ndown_revision: Union[str, None] = \"69e50fdaf067\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/0a1bb637fa26_initial.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/0a1bb637fa26_initial.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"0a1bb637fa26\"\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/08d71952ec2f_refactor_specification_template_to_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/08d71952ec2f_refactor_specification_template_to_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"08d71952ec2f\"\ndown_revision: Union[str, None] = \"ff891d366761\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/69e50fdaf067_move_knowledge_base_to_separate_table.py_18",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 18. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/69e50fdaf067_move_knowledge_base_to_separate_table.py",
      "line_number": 18,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"69e50fdaf067\"\ndown_revision: Union[str, None] = \"0173e14719aa\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/69e50fdaf067_move_knowledge_base_to_separate_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/69e50fdaf067_move_knowledge_base_to_separate_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"move knowledge base to separate table\n\nRevision ID: 69e50fdaf067\nRevises: 0173e14719aa\nCreate Date: 2025-05-15 17:27:50.312917\n\n\"\"\"\n\nimport json\nfrom typing import Sequence, Union",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/b760f66138c0_add_docs_column_to_project_states.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/b760f66138c0_add_docs_column_to_project_states.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"b760f66138c0\"\ndown_revision: Union[str, None] = \"f352dbe45751\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/f352dbe45751_make_relevant_files_nullable.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/f352dbe45751_make_relevant_files_nullable.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"f352dbe45751\"\ndown_revision: Union[str, None] = \"0a1bb637fa26\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/3968d770dced_add_project_type_to_project.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/3968d770dced_add_project_type_to_project.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"3968d770dced\"\ndown_revision: Union[str, None] = \"f708791b9270\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/c8905d4ce784_add_original_description_and_template_.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpu83nuskq/gpt-pilot/core/db/migrations/versions/c8905d4ce784_add_original_description_and_template_.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"c8905d4ce784\"\ndown_revision: Union[str, None] = \"08d71952ec2f\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 139,
      "kept": 98,
      "filtered": 41,
      "reduction_pct": 29.5,
      "avg_tp_probability": 0.541,
      "filter_reasons": {
        "high severity with context": 39,
        "test file": 29,
        "build tool subprocess": 22,
        "asyncio.run pattern": 5
      }
    }
  }
}