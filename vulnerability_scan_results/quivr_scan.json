{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:15:09.884947Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr",
    "files_scanned": 77,
    "overall_score": 1.59,
    "confidence": 0.66,
    "duration_seconds": 1.508,
    "findings_count": 61,
    "severity_breakdown": {
      "CRITICAL": 33,
      "HIGH": 2,
      "MEDIUM": 22,
      "LOW": 3,
      "INFO": 1
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 6,
      "confidence": 0.48,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 0,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 0,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Token Limiting"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 0,
      "confidence": 0.43,
      "subscores": {
        "model_protection": 33,
        "extraction_defense": 25,
        "supply_chain_security": 25,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": [
        "Model protection needs improvement",
        "Consider implementing encryption and access controls",
        "Extraction defense is weak",
        "Implement rate limiting and output protections",
        "Supply chain security needs attention",
        "Add model verification and dependency scanning"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.42,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "NER models for PII",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.38,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.94,
      "subscores": {
        "LLM01": 0,
        "LLM02": 36,
        "LLM03": 100,
        "LLM04": 36,
        "LLM05": 89,
        "LLM06": 70,
        "LLM07": 100,
        "LLM08": 43,
        "LLM09": 100,
        "LLM10": 100
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)",
        "Model Theft (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 3 critical",
        "Insecure Output Handling: 2 critical",
        "Model Denial of Service: 2 critical",
        "Supply Chain Vulnerabilities: 3 low",
        "Sensitive Information Disclosure: 1 critical",
        "Excessive Agency: 2 high, 4 medium",
        "ML: 25 critical, 18 medium"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/simple_question_megaparse.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/simple_question_megaparse.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom quivr_core import Brain\nfrom quivr_core.llm.llm_endpoint import LLMEndpoint\nfrom quivr_core.rag.entities.config import LLMEndpointConfig\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/pdf_document_from_yaml.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/pdf_document_from_yaml.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport logging\nimport os\nfrom pathlib import Path\n\nimport dotenv\nfrom quivr_core import Brain\nfrom quivr_core.rag.entities.config import AssistantConfig\nfrom rich.traceback import install as rich_install\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/pdf_parsing_tika.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/pdf_parsing_tika.py",
      "line_number": 1,
      "code_snippet": "from langchain_core.embeddings import DeterministicFakeEmbedding\nfrom langchain_core.language_models import FakeListChatModel\nfrom quivr_core import Brain\nfrom quivr_core.rag.entities.config import LLMEndpointConfig\nfrom quivr_core.llm.llm_endpoint import LLMEndpoint\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\n\nif __name__ == \"__main__\":",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py_135",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
      "line_number": 135,
      "code_snippet": "\n\ndef synthesize_speech(text):\n    speech_response = openai.audio.speech.create(\n        model=\"tts-1\", voice=\"nova\", input=text\n    )\n    audio_content = speech_response.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py_135",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'openai.audio.speech.create' is used in 'run(' on line 135 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
      "line_number": 135,
      "code_snippet": "def synthesize_speech(text):\n    speech_response = openai.audio.speech.create(\n        model=\"tts-1\", voice=\"nova\", input=text\n    )",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py_21",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 21. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
      "line_number": 21,
      "code_snippet": "\napp = Flask(__name__)\napp.secret_key = \"secret\"\napp.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\napp.config[\"CACHE_TYPE\"] = \"SimpleCache\"  # In-memory cache for development",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py_134",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/quivr-whisper/app.py",
      "line_number": 134,
      "code_snippet": "        os.unlink(temp_audio_file_path)\n\n    return transcript\n\n\ndef synthesize_speech(text):\n    speech_response = openai.audio.speech.create(\n        model=\"tts-1\", voice=\"nova\", input=text\n    )\n    audio_content = speech_response.content\n    audio_base64 = base64.b64encode(audio_content).decode(\"utf-8\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/chatbot_voice/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/examples/chatbot_voice/main.py",
      "line_number": 1,
      "code_snippet": "import tempfile\nimport os\nimport chainlit as cl\nfrom quivr_core import Brain\nfrom quivr_core.rag.entities.config import RetrievalConfig\nfrom openai import AsyncOpenAI\nfrom chainlit.element import Element\n\nfrom io import BytesIO\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/config.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\nimport yaml\nfrom pydantic import BaseModel\n\n\nclass ParserType(str, Enum):\n    \"\"\"Parser type enumeration.\"\"\"\n\n    UNSTRUCTURED = \"unstructured\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/__init__.py",
      "line_number": 1,
      "code_snippet": "from importlib.metadata import entry_points\n\nfrom .brain import Brain\nfrom .processor.registry import register_processor, registry\n\n__all__ = [\"Brain\", \"registry\", \"register_processor\"]\n\n\ndef register_entries():\n    if entry_points is not None:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py_244_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'config.model' is used without version pinning on line 244. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py",
      "line_number": 244,
      "code_snippet": "            elif config.supplier == DefaultModelSuppliers.ANTHROPIC:\n                assert config.llm_api_key, \"Can't load model config\"\n                _llm = ChatAnthropic(\n                    model_name=config.model,\n                    api_key=SecretStr(config.llm_api_key),\n                    base_url=config.llm_base_url,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py_254_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'config.model' is used without version pinning on line 254. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py",
      "line_number": 254,
      "code_snippet": "                )\n            elif config.supplier == DefaultModelSuppliers.OPENAI:\n                _llm = ChatOpenAI(\n                    model=config.model,\n                    api_key=SecretStr(config.llm_api_key)\n                    if config.llm_api_key",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py_296_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'config.model' is used without version pinning on line 296. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py",
      "line_number": 296,
      "code_snippet": "\n            else:\n                _llm = ChatOpenAI(\n                    model=config.model,\n                    api_key=SecretStr(config.llm_api_key)\n                    if config.llm_api_key",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py_212_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'from_config'",
      "description": "Function 'from_config' on line 212 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py",
      "line_number": 212,
      "code_snippet": "\n    @classmethod\n    def from_config(cls, config: LLMEndpointConfig = LLMEndpointConfig()):\n        hashed_config = hash(config)\n        if hashed_config in cls._cache:\n            return cls._cache[hashed_config]",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py_329_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'clone_llm'",
      "description": "Function 'clone_llm' on line 329 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py",
      "line_number": 329,
      "code_snippet": "        )\n\n    def clone_llm(self):\n        \"\"\"Create a new instance of the LLM with the same configuration.\"\"\"\n        return self._llm.__class__(**self._llm.__dict__)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py_232",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm/llm_endpoint.py",
      "line_number": 232,
      "code_snippet": "                deployment = parsed_url.path.split(\"/\")[3]  # type: ignore\n                api_version = parse_qs(parsed_url.query).get(\"api-version\", [None])[0]  # type: ignore\n                azure_endpoint = f\"https://{parsed_url.netloc}\"  # type: ignore\n                _llm = AzureChatOpenAI(\n                    azure_deployment=deployment,  # type: ignore\n                    api_version=api_version,\n                    api_key=SecretStr(config.llm_api_key)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm_tools/web_search_tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/llm_tools/web_search_tools.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Dict, List, Any\nfrom langchain_community.tools import TavilySearchResults\nfrom langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\nfrom quivr_core.llm_tools.entity import ToolsCategory\nimport os\nfrom pydantic import SecretStr  # Ensure correct import\nfrom quivr_core.llm_tools.entity import ToolWrapper, ToolRegistry\nfrom langchain_core.documents import Document\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/registry.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/registry.py",
      "line_number": 1,
      "code_snippet": "import importlib\nimport logging\nimport types\nfrom dataclasses import dataclass, field\nfrom heapq import heappop, heappush\nfrom typing import List, Type, TypeAlias\n\nfrom quivr_core.files.file import FileExtension\n\nfrom .processor_base import ProcessorBase",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/processor_base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/processor_base.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom abc import ABC, abstractmethod\nfrom importlib.metadata import PackageNotFoundError, version\nfrom typing import Any, Generic, List, TypeVar\n\nfrom attr import dataclass\nfrom langchain_core.documents import Document\n\nfrom quivr_core.files.file import FileExtension, QuivrFile\nfrom quivr_core.language.utils import detect_language",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/language/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/language/models.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass Language(str, Enum):\n    AF = \"af\"  # Afrikaans\n    ALS = \"als\"  # Alemannic\n    AM = \"am\"  # Amharic\n    AN = \"an\"  # Aragonese\n    AR = \"ar\"  # Arabic\n    ARZ = \"arz\"  # Egyptian Arabic",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/prompts.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport types\nfrom enum import Enum\n\nfrom langchain_core.prompts import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n    PromptTemplate,\n    SystemMessagePromptTemplate,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1194",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'structured_llm.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1194,
      "code_snippet": "            )\n            return structured_llm.invoke(prompt)\n        except openai.BadRequestError:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1197",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'structured_llm.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1197,
      "code_snippet": "            structured_llm = self.llm_endpoint._llm.with_structured_output(output_class)\n            return structured_llm.invoke(prompt)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_429",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.invoke_structured_output' is used in 'UPDATE' on line 429 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 429,
      "code_snippet": "\n        response: UpdatedPromptAndTools = self.invoke_structured_output(\n            msg, UpdatedPromptAndTools\n        )",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_923",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'generate_zendesk_rag' on line 923 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 923,
      "code_snippet": "    def generate_zendesk_rag(self, state: AgentState) -> AgentState:\n        tasks = state[\"tasks\"]\n        docs: List[Document] = tasks.docs if tasks else []\n        messages = state[\"messages\"]\n        user_task = messages[0].content\n        prompt_template: BasePromptTemplate = custom_prompts[\n            TemplatePromptName.ZENDESK_TEMPLATE_PROMPT\n        ]\n\n        ticket_metadata = state[\"ticket_metadata\"] or {}\n        user_metadata = state[\"user_metadata\"] or {}",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_920_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'bind_tools_to_llm'",
      "description": "Function 'bind_tools_to_llm' on line 916 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 920,
      "code_snippet": "            tools = self.retrieval_config.workflow_config.get_node_tools(node_name)\n            if tools:  # Only bind tools if there are any available\n                return self.llm_endpoint._llm.bind_tools(tools, tool_choice=\"any\")\n        return self.llm_endpoint._llm\n\n    def generate_zendesk_rag(self, state: AgentState) -> AgentState:",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_916_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'bind_tools_to_llm'",
      "description": "Function 'bind_tools_to_llm' on line 916 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 916,
      "code_snippet": "        return {**state, \"tasks\": tasks}, inputs\n\n    def bind_tools_to_llm(self, node_name: str):\n        if self.llm_endpoint.supports_func_calling():\n            tools = self.retrieval_config.workflow_config.get_node_tools(node_name)\n            if tools:  # Only bind tools if there are any available",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_414_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'edit_system_prompt'",
      "description": "Function 'edit_system_prompt' on line 414 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 414,
      "code_snippet": "                        )\n\n    def edit_system_prompt(self, state: AgentState) -> AgentState:\n        user_instruction = state[\"instructions\"]\n        prompt = self.retrieval_config.prompt\n        available_tools, activated_tools = collect_tools(",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_370",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 370,
      "code_snippet": "        return send_list\n\n    def routing_split(self, state: AgentState):\n        response: SplittedInput = self.invoke_structured_output(\n            custom_prompts[TemplatePromptName.SPLIT_PROMPT].format(\n                chat_history=state[\"chat_history\"].to_list(),\n                user_input=state[\"messages\"][0].content,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_429",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 429,
      "code_snippet": "\n        msg = custom_prompts[TemplatePromptName.UPDATE_PROMPT].format(**inputs)\n\n        response: UpdatedPromptAndTools = self.invoke_structured_output(\n            msg, UpdatedPromptAndTools\n        )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_952",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 952,
      "code_snippet": "        msg = prompt_template.format_prompt(**inputs)\n        llm = self.bind_tools_to_llm(self.generate_zendesk_rag.__name__)\n\n        response = llm.invoke(msg)\n\n        return {**state, \"messages\": [response]}\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_964",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 964,
      "code_snippet": "        state, inputs = self.reduce_rag_context(state, inputs, prompt)\n        msg = prompt.format(**inputs)\n        llm = self.bind_tools_to_llm(self.generate_rag.__name__)\n        response = llm.invoke(msg)\n\n        return {**state, \"messages\": [response]}\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1017",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1017,
      "code_snippet": "            ]\n        )\n        # Run\n        chat_llm_prompt = CHAT_LLM_PROMPT.invoke(\n            {\"chat_history\": final_inputs[\"chat_history\"]}\n        )\n        response = llm.invoke(chat_llm_prompt)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1020",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1020,
      "code_snippet": "        chat_llm_prompt = CHAT_LLM_PROMPT.invoke(\n            {\"chat_history\": final_inputs[\"chat_history\"]}\n        )\n        response = llm.invoke(chat_llm_prompt)\n        return {**state, \"messages\": [response]}\n\n    def build_chain(self):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_334",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 334,
      "code_snippet": "        response: SplittedInput\n\n        try:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                SplittedInput, method=\"json_schema\"\n            )\n            response = structured_llm.invoke(msg)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_337",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 337,
      "code_snippet": "            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                SplittedInput, method=\"json_schema\"\n            )\n            response = structured_llm.invoke(msg)\n\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1179",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1179,
      "code_snippet": "        self, prompt: str, output_class: Type[BaseModel]\n    ) -> Any:\n        try:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                output_class, method=\"json_schema\"\n            )\n            return await structured_llm.ainvoke(prompt)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1191",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1191,
      "code_snippet": "        self, prompt: str, output_class: Type[BaseModel]\n    ) -> Any:\n        try:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                output_class, method=\"json_schema\"\n            )\n            return structured_llm.invoke(prompt)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_340",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 340,
      "code_snippet": "            response = structured_llm.invoke(msg)\n\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                SplittedInput\n            )\n            response = structured_llm.invoke(msg)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_343",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 343,
      "code_snippet": "            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                SplittedInput\n            )\n            response = structured_llm.invoke(msg)\n\n        send_list: List[Send] = []\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1182",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1182,
      "code_snippet": "            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                output_class, method=\"json_schema\"\n            )\n            return await structured_llm.ainvoke(prompt)\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(output_class)\n            return await structured_llm.ainvoke(prompt)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1184",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1184,
      "code_snippet": "            )\n            return await structured_llm.ainvoke(prompt)\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(output_class)\n            return await structured_llm.ainvoke(prompt)\n\n    def invoke_structured_output(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1196",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1196,
      "code_snippet": "            )\n            return structured_llm.invoke(prompt)\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(output_class)\n            return structured_llm.invoke(prompt)\n\n    def _build_rag_prompt_inputs(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1185",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1185,
      "code_snippet": "            return await structured_llm.ainvoke(prompt)\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(output_class)\n            return await structured_llm.ainvoke(prompt)\n\n    def invoke_structured_output(\n        self, prompt: str, output_class: Type[BaseModel]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py_1187",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag_langgraph.py",
      "line_number": 1187,
      "code_snippet": "            return await structured_llm.ainvoke(prompt)\n        except openai.BadRequestError:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(output_class)\n            return await structured_llm.ainvoke(prompt)\n\n    def invoke_structured_output(\n        self, prompt: str, output_class: Type[BaseModel]\n    ) -> Any:\n        try:\n            structured_llm = self.llm_endpoint._llm.with_structured_output(\n                output_class, method=\"json_schema\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py_175",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input parameter 'question' is directly passed to LLM API call 'conversational_qa_chain.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py",
      "line_number": 175,
      "code_snippet": "        conversational_qa_chain = self.build_chain(concat_list_files)\n        raw_llm_response = conversational_qa_chain.invoke(\n            {",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py_147_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'build_chain'",
      "description": "Function 'build_chain' on line 105 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py",
      "line_number": 147,
      "code_snippet": "        llm = self.llm_endpoint._llm\n        if self.llm_endpoint.supports_func_calling():\n            llm = self.llm_endpoint._llm.bind_tools(\n                [cited_answer],\n                tool_choice=\"any\",\n            )",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/quivr_rag.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom operator import itemgetter\nfrom typing import AsyncGenerator, Optional, Sequence\n\n# TODO(@aminediro): this is the only dependency to langchain package, we should remove it\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_core.callbacks import Callbacks\nfrom langchain_core.documents import BaseDocumentCompressor, Document\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langchain_core.messages.ai import AIMessageChunk",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/storage/file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/storage/file.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport mimetypes\nimport os\nimport warnings\nfrom contextlib import asynccontextmanager\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, AsyncGenerator, AsyncIterable\nfrom uuid import UUID, uuid4\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/files/file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/files/file.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport mimetypes\nimport os\nimport warnings\nfrom contextlib import asynccontextmanager\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, AsyncGenerator, AsyncIterable, Self\nfrom uuid import UUID, uuid4\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain.py_375_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'from_files'",
      "description": "Function 'from_files' on line 375 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain.py",
      "line_number": 375,
      "code_snippet": "\n    @classmethod\n    def from_files(\n        cls,\n        *,\n        name: str,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport logging\nimport os\nfrom pathlib import Path\nfrom pprint import PrettyPrinter\nfrom typing import Any, AsyncGenerator, Callable, Dict, Self, Type, Union\nfrom uuid import UUID, uuid4\n\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain_defaults.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/brain_defaults.py",
      "line_number": 1,
      "code_snippet": "import logging\n\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStore\n\nfrom quivr_core.rag.entities.config import DefaultModelSuppliers, LLMEndpointConfig\nfrom quivr_core.llm import LLMEndpoint\n\nlogger = logging.getLogger(\"quivr_core\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/info.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/brain/info.py",
      "line_number": 1,
      "code_snippet": "from dataclasses import dataclass\nfrom uuid import UUID\n\nfrom rich.tree import Tree\n\n\n@dataclass\nclass ChatHistoryInfo:\n    nb_chats: int\n    current_default_chat: UUID",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/config.py_281",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_supplier_by_model_name' on line 281 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/config.py",
      "line_number": 281,
      "code_snippet": "    def get_supplier_by_model_name(cls, model: str) -> DefaultModelSuppliers | None:\n        # Iterate over the suppliers and their models\n        for supplier, models in cls._model_defaults.items():\n            # Check if the model name or a base part of the model name is in the supplier's models\n            for base_model_name in models:\n                if model.startswith(base_model_name):\n                    return supplier\n        # Return None if no supplier matches the model name\n        return None\n\n    @classmethod",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/config.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport re\nfrom enum import Enum\nfrom typing import Any, Dict, Hashable, List, Optional, Type, Union\nfrom uuid import UUID\n\nfrom langchain_core.prompts.base import BasePromptTemplate\nfrom langchain_core.tools import BaseTool\nfrom langgraph.graph import END, START",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/rag/entities/models.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, Optional\nfrom uuid import UUID\n\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/default.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/default.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Any, List, Type, TypeVar\n\nimport tiktoken\nfrom langchain_community.document_loaders import (\n    BibtexLoader,\n    CSVLoader,\n    Docx2txtLoader,\n    NotebookLoader,\n    PythonLoader,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/tika_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/tika_processor.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom typing import AsyncIterable\n\nimport httpx\nimport tiktoken\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n\nfrom quivr_core.files.file import QuivrFile",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/simple_txt_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/simple_txt_processor.py",
      "line_number": 1,
      "code_snippet": "from typing import Any\n\nimport aiofiles\nfrom langchain_core.documents import Document\n\nfrom quivr_core.files.file import QuivrFile\nfrom quivr_core.processor.processor_base import ProcessedDocument, ProcessorBase\nfrom quivr_core.processor.registry import FileExtension\nfrom quivr_core.processor.splitter import SplitterConfig\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/megaparse_processor.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp5sqrizcb/quivr/core/quivr_core/processor/implementations/megaparse_processor.py",
      "line_number": 1,
      "code_snippet": "import logging\n\nimport tiktoken\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\nfrom megaparse_sdk.client import MegaParseNATSClient\nfrom megaparse_sdk.config import ClientNATSConfig\nfrom megaparse_sdk.schema.document import Document as MPDocument\n\nfrom quivr_core.config import MegaparseConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 102,
      "kept": 61,
      "filtered": 41,
      "reduction_pct": 40.2,
      "avg_tp_probability": 0.466,
      "filter_reasons": {
        "high severity with context": 38,
        "test file": 25,
        "build tool subprocess": 13,
        "example file": 10,
        "asyncio.run pattern": 2,
        "callback handler pattern": 2
      }
    }
  }
}