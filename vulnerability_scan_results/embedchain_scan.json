{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:17:03.044608Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain",
    "files_scanned": 542,
    "overall_score": 3.92,
    "confidence": 0.66,
    "duration_seconds": 14.267,
    "findings_count": 702,
    "severity_breakdown": {
      "CRITICAL": 406,
      "HIGH": 96,
      "MEDIUM": 115,
      "LOW": 11,
      "INFO": 74
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.5,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Token Limiting"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.48,
      "subscores": {
        "model_protection": 63,
        "extraction_defense": 37,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "OAuth",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": [
        "Extraction defense is weak",
        "Implement rate limiting and output protections"
      ]
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.53,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII masking",
        "Right to delete",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.89,
      "subscores": {
        "LLM01": 0,
        "LLM02": 0,
        "LLM03": 66,
        "LLM04": 0,
        "LLM05": 0,
        "LLM06": 0,
        "LLM07": 100,
        "LLM08": 0,
        "LLM09": 0,
        "LLM10": 0
      },
      "detected_controls": [
        "Insecure Plugin Design (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 27 critical",
        "Insecure Output Handling: 63 critical, 2 high",
        "Training Data Poisoning: 1 critical",
        "Model Denial of Service: 32 critical",
        "Supply Chain Vulnerabilities: 5 critical, 8 medium, 11 low",
        "Sensitive Information Disclosure: 23 critical, 3 high, 2 medium",
        "Excessive Agency: 20 critical, 32 high, 36 medium",
        "Overreliance: 26 critical, 2 high",
        "Model Theft: 11 high",
        "ML: 209 critical, 69 medium",
        "SQLI: 46 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/server/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/server/main.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom typing import Any, Dict, List, Optional\n\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import JSONResponse, RedirectResponse\nfrom pydantic import BaseModel, Field\n\nfrom mem0 import Memory",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/generate_scores.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/generate_scores.py",
      "line_number": 1,
      "code_snippet": "import json\n\nimport pandas as pd\n\n# Load the evaluation metrics data\nwith open(\"evaluation_metrics.json\", \"r\") as f:\n    data = json.load(f)\n\n# Flatten the data into a list of question items\nall_items = []",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/run_experiments.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/run_experiments.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\n\nfrom src.langmem import LangMemManager\nfrom src.memzero.add import MemoryADD\nfrom src.memzero.search import MemorySearch\nfrom src.openai.predict import OpenAIPredict\nfrom src.rag import RAGManager\nfrom src.utils import METHODS, TECHNIQUES\nfrom src.zep.add import ZepAdd",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/exceptions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/exceptions.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Structured exception classes for Mem0 with error codes, suggestions, and debug information.\n\nThis module provides a comprehensive set of exception classes that replace the generic\nAPIError with specific, actionable exceptions. Each exception includes error codes,\nuser-friendly suggestions, and debug information to enable better error handling\nand recovery in applications using Mem0.\n\nExample:\n    Basic usage:\n        try:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py_60_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 60. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py",
      "line_number": 60,
      "code_snippet": "        expanded_text = text\n        if self.memory.get_all(agent_id=self.agent_id):\n            expanded_text = self._consider_memo_retrieval(text)\n        self._consider_memo_storage(text)\n        return expanded_text\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py_114_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 114. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py",
      "line_number": 114,
      "code_snippet": "            )\n\n    def _consider_memo_retrieval(self, comment: Union[Dict, str]):\n        if self.verbosity >= 1:\n            print(colored(\"\\nLOOK FOR RELEVANT MEMOS, AS QUESTION-ANSWER PAIRS\", \"light_yellow\"))\n        memo_list = self._retrieve_relevant_memos(comment)",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/cookbooks/helper/mem0_teachability.py",
      "line_number": 1,
      "code_snippet": "# Copyright (c) 2023 - 2024, Owners of https://github.com/autogen-ai\n#\n# SPDX-License-Identifier: Apache-2.0\n#\n# Portions derived from  https://github.com/microsoft/autogen are under the MIT License.\n# SPDX-License-Identifier: MIT\n# forked from autogen.agentchat.contrib.capabilities.teachability.Teachability\n\nfrom typing import Dict, Optional, Union\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/main.py",
      "line_number": 1,
      "code_snippet": "import datetime\nfrom uuid import uuid4\n\nfrom app.config import DEFAULT_APP_ID, USER_ID\nfrom app.database import Base, SessionLocal, engine\nfrom app.mcp_server import setup_mcp_server\nfrom app.models import App, User\nfrom app.routers import apps_router, backup_router, config_router, memories_router, stats_router\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/models.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport enum\nimport uuid\n\nimport sqlalchemy as sa\nfrom app.database import Base\nfrom app.utils.categorization import get_categories_for_memory\nfrom sqlalchemy import (\n    JSON,\n    UUID,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/database.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/database.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import declarative_base, sessionmaker\n\n# load .env file (make sure you have DATABASE_URL set)\nload_dotenv()\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite:///./openmemory.db\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/mcp_server.py_172",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/mcp_server.py",
      "line_number": 172,
      "code_snippet": "                \"user_id\": uid\n            }\n\n            embeddings = memory_client.embedding_model.embed(query, \"search\")\n\n            hits = memory_client.vector_store.search(\n                query=query, ",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/mcp_server.py_446",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/mcp_server.py",
      "line_number": 446,
      "code_snippet": "            request.receive,\n            request._send,\n        ) as (read_stream, write_stream):\n            await mcp._mcp_server.run(\n                read_stream,\n                write_stream,\n                mcp._mcp_server.create_initialization_options(),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py_37_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 37 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
      "line_number": 37,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py_61_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 61 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
      "line_number": 61,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py_37_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 37 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
      "line_number": 37,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py_61_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 61 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
      "line_number": 61,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/env.py",
      "line_number": 1,
      "code_snippet": "import os\nimport sys\nfrom logging.config import fileConfig\n\nfrom alembic import context\nfrom dotenv import load_dotenv\nfrom sqlalchemy import engine_from_config, pool\n\n# Add the parent directory to the Python path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/add_config_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/add_config_table.py",
      "line_number": 15,
      "code_snippet": "# revision identifiers, used by Alembic.\nrevision = 'add_config_table'\ndown_revision = '0b53c747049a'\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/add_config_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/add_config_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_config_table\n\nRevision ID: add_config_table\nRevises: 0b53c747049a\nCreate Date: 2023-06-01 10:00:00.000000\n\n\"\"\"\nimport uuid\n\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py",
      "line_number": 14,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = '0b53c747049a'\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/backup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/backup.py",
      "line_number": 1,
      "code_snippet": "from datetime import UTC, datetime\nimport io \nimport json \nimport gzip \nimport zipfile\nfrom typing import Optional, List, Dict, Any\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Query, Form\nfrom fastapi.responses import StreamingResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py_62",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 62. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py",
      "line_number": 62,
      "code_snippet": "                    \"temperature\": 0.1,\n                    \"max_tokens\": 2000,\n                    \"api_key\": \"env:OPENAI_API_KEY\"\n                }\n            },",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py_69",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 69. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py",
      "line_number": 69,
      "code_snippet": "                \"config\": {\n                    \"model\": \"text-embedding-3-small\",\n                    \"api_key\": \"env:OPENAI_API_KEY\"\n                }\n            },",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/config.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, Optional\n\nfrom app.database import get_db\nfrom app.models import Config as ConfigModel\nfrom app.utils.memory import reset_memory_client\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom pydantic import BaseModel, Field\nfrom sqlalchemy.orm import Session\n\nrouter = APIRouter(prefix=\"/api/v1/config\", tags=[\"config\"])",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/apps.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/apps.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\nfrom uuid import UUID\n\nfrom app.database import get_db\nfrom app.models import App, Memory, MemoryAccessLog, MemoryState\nfrom fastapi import APIRouter, Depends, HTTPException, Query\nfrom sqlalchemy import desc, func\nfrom sqlalchemy.orm import Session, joinedload\n\nrouter = APIRouter(prefix=\"/api/v1/apps\", tags=[\"apps\"])",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/stats.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/stats.py",
      "line_number": 1,
      "code_snippet": "from app.database import get_db\nfrom app.models import App, Memory, MemoryState, User\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom sqlalchemy.orm import Session\n\nrouter = APIRouter(prefix=\"/api/v1/stats\", tags=[\"stats\"])\n\n@router.get(\"/\")\nasync def get_profile(\n    user_id: str,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/memories.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/routers/memories.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom datetime import UTC, datetime\nfrom typing import List, Optional, Set\nfrom uuid import UUID\n\nfrom app.database import get_db\nfrom app.models import (\n    AccessControl,\n    App,\n    Category,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/db.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/db.py",
      "line_number": 1,
      "code_snippet": "from typing import Tuple\n\nfrom app.models import App, User\nfrom sqlalchemy.orm import Session\n\n\ndef get_or_create_user(db: Session, user_id: str) -> User:\n    \"\"\"Get or create a user with the given user_id\"\"\"\n    user = db.query(User).filter(User.user_id == user_id).first()\n    if not user:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py_250",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 250. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py",
      "line_number": 250,
      "code_snippet": "                \"temperature\": 0.1,\n                \"max_tokens\": 2000,\n                \"api_key\": \"env:OPENAI_API_KEY\"\n            }\n        },",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py_257",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 257. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py",
      "line_number": 257,
      "code_snippet": "            \"config\": {\n                \"model\": \"text-embedding-3-small\",\n                \"api_key\": \"env:OPENAI_API_KEY\"\n            }\n        },",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/memory.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nMemory client utilities for OpenMemory.\n\nThis module provides functionality to initialize and manage the Mem0 memory client\nwith automatic configuration management and Docker environment support.\n\nDocker Ollama Configuration:\nWhen running inside a Docker container and using Ollama as the LLM or embedder provider,\nthe system automatically detects the Docker environment and adjusts localhost URLs\nto properly reach the host machine where Ollama is running.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/categorization.py_19_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_categories_for_memory'",
      "description": "Function 'get_categories_for_memory' on line 19 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/openmemory/api/app/utils/categorization.py",
      "line_number": 19,
      "code_snippet": "\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=15))\ndef get_categories_for_memory(memory: str) -> List[str]:\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": MEMORY_CATEGORIZATION_PROMPT},",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/cohere_reranker.py_32",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/cohere_reranker.py",
      "line_number": 32,
      "code_snippet": "            raise ValueError(\"Cohere API key is required. Set COHERE_API_KEY environment variable or pass api_key in config.\")\n            \n        self.model = config.model\n        self.client = cohere.Client(self.api_key)\n        \n    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n        \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/huggingface_reranker.py_19_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in '__init__'",
      "description": "Function '__init__' on line 19 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/huggingface_reranker.py",
      "line_number": 19,
      "code_snippet": "\nclass HuggingFaceReranker(BaseReranker):\n    \"\"\"HuggingFace Transformers based reranker implementation.\"\"\"\n\n    def __init__(self, config: Union[BaseRerankerConfig, HuggingFaceRerankerConfig, Dict]):\n        \"\"\"\n        Initialize HuggingFace reranker.\n\n        Args:\n            config: Configuration object with reranker parameters",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/huggingface_reranker.py_19_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in '__init__'",
      "description": "Function '__init__' on line 19 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/huggingface_reranker.py",
      "line_number": 19,
      "code_snippet": "    \"\"\"HuggingFace Transformers based reranker implementation.\"\"\"\n\n    def __init__(self, config: Union[BaseRerankerConfig, HuggingFaceRerankerConfig, Dict]):\n        \"\"\"\n        Initialize HuggingFace reranker.\n",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py_120",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to 'self._extract_score' on line 120 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py",
      "line_number": 120,
      "code_snippet": "                # Extract score from response\n                score = self._extract_score(response)\n                \n                # Create scored document",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py_82_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'rerank'",
      "description": "Function 'rerank' on line 82 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py",
      "line_number": 82,
      "code_snippet": "        return 0.5\n    \n    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Rerank documents using LLM scoring.\n        ",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py_115",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py",
      "line_number": 115,
      "code_snippet": "                prompt = self.scoring_prompt.format(query=query, document=doc_text)\n                \n                # Get LLM response\n                response = self.llm.generate_response(\n                    messages=[{\"role\": \"user\", \"content\": prompt}]\n                )\n                ",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py_82",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/llm_reranker.py",
      "line_number": 82,
      "code_snippet": "            return min(max(score, 0.0), 1.0)  # Clamp between 0.0 and 1.0\n        \n        # Fallback: return 0.5 if no valid score found\n        return 0.5\n    \n    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Rerank documents using LLM scoring.\n        \n        Args:\n            query: The search query",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/zero_entropy_reranker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/zero_entropy_reranker.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import List, Dict, Any\n\nfrom mem0.reranker.base import BaseReranker\n\ntry:\n    from zeroentropy import ZeroEntropy\n    ZERO_ENTROPY_AVAILABLE = True\nexcept ImportError:\n    ZERO_ENTROPY_AVAILABLE = False",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py_46_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'rerank'",
      "description": "Function 'rerank' on line 46 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py",
      "line_number": 46,
      "code_snippet": "        self.model = SentenceTransformer(self.config.model, device=self.config.device)\n        \n    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Rerank documents using sentence transformer cross-encoder.\n        ",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py_78",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py",
      "line_number": 78,
      "code_snippet": "            pairs = [[query, doc_text] for doc_text in doc_texts]\n            \n            # Get similarity scores\n            scores = self.model.predict(pairs)\n            if isinstance(scores, np.ndarray):\n                scores = scores.tolist()\n            ",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py_46",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/reranker/sentence_transformer_reranker.py",
      "line_number": 46,
      "code_snippet": "            )\n\n        self.config = config\n        self.model = SentenceTransformer(self.config.model, device=self.config.device)\n        \n    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Rerank documents using sentence transformer cross-encoder.\n        \n        Args:\n            query: The search query",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_137",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 137 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 137,
      "code_snippet": "        try:\n            # Create table with vector stored as list<float> and payload as text (JSON)\n            query = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {self.keyspace}.{self.collection_name} (\n                    id text PRIMARY KEY,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_163",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 163 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 163,
      "code_snippet": "\n        try:\n            query = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {self.keyspace}.{table_name} (\n                    id text PRIMARY KEY,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_198",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 198 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 198,
      "code_snippet": "\n        try:\n            query = f\"\"\"\n                INSERT INTO {self.keyspace}.{self.collection_name} (id, vector, payload)\n                VALUES (?, ?, ?)",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_234",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 234 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 234,
      "code_snippet": "        try:\n            # Fetch all vectors (in production, you'd want pagination or filtering)\n            query_cql = f\"\"\"\n                SELECT id, vector, payload\n                FROM {self.keyspace}.{self.collection_name}",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_290",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 290 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 290,
      "code_snippet": "        \"\"\"\n        try:\n            query = f\"\"\"\n                DELETE FROM {self.keyspace}.{self.collection_name}\n                WHERE id = ?",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_317",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 317 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 317,
      "code_snippet": "        try:\n            if vector is not None:\n                query = f\"\"\"\n                    UPDATE {self.keyspace}.{self.collection_name}\n                    SET vector = ?",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_326",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 326 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 326,
      "code_snippet": "\n            if payload is not None:\n                query = f\"\"\"\n                    UPDATE {self.keyspace}.{self.collection_name}\n                    SET payload = ?",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_350",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 350 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 350,
      "code_snippet": "        \"\"\"\n        try:\n            query = f\"\"\"\n                SELECT id, vector, payload\n                FROM {self.keyspace}.{self.collection_name}",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_378",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 378 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 378,
      "code_snippet": "        \"\"\"\n        try:\n            query = f\"\"\"\n                SELECT table_name\n                FROM system_schema.tables",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_392",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 392 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 392,
      "code_snippet": "        \"\"\"Delete the collection (table).\"\"\"\n        try:\n            query = f\"\"\"\n                DROP TABLE IF EXISTS {self.keyspace}.{self.collection_name}\n            \"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_410",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 410 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 410,
      "code_snippet": "        try:\n            # Get row count (approximate)\n            query = f\"\"\"\n                SELECT COUNT(*) as count\n                FROM {self.keyspace}.{self.collection_name}",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_443",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 443 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 443,
      "code_snippet": "        \"\"\"\n        try:\n            query = f\"\"\"\n                SELECT id, vector, payload\n                FROM {self.keyspace}.{self.collection_name}",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/cassandra.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport uuid\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nfrom pydantic import BaseModel\n\ntry:\n    from cassandra.cluster import Cluster",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/valkey.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/valkey.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom datetime import datetime\nfrom typing import Dict\n\nimport numpy as np\nimport pytz\nimport valkey\nfrom pydantic import BaseModel\nfrom valkey.exceptions import ResponseError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/s3_vectors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/s3_vectors.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom typing import Dict, List, Optional\n\nfrom pydantic import BaseModel\n\nfrom mem0.vector_stores.base import VectorStoreBase\n\ntry:\n    import boto3",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/milvus.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/milvus.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Dict, Optional\n\nfrom pydantic import BaseModel\n\nfrom mem0.configs.vector_stores.milvus import MetricType\nfrom mem0.vector_stores.base import VectorStoreBase\n\ntry:\n    import pymilvus  # noqa: F401",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_182",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 182 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 182,
      "code_snippet": "        with self._get_cursor(commit=True) as cur:\n            # Create table with vector column\n            cur.execute(f\"\"\"\n                CREATE TABLE IF NOT EXISTS `{table_name}` (\n                    id VARCHAR(255) PRIMARY KEY,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_214",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting, % formatting",
      "description": "SQL query on line 214 uses f-string formatting, % formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 214,
      "code_snippet": "\n        with self._get_cursor(commit=True) as cur:\n            cur.executemany(\n                f\"INSERT INTO `{self.collection_name}` (id, vector, payload) VALUES (%s, %s, %s) \"\n                f\"ON DUPLICATE KEY UPDATE vector = VALUES(vector), payload = VALUES(payload)\",",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_377",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 377 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 377,
      "code_snippet": "        \"\"\"Delete the collection (table).\"\"\"\n        with self._get_cursor(commit=True) as cur:\n            cur.execute(f\"DROP TABLE IF EXISTS `{self.collection_name}`\")\n        logger.info(f\"Deleted collection '{self.collection_name}'\")\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_388",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "SQL injection: % formatting",
      "description": "SQL query on line 388 uses % formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 388,
      "code_snippet": "        \"\"\"\n        with self._get_cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT\n                    TABLE_NAME as name,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_432",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting, % formatting",
      "description": "SQL query on line 432 uses f-string formatting, % formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 432,
      "code_snippet": "\n        with self._get_cursor() as cur:\n            cur.execute(\n                f\"\"\"\n                SELECT id, vector, payload",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_274",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 274 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 274,
      "code_snippet": "        # In production, you'd want to use MySQL stored procedures or UDFs\n        with self._get_cursor() as cur:\n            query_sql = f\"\"\"\n                SELECT id, vector, payload\n                FROM `{self.collection_name}`",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_311",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 311 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 311,
      "code_snippet": "        \"\"\"\n        with self._get_cursor(commit=True) as cur:\n            cur.execute(f\"DELETE FROM `{self.collection_name}` WHERE id = %s\", (vector_id,))\n\n    def update(",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_330",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 330 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 330,
      "code_snippet": "            if vector is not None:\n                cur.execute(\n                    f\"UPDATE `{self.collection_name}` SET vector = %s WHERE id = %s\",\n                    (json.dumps(vector), vector_id),\n                )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_335",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 335 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 335,
      "code_snippet": "            if payload is not None:\n                cur.execute(\n                    f\"UPDATE `{self.collection_name}` SET payload = %s WHERE id = %s\",\n                    (json.dumps(payload), vector_id),\n                )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_351",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 351 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 351,
      "code_snippet": "        with self._get_cursor() as cur:\n            cur.execute(\n                f\"SELECT id, vector, payload FROM `{self.collection_name}` WHERE id = %s\",\n                (vector_id,),\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_mysql.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel\n\ntry:\n    import pymysql\n    from pymysql.cursors import DictCursor",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/configs.py",
      "line_number": 1,
      "code_snippet": "from typing import Dict, Optional\n\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass VectorStoreConfig(BaseModel):\n    provider: str = Field(\n        description=\"Provider of the vector store (e.g., 'qdrant', 'chroma', 'upstash_vector')\",\n        default=\"qdrant\",\n    )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pinecone.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pinecone.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom pydantic import BaseModel\n\ntry:\n    from pinecone import Pinecone, PodSpec, ServerlessSpec, Vector\nexcept ImportError:\n    raise ImportError(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/qdrant.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/qdrant.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport shutil\n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance,\n    FieldCondition,\n    Filter,\n    MatchValue,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/upstash_vector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/upstash_vector.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Dict, List, Optional\n\nfrom pydantic import BaseModel\n\nfrom mem0.vector_stores.base import VectorStoreBase\n\ntry:\n    from upstash_vector import Index\nexcept ImportError:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/mongodb.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/mongodb.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom importlib.metadata import version\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel\n\ntry:\n    from pymongo import MongoClient\n    from pymongo.driver_info import DriverInfo\n    from pymongo.errors import PyMongoError",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/neptune_analytics.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/neptune_analytics.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nimport uuid\nfrom typing import Dict, List, Optional\n\nfrom pydantic import BaseModel\n\ntry:\n    from langchain_aws import NeptuneAnalyticsGraph\nexcept ImportError:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM03_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py_94_unsafe_load",
      "category": "LLM03: Training Data Poisoning",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Unsafe data loading with pickle.load in training context",
      "description": "Function '_load' uses pickle.load on line 94. Pickle-based deserialization can execute arbitrary code, allowing attackers to inject malicious code through poisoned training data or models.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py",
      "line_number": 94,
      "code_snippet": "            with open(docstore_path, \"rb\") as f:\n                self.docstore, self.index_to_id = pickle.load(f)\n            logger.info(f\"Loaded FAISS index from {index_path} with {self.index.ntotal} vectors\")\n        except Exception as e:",
      "recommendation": "Secure Data Loading:\n1. Use safetensors instead of pickle for model weights\n2. For torch.load, use weights_only=True\n3. Verify checksums/signatures before loading\n4. Only load data from trusted, verified sources\n5. Implement content scanning before deserialization\n6. Consider using JSON/YAML for configuration data"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py_3_pickle",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.85,
      "title": "Use of pickle for serialization",
      "description": "Import of 'pickle' on line 3. This library can execute arbitrary code during deserialization. (Advisory: safe if only used with trusted local data.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py",
      "line_number": 3,
      "code_snippet": "import pickle",
      "recommendation": "Secure Serialization:\n1. Use safer alternatives like safetensors\n2. Never deserialize from untrusted sources\n3. Consider using ONNX for model exchange"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py_102_exposed_artifacts",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Model artifacts exposed without protection in '_save'",
      "description": "Function '_save' on line 102 exposes sklearn model artifacts without proper access control. This allows unauthorized users to download the full model, stealing intellectual property and training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py",
      "line_number": 102,
      "code_snippet": "            self.index_to_id = {}\n\n    def _save(self):\n        \"\"\"Save FAISS index and docstore to disk.\"\"\"\n        if not self.path or not self.index:\n            return",
      "recommendation": "Protect model artifacts from unauthorized access:\n\n1. Implement strict access control:\n   - Require authentication for model downloads\n   - Use role-based access control (RBAC)\n   - Log all artifact access attempts\n\n2. Store artifacts securely:\n   - Use private S3 buckets with signed URLs\n   - Never store in /static or /public directories\n   - Encrypt at rest and in transit\n\n3. Model obfuscation:\n   - Use model encryption\n   - Implement model quantization\n   - Remove unnecessary metadata\n\n4. Add legal protection:\n   - Include license files with models\n   - Add watermarks to model outputs\n   - Use model fingerprinting\n\n5. Monitor access:\n   - Track who downloads models\n   - Alert on unauthorized access\n   - Maintain audit logs"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/faiss.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport pickle\nimport uuid\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\nimport numpy as np\nfrom pydantic import BaseModel\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_154",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 154 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 154,
      "code_snippet": "        with self._get_cursor(commit=True) as cur:\n            cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n            cur.execute(\n                f\"\"\"\n                CREATE TABLE IF NOT EXISTS {self.collection_name} (",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_167",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 167 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 167,
      "code_snippet": "                if cur.fetchone():\n                    # Create DiskANN index if extension is installed for faster search\n                    cur.execute(\n                        f\"\"\"\n                        CREATE INDEX IF NOT EXISTS {self.collection_name}_diskann_idx",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_175",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 175 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 175,
      "code_snippet": "                    )\n            elif self.use_hnsw:\n                cur.execute(\n                    f\"\"\"\n                    CREATE INDEX IF NOT EXISTS {self.collection_name}_hnsw_idx",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_190",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting, % formatting",
      "description": "SQL query on line 190 uses f-string formatting, % formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 190,
      "code_snippet": "        if PSYCOPG_VERSION == 3:\n            with self._get_cursor(commit=True) as cur:\n                cur.executemany(\n                    f\"INSERT INTO {self.collection_name} (id, vector, payload) VALUES (%s, %s, %s)\",\n                    data,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_232",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting, % formatting",
      "description": "SQL query on line 232 uses f-string formatting, % formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 232,
      "code_snippet": "\n        with self._get_cursor() as cur:\n            cur.execute(\n                f\"\"\"\n                SELECT id, vector <=> %s::vector AS distance, payload",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_326",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 326 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 326,
      "code_snippet": "        \"\"\"Delete a collection.\"\"\"\n        with self._get_cursor(commit=True) as cur:\n            cur.execute(f\"DROP TABLE IF EXISTS {self.collection_name}\")\n\n    def col_info(self) -> dict[str, Any]:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_198",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 198 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 198,
      "code_snippet": "                execute_values(\n                    cur,\n                    f\"INSERT INTO {self.collection_name} (id, vector, payload) VALUES %s\",\n                    data,\n                )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_254",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 254 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 254,
      "code_snippet": "        \"\"\"\n        with self._get_cursor(commit=True) as cur:\n            cur.execute(f\"DELETE FROM {self.collection_name} WHERE id = %s\", (vector_id,))\n\n    def update(",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_273",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 273 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 273,
      "code_snippet": "            if vector:\n               cur.execute(\n                    f\"UPDATE {self.collection_name} SET vector = %s WHERE id = %s\",\n                    (vector, vector_id),\n                )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_281",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 281 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 281,
      "code_snippet": "                    # psycopg3 uses psycopg.types.json.Json\n                    cur.execute(\n                        f\"UPDATE {self.collection_name} SET payload = %s WHERE id = %s\",\n                        (Json(payload), vector_id),\n                    )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_287",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 287 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 287,
      "code_snippet": "                    # psycopg2 uses psycopg2.extras.Json\n                    cur.execute(\n                        f\"UPDATE {self.collection_name} SET payload = %s WHERE id = %s\",\n                        (Json(payload), vector_id),\n                    )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_304",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 304 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 304,
      "code_snippet": "        with self._get_cursor() as cur:\n            cur.execute(\n                f\"SELECT id, vector, payload FROM {self.collection_name} WHERE id = %s\",\n                (vector_id,),\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_337",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 337 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 337,
      "code_snippet": "        with self._get_cursor() as cur:\n            cur.execute(\n                f\"\"\"\n                SELECT\n                    table_name,",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_375",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 375 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 375,
      "code_snippet": "        filter_clause = \"WHERE \" + \" AND \".join(filter_conditions) if filter_conditions else \"\"\n\n        query = f\"\"\"\n            SELECT id, vector, payload\n            FROM {self.collection_name}",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/pgvector.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom contextlib import contextmanager\nfrom typing import Any, List, Optional\n\nfrom pydantic import BaseModel\n\n# Try to import psycopg (psycopg3) first, then fall back to psycopg2\ntry:\n    from psycopg.types.json import Json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/redis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/redis.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom datetime import datetime\nfrom functools import reduce\n\nimport numpy as np\nimport pytz\nimport redis\nfrom redis.commands.search.query import Query\nfrom redisvl.index import SearchIndex",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/weaviate.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/weaviate.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport uuid\nfrom typing import Dict, List, Mapping, Optional\nfrom urllib.parse import urlparse\n\nfrom pydantic import BaseModel\n\ntry:\n    import weaviate\nexcept ImportError:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_ai_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/azure_ai_search.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport re\nfrom typing import List, Optional\n\nfrom pydantic import BaseModel\n\nfrom mem0.memory.utils import extract_json\nfrom mem0.vector_stores.base import VectorStoreBase\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py_407",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 407 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
      "line_number": 407,
      "code_snippet": "            value_tuples.append(f\"({', '.join(formatted)})\")\n\n        insert_sql = f\"INSERT INTO {self.fully_qualified_table_name} ({', '.join(self.column_names)}) VALUES {', '.join(value_tuples)}\"\n\n        # Execute the insert",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py_497",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 497 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
      "line_number": 497,
      "code_snippet": "            logger.info(f\"Deleting vector with ID {vector_id} from Delta table {self.fully_qualified_table_name}\")\n\n            delete_sql = f\"DELETE FROM {self.fully_qualified_table_name} WHERE memory_id = '{vector_id}'\"\n\n            response = self.client.statement_execution.execute_statement(",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py_522",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 522 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
      "line_number": 522,
      "code_snippet": "        \"\"\"\n\n        update_sql = f\"UPDATE {self.fully_qualified_table_name} SET \"\n        set_clauses = []\n        if not vector_id:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py_742",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 742 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
      "line_number": 742,
      "code_snippet": "            # Drop the backing table (if it exists)\n            try:\n                drop_sql = f\"DROP TABLE IF EXISTS {self.fully_qualified_table_name}\"\n                resp = self.client.statement_execution.execute_statement(\n                    statement=drop_sql, warehouse_id=self.warehouse_id, wait_timeout=\"30s\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py_750",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 750 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
      "line_number": 750,
      "code_snippet": "                else:\n                    logger.warning(\n                        f\"Attempted to drop table '{self.fully_qualified_table_name}' but state was {getattr(resp.status, 'state', 'UNKNOWN')}: {getattr(resp.status, 'error', None)}\"\n                    )\n            except Exception as e_drop:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/databricks.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport uuid\nfrom typing import Optional, List\nfrom datetime import datetime, date\nfrom databricks.sdk.service.catalog import ColumnInfo, ColumnTypeName, TableType, DataSourceFormat\nfrom databricks.sdk.service.catalog import TableConstraint, PrimaryKeyConstraint\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.vectorsearch import (\n    VectorIndexType,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/baidu.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/baidu.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nfrom typing import Dict, Optional\n\nfrom pydantic import BaseModel\n\nfrom mem0.vector_stores.base import VectorStoreBase\n\ntry:\n    import pymochow",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py_73_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'credentials' containing sensitive data is being logged on line 73. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py",
      "line_number": 73,
      "code_snippet": "        # Support both credentials_path and service_account_json\n        if hasattr(config, \"credentials_path\") and config.credentials_path:\n            logger.debug(\"Using credentials from file: %s\", config.credentials_path)\n            credentials = service_account.Credentials.from_service_account_file(config.credentials_path)\n            init_args[\"credentials\"] = credentials",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py_77_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'credentials' containing sensitive data is being logged on line 77. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py",
      "line_number": 77,
      "code_snippet": "            init_args[\"credentials\"] = credentials\n        elif hasattr(config, \"service_account_json\") and config.service_account_json:\n            logger.debug(\"Using credentials from provided JSON dict\")\n            credentials = service_account.Credentials.from_service_account_info(config.service_account_json)\n            init_args[\"credentials\"] = credentials",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/vector_stores/vertex_ai_vector_search.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport traceback\nimport uuid\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport google.api_core.exceptions\nfrom google.cloud import aiplatform, aiplatform_v1\nfrom google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import Namespace\nfrom google.oauth2 import service_account\nfrom pydantic import BaseModel",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py_44_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'embed'",
      "description": "Function 'embed' on line 44 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py",
      "line_number": 44,
      "code_snippet": "        self.model = TextEmbeddingModel.from_pretrained(self.config.model)\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Vertex AI.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py_44_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'embed'",
      "description": "Function 'embed' on line 44 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py",
      "line_number": 44,
      "code_snippet": "        self.model = TextEmbeddingModel.from_pretrained(self.config.model)\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Vertex AI.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py_62",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py",
      "line_number": 62,
      "code_snippet": "            embedding_type = self.embedding_types[memory_action]\n\n        text_input = TextEmbeddingInput(text=text, task_type=embedding_type)\n        embeddings = self.model.get_embeddings(texts=[text_input], output_dimensionality=self.config.embedding_dims)\n\n        return embeddings[0].values",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py_44",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/vertexai.py",
      "line_number": 44,
      "code_snippet": "                    \"Google application credentials JSON is not provided. Please provide a valid JSON path or set the 'GOOGLE_APPLICATION_CREDENTIALS' environment variable.\"\n                )\n\n        self.model = TextEmbeddingModel.from_pretrained(self.config.model)\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Vertex AI.\n\n        Args:\n            text (str): The text to embed.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/gemini.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/gemini.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Literal, Optional\n\nfrom google import genai\nfrom google.genai import types\n\nfrom mem0.configs.embeddings.base import BaseEmbedderConfig\nfrom mem0.embeddings.base import EmbeddingBase\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py_55_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_get_embedding'",
      "description": "Function '_get_embedding' on line 55 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
      "line_number": 55,
      "code_snippet": "        return norm_emb.tolist()\n\n    def _get_embedding(self, text):\n        \"\"\"Call out to Bedrock embedding endpoint.\"\"\"\n\n        # Format input body based on the provider",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py_59",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
      "line_number": 59,
      "code_snippet": "        \"\"\"Call out to Bedrock embedding endpoint.\"\"\"\n\n        # Format input body based on the provider\n        provider = self.config.model.split(\".\")[0]\n        input_body = {}\n\n        if provider == \"cohere\":",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py_72",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
      "line_number": 72,
      "code_snippet": "        body = json.dumps(input_body)\n\n        try:\n            response = self.client.invoke_model(\n                body=body,\n                modelId=self.config.model,\n                accept=\"application/json\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py_55",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/aws_bedrock.py",
      "line_number": 55,
      "code_snippet": "        \"\"\"Normalize the embedding to a unit vector.\"\"\"\n        emb = np.array(embeddings)\n        norm_emb = emb / np.linalg.norm(emb)\n        return norm_emb.tolist()\n\n    def _get_embedding(self, text):\n        \"\"\"Call out to Bedrock embedding endpoint.\"\"\"\n\n        # Format input body based on the provider\n        provider = self.config.model.split(\".\")[0]\n        input_body = {}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/azure_openai.py_34",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'AzureOpenAI' is used in 'UPDATE' on line 34 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/azure_openai.py",
      "line_number": 34,
      "code_snippet": "\n        self.client = AzureOpenAI(\n            azure_deployment=azure_deployment,\n            azure_endpoint=azure_endpoint,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/azure_openai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/azure_openai.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Literal, Optional\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom openai import AzureOpenAI\n\nfrom mem0.configs.embeddings.base import BaseEmbedderConfig\nfrom mem0.embeddings.base import EmbeddingBase\n\nSCOPE = \"https://cognitiveservices.azure.com/.default\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py_27",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.model.get_sentence_embedding_dimension' is used in 'UPDATE' on line 27 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
      "line_number": 27,
      "code_snippet": "\n            self.config.embedding_dims = self.config.embedding_dims or self.model.get_sentence_embedding_dimension()\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py_29_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'embed'",
      "description": "Function 'embed' on line 29 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
      "line_number": 29,
      "code_snippet": "            self.config.embedding_dims = self.config.embedding_dims or self.model.get_sentence_embedding_dimension()\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Hugging Face.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py_29_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'embed'",
      "description": "Function 'embed' on line 29 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
      "line_number": 29,
      "code_snippet": "            self.config.embedding_dims = self.config.embedding_dims or self.model.get_sentence_embedding_dimension()\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Hugging Face.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py_44",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
      "line_number": 44,
      "code_snippet": "                input=text, model=self.config.model, **self.config.model_kwargs\n            ).data[0].embedding\n        else:\n            return self.model.encode(text, convert_to_numpy=True).tolist()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py_29",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/huggingface.py",
      "line_number": 29,
      "code_snippet": "\n            self.model = SentenceTransformer(self.config.model, **self.config.model_kwargs)\n\n            self.config.embedding_dims = self.config.embedding_dims or self.model.get_sentence_embedding_dimension()\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Hugging Face.\n\n        Args:\n            text (str): The text to embed.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/langchain.py_24_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.75,
      "title": "Critical decision without oversight in 'embed'",
      "description": "Function 'embed' on line 24 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/langchain.py",
      "line_number": 24,
      "code_snippet": "        self.langchain_model = self.config.model\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Get the embedding for the given text using Langchain.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/ollama.py_39",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'model.get' is used in 'UPDATE' on line 39 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/ollama.py",
      "line_number": 39,
      "code_snippet": "        local_models = self.client.list()[\"models\"]\n        if not any(model.get(\"name\") == self.config.model or model.get(\"model\") == self.config.model for model in local_models):\n            self.client.pull(self.config.model)\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/fastembed.py_18_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'embed'",
      "description": "Function 'embed' on line 18 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/fastembed.py",
      "line_number": 18,
      "code_snippet": "        self.dense_model = TextEmbedding(model_name = self.config.model)\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Convert the text to embeddings using FastEmbed running in the Onnx runtime\n        Args:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/fastembed.py_18_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'embed'",
      "description": "Function 'embed' on line 18 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/embeddings/fastembed.py",
      "line_number": 18,
      "code_snippet": "        self.dense_model = TextEmbedding(model_name = self.config.model)\n\n    def embed(self, text, memory_action: Optional[Literal[\"add\", \"search\", \"update\"]] = None):\n        \"\"\"\n        Convert the text to embeddings using FastEmbed running in the Onnx runtime\n        Args:",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_204",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'data' embedded in LLM prompt",
      "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 204,
      "code_snippet": "            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_347",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 347 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 347,
      "code_snippet": "\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_278",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.embedding_model.embed' is used in 'WHERE' on line 278 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 278,
      "code_snippet": "        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n\n            # Build query based on whether agent_id is provided",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_273",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_search_graph_db' on line 273 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 273,
      "code_snippet": "    def _search_graph_db(self, node_list, filters, limit=100):\n        \"\"\"Search similar nodes among and their respective incoming and outgoing relations.\"\"\"\n        result_relations = []\n\n        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n\n            # Build query based on whether agent_id is provided\n            if filters.get(\"agent_id\"):\n                cypher_query = \"\"\"\n                CALL vector_search.search(\"memzero\", $limit, $n_embedding)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_204_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 199 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 204,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[\n                {\n                    \"role\": \"system\",",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_260_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 232 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 260,
      "code_snippet": "            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_347_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 336 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 347,
      "code_snippet": "            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_199_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 199 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 199,
      "code_snippet": "        return final_results\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"Extracts all the entities mentioned in the query.\"\"\"\n        _tools = [EXTRACT_ENTITIES_TOOL]\n        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_405_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_add_entities'",
      "description": "Function '_add_entities' on line 405 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 405,
      "code_snippet": "\n    # added Entity label to all nodes for vector search to work\n    def _add_entities(self, to_be_added, filters, entity_type_map):\n        \"\"\"Add the new entities to the graph. Merge the nodes if they already exist.\"\"\"\n        user_id = filters[\"user_id\"]\n        agent_id = filters.get(\"agent_id\", None)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_199_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 199 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 199,
      "code_snippet": "        return final_results\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"Extracts all the entities mentioned in the query.\"\"\"\n        _tools = [EXTRACT_ENTITIES_TOOL]\n        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_232_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 232 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 232,
      "code_snippet": "        return entity_type_map\n\n    def _establish_nodes_relations_from_data(self, data, filters, entity_type_map):\n        \"\"\"Eshtablish relations among the extracted nodes.\"\"\"\n        if self.config.graph_store.custom_prompt:\n            messages = [",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_336_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 336 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 336,
      "code_snippet": "        return result_relations\n\n    def _get_delete_entities_from_search_output(self, search_output, data, filters):\n        \"\"\"Get the entities to be deleted from the search output.\"\"\"\n        search_output_string = format_entities(search_output)\n        system_prompt, user_prompt = get_delete_messages(search_output_string, data, filters[\"user_id\"])",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_405_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_add_entities'",
      "description": "Function '_add_entities' on line 405 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 405,
      "code_snippet": "\n    # added Entity label to all nodes for vector search to work\n    def _add_entities(self, to_be_added, filters, entity_type_map):\n        \"\"\"Add the new entities to the graph. Merge the nodes if they already exist.\"\"\"\n        user_id = filters[\"user_id\"]\n        agent_id = filters.get(\"agent_id\", None)",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_70",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 70 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 70,
      "code_snippet": "        if not self._vector_index_exists(index_info, \"memzero\"):\n            self.graph.query(\n                f\"CREATE VECTOR INDEX memzero ON :Entity(embedding) WITH CONFIG {{'dimension': {embedding_dims}, 'capacity': 1000, 'metric': 'cos'}};\"\n            )\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_260",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 260,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_347",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 347,
      "code_snippet": "                DELETE_MEMORY_STRUCT_TOOL_GRAPH,\n            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_278",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 278,
      "code_snippet": "        result_relations = []\n\n        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n\n            # Build query based on whether agent_id is provided\n            if filters.get(\"agent_id\"):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py_422",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/memgraph_memory.py",
      "line_number": 422,
      "code_snippet": "            destination_type = entity_type_map.get(destination, \"__User__\")\n\n            # embeddings\n            source_embedding = self.embedding_model.embed(source)\n            dest_embedding = self.embedding_model.embed(destination)\n\n            # search for the nodes with the closest embeddings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_227",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'data' embedded in LLM prompt",
      "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 227,
      "code_snippet": "            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_368",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 368 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 368,
      "code_snippet": "\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_317",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.embedding_model.embed' is used in 'execute(' on line 317 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 317,
      "code_snippet": "        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n            params[\"n_embedding\"] = n_embedding\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_629",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.embedding_model.embed' is used in 'execute(' on line 629 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 629,
      "code_snippet": "\n            result = self.kuzu_execute(cypher, parameters=params)\n            results.append(result)\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_297",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_search_graph_db' on line 297 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 297,
      "code_snippet": "    def _search_graph_db(self, node_list, filters, limit=100, threshold=None):\n        \"\"\"Search similar nodes among and their respective incoming and outgoing relations.\"\"\"\n        result_relations = []\n\n        params = {\n            \"threshold\": threshold if threshold else self.threshold,\n            \"user_id\": filters[\"user_id\"],\n            \"limit\": limit,\n        }\n        # Build node properties for filtering\n        node_props = [\"user_id: $user_id\"]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_227_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 222 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 227,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[\n                {\n                    \"role\": \"system\",",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_284_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 255 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 284,
      "code_snippet": "            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_368_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 349 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 368,
      "code_snippet": "            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_222_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 222 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 222,
      "code_snippet": "        return final_results\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"Extracts all the entities mentioned in the query.\"\"\"\n        _tools = [EXTRACT_ENTITIES_TOOL]\n        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_297_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_search_graph_db'",
      "description": "Function '_search_graph_db' on line 297 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 297,
      "code_snippet": "        return entities\n\n    def _search_graph_db(self, node_list, filters, limit=100, threshold=None):\n        \"\"\"Search similar nodes among and their respective incoming and outgoing relations.\"\"\"\n        result_relations = []\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_434_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in '_add_entities'",
      "description": "Function '_add_entities' on line 434 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 434,
      "code_snippet": "        return results\n\n    def _add_entities(self, to_be_added, filters, entity_type_map):\n        \"\"\"Add the new entities to the graph. Merge the nodes if they already exist.\"\"\"\n        user_id = filters[\"user_id\"]\n        agent_id = filters.get(\"agent_id\", None)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_222_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 222 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 222,
      "code_snippet": "        return final_results\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"Extracts all the entities mentioned in the query.\"\"\"\n        _tools = [EXTRACT_ENTITIES_TOOL]\n        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_255_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 255 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 255,
      "code_snippet": "        return entity_type_map\n\n    def _establish_nodes_relations_from_data(self, data, filters, entity_type_map):\n        \"\"\"Establish relations among the extracted nodes.\"\"\"\n\n        # Compose user identification string for prompt",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_349_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 349 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 349,
      "code_snippet": "        return result_relations\n\n    def _get_delete_entities_from_search_output(self, search_output, data, filters):\n        \"\"\"Get the entities to be deleted from the search output.\"\"\"\n        search_output_string = format_entities(search_output)\n",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_434_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_add_entities'",
      "description": "Function '_add_entities' on line 434 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 434,
      "code_snippet": "        return results\n\n    def _add_entities(self, to_be_added, filters, entity_type_map):\n        \"\"\"Add the new entities to the graph. Merge the nodes if they already exist.\"\"\"\n        user_id = filters[\"user_id\"]\n        agent_id = filters.get(\"agent_id\", None)",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_284",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 284,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py_368",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/kuzu_memory.py",
      "line_number": 368,
      "code_snippet": "                DELETE_MEMORY_STRUCT_TOOL_GRAPH,\n            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/telemetry.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/telemetry.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport platform\nimport sys\n\nfrom posthog import Posthog\n\nimport mem0\nfrom mem0.memory.setup import get_or_create_user_id\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/setup.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport uuid\n\n# Set up the directory path\nVECTOR_ID = str(uuid.uuid4())\nhome_dir = os.path.expanduser(\"~\")\nmem0_dir = os.environ.get(\"MEM0_DIR\") or os.path.join(home_dir, \".mem0\")\nos.makedirs(mem0_dir, exist_ok=True)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/utils.py_86_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.75,
      "title": "Critical decision without oversight in 'get_image_description'",
      "description": "Function 'get_image_description' on line 86 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/utils.py",
      "line_number": 86,
      "code_snippet": "\n\ndef get_image_description(image_obj, llm, vision_details):\n    \"\"\"\n    Get the description of the image\n    \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/utils.py_107",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/utils.py",
      "line_number": 107,
      "code_snippet": "    else:\n        messages = [image_obj]\n\n    response = llm.generate_response(messages=messages)\n    return response\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/storage.py_85",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 85 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/storage.py",
      "line_number": 85,
      "code_snippet": "                if intersecting:\n                    cols_csv = \", \".join(intersecting)\n                    cur.execute(f\"INSERT INTO history ({cols_csv}) SELECT {cols_csv} FROM history_old\")\n\n                # Drop the old table",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/storage.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/storage.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport sqlite3\nimport threading\nimport uuid\nfrom typing import Any, Dict, List, Optional\n\nlogger = logging.getLogger(__name__)\n\n\nclass SQLiteManager:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1114",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input 'messages' flows to LLM call via assignment in variable 'parsed_messages'. Function '_create_procedural_memory' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1114,
      "code_snippet": "\n        parsed_messages = [\n            {\"role\": \"system\", \"content\": prompt or PROCEDURAL_MEMORY_SYSTEM_PROMPT},\n            *messages,\n            {\n                \"role\": \"user\",\n                \"content\": \"Create procedural memory of the above conversation.\",\n            },\n        ]\n\n        try:\n            procedural_memory = self.llm.generate_response(messages=parsed_messages)\n            procedural_memory = remove_code_blocks(procedural_memory)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_452",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to 'extract_json' on line 452 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 452,
      "code_snippet": "                    # Try extracting JSON from response using built-in function\n                    extracted_json = extract_json(response)\n                    new_retrieved_facts = json.loads(extracted_json)[\"facts\"]\n        except Exception as e:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1087",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'embeddings' flows to 'self.vector_store.insert' on line 1087 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1087,
      "code_snippet": "\n        self.vector_store.insert(\n            vectors=[embeddings],\n            ids=[memory_id],",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1177",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'embeddings' flows to 'self.vector_store.update' on line 1177 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1177,
      "code_snippet": "\n        self.vector_store.update(\n            vector_id=memory_id,\n            vector=embeddings,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_281",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'add' on line 281 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 281,
      "code_snippet": "    def add(\n        self,\n        messages,\n        *,\n        user_id: Optional[str] = None,\n        agent_id: Optional[str] = None,\n        run_id: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        infer: bool = True,\n        memory_type: Optional[str] = None,\n        prompt: Optional[str] = None,",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1103",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_create_procedural_memory' on line 1103 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1103,
      "code_snippet": "    def _create_procedural_memory(self, messages, metadata=None, prompt=None):\n        \"\"\"\n        Create a procedural memory\n\n        Args:\n            messages (list): List of messages to create a procedural memory from.\n            metadata (dict): Metadata to create a procedural memory from.\n            prompt (str, optional): Prompt to use for the procedural memory creation. Defaults to None.\n        \"\"\"\n        logger.info(\"Creating procedural memory\")\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_502_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_add_to_vector_store'",
      "description": "Function '_add_to_vector_store' on line 386 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 502,
      "code_snippet": "\n            try:\n                response: str = self.llm.generate_response(\n                    messages=[{\"role\": \"user\", \"content\": function_calling_prompt}],\n                    response_format={\"type\": \"json_object\"},\n                )",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_954_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in '_search_vector_store'",
      "description": "Function '_search_vector_store' on line 954 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 954,
      "code_snippet": "        return False\n\n    def _search_vector_store(self, query, filters, limit, threshold: Optional[float] = None):\n        embeddings = self.embedding_model.embed(query, \"search\")\n        memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_992_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'update'",
      "description": "Function 'update' on line 992 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 992,
      "code_snippet": "        return original_memories\n\n    def update(self, memory_id, data):\n        \"\"\"\n        Update a memory by ID.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1075_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in '_create_memory'",
      "description": "Function '_create_memory' on line 1075 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1075,
      "code_snippet": "        return self.db.get_history(memory_id)\n\n    def _create_memory(self, data, existing_embeddings, metadata=None):\n        logger.debug(f\"Creating memory with {data=}\")\n        if data in existing_embeddings:\n            embeddings = existing_embeddings[data]",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1142_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in '_update_memory'",
      "description": "Function '_update_memory' on line 1142 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1142,
      "code_snippet": "        return result\n\n    def _update_memory(self, memory_id, data, existing_embeddings, metadata=None):\n        logger.info(f\"Updating memory with {data=}\")\n\n        try:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_281_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'add'",
      "description": "Function 'add' on line 281 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 281,
      "code_snippet": "        return has_agent_id and has_assistant_messages\n\n    def add(\n        self,\n        messages,\n        *,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_386_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.75,
      "title": "Critical decision without oversight in '_add_to_vector_store'",
      "description": "Function '_add_to_vector_store' on line 386 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 386,
      "code_snippet": "        return {\"results\": vector_store_result}\n\n    def _add_to_vector_store(self, messages, metadata, filters, infer):\n        if not infer:\n            returned_memories = []\n            for message_dict in messages:",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_954_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_search_vector_store'",
      "description": "Function '_search_vector_store' on line 954 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 954,
      "code_snippet": "        return False\n\n    def _search_vector_store(self, query, filters, limit, threshold: Optional[float] = None):\n        embeddings = self.embedding_model.embed(query, \"search\")\n        memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_992_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'update'",
      "description": "Function 'update' on line 992 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 992,
      "code_snippet": "        return original_memories\n\n    def update(self, memory_id, data):\n        \"\"\"\n        Update a memory by ID.\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1075_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_create_memory'",
      "description": "Function '_create_memory' on line 1075 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1075,
      "code_snippet": "        return self.db.get_history(memory_id)\n\n    def _create_memory(self, data, existing_embeddings, metadata=None):\n        logger.debug(f\"Creating memory with {data=}\")\n        if data in existing_embeddings:\n            embeddings = existing_embeddings[data]",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1103_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_create_procedural_memory'",
      "description": "Function '_create_procedural_memory' on line 1103 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1103,
      "code_snippet": "        return memory_id\n\n    def _create_procedural_memory(self, messages, metadata=None, prompt=None):\n        \"\"\"\n        Create a procedural memory\n",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1142_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_update_memory'",
      "description": "Function '_update_memory' on line 1142 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1142,
      "code_snippet": "        return result\n\n    def _update_memory(self, memory_id, data, existing_embeddings, metadata=None):\n        logger.info(f\"Updating memory with {data=}\")\n\n        try:",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_364",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 364,
      "code_snippet": "            results = self._create_procedural_memory(messages, metadata=processed_metadata, prompt=prompt)\n            return results\n\n        if self.config.llm.config.get(\"enable_vision\"):\n            messages = parse_vision_messages(messages, self.llm, self.config.llm.config.get(\"vision_details\"))\n        else:\n            messages = parse_vision_messages(messages)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_434",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 434,
      "code_snippet": "            is_agent_memory = self._should_use_agent_memory_extraction(messages, metadata)\n            system_prompt, user_prompt = get_fact_retrieval_messages(parsed_messages, is_agent_memory)\n\n        response = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_955",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 955,
      "code_snippet": "        return False\n\n    def _search_vector_store(self, query, filters, limit, threshold: Optional[float] = None):\n        embeddings = self.embedding_model.embed(query, \"search\")\n        memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n\n        promoted_payload_keys = [",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1390",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1390,
      "code_snippet": "            )\n            return results\n\n        if self.config.llm.config.get(\"enable_vision\"):\n            messages = parse_vision_messages(messages, self.llm, self.config.llm.config.get(\"vision_details\"))\n        else:\n            messages = parse_vision_messages(messages)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_473",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 473,
      "code_snippet": "        if filters.get(\"run_id\"):\n            search_filters[\"run_id\"] = filters[\"run_id\"]\n        for new_mem in new_retrieved_facts:\n            messages_embeddings = self.embedding_model.embed(new_mem, \"add\")\n            new_message_embeddings[new_mem] = messages_embeddings\n            existing_memories = self.vector_store.search(\n                query=new_mem,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1009",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1009,
      "code_snippet": "        \"\"\"\n        capture_event(\"mem0.update\", self, {\"memory_id\": memory_id, \"sync_type\": \"sync\"})\n\n        existing_embeddings = {data: self.embedding_model.embed(data, \"update\")}\n\n        self._update_memory(memory_id, data, existing_embeddings)\n        return {\"message\": \"Memory updated successfully!\"}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1124",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1124,
      "code_snippet": "        ]\n\n        try:\n            procedural_memory = self.llm.generate_response(messages=parsed_messages)\n            procedural_memory = remove_code_blocks(procedural_memory)\n        except Exception as e:\n            logger.error(f\"Error generating procedural memory summary: {e}\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_365",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 365,
      "code_snippet": "            return results\n\n        if self.config.llm.config.get(\"enable_vision\"):\n            messages = parse_vision_messages(messages, self.llm, self.config.llm.config.get(\"vision_details\"))\n        else:\n            messages = parse_vision_messages(messages)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_409",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 409,
      "code_snippet": "                    per_msg_meta[\"actor_id\"] = actor_name\n\n                msg_content = message_dict[\"content\"]\n                msg_embeddings = self.embedding_model.embed(msg_content, \"add\")\n                mem_id = self._create_memory(msg_content, msg_embeddings, per_msg_meta)\n\n                returned_memories.append(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_502",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 502,
      "code_snippet": "            )\n\n            try:\n                response: str = self.llm.generate_response(\n                    messages=[{\"role\": \"user\", \"content\": function_calling_prompt}],\n                    response_format={\"type\": \"json_object\"},\n                )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1391",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1391,
      "code_snippet": "            return results\n\n        if self.config.llm.config.get(\"enable_vision\"):\n            messages = parse_vision_messages(messages, self.llm, self.config.llm.config.get(\"vision_details\"))\n        else:\n            messages = parse_vision_messages(messages)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_954",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 954,
      "code_snippet": "            # Check for wildcard values\n            if value == \"*\":\n                return True\n        return False\n\n    def _search_vector_store(self, query, filters, limit, threshold: Optional[float] = None):\n        embeddings = self.embedding_model.embed(query, \"search\")\n        memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters)\n\n        promoted_payload_keys = [\n            \"user_id\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py_1103",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/main.py",
      "line_number": 1103,
      "code_snippet": "            actor_id=metadata.get(\"actor_id\"),\n            role=metadata.get(\"role\"),\n        )\n        return memory_id\n\n    def _create_procedural_memory(self, messages, metadata=None, prompt=None):\n        \"\"\"\n        Create a procedural memory\n\n        Args:\n            messages (list): List of messages to create a procedural memory from.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_201",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'data' embedded in LLM prompt",
      "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 201,
      "code_snippet": "            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_341",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 341 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 341,
      "code_snippet": "\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_284",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.embedding_model.embed' is used in 'WHERE' on line 284 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 284,
      "code_snippet": "        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n\n            cypher_query = f\"\"\"",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_271",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_search_graph_db' on line 271 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 271,
      "code_snippet": "    def _search_graph_db(self, node_list, filters, limit=100):\n        \"\"\"Search similar nodes among and their respective incoming and outgoing relations.\"\"\"\n        result_relations = []\n\n        # Build node properties for filtering\n        node_props = [\"user_id: $user_id\"]\n        if filters.get(\"agent_id\"):\n            node_props.append(\"agent_id: $agent_id\")\n        if filters.get(\"run_id\"):\n            node_props.append(\"run_id: $run_id\")\n        node_props_str = \", \".join(node_props)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_201_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 196 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 201,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[\n                {\n                    \"role\": \"system\",",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_258_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 229 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 258,
      "code_snippet": "            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_341_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 322 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 341,
      "code_snippet": "            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_196_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 196 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 196,
      "code_snippet": "        return final_results\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"Extracts all the entities mentioned in the query.\"\"\"\n        _tools = [EXTRACT_ENTITIES_TOOL]\n        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_271_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_search_graph_db'",
      "description": "Function '_search_graph_db' on line 271 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 271,
      "code_snippet": "        return entities\n\n    def _search_graph_db(self, node_list, filters, limit=100):\n        \"\"\"Search similar nodes among and their respective incoming and outgoing relations.\"\"\"\n        result_relations = []\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_413_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in '_add_entities'",
      "description": "Function '_add_entities' on line 413 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 413,
      "code_snippet": "        return results\n\n    def _add_entities(self, to_be_added, filters, entity_type_map):\n        \"\"\"Add the new entities to the graph. Merge the nodes if they already exist.\"\"\"\n        user_id = filters[\"user_id\"]\n        agent_id = filters.get(\"agent_id\", None)",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_196_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 196 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 196,
      "code_snippet": "        return final_results\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"Extracts all the entities mentioned in the query.\"\"\"\n        _tools = [EXTRACT_ENTITIES_TOOL]\n        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_229_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 229 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 229,
      "code_snippet": "        return entity_type_map\n\n    def _establish_nodes_relations_from_data(self, data, filters, entity_type_map):\n        \"\"\"Establish relations among the extracted nodes.\"\"\"\n\n        # Compose user identification string for prompt",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_322_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 322 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 322,
      "code_snippet": "        return result_relations\n\n    def _get_delete_entities_from_search_output(self, search_output, data, filters):\n        \"\"\"Get the entities to be deleted from the search output.\"\"\"\n        search_output_string = format_entities(search_output)\n",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_413_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_add_entities'",
      "description": "Function '_add_entities' on line 413 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 413,
      "code_snippet": "        return results\n\n    def _add_entities(self, to_be_added, filters, entity_type_map):\n        \"\"\"Add the new entities to the graph. Merge the nodes if they already exist.\"\"\"\n        user_id = filters[\"user_id\"]\n        agent_id = filters.get(\"agent_id\", None)",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_48",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 48 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 48,
      "code_snippet": "            # Safely add user_id index\n            try:\n                self.graph.query(f\"CREATE INDEX entity_single IF NOT EXISTS FOR (n {self.node_label}) ON (n.user_id)\")\n            except Exception:\n                pass",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_53",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 53 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 53,
      "code_snippet": "            try:  # Safely try to add composite index (Enterprise only)\n                self.graph.query(\n                    f\"CREATE INDEX entity_composite IF NOT EXISTS FOR (n {self.node_label}) ON (n.name, n.user_id)\"\n                )\n            except Exception:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_258",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 258,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_341",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 341,
      "code_snippet": "                DELETE_MEMORY_STRUCT_TOOL_GRAPH,\n            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py_284",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/memory/graph_memory.py",
      "line_number": 284,
      "code_snippet": "        node_props_str = \", \".join(node_props)\n\n        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n\n            cypher_query = f\"\"\"\n            MATCH (n {self.node_label} {{{node_props_str}}})",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py_110_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model' is used without version pinning on line 110. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py",
      "line_number": 110,
      "code_snippet": "            prepared_messages[-1][\"content\"] = self._format_query_with_memories(messages, relevant_memories)\n\n        response = litellm.completion(\n            model=model,\n            messages=prepared_messages,\n            temperature=temperature,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py_110_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in 'create'",
      "description": "Function 'create' on line 52 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py",
      "line_number": 110,
      "code_snippet": "            prepared_messages[-1][\"content\"] = self._format_query_with_memories(messages, relevant_memories)\n\n        response = litellm.completion(\n            model=model,\n            messages=prepared_messages,\n            temperature=temperature,",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py_52_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'create'",
      "description": "Function 'create' on line 52 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py",
      "line_number": 52,
      "code_snippet": "        self.mem0_client = mem0_client\n\n    def create(\n        self,\n        model: str,\n        messages: List = [],",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py_52_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'create'",
      "description": "Function 'create' on line 52 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py",
      "line_number": 52,
      "code_snippet": "        self.mem0_client = mem0_client\n\n    def create(\n        self,\n        model: str,\n        messages: List = [],",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py_110",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py",
      "line_number": 110,
      "code_snippet": "            logger.debug(f\"Retrieved {len(relevant_memories)} relevant memories\")\n            prepared_messages[-1][\"content\"] = self._format_query_with_memories(messages, relevant_memories)\n\n        response = litellm.completion(\n            model=model,\n            messages=prepared_messages,\n            temperature=temperature,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py_98",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/proxy/main.py",
      "line_number": 98,
      "code_snippet": "        if not any([user_id, agent_id, run_id]):\n            raise ValueError(\"One of user_id, agent_id, run_id must be provided\")\n\n        if not litellm.supports_function_calling(model):\n            raise ValueError(\n                f\"Model '{model}' does not support function calling. Please use a model that supports function calling.\"\n            )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/utils/factory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/utils/factory.py",
      "line_number": 1,
      "code_snippet": "import importlib\nfrom typing import Dict, Optional, Union\n\nfrom mem0.configs.embeddings.base import BaseEmbedderConfig\nfrom mem0.configs.llms.anthropic import AnthropicConfig\nfrom mem0.configs.llms.azure import AzureOpenAIConfig\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.deepseek import DeepSeekConfig\nfrom mem0.configs.llms.lmstudio import LMStudioConfig\nfrom mem0.configs.llms.ollama import OllamaConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/prompts.py_426",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 426 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/prompts.py",
      "line_number": 426,
      "code_snippet": "    \"\"\"\n\n    return f\"\"\"{custom_update_memory_prompt}\n\n    {current_memory_part}",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/prompts.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\n\nMEMORY_ANSWER_PROMPT = \"\"\"\nYou are an expert at answering questions based on the provided memories. Your task is to provide accurate and concise answers to the questions by leveraging the information given in the memories.\n\nGuidelines:\n- Extract relevant information from the memories based on the question.\n- If no relevant information is found, make sure you don't say no information is found. Instead, accept the question and provide a general response.\n- Ensure that the answers are clear, concise, and directly address the question.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/tools.py",
      "line_number": 1,
      "code_snippet": "UPDATE_MEMORY_TOOL_GRAPH = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"update_graph_memory\",\n        \"description\": \"Update the relationship key of an existing graph memory based on new information. This function should be called when there's a need to modify an existing relationship in the knowledge graph. The update should only be performed if the new information is more recent, more accurate, or provides additional context compared to the existing information. The source and destination nodes of the relationship must remain the same as in the existing graph memory; only the relationship itself can be updated.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"source\": {\n                    \"type\": \"string\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py_73",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py",
      "line_number": 73,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a response based on the given messages using vLLM.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py_73_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 73 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py",
      "line_number": 73,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py_73_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 73 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py",
      "line_number": 73,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/vllm.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom typing import Dict, List, Optional, Union\n\nfrom openai import OpenAI\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.vllm import VllmConfig\nfrom mem0.llms.base import LLMBase\nfrom mem0.memory.utils import extract_json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py_73",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py",
      "line_number": 73,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a response based on the given messages using LM Studio.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py_73_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 73 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py",
      "line_number": 73,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py_73_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 73 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py",
      "line_number": 73,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/lmstudio.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom typing import Dict, List, Optional, Union\n\nfrom openai import OpenAI\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.lmstudio import LMStudioConfig\nfrom mem0.llms.base import LLMBase\nfrom mem0.memory.utils import extract_json\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai_structured.py_49_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 49 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai_structured.py",
      "line_number": 49,
      "code_snippet": "        )\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format: Optional[str] = None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai_structured.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai_structured.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Dict, List, Optional\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom openai import AzureOpenAI\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.llms.base import LLMBase\n\nSCOPE = \"https://cognitiveservices.azure.com/.default\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/gemini.py_197_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.config.model' is used without version pinning on line 197. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/gemini.py",
      "line_number": 197,
      "code_snippet": "        generation_config = types.GenerateContentConfig(**config_params)\n\n        response = self.client.models.generate_content(\n            model=self.config.model, contents=contents, config=generation_config\n        )\n",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/gemini.py_134_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 134 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/gemini.py",
      "line_number": 134,
      "code_snippet": "            return None\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/gemini.py_134_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 134 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/gemini.py",
      "line_number": 134,
      "code_snippet": "            return None\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py_600",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'list_available_models' on line 600 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py",
      "line_number": 600,
      "code_snippet": "    def list_available_models(self) -> List[Dict[str, Any]]:\n        \"\"\"List all available models in the current region.\"\"\"\n        try:\n            bedrock_client = boto3.client(\"bedrock\", **self.config.get_aws_config())\n            response = bedrock_client.list_foundation_models()\n\n            models = []\n            for model in response[\"modelSummaries\"]:\n                provider = extract_provider(model[\"modelId\"])\n                models.append(\n                    {",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py_352_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_parse_response'",
      "description": "Function '_parse_response' on line 352 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py",
      "line_number": 352,
      "code_snippet": "        return new_tools\n\n    def _parse_response(\n        self, response: Dict[str, Any], tools: Optional[List[Dict]] = None\n    ) -> Union[str, Dict[str, Any]]:\n        \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py_600_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'list_available_models'",
      "description": "Function 'list_available_models' on line 600 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py",
      "line_number": 600,
      "code_snippet": "            return self._parse_response(response)\n\n    def list_available_models(self) -> List[Dict[str, Any]]:\n        \"\"\"List all available models in the current region.\"\"\"\n        try:\n            bedrock_client = boto3.client(\"bedrock\", **self.config.get_aws_config())",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py_231_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_prepare_input'",
      "description": "Function '_prepare_input' on line 231 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/aws_bedrock.py",
      "line_number": 231,
      "code_snippet": "        return \"\\n\\nHuman: \" + \"\".join(formatted_messages) + \"\\n\\nAssistant:\"\n\n    def _prepare_input(self, prompt: str) -> Dict[str, Any]:\n        \"\"\"\n        Prepare input for the current provider's model.\n",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/litellm.py_51_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 51 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/litellm.py",
      "line_number": 51,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/litellm.py_51_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 51 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/litellm.py",
      "line_number": 51,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/groq.py_55_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 55 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/groq.py",
      "line_number": 55,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/groq.py_55_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 55 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/groq.py",
      "line_number": 55,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py_142",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'response' flows to 'self.config.response_callback' on line 142 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
      "line_number": 142,
      "code_snippet": "            try:\n                self.config.response_callback(self, response, params)\n            except Exception as e:\n                # Log error but don't propagate",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py_83",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 83 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
      "line_number": 83,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a JSON response based on the given messages using OpenAI.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py_83_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 83 performs high-risk delete/write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
      "line_number": 83,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py_83_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 83 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
      "line_number": 83,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nfrom typing import Dict, List, Optional, Union\n\nfrom openai import OpenAI\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.openai import OpenAIConfig\nfrom mem0.llms.base import LLMBase",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py_100",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 100 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py",
      "line_number": 100,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a response based on the given messages using Azure OpenAI.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py_100_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 100 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py",
      "line_number": 100,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/azure_openai.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom typing import Dict, List, Optional, Union\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom openai import AzureOpenAI\n\nfrom mem0.configs.llms.azure import AzureOpenAIConfig\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.llms.base import LLMBase",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py_73",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py",
      "line_number": 73,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a response based on the given messages using DeepSeek.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py_73_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 73 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py",
      "line_number": 73,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py_73_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 73 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py",
      "line_number": 73,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/deepseek.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nfrom typing import Dict, List, Optional, Union\n\nfrom openai import OpenAI\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.deepseek import DeepSeekConfig\nfrom mem0.llms.base import LLMBase\nfrom mem0.memory.utils import extract_json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py_31_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 31 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py",
      "line_number": 31,
      "code_snippet": "        )\n\n    def generate_response(self, messages: List[Dict[str, str]], response_format=None) -> str:\n        \"\"\"\n        Generate a response based on the given messages using Sarvam-M.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py_31_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 31 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py",
      "line_number": 31,
      "code_snippet": "        )\n\n    def generate_response(self, messages: List[Dict[str, str]], response_format=None) -> str:\n        \"\"\"\n        Generate a response based on the given messages using Sarvam-M.\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/sarvam.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Dict, List, Optional\n\nimport requests\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.llms.base import LLMBase\n\n\nclass SarvamLLM(LLMBase):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py_43",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 43 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py",
      "line_number": 43,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a response based on the given messages using Anthropic.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py_43_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 43 performs high-risk delete/write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py",
      "line_number": 43,
      "code_snippet": "        self.client = anthropic.Anthropic(api_key=api_key)\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py_43_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 43 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py",
      "line_number": 43,
      "code_snippet": "        self.client = anthropic.Anthropic(api_key=api_key)\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py_86",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/anthropic.py",
      "line_number": 86,
      "code_snippet": "            params[\"tools\"] = tools\n            params[\"tool_choice\"] = tool_choice\n\n        response = self.client.messages.create(**params)\n        return response.content[0].text",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py_21_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 21 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py",
      "line_number": 21,
      "code_snippet": "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py_21_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 21 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py",
      "line_number": 21,
      "code_snippet": "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py_51",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/xai.py",
      "line_number": 51,
      "code_snippet": "        if response_format:\n            params[\"response_format\"] = response_format\n\n        response = self.client.chat.completions.create(**params)\n        return response.choices[0].message.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/together.py_55_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 55 performs high-risk delete/write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/together.py",
      "line_number": 55,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/together.py_55_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 55 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/together.py",
      "line_number": 55,
      "code_snippet": "            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py_54",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 54 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
      "line_number": 54,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n    ):\n        \"\"\"\n        Generate a response based on the given messages using langchain_community.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py_54_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 54 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
      "line_number": 54,
      "code_snippet": "        return processed_response\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py_54_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 54 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
      "line_number": 54,
      "code_snippet": "        return processed_response\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py_80",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
      "line_number": 80,
      "code_snippet": "            content = message[\"content\"]\n\n            if role == \"system\":\n                langchain_messages.append((\"system\", content))\n            elif role == \"user\":\n                langchain_messages.append((\"human\", content))\n            elif role == \"assistant\":",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py_82",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
      "line_number": 82,
      "code_snippet": "            if role == \"system\":\n                langchain_messages.append((\"system\", content))\n            elif role == \"user\":\n                langchain_messages.append((\"human\", content))\n            elif role == \"assistant\":\n                langchain_messages.append((\"ai\", content))\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py_84",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/langchain.py",
      "line_number": 84,
      "code_snippet": "            elif role == \"user\":\n                langchain_messages.append((\"human\", content))\n            elif role == \"assistant\":\n                langchain_messages.append((\"ai\", content))\n\n        if not langchain_messages:\n            raise ValueError(\"No valid messages found in the messages list\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py_21",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 21 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py",
      "line_number": 21,
      "code_snippet": "    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format: Optional[str] = None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n    ) -> str:\n        \"\"\"\n        Generate a response based on the given messages using OpenAI.\n\n        Args:",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py_21_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 21 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py",
      "line_number": 21,
      "code_snippet": "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format: Optional[str] = None,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py_51",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/openai_structured.py",
      "line_number": 51,
      "code_snippet": "            params[\"tools\"] = tools\n            params[\"tool_choice\"] = tool_choice\n\n        response = self.client.beta.chat.completions.parse(**params)\n        return response.choices[0].message.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/ollama.py_69_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete operation without confirmation in 'generate_response'",
      "description": "Function 'generate_response' on line 69 performs high-risk delete operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/ollama.py",
      "line_number": 69,
      "code_snippet": "            return content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/ollama.py_69_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 69 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/ollama.py",
      "line_number": 69,
      "code_snippet": "            return content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/llms/base.py",
      "line_number": 1,
      "code_snippet": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union\n\nfrom mem0.configs.llms.base import BaseLlmConfig\n\n\nclass LLMBase(ABC):\n    \"\"\"\n    Base class for all LLM providers.\n    Handles common functionality and delegates provider-specific logic to subclasses.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport httpx\n\nfrom mem0.exceptions import (\n    NetworkError,\n    create_exception_from_response,\n)\n\nlogger = logging.getLogger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/main.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nimport os\nimport warnings\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\nimport requests\n\nfrom mem0.client.project import AsyncProject, Project",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/project.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/client/project.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\nfrom pydantic import BaseModel, ConfigDict, Field\n\nfrom mem0.client.utils import api_error_handler\nfrom mem0.memory.telemetry import capture_client_event\n# Exception classes are referenced in docstrings only",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/neptunedb.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/neptunedb.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport uuid\nfrom datetime import datetime\nimport pytz\n\nfrom .base import NeptuneBase\n\ntry:\n    from langchain_aws import NeptuneGraph\nexcept ImportError:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/neptunegraph.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/neptunegraph.py",
      "line_number": 1,
      "code_snippet": "import logging\n\nfrom .base import NeptuneBase\n\ntry:\n    from langchain_aws import NeptuneAnalyticsGraph\n    from botocore.config import Config\nexcept ImportError:\n    raise ImportError(\"langchain_aws is not installed. Please install it using 'make install_all'.\")\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_83",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'data' embedded in LLM prompt",
      "description": "User input parameter 'data' is directly passed to LLM API call 'self.llm.generate_response'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 83,
      "code_snippet": "            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_174",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.generate_response' is used in 'DELETE' on line 174 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 174,
      "code_snippet": "\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_216",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_add_entities' on line 216 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 216,
      "code_snippet": "    def _add_entities(self, to_be_added, user_id, entity_type_map):\n        \"\"\"\n        Add the new entities to the graph. Merge the nodes if they already exist.\n        \"\"\"\n\n        results = []\n        for item in to_be_added:\n            # entities\n            source = item[\"source\"]\n            destination = item[\"destination\"]\n            relationship = item[\"relationship\"]",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_461",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_search_graph_db' on line 461 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 461,
      "code_snippet": "    def _search_graph_db(self, node_list, filters, limit=100):\n        \"\"\"\n        Search similar nodes among and their respective incoming and outgoing relations.\n        \"\"\"\n        result_relations = []\n\n        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n            cypher_query, params = self._search_graph_db_cypher(n_embedding, filters, limit)\n            ans = self.graph.query(cypher_query, params=params)\n            result_relations.extend(ans)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_83_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 76 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 83,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [EXTRACT_ENTITIES_STRUCT_TOOL]\n        search_results = self.llm.generate_response(\n            messages=[\n                {\n                    \"role\": \"system\",",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_140_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 110 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 140,
      "code_snippet": "            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_174_tool_calling",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "LLM tool calling without permission checks in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 160 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 174,
      "code_snippet": "            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "Tool Calling Security Best Practices:\n1. Implement permission checks before tool execution (check_permission, authorize)\n2. Use allowlists to restrict which tools can be called\n3. Require human confirmation for sensitive operations\n4. Log all tool executions with context for audit trails\n5. Implement rate limiting on tool calls to prevent abuse\n6. Use least-privilege principle - only grant necessary permissions\n7. Add input validation for tool parameters\n8. Consider implementing a \"dry-run\" mode for testing\n9. Set up alerts for unusual tool usage patterns\n10. Document tool permissions and restrictions clearly"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_76_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 76 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 76,
      "code_snippet": "        return {\"deleted_entities\": deleted_entities, \"added_entities\": added_entities}\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"\n        Extract all entities mentioned in the query.\n        \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_76_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_retrieve_nodes_from_data'",
      "description": "Function '_retrieve_nodes_from_data' on line 76 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 76,
      "code_snippet": "        return {\"deleted_entities\": deleted_entities, \"added_entities\": added_entities}\n\n    def _retrieve_nodes_from_data(self, data, filters):\n        \"\"\"\n        Extract all entities mentioned in the query.\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_110_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_establish_nodes_relations_from_data'",
      "description": "Function '_establish_nodes_relations_from_data' on line 110 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 110,
      "code_snippet": "        return entity_type_map\n\n    def _establish_nodes_relations_from_data(self, data, filters, entity_type_map):\n        \"\"\"\n        Establish relations among the extracted nodes.\n        \"\"\"",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_160_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_delete_entities_from_search_output'",
      "description": "Function '_get_delete_entities_from_search_output' on line 160 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 160,
      "code_snippet": "        return entity_list\n\n    def _get_delete_entities_from_search_output(self, search_output, data, filters):\n        \"\"\"\n        Get the entities to be deleted from the search output.\n        \"\"\"",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_140",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 140,
      "code_snippet": "        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\n            _tools = [RELATIONS_STRUCT_TOOL]\n\n        extracted_entities = self.llm.generate_response(\n            messages=messages,\n            tools=_tools,\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_174",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 174,
      "code_snippet": "                DELETE_MEMORY_STRUCT_TOOL_GRAPH,\n            ]\n\n        memory_updates = self.llm.generate_response(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_233",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 233,
      "code_snippet": "            destination_type = entity_type_map.get(destination, \"__User__\")\n\n            # embeddings\n            source_embedding = self.embedding_model.embed(source)\n            dest_embedding = self.embedding_model.embed(destination)\n\n            # search for the nodes with the closest embeddings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_234",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 234,
      "code_snippet": "\n            # embeddings\n            source_embedding = self.embedding_model.embed(source)\n            dest_embedding = self.embedding_model.embed(destination)\n\n            # search for the nodes with the closest embeddings\n            source_node_search_result = self._search_source_node(source_embedding, user_id, threshold=self.threshold)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py_468",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/graphs/neptune/base.py",
      "line_number": 468,
      "code_snippet": "        result_relations = []\n\n        for node in node_list:\n            n_embedding = self.embedding_model.embed(node)\n            cypher_query, params = self._search_graph_db_cypher(n_embedding, filters, limit)\n            ans = self.graph.query(cypher_query, params=params)\n            result_relations.extend(ans)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/cassandra.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/cassandra.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass CassandraConfig(BaseModel):\n    \"\"\"Configuration for Apache Cassandra vector database.\"\"\"\n\n    contact_points: List[str] = Field(\n        ...,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/elasticsearch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/elasticsearch.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Callable\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass ElasticsearchConfig(BaseModel):\n    collection_name: str = Field(\"mem0\", description=\"Name of the index\")\n    host: str = Field(\"localhost\", description=\"Elasticsearch host\")\n    port: int = Field(9200, description=\"Elasticsearch port\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/s3_vectors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/s3_vectors.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass S3VectorsConfig(BaseModel):\n    vector_bucket_name: str = Field(description=\"Name of the S3 Vector bucket\")\n    collection_name: str = Field(\"mem0\", description=\"Name of the vector index\")\n    embedding_model_dims: int = Field(1536, description=\"Dimension of the embedding vector\")\n    distance_metric: str = Field(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/milvus.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/milvus.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Any, Dict\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass MetricType(str, Enum):\n    \"\"\"\n    Metric Constant for milvus/ zilliz server.\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/azure_mysql.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/azure_mysql.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass AzureMySQLConfig(BaseModel):\n    \"\"\"Configuration for Azure MySQL vector database.\"\"\"\n\n    host: str = Field(..., description=\"MySQL server host (e.g., myserver.mysql.database.azure.com)\")\n    port: int = Field(3306, description=\"MySQL server port\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/pinecone.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/pinecone.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass PineconeConfig(BaseModel):\n    \"\"\"Configuration for Pinecone vector database.\"\"\"\n\n    collection_name: str = Field(\"mem0\", description=\"Name of the index/collection\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/supabase.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/supabase.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\nfrom typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, Field, model_validator\n\n\nclass IndexMethod(str, Enum):\n    AUTO = \"auto\"\n    HNSW = \"hnsw\"\n    IVFFLAT = \"ivfflat\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/qdrant.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/qdrant.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, ClassVar, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass QdrantConfig(BaseModel):\n    from qdrant_client import QdrantClient\n\n    QdrantClient: ClassVar[type] = QdrantClient\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/faiss.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/faiss.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass FAISSConfig(BaseModel):\n    collection_name: str = Field(\"mem0\", description=\"Default name for the collection\")\n    path: Optional[str] = Field(None, description=\"Path to store FAISS index and metadata\")\n    distance_strategy: str = Field(\n        \"euclidean\", description=\"Distance strategy to use. Options: 'euclidean', 'inner_product', 'cosine'\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/redis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/redis.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\n# TODO: Upgrade to latest pydantic version\nclass RedisDBConfig(BaseModel):\n    redis_url: str = Field(..., description=\"Redis URL\")\n    collection_name: str = Field(\"mem0\", description=\"Collection name\")\n    embedding_model_dims: int = Field(1536, description=\"Embedding model dimensions\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/weaviate.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/weaviate.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, ClassVar, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass WeaviateConfig(BaseModel):\n    from weaviate import WeaviateClient\n\n    WeaviateClient: ClassVar[type] = WeaviateClient\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/azure_ai_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/azure_ai_search.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass AzureAISearchConfig(BaseModel):\n    collection_name: str = Field(\"mem0\", description=\"Name of the collection\")\n    service_name: str = Field(None, description=\"Azure AI Search service name\")\n    api_key: str = Field(None, description=\"API key for the Azure AI Search service\")\n    embedding_model_dims: int = Field(1536, description=\"Dimension of the embedding vector\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/databricks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/databricks.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\nfrom databricks.sdk.service.vectorsearch import EndpointType, VectorIndexType, PipelineType\n\n\nclass DatabricksConfig(BaseModel):\n    \"\"\"Configuration for Databricks Vector Search vector store.\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/langchain.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/langchain.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, ClassVar, Dict\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass LangchainConfig(BaseModel):\n    try:\n        from langchain_community.vectorstores import VectorStore\n    except ImportError:\n        raise ImportError(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/baidu.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/vector_stores/baidu.py",
      "line_number": 1,
      "code_snippet": "from typing import Any, Dict\n\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\n\n\nclass BaiduDBConfig(BaseModel):\n    endpoint: str = Field(\"http://localhost:8287\", description=\"Endpoint URL for Baidu VectorDB\")\n    account: str = Field(\"root\", description=\"Account for Baidu VectorDB\")\n    api_key: str = Field(None, description=\"API Key for Baidu VectorDB\")\n    database_name: str = Field(\"mem0\", description=\"Name of the database\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/llms/aws_bedrock.py_110_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write operation without confirmation in 'validate_model_format'",
      "description": "Function 'validate_model_format' on line 110 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/llms/aws_bedrock.py",
      "line_number": 110,
      "code_snippet": "        return config\n\n    def validate_model_format(self) -> bool:\n        \"\"\"\n        Validate that the model identifier follows Bedrock naming convention.\n        ",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/llms/aws_bedrock.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/mem0/configs/llms/aws_bedrock.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Any, Dict, List, Optional\n\nfrom mem0.configs.llms.base import BaseLlmConfig\n\n\nclass AWSBedrockConfig(BaseLlmConfig):\n    \"\"\"\n    Configuration class for AWS Bedrock LLM integration.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/utils.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nBorrowed from https://github.com/WujiangXu/AgenticMemory/blob/main/utils.py\n\n@article{xu2025mem,\n    title={A-mem: Agentic memory for llm agents},\n    author={Xu, Wujiang and Liang, Zujie and Mei, Kai and Gao, Hang and Tan, Juntao\n           and Zhang, Yongfeng},\n    journal={arXiv preprint arXiv:2502.12110},\n    year={2025}\n}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py_41",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py",
      "line_number": 41,
      "code_snippet": "\ndef evaluate_llm_judge(question, gold_answer, generated_answer):\n    \"\"\"Evaluate the generated answer against the gold answer using an LLM judge.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py_54",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'response' flows to 'extract_json' on line 54 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py",
      "line_number": 54,
      "code_snippet": "    )\n    label = json.loads(extract_json(response.choices[0].message.content))[\"label\"]\n    return 1 if label == \"CORRECT\" else 0\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py_41_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4o-mini'' is used without version pinning on line 41. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py",
      "line_number": 41,
      "code_snippet": "def evaluate_llm_judge(question, gold_answer, generated_answer):\n    \"\"\"Evaluate the generated answer against the gold answer using an LLM judge.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py_39_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'evaluate_llm_judge'",
      "description": "Function 'evaluate_llm_judge' on line 39 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/metrics/llm_judge.py",
      "line_number": 39,
      "code_snippet": "\n\ndef evaluate_llm_judge(question, gold_answer, generated_answer):\n    \"\"\"Evaluate the generated answer against the gold answer using an LLM judge.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_26",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input 'question' flows to LLM call via call in variable 'prompt'. Function 'get_answer' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 26,
      "code_snippet": "def get_answer(question, speaker_1_user_id, speaker_1_memories, speaker_2_user_id, speaker_2_memories):\n    prompt = ANSWER_PROMPT_TEMPLATE.render(\n        question=question,\n        speaker_1_user_id=speaker_1_user_id,\n        speaker_1_memories=speaker_1_memories,\n        speaker_2_user_id=speaker_2_user_id,\n        speaker_2_memories=speaker_2_memories,\n    )\n\n    t1 = time.time()\n    response = client.chat.completions.create(\n        model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": prompt}], temperature=0.0",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_83",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM API call 'self.agent.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 83,
      "code_snippet": "    def add_memory(self, message, config):\n        return self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]}, config=config)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_88",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.agent.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 88,
      "code_snippet": "            t1 = time.time()\n            response = self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n            t2 = time.time()",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_25",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_answer' on line 25 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 25,
      "code_snippet": "def get_answer(question, speaker_1_user_id, speaker_1_memories, speaker_2_user_id, speaker_2_memories):\n    prompt = ANSWER_PROMPT_TEMPLATE.render(\n        question=question,\n        speaker_1_user_id=speaker_1_user_id,\n        speaker_1_memories=speaker_1_memories,\n        speaker_2_user_id=speaker_2_user_id,\n        speaker_2_memories=speaker_2_memories,\n    )\n\n    t1 = time.time()\n    response = client.chat.completions.create(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_82",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'add_memory' on line 82 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 82,
      "code_snippet": "    def add_memory(self, message, config):\n        return self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]}, config=config)\n\n    def search_memory(self, query, config):\n        try:\n            t1 = time.time()\n            response = self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n            t2 = time.time()\n            return response[\"messages\"][-1].content, t2 - t1\n        except Exception as e:\n            print(f\"Error in search_memory: {e}\")",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_85",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'search_memory' on line 85 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 85,
      "code_snippet": "    def search_memory(self, query, config):\n        try:\n            t1 = time.time()\n            response = self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n            t2 = time.time()\n            return response[\"messages\"][-1].content, t2 - t1\n        except Exception as e:\n            print(f\"Error in search_memory: {e}\")\n            return \"\", t2 - t1\n\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_35_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'os.getenv('MODEL')' is used without version pinning on line 35. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 35,
      "code_snippet": "\n    t1 = time.time()\n    response = client.chat.completions.create(\n        model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": prompt}], temperature=0.0\n    )\n    t2 = time.time()",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_25_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_answer'",
      "description": "Function 'get_answer' on line 25 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 25,
      "code_snippet": "\n\ndef get_answer(question, speaker_1_user_id, speaker_1_memories, speaker_2_user_id, speaker_2_memories):\n    prompt = ANSWER_PROMPT_TEMPLATE.render(\n        question=question,\n        speaker_1_user_id=speaker_1_user_id,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_82_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'add_memory'",
      "description": "Function 'add_memory' on line 82 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 82,
      "code_snippet": "        )\n\n    def add_memory(self, message, config):\n        return self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]}, config=config)\n\n    def search_memory(self, query, config):",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_85_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'search_memory'",
      "description": "Function 'search_memory' on line 85 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 85,
      "code_snippet": "        return self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]}, config=config)\n\n    def search_memory(self, query, config):\n        try:\n            t1 = time.time()\n            response = self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_35",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 35,
      "code_snippet": "    )\n\n    t1 = time.time()\n    response = client.chat.completions.create(\n        model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": prompt}], temperature=0.0\n    )\n    t2 = time.time()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_82",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 82,
      "code_snippet": "            ],\n            store=self.store,\n            checkpointer=self.checkpointer,\n        )\n\n    def add_memory(self, message, config):\n        return self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]}, config=config)\n\n    def search_memory(self, query, config):\n        try:\n            t1 = time.time()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py_85",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/langmem.py",
      "line_number": 85,
      "code_snippet": "        )\n\n    def add_memory(self, message, config):\n        return self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]}, config=config)\n\n    def search_memory(self, query, config):\n        try:\n            t1 = time.time()\n            response = self.agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n            t2 = time.time()\n            return response[\"messages\"][-1].content, t2 - t1",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py_34",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'generate_response' on line 34 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
      "line_number": 34,
      "code_snippet": "    def generate_response(self, question, context):\n        template = Template(PROMPT)\n        prompt = template.render(CONTEXT=context, QUESTION=question)\n\n        max_retries = 3\n        retries = 0\n\n        while retries <= max_retries:\n            try:\n                t1 = time.time()\n                response = self.client.chat.completions.create(",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py_144",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'process_all_conversations' on line 144 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
      "line_number": 144,
      "code_snippet": "    def process_all_conversations(self, output_file_path):\n        with open(self.data_path, \"r\") as f:\n            data = json.load(f)\n\n        FINAL_RESULTS = defaultdict(list)\n        for key, value in tqdm(data.items(), desc=\"Processing conversations\"):\n            chat_history = value[\"conversation\"]\n            questions = value[\"question\"]\n\n            chunks, embeddings = self.create_chunks(chat_history, self.chunk_size)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py_44_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.model' is used without version pinning on line 44. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
      "line_number": 44,
      "code_snippet": "            try:\n                t1 = time.time()\n                response = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=[\n                        {",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py_144_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'process_all_conversations'",
      "description": "Function 'process_all_conversations' on line 144 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
      "line_number": 144,
      "code_snippet": "        return chunks, embeddings\n\n    def process_all_conversations(self, output_file_path):\n        with open(self.data_path, \"r\") as f:\n            data = json.load(f)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py_34_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'generate_response'",
      "description": "Function 'generate_response' on line 34 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
      "line_number": 34,
      "code_snippet": "        self.k = k\n\n    def generate_response(self, question, context):\n        template = Template(PROMPT)\n        prompt = template.render(CONTEXT=context, QUESTION=question)\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py_44",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/rag.py",
      "line_number": 44,
      "code_snippet": "        while retries <= max_retries:\n            try:\n                t1 = time.time()\n                response = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=[\n                        {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/add.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/add.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport os\n\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\nfrom zep_cloud import Message\nfrom zep_cloud.client import Zep\n\nload_dotenv()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py_103",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input 'question' flows to LLM call via call in variable 'answer_prompt'. Function 'answer_question' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
      "line_number": 103,
      "code_snippet": "        template = Template(ANSWER_PROMPT_ZEP)\n        answer_prompt = template.render(memories=context, question=question)\n\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py_106",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.openai_client.chat.completions.create' is used in 'render' on line 106 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
      "line_number": 106,
      "code_snippet": "        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py_106_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'os.getenv('MODEL')' is used without version pinning on line 106. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
      "line_number": 106,
      "code_snippet": "\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )\n        t2 = time.time()",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py_99_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'answer_question'",
      "description": "Function 'answer_question' on line 99 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
      "line_number": 99,
      "code_snippet": "        return result\n\n    def answer_question(self, run_id, idx, question):\n        context, search_memory_time = self.search_memory(run_id, idx, question)\n\n        template = Template(ANSWER_PROMPT_ZEP)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py_106",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/zep/search.py",
      "line_number": 106,
      "code_snippet": "        answer_prompt = template.render(memories=context, question=question)\n\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )\n        t2 = time.time()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py_102",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input 'question' flows to LLM call via call in variable 'answer_prompt'. Function 'answer_question' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
      "line_number": 102,
      "code_snippet": "        template = Template(self.ANSWER_PROMPT)\n        answer_prompt = template.render(\n            speaker_1_user_id=speaker_1_user_id.split(\"_\")[0],\n            speaker_2_user_id=speaker_2_user_id.split(\"_\")[0],\n            speaker_1_memories=json.dumps(search_1_memory, indent=4),\n            speaker_2_memories=json.dumps(search_2_memory, indent=4),\n            speaker_1_graph_memories=json.dumps(speaker_1_graph_memories, indent=4),\n            speaker_2_graph_memories=json.dumps(speaker_2_graph_memories, indent=4),\n            question=question,\n        )\n\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py_90",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'answer_question' on line 90 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
      "line_number": 90,
      "code_snippet": "    def answer_question(self, speaker_1_user_id, speaker_2_user_id, question, answer, category):\n        speaker_1_memories, speaker_1_graph_memories, speaker_1_memory_time = self.search_memory(\n            speaker_1_user_id, question\n        )\n        speaker_2_memories, speaker_2_graph_memories, speaker_2_memory_time = self.search_memory(\n            speaker_2_user_id, question\n        )\n\n        search_1_memory = [f\"{item['timestamp']}: {item['memory']}\" for item in speaker_1_memories]\n        search_2_memory = [f\"{item['timestamp']}: {item['memory']}\" for item in speaker_2_memories]\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py_113_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'os.getenv('MODEL')' is used without version pinning on line 113. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
      "line_number": 113,
      "code_snippet": "\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )\n        t2 = time.time()",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py_90_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'answer_question'",
      "description": "Function 'answer_question' on line 90 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
      "line_number": 90,
      "code_snippet": "        return semantic_memories, graph_memories, end_time - start_time\n\n    def answer_question(self, speaker_1_user_id, speaker_2_user_id, question, answer, category):\n        speaker_1_memories, speaker_1_graph_memories, speaker_1_memory_time = self.search_memory(\n            speaker_1_user_id, question\n        )",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py_113",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/memzero/search.py",
      "line_number": 113,
      "code_snippet": "        )\n\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )\n        t2 = time.time()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py_94",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'question' embedded in LLM prompt",
      "description": "User input 'question' flows to LLM call via call in variable 'answer_prompt'. Function 'answer_question' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
      "line_number": 94,
      "code_snippet": "        template = Template(ANSWER_PROMPT)\n        answer_prompt = template.render(memories=memories, question=question)\n\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py_97",
      "category": "LLM02: Insecure Output Handling",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "LLM output used in dangerous xss sink",
      "description": "LLM output from 'self.openai_client.chat.completions.create' is used in 'render' on line 97 without sanitization. This creates a xss vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
      "line_number": 97,
      "code_snippet": "        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )",
      "recommendation": "Mitigations for XSS:\n1. Use auto-escaping templates (Jinja2, Django templates)\n2. Apply HTML escaping: html.escape(response)\n3. Use Content Security Policy (CSP) headers\n4. Sanitize with bleach.clean() for rich text\n5. Never use innerHTML or dangerouslySetInnerHTML with LLM output"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py_97_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'os.getenv('MODEL')' is used without version pinning on line 97. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
      "line_number": 97,
      "code_snippet": "\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )\n        t2 = time.time()",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py_90_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'answer_question'",
      "description": "Function 'answer_question' on line 90 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
      "line_number": 90,
      "code_snippet": "        return result\n\n    def answer_question(self, idx, question):\n        memories, search_memory_time = self.search_memory(idx)\n\n        template = Template(ANSWER_PROMPT)",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py_97",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/evaluation/src/openai/predict.py",
      "line_number": 97,
      "code_snippet": "        answer_prompt = template.render(memories=memories, question=question)\n\n        t1 = time.time()\n        response = self.openai_client.chat.completions.create(\n            model=os.getenv(\"MODEL\"), messages=[{\"role\": \"system\", \"content\": answer_prompt}], temperature=0.0\n        )\n        t2 = time.time()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py_50",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'user_query' embedded in LLM prompt",
      "description": "User input 'user_query' flows to LLM call via assignment in variable 'prompt'. Function 'recommend_movie_with_memory' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py",
      "line_number": 50,
      "code_snippet": "\n    prompt = user_query\n    if past_memories:\n        prompt += f\"\\nPreviously, the user mentioned: {past_memories}\"\n\n    # Generate movie recommendation using Grok 3\n    response = grok_client.chat.completions.create(model=\"grok-3-beta\", messages=[{\"role\": \"user\", \"content\": prompt}])\n    recommendation = response.choices[0].message.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py_7",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py",
      "line_number": 7,
      "code_snippet": "\nIn order to run this file, you need to set up your Mem0 API at Mem0 platform and also need an XAI API key.\nexport XAI_API_KEY=\"your_xai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py_8",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/movie_recommendation_grok3.py",
      "line_number": 8,
      "code_snippet": "In order to run this file, you need to set up your Mem0 API at Mem0 platform and also need an XAI API key.\nexport XAI_API_KEY=\"your_xai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py_69",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'agent.run' is used in 'run(' on line 69 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
      "line_number": 69,
      "code_snippet": "        if image_path:\n            response = agent.run(prompt, images=[Image(filepath=Path(image_path))])\n        else:\n            response = agent.run(prompt)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py_71",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'agent.run' is used in 'run(' on line 71 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
      "line_number": 71,
      "code_snippet": "        else:\n            response = agent.run(prompt)\n        client.add(f\"User: {user_input}\\nAssistant: {response.content}\", user_id=user_id)\n        return response.content",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py_5",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 5. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
      "line_number": 5,
      "code_snippet": "\nIn order to run this file, you need to set up your Mem0 API at Mem0 platform and also need a OpenAI API key.\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py_6",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 6. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personal_assistant_agno.py",
      "line_number": 6,
      "code_snippet": "In order to run this file, you need to set up your Mem0 API at Mem0 platform and also need a OpenAI API key.\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/diet_assistant_voice_cartesia.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/diet_assistant_voice_cartesia.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Simple Voice Agent with Memory: Personal Food Assistant.\nA food assistant that remembers your dietary preferences and speaks recommendations\nPowered by Agno + Cartesia + Mem0\n\nexport MEM0_API_KEY=your_mem0_api_key\nexport OPENAI_API_KEY=your_openai_api_key\nexport CARTESIA_API_KEY=your_cartesia_api_key\n\"\"\"\n\nfrom textwrap import dedent",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
      "line_number": 12,
      "code_snippet": "To run this file, you need to set the following environment variables:\n\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\nexport ELEVENLABS_API_KEY=\"your_elevenlabs_api_key\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
      "line_number": 13,
      "code_snippet": "\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\nexport ELEVENLABS_API_KEY=\"your_elevenlabs_api_key\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
      "line_number": 14,
      "code_snippet": "export OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\nexport ELEVENLABS_API_KEY=\"your_elevenlabs_api_key\"\n\nYou must also have:",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/voice_assistant_elevenlabs.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nPersonal Voice Assistant with Memory (Whisper + CrewAI + Mem0 + ElevenLabs)\nThis script creates a personalized AI assistant that can:\n- Understand voice commands using Whisper (OpenAI STT)\n- Respond intelligently using CrewAI Agent and LLMs\n- Remember user preferences and facts using Mem0 memory\n- Speak responses back using ElevenLabs text-to-speech\nInitial user memory is bootstrapped from predefined preferences, and the assistant can remember new context dynamically over time.\n\nTo run this file, you need to set the following environment variables:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/test.py_74",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/test.py",
      "line_number": 74,
      "code_snippet": "        The agent's response\n    \"\"\"\n    # Run the triage agent (it will automatically handoffs when needed)\n    result = Runner.run_sync(triage_agent, user_input)\n\n    # Store the original conversation in memory\n    conversation = [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": result.final_output}]",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/test.py_62",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/test.py",
      "line_number": 62,
      "code_snippet": "    handoffs=[travel_agent, health_agent],\n    model=\"gpt-4.1-nano-2025-04-14\",\n)\n\n\ndef chat_with_handoffs(user_input: str, user_id: str) -> str:\n    \"\"\"\n    Handle user input with automatic agent handoffs and memory integration.\n\n    Args:\n        user_input: The user's message",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py_172",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py",
      "line_number": 172,
      "code_snippet": "        agent_executor = create_personalized_search_agent(user_context)\n\n        # Run the agent\n        response = agent_executor.invoke({\n            \"messages\": [HumanMessage(content=query)]\n        })\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py_38",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py",
      "line_number": 38,
      "code_snippet": "'''\n)\n\nllm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.2)\n\n\ndef setup_user_history(user_id):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py_155",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/personalized_search.py",
      "line_number": 155,
      "code_snippet": "    )\n\n    return agent_executor\n\n\ndef conduct_personalized_search(user_id, query):\n    \"\"\"\n    Personalized search workflow using LangChain agent + Tavily + Mem0\n\n    Returns search results with user personalization details\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/vllm_example.py_22",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 22. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/vllm_example.py",
      "line_number": 22,
      "code_snippet": "Optional environment variables:\n   export VLLM_BASE_URL=\"http://localhost:8000/v1\"\n   export VLLM_API_KEY=\"vllm-api-key\"\n\"\"\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/vllm_example.py_34",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 34. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/vllm_example.py",
      "line_number": 34,
      "code_snippet": "            \"model\": \"Qwen/Qwen2.5-32B-Instruct\",\n            \"vllm_base_url\": \"http://localhost:8000/v1\",\n            \"api_key\": \"vllm-api-key\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 100,",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/study_buddy.py_7",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/study_buddy.py",
      "line_number": 7,
      "code_snippet": "\nIn order to run this file, you need to set up your Mem0 API at Mem0 platform and also need a OpenAI API key.\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/study_buddy.py_8",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/study_buddy.py",
      "line_number": 8,
      "code_snippet": "In order to run this file, you need to set up your Mem0 API at Mem0 platform and also need a OpenAI API key.\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/multillm_memory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/multillm_memory.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nMulti-LLM Research Team with Shared Knowledge Base\n\nUse Case: AI Research Team where each model has different strengths:\n- GPT-4: Technical analysis and code review\n- Claude: Writing and documentation\n\nAll models share a common knowledge base, building on each other's work.\nExample: GPT-4 analyzes a tech stack \u2192 Claude writes documentation \u2192\nData analyst analyzes user data \u2192 All models can reference previous research.",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py_38",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "User input 'user_input' embedded in LLM prompt",
      "description": "User input 'user_input' flows to LLM call via f-string in variable 'prompt'. Function 'fitness_coach' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
      "line_number": 38,
      "code_snippet": "\n    prompt = f\"\"\"You are a fitness assistant who helps Anish with his training, recovery, and diet. You have long-term memory of his health, routines, preferences, and past conversations.\n\nUse your memory to personalize suggestions \u2014 consider his constraints, goals, patterns, and lifestyle when responding.\n\nHere is what you remember about {user_id}:\n{memory_context}\n\nUser query:\n{user_input}\"\"\"\n    response = agent.run(prompt)\n    memory_client.add(f\"User: {user_input}\\nAssistant: {response.content}\", user_id=user_id)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py_47",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'agent.run' is used in 'run(' on line 47 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
      "line_number": 47,
      "code_snippet": "{user_input}\"\"\"\n    response = agent.run(prompt)\n    memory_client.add(f\"User: {user_input}\\nAssistant: {response.content}\", user_id=user_id)\n    return response.content",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py_6",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 6. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
      "line_number": 6,
      "code_snippet": "\nIn order to run this file, you need to set up your Mem0 API at Mem0 platform and also need an OpenAI API key.\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py_7",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/examples/misc/fitness_checker.py",
      "line_number": 7,
      "code_snippet": "In order to run this file, you need to set up your Mem0 API at Mem0 platform and also need an OpenAI API key.\nexport OPENAI_API_KEY=\"your_openai_api_key\"\nexport MEM0_API_KEY=\"your_mem0_api_key\"\n\"\"\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_90",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.update_history' is used in 'UPDATE' on line 90 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 90,
      "code_snippet": "        # Fetch the history from the database if exists\n        self.llm.update_history(app_id=self.config.id)\n\n        # Attributes that aren't subclass related.",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_618",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.update_history' is used in 'UPDATE' on line 618 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 618,
      "code_snippet": "        # Update the history beforehand so that we can handle multiple chat sessions in the same python session\n        self.llm.update_history(app_id=self.config.id, session_id=session_id)\n\n        if self.cache_config is not None:",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_755",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.memory.get' is used in 'DELETE' on line 755 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 755,
      "code_snippet": "    ):\n        history = self.llm.memory.get(\n            app_id=self.config.id,\n            session_id=session_id,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_765",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.memory.delete' is used in 'DELETE' on line 765 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 765,
      "code_snippet": "    def delete_session_chat_history(self, session_id: str = \"default\"):\n        self.llm.memory.delete(app_id=self.config.id, session_id=session_id)\n        self.llm.update_history(app_id=self.config.id)\n",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_766",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.llm.update_history' is used in 'DELETE' on line 766 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 766,
      "code_snippet": "        self.llm.memory.delete(app_id=self.config.id, session_id=session_id)\n        self.llm.update_history(app_id=self.config.id)\n\n    def delete_all_chat_history(self, app_id: str):",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_748_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'get_history'",
      "description": "Function 'get_history' on line 748 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 748,
      "code_snippet": "            self.telemetry.capture(event_name=\"reset\", properties=self._telemetry_props)\n\n    def get_history(\n        self,\n        num_rounds: int = 10,\n        display_format: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_764_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write operation without confirmation in 'delete_session_chat_history'",
      "description": "Function 'delete_session_chat_history' on line 764 performs high-risk delete/write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 764,
      "code_snippet": "        return history\n\n    def delete_session_chat_history(self, session_id: str = \"default\"):\n        self.llm.memory.delete(app_id=self.config.id, session_id=session_id)\n        self.llm.update_history(app_id=self.config.id)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_768_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write operation without confirmation in 'delete_all_chat_history'",
      "description": "Function 'delete_all_chat_history' on line 768 performs high-risk delete/write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 768,
      "code_snippet": "        self.llm.update_history(app_id=self.config.id)\n\n    def delete_all_chat_history(self, app_id: str):\n        self.llm.memory.delete(app_id=app_id)\n        self.llm.update_history(app_id=app_id)\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_39_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 39 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 39,
      "code_snippet": "\nclass EmbedChain(JSONSerializable):\n    def __init__(\n        self,\n        config: BaseAppConfig,\n        llm: BaseLlm,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_764_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_session_chat_history'",
      "description": "Function 'delete_session_chat_history' on line 764 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 764,
      "code_snippet": "        return history\n\n    def delete_session_chat_history(self, session_id: str = \"default\"):\n        self.llm.memory.delete(app_id=self.config.id, session_id=session_id)\n        self.llm.update_history(app_id=self.config.id)\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_768_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'delete_all_chat_history'",
      "description": "Function 'delete_all_chat_history' on line 768 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 768,
      "code_snippet": "        self.llm.update_history(app_id=self.config.id)\n\n    def delete_all_chat_history(self, app_id: str):\n        self.llm.memory.delete(app_id=app_id)\n        self.llm.update_history(app_id=app_id)\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_90",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 90,
      "code_snippet": "            self.llm.config.system_prompt = system_prompt\n\n        # Fetch the history from the database if exists\n        self.llm.update_history(app_id=self.config.id)\n\n        # Attributes that aren't subclass related.\n        self.user_asks = []",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_658",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 658,
      "code_snippet": "            self.mem0_memory.add(data=answer, agent_id=self.config.id, user_id=session_id)\n\n        # add conversation in memory\n        self.llm.add_history(self.config.id, input_query, answer, session_id=session_id)\n\n        # Send anonymous telemetry\n        if self.config.collect_metrics:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_539",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 539,
      "code_snippet": "            )\n        else:\n            if self.llm.config.token_usage:\n                answer, token_info = self.llm.query(\n                    input_query=input_query, contexts=contexts_data_for_llm_query, config=config, dry_run=dry_run\n                )\n            else:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_543",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 543,
      "code_snippet": "                    input_query=input_query, contexts=contexts_data_for_llm_query, config=config, dry_run=dry_run\n                )\n            else:\n                answer = self.llm.query(\n                    input_query=input_query, contexts=contexts_data_for_llm_query, config=config, dry_run=dry_run\n                )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_636",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 636,
      "code_snippet": "        else:\n            logger.debug(\"Cache disabled. Running chat without cache.\")\n            if self.llm.config.token_usage:\n                answer, token_info = self.llm.query(\n                    input_query=input_query,\n                    contexts=contexts_data_for_llm_query,\n                    config=config,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py_644",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedchain.py",
      "line_number": 644,
      "code_snippet": "                    memories=memories,\n                )\n            else:\n                answer = self.llm.query(\n                    input_query=input_query,\n                    contexts=contexts_data_for_llm_query,\n                    config=config,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/client.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nimport uuid\n\nimport requests\n\nfrom embedchain.constants import CONFIG_DIR, CONFIG_FILE\n\nlogger = logging.getLogger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/constants.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom pathlib import Path\n\nABS_PATH = os.getcwd()\nHOME_DIR = os.environ.get(\"EMBEDCHAIN_CONFIG_DIR\", str(Path.home()))\nCONFIG_DIR = os.path.join(HOME_DIR, \".embedchain\")\nCONFIG_FILE = os.path.join(CONFIG_DIR, \"config.json\")\nSQLITE_PATH = os.path.join(CONFIG_DIR, \"embedchain.db\")\n\n# Set the environment variable for the database URI",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/factory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/factory.py",
      "line_number": 1,
      "code_snippet": "import importlib\n\n\ndef load_class(class_type):\n    module_path, class_name = class_type.rsplit(\".\", 1)\n    module = importlib.import_module(module_path)\n    return getattr(module, class_name)\n\n\nclass LlmFactory:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_111",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 111 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 111,
      "code_snippet": "    if docker:\n        subprocess.run([\"docker-compose\", \"build\"], check=True)\n    else:\n        ctx.invoke(install_reqs)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_113",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'ctx.invoke' is used in 'run(' on line 113 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 113,
      "code_snippet": "    else:\n        ctx.invoke(install_reqs)\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_122",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 122 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 122,
      "code_snippet": "        os.chdir(\"api\")\n        subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True)\n        os.chdir(\"..\")\n        console.print(\"\\n \u2705 [bold green]Installed API requirements successfully.[/bold green]\\n\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_132",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 132 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 132,
      "code_snippet": "        os.chdir(\"ui\")\n        subprocess.run([\"yarn\"], check=True)\n        console.print(\"\\n\u2705 [bold green]Successfully installed frontend requirements.[/bold green]\")\n        anonymous_telemetry.capture(event_name=\"ec_install_reqs\", properties={\"success\": True})",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_144",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 144 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 144,
      "code_snippet": "    if docker:\n        subprocess.run([\"docker-compose\", \"up\"], check=True)\n        return\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_168",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 168 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 168,
      "code_snippet": "        os.chdir(\"ui\")\n        subprocess.run([\"yarn\"], check=True)\n        ui_process = subprocess.Popen([\"yarn\", \"dev\"])\n        console.print(\"\u2705 [bold green]UI server started successfully.[/bold green]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_229",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 229 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 229,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(uvicorn_command)}[/bold cyan]\")\n        subprocess.run(uvicorn_command, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_240",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 240 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 240,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")\n        subprocess.run(modal_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_251",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 251 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 251,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running Streamlit app with command: {' '.join(streamlit_run_cmd)}[/bold cyan]\")\n        subprocess.run(streamlit_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_268",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 268 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 268,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(uvicorn_command)}[/bold cyan]\")\n        subprocess.run(uvicorn_command, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_279",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 279 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 279,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running Gradio app with command: {' '.join(gradio_run_cmd)}[/bold cyan]\")\n        subprocess.run(gradio_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_111_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [76, 82]) and executes code (lines [111, 122, 132]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 111,
      "code_snippet": "        anonymous_telemetry.capture(event_name=\"ec_create_app\", properties={\"success\": False})\n        return\n\n    if docker:\n        subprocess.run([\"docker-compose\", \"build\"], check=True)\n    else:\n        ctx.invoke(install_reqs)\n\n\n@cli.command()",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_62_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'create_app'",
      "description": "Function 'create_app' on line 62 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 62,
      "code_snippet": "@cli.command()\n@click.argument(\"app_name\")\n@click.option(\"--docker\", is_flag=True, help=\"Use docker to create the app.\")\n@click.pass_context\ndef create_app(ctx, app_name, docker):\n    if Path(app_name).exists():\n        console.print(\n            f\"\u274c [red]Directory '{app_name}' already exists. Try using a new directory name, or remove it.[/red]\"\n        )\n        return",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits\n\nAPI Access Control:\n1. Implement strict URL allowlists for permitted endpoints\n2. Validate and sanitize all URLs before making requests\n3. Use separate API keys with minimal permissions\n4. Implement rate limiting on outbound requests\n5. Log all API calls with full context\n6. Block private/internal IP ranges (SSRF prevention)\n7. Validate response content before processing\n8. Set timeouts on API calls\n9. Use circuit breakers for failing endpoints\n10. Monitor for unusual access patterns"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_117_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'install_reqs'",
      "description": "Function 'install_reqs' on line 117 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 117,
      "code_snippet": "        ctx.invoke(install_reqs)\n\n\n@cli.command()\ndef install_reqs():\n    try:\n        console.print(\"Installing python requirements...\\n\")\n        time.sleep(2)\n        os.chdir(\"api\")\n        subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_142_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'start'",
      "description": "Function 'start' on line 142 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 142,
      "code_snippet": "\n\n@cli.command()\n@click.option(\"--docker\", is_flag=True, help=\"Run inside docker.\")\ndef start(docker):\n    if docker:\n        subprocess.run([\"docker-compose\", \"up\"], check=True)\n        return\n\n    # Set up signal handling",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_219_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_fly_io'",
      "description": "Function 'run_dev_fly_io' on line 219 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 219,
      "code_snippet": "            f\"\ud83c\udf89 [green]All done! Successfully created `embedchain.json` with '{template}' as provider.[/green]\"\n        )\n\n\ndef run_dev_fly_io(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:\n        uvicorn_command.append(\"--reload\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_236_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_modal_com'",
      "description": "Function 'run_dev_modal_com' on line 236 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 236,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]FastAPI server stopped[/bold yellow]\")\n\n\ndef run_dev_modal_com():\n    modal_run_cmd = [\"modal\", \"serve\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")\n        subprocess.run(modal_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_247_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_streamlit_io'",
      "description": "Function 'run_dev_streamlit_io' on line 247 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 247,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]FastAPI server stopped[/bold yellow]\")\n\n\ndef run_dev_streamlit_io():\n    streamlit_run_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Streamlit app with command: {' '.join(streamlit_run_cmd)}[/bold cyan]\")\n        subprocess.run(streamlit_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_258_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_render_com'",
      "description": "Function 'run_dev_render_com' on line 258 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 258,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]Streamlit server stopped[/bold yellow]\")\n\n\ndef run_dev_render_com(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:\n        uvicorn_command.append(\"--reload\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_275_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_gradio'",
      "description": "Function 'run_dev_gradio' on line 275 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 275,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]FastAPI server stopped[/bold yellow]\")\n\n\ndef run_dev_gradio():\n    gradio_run_cmd = [\"gradio\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Gradio app with command: {' '.join(gradio_run_cmd)}[/bold cyan]\")\n        subprocess.run(gradio_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_62_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'create_app'",
      "description": "Function 'create_app' on line 62 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 62,
      "code_snippet": "@click.option(\"--docker\", is_flag=True, help=\"Use docker to create the app.\")\n@click.pass_context\ndef create_app(ctx, app_name, docker):\n    if Path(app_name).exists():\n        console.print(\n            f\"\u274c [red]Directory '{app_name}' already exists. Try using a new directory name, or remove it.[/red]\"",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_117_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'install_reqs'",
      "description": "Function 'install_reqs' on line 117 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 117,
      "code_snippet": "\n@cli.command()\ndef install_reqs():\n    try:\n        console.print(\"Installing python requirements...\\n\")\n        time.sleep(2)",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_142_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'start'",
      "description": "Function 'start' on line 142 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 142,
      "code_snippet": "@cli.command()\n@click.option(\"--docker\", is_flag=True, help=\"Run inside docker.\")\ndef start(docker):\n    if docker:\n        subprocess.run([\"docker-compose\", \"up\"], check=True)\n        return",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_219_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_fly_io'",
      "description": "Function 'run_dev_fly_io' on line 219 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 219,
      "code_snippet": "\n\ndef run_dev_fly_io(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_236_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_modal_com'",
      "description": "Function 'run_dev_modal_com' on line 236 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 236,
      "code_snippet": "\n\ndef run_dev_modal_com():\n    modal_run_cmd = [\"modal\", \"serve\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_247_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_streamlit_io'",
      "description": "Function 'run_dev_streamlit_io' on line 247 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 247,
      "code_snippet": "\n\ndef run_dev_streamlit_io():\n    streamlit_run_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Streamlit app with command: {' '.join(streamlit_run_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_258_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_render_com'",
      "description": "Function 'run_dev_render_com' on line 258 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 258,
      "code_snippet": "\n\ndef run_dev_render_com(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_275_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_gradio'",
      "description": "Function 'run_dev_gradio' on line 275 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 275,
      "code_snippet": "\n\ndef run_dev_gradio():\n    gradio_run_cmd = [\"gradio\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Gradio app with command: {' '.join(gradio_run_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_62_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'create_app'",
      "description": "API endpoint 'create_app' on line 62 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 62,
      "code_snippet": "@click.option(\"--docker\", is_flag=True, help=\"Use docker to create the app.\")\n@click.pass_context\ndef create_app(ctx, app_name, docker):\n    if Path(app_name).exists():\n        console.print(\n            f\"\u274c [red]Directory '{app_name}' already exists. Try using a new directory name, or remove it.[/red]\"",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_219_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'run_dev_fly_io'",
      "description": "API endpoint 'run_dev_fly_io' on line 219 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 219,
      "code_snippet": "\n\ndef run_dev_fly_io(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_236_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'run_dev_modal_com'",
      "description": "API endpoint 'run_dev_modal_com' on line 236 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 236,
      "code_snippet": "\n\ndef run_dev_modal_com():\n    modal_run_cmd = [\"modal\", \"serve\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_258_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'run_dev_render_com'",
      "description": "API endpoint 'run_dev_render_com' on line 258 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 258,
      "code_snippet": "\n\ndef run_dev_render_com(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/cli.py",
      "line_number": 1,
      "code_snippet": "import json\nimport os\nimport shutil\nimport signal\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport zipfile\nfrom pathlib import Path",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/app.py_14_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [191, 215, 235]) and executes code (lines [14, 15, 27]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/app.py",
      "line_number": 14,
      "code_snippet": "from tqdm import tqdm\n\nfrom embedchain.cache import (\n    Config,\n    ExactMatchEvaluation,\n    SearchDistanceEvaluation,\n    cache,\n    gptcache_data_manager,\n    gptcache_pre_function,\n)",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/app.py_288_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 288. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/app.py",
      "line_number": 288,
      "code_snippet": "        else:\n            if data_type == \"qna_pair\":\n                data_value = list(ast.literal_eval(data_value))\n            metadata = {}\n\n        try:",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/telegram_bot/telegram_bot.py_51",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/telegram_bot/telegram_bot.py",
      "line_number": 51,
      "code_snippet": "\ndef query_chat_bot(question):\n    try:\n        response = chat_bot.chat(question)\n        response_text = response\n    except Exception as e:\n        response_text = \"An error occurred. Please try again!\"",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/api_server/api_server.py_48",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'App().chat' is used in 'run(' on line 48 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/api_server/api_server.py",
      "line_number": 48,
      "code_snippet": "        try:\n            response = App().chat(question)\n            return jsonify({\"data\": response}), 200\n        except Exception:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py_69",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
      "line_number": 69,
      "code_snippet": "    with st.chat_message(role, avatar=assistant_avatar_url if role == \"assistant\" else None):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"Ask me anything!\"):\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py_66",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
      "line_number": 66,
      "code_snippet": "\nfor message in st.session_state.messages:\n    role = message[\"role\"]\n    with st.chat_message(role, avatar=assistant_avatar_url if role == \"assistant\" else None):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"Ask me anything!\"):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py_70",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
      "line_number": 70,
      "code_snippet": "        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"Ask me anything!\"):\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py_74",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential indirect prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Indirect prompt injection: Malicious instructions from external data sources\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Indirect injection allows attackers to embed malicious instructions in external data sources (websites, documents) that are processed by the LLM.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/unacademy-ai/app.py",
      "line_number": 74,
      "code_snippet": "        st.markdown(prompt)\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n    with st.chat_message(\"assistant\", avatar=assistant_avatar_url):\n        msg_placeholder = st.empty()\n        msg_placeholder.markdown(\"Thinking...\")\n        full_response = \"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Validate and sanitize all external data before including in prompts.\n5. Use data isolation - don't mix untrusted content with instructions.\n6. Consider using separate LLM calls for processing external data."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py_44",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py",
      "line_number": 44,
      "code_snippet": "\ndef query(message):\n    try:\n        response = chat_bot.chat(message)\n    except Exception:\n        response = \"An error occurred. Please try again!\"\n    return response",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py_44",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'chat_bot.chat' is used in 'run(' on line 44 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py",
      "line_number": 44,
      "code_snippet": "    try:\n        response = chat_bot.chat(message)\n    except Exception:\n        response = \"An error occurred. Please try again!\"",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py_42",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/whatsapp_bot/whatsapp_bot.py",
      "line_number": 42,
      "code_snippet": "    else:\n        response = \"Invalid 'add' command format.\\nUse: add <data_type> <url_or_text>\"\n    return response\n\n\ndef query(message):\n    try:\n        response = chat_bot.chat(message)\n    except Exception:\n        response = \"An error occurred. Please try again!\"\n    return response",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/models.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nfrom database import Base\nfrom pydantic import BaseModel, Field\nfrom sqlalchemy import Column, Integer, String\n\n\nclass QueryApp(BaseModel):\n    query: str = Field(\"\", description=\"The query that you want to ask the App.\")\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/database.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/database.py",
      "line_number": 1,
      "code_snippet": "from sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nSQLALCHEMY_DATABASE_URI = \"sqlite:///./app.db\"\n\nengine = create_engine(SQLALCHEMY_DATABASE_URI, connect_args={\"check_same_thread\": False})\n\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py_3",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 3. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
      "line_number": 3,
      "code_snippet": "def generate_error_message_for_api_keys(error: ValueError) -> str:\n    env_mapping = {\n        \"OPENAI_API_KEY\": \"OPENAI_API_KEY\",\n        \"OPENAI_API_TYPE\": \"OPENAI_API_TYPE\",\n        \"OPENAI_API_BASE\": \"OPENAI_API_BASE\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py_7",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 7. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
      "line_number": 7,
      "code_snippet": "        \"OPENAI_API_BASE\": \"OPENAI_API_BASE\",\n        \"OPENAI_API_VERSION\": \"OPENAI_API_VERSION\",\n        \"COHERE_API_KEY\": \"COHERE_API_KEY\",\n        \"TOGETHER_API_KEY\": \"TOGETHER_API_KEY\",\n        \"ANTHROPIC_API_KEY\": \"ANTHROPIC_API_KEY\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py_8",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 8. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
      "line_number": 8,
      "code_snippet": "        \"OPENAI_API_VERSION\": \"OPENAI_API_VERSION\",\n        \"COHERE_API_KEY\": \"COHERE_API_KEY\",\n        \"TOGETHER_API_KEY\": \"TOGETHER_API_KEY\",\n        \"ANTHROPIC_API_KEY\": \"ANTHROPIC_API_KEY\",\n        \"JINACHAT_API_KEY\": \"JINACHAT_API_KEY\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py_9",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 9. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
      "line_number": 9,
      "code_snippet": "        \"COHERE_API_KEY\": \"COHERE_API_KEY\",\n        \"TOGETHER_API_KEY\": \"TOGETHER_API_KEY\",\n        \"ANTHROPIC_API_KEY\": \"ANTHROPIC_API_KEY\",\n        \"JINACHAT_API_KEY\": \"JINACHAT_API_KEY\",\n        \"HUGGINGFACE_ACCESS_TOKEN\": \"HUGGINGFACE_ACCESS_TOKEN\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py_10",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 10. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/utils.py",
      "line_number": 10,
      "code_snippet": "        \"TOGETHER_API_KEY\": \"TOGETHER_API_KEY\",\n        \"ANTHROPIC_API_KEY\": \"ANTHROPIC_API_KEY\",\n        \"JINACHAT_API_KEY\": \"JINACHAT_API_KEY\",\n        \"HUGGINGFACE_ACCESS_TOKEN\": \"HUGGINGFACE_ACCESS_TOKEN\",\n        \"REPLICATE_API_TOKEN\": \"REPLICATE_API_TOKEN\",",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/rest-api/main.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\n\nimport aiofiles\nimport yaml\nfrom database import Base, SessionLocal, engine\nfrom fastapi import Depends, FastAPI, HTTPException, UploadFile\nfrom models import DefaultResponse, DeployAppRequest, QueryApp, SourceApp\nfrom services import get_app, get_apps, remove_app, save_app\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/models.py",
      "line_number": 1,
      "code_snippet": "from flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\n\n\nclass APIKey(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    key = db.Column(db.String(255), nullable=False)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/dashboard.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/dashboard.py",
      "line_number": 1,
      "code_snippet": "from flask import Blueprint, jsonify, make_response, request\nfrom models import APIKey, BotList, db\n\ndashboard_bp = Blueprint(\"dashboard\", __name__)\n\n\n# Set Open AI Key\n@dashboard_bp.route(\"/api/set_key\", methods=[\"POST\"])\ndef set_key():\n    data = request.get_json()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/sources.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/sources.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom flask import Blueprint, jsonify, make_response, request\nfrom models import APIKey\nfrom paths import DB_DIRECTORY_OPEN_AI\n\nfrom embedchain import App\n\nsources_bp = Blueprint(\"sources\", __name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/chat_response.py_28",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/full_stack/backend/routes/chat_response.py",
      "line_number": 28,
      "code_snippet": "            if app_type == \"app\":\n                chat_bot = App()\n\n        response = chat_bot.chat(query)\n        return make_response(jsonify({\"response\": response}), 200)\n\n    except Exception as e:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/ec_app/app.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/ec_app/app.py",
      "line_number": 1,
      "code_snippet": "from dotenv import load_dotenv\nfrom fastapi import FastAPI, responses\nfrom pydantic import BaseModel\n\nfrom embedchain import App\n\nload_dotenv(\".env\")\n\napp = FastAPI(title=\"Embedchain FastAPI App\")\nembedchain_app = App()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/nextjs_discord/app.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/nextjs_discord/app.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\n\nimport discord\nimport dotenv\nimport requests\n\ndotenv.load_dotenv(\".env\")\n\nintents = discord.Intents.default()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/nextjs_slack/app.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/examples/nextjs/nextjs_slack/app.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport re\n\nimport requests\nfrom dotenv import load_dotenv\nfrom slack_bolt import App as SlackApp\nfrom slack_bolt.adapter.socket_mode import SocketModeHandler\n\nload_dotenv(\".env\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py_69",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'app.run' is used in 'run(' on line 69 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py",
      "line_number": 69,
      "code_snippet": "\n        app.run(host=host, port=port, debug=debug)\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py_52_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'start'",
      "description": "Function 'start' on line 52 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py",
      "line_number": 52,
      "code_snippet": "        return response\n\n    def start(self, host=\"0.0.0.0\", port=5000, debug=True):\n        app = self.flask.Flask(__name__)\n\n        def signal_handler(sig, frame):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py_52_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'start'",
      "description": "API endpoint 'start' on line 52 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py",
      "line_number": 52,
      "code_snippet": "        return response\n\n    def start(self, host=\"0.0.0.0\", port=5000, debug=True):\n        app = self.flask.Flask(__name__)\n\n        def signal_handler(sig, frame):",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py_69",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/whatsapp.py",
      "line_number": 69,
      "code_snippet": "            twilio_response.message(response)\n            return str(twilio_response)\n\n        app.run(host=host, port=port, debug=debug)\n\n\ndef start_command():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py_73",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'ask_bot' on line 73 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py",
      "line_number": 73,
      "code_snippet": "    def ask_bot(self, message, history: list[str]):\n        try:\n            self.app.llm.set_history(history=history)\n            response = self.query(message)\n        except Exception:\n            logging.exception(f\"Failed to query {message}.\")\n            response = \"An error occurred. Please try again!\"\n        return response\n\n    def start(self):\n        start_command()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py_75",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py",
      "line_number": 75,
      "code_snippet": "\n    def ask_bot(self, message, history: list[str]):\n        try:\n            self.app.llm.set_history(history=history)\n            response = self.query(message)\n        except Exception:\n            logging.exception(f\"Failed to query {message}.\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py_73",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/poe.py",
      "line_number": 73,
      "code_snippet": "    #     except Exception:\n    #         logging.exception(f\"Failed to add data {data}.\")\n    #         response = \"Some error occurred while adding data.\"\n    #     return response\n\n    def ask_bot(self, message, history: list[str]):\n        try:\n            self.app.llm.set_history(history=history)\n            response = self.query(message)\n        except Exception:\n            logging.exception(f\"Failed to query {message}.\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/discord.py_54",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'client.run' is used in 'run(' on line 54 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/discord.py",
      "line_number": 54,
      "code_snippet": "    def start(self):\n        client.run(os.environ[\"DISCORD_BOT_TOKEN\"])\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/discord.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/discord.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport logging\nimport os\n\nfrom embedchain.helpers.json_serializable import register_deserializable\n\nfrom .base import BaseBot\n\ntry:\n    import discord",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_66",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM API call 'self.client.chat_postMessage'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 66,
      "code_snippet": "    def send_slack_message(self, channel, message):\n        response = self.client.chat_postMessage(channel=channel, text=message)\n        return response",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_87",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'app.run' is used in 'run(' on line 87 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 87,
      "code_snippet": "\n        app.run(host=host, port=port, debug=debug)\n\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_65",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'send_slack_message' on line 65 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 65,
      "code_snippet": "    def send_slack_message(self, channel, message):\n        response = self.client.chat_postMessage(channel=channel, text=message)\n        return response\n\n    def start(self, host=\"0.0.0.0\", port=5000, debug=True):\n        app = Flask(__name__)\n\n        def signal_handler(sig, frame):\n            logger.info(\"\\nGracefully shutting down the SlackBot...\")\n            sys.exit(0)\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_35_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'handle_message'",
      "description": "Function 'handle_message' on line 35 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 35,
      "code_snippet": "        super().__init__()\n\n    def handle_message(self, event_data):\n        message = event_data.get(\"event\")\n        if message and \"text\" in message and message.get(\"subtype\") != \"bot_message\":\n            text: str = message[\"text\"]",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_65_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'send_slack_message'",
      "description": "Function 'send_slack_message' on line 65 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 65,
      "code_snippet": "                        logger.error(\"Error occurred during 'add' command:\", e)\n\n    def send_slack_message(self, channel, message):\n        response = self.client.chat_postMessage(channel=channel, text=message)\n        return response\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_69_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'start'",
      "description": "Function 'start' on line 69 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 69,
      "code_snippet": "        return response\n\n    def start(self, host=\"0.0.0.0\", port=5000, debug=True):\n        app = Flask(__name__)\n\n        def signal_handler(sig, frame):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_65_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'send_slack_message'",
      "description": "Function 'send_slack_message' on line 65 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 65,
      "code_snippet": "                        logger.error(\"Error occurred during 'add' command:\", e)\n\n    def send_slack_message(self, channel, message):\n        response = self.client.chat_postMessage(channel=channel, text=message)\n        return response\n",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_69_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'start'",
      "description": "API endpoint 'start' on line 69 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 69,
      "code_snippet": "        return response\n\n    def start(self, host=\"0.0.0.0\", port=5000, debug=True):\n        app = Flask(__name__)\n\n        def signal_handler(sig, frame):",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_87",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 87,
      "code_snippet": "            response = self.handle_message(request.json)\n            return str(response)\n\n        app.run(host=host, port=port, debug=debug)\n\n\ndef start_command():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_45",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 45,
      "code_snippet": "                if text.startswith(\"query\"):\n                    _, question = text.split(\" \", 1)\n                    try:\n                        response = self.chat_bot.chat(question)\n                        self.send_slack_message(message[\"channel\"], response)\n                        logger.info(\"Query answered successfully!\")\n                    except Exception as e:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_56",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 56,
      "code_snippet": "                    if url_or_text.startswith(\"<\") and url_or_text.endswith(\">\"):\n                        url_or_text = url_or_text[1:-1]\n                    try:\n                        self.chat_bot.add(url_or_text, data_type)\n                        self.send_slack_message(message[\"channel\"], f\"Added {data_type} : {url_or_text}\")\n                    except ValueError as e:\n                        self.send_slack_message(message[\"channel\"], f\"Error: {str(e)}\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py_65",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/bots/slack.py",
      "line_number": 65,
      "code_snippet": "                        logger.error(\"Error occurred during 'add' command:\", e)\n                    except Exception as e:\n                        self.send_slack_message(message[\"channel\"], f\"Failed to add {data_type} : {url_or_text}\")\n                        logger.error(\"Error occurred during 'add' command:\", e)\n\n    def send_slack_message(self, channel, message):\n        response = self.client.chat_postMessage(channel=channel, text=message)\n        return response\n\n    def start(self, host=\"0.0.0.0\", port=5000, debug=True):\n        app = Flask(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py_21_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 21 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
      "line_number": 21,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py_45_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 45 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
      "line_number": 45,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py_21_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 21 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
      "line_number": 21,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py_45_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 45 makes critical financial decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
      "line_number": 45,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/env.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\n\nfrom embedchain.core.db.models import Base\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/nvidia.py_65",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/nvidia.py",
      "line_number": 65,
      "code_snippet": "        if labels:\n            params[\"labels\"] = labels\n        llm = ChatNVIDIA(**params, callback_manager=CallbackManager(callback_manager))\n        chat_response = llm.invoke(prompt) if labels is None else llm.invoke(prompt, labels=labels)\n        if config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"token_usage\"]\n        return chat_response.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vllm.py_40",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vllm.py",
      "line_number": 40,
      "code_snippet": "        llm = BaseVLLM(**llm_args)\n        return llm.invoke(prompt)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vllm.py_40",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'llm.invoke' is used in 'UPDATE' on line 40 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vllm.py",
      "line_number": 40,
      "code_snippet": "        llm = BaseVLLM(**llm_args)\n        return llm.invoke(prompt)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/jina.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/jina.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Optional\n\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom langchain_community.chat_models import JinaChat\n\nfrom embedchain.config import BaseLlmConfig\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.llm.base import BaseLlm\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py_52",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'model.generate_content'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py",
      "line_number": 52,
      "code_snippet": "\n        response = model.generate_content(\n            prompt,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py_37_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'model_name' is used without version pinning on line 37. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py",
      "line_number": 37,
      "code_snippet": "        model_name = self.config.model or \"gemini-pro\"\n        logger.info(f\"Using Google LLM model: {model_name}\")\n        model = genai.GenerativeModel(model_name=model_name)\n\n        generation_config_params = {\n            \"candidate_count\": 1,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py_37",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py",
      "line_number": 37,
      "code_snippet": "    def _get_answer(self, prompt: str) -> Union[str, Generator[Any, Any, None]]:\n        model_name = self.config.model or \"gemini-pro\"\n        logger.info(f\"Using Google LLM model: {model_name}\")\n        model = genai.GenerativeModel(model_name=model_name)\n\n        generation_config_params = {\n            \"candidate_count\": 1,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py_34",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/google.py",
      "line_number": 34,
      "code_snippet": "        if self.config.system_prompt:\n            raise ValueError(\"GoogleLlm does not support `system_prompt`\")\n        response = self._get_answer(prompt)\n        return response\n\n    def _get_answer(self, prompt: str) -> Union[str, Generator[Any, Any, None]]:\n        model_name = self.config.model or \"gemini-pro\"\n        logger.info(f\"Using Google LLM model: {model_name}\")\n        model = genai.GenerativeModel(model_name=model_name)\n\n        generation_config_params = {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/gpt4all.py_44",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/gpt4all.py",
      "line_number": 44,
      "code_snippet": "            else:\n                raise ValueError(f\"Model does not exist at {model_path=}\")\n        else:\n            return LangchainGPT4All(model=model, allow_download=True)\n\n    def _get_answer(self, prompt: str, config: BaseLlmConfig) -> Union[str, Iterable]:\n        if config.model and config.model != self.config.model:\n            raise RuntimeError(\n                \"GPT4ALLLlm does not support switching models at runtime. Please create a new app instance.\"\n            )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/aws_bedrock.py_57",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/aws_bedrock.py",
      "line_number": 57,
      "code_snippet": "\n        return llm.invoke(prompt)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/groq.py_64",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/groq.py",
      "line_number": 64,
      "code_snippet": "        else:\n            chat = ChatGroq(**kwargs)\n\n        chat_response = chat.invoke(prompt)\n        if self.config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"token_usage\"]\n        return chat_response.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/groq.py_47",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/groq.py",
      "line_number": 47,
      "code_snippet": "                \"cost_currency\": \"USD\",\n            }\n            return response, response_token_info\n        return self._get_answer(prompt, self.config)\n\n    def _get_answer(self, prompt: str, config: BaseLlmConfig) -> str:\n        messages = []\n        if config.system_prompt:\n            messages.append(SystemMessage(content=config.system_prompt))\n        messages.append(HumanMessage(content=prompt))\n        api_key = config.api_key or os.environ[\"GROQ_API_KEY\"]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_118",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 118,
      "code_snippet": "        openai_tools = [convert_to_openai_tool(tools)]\n        chat = chat.bind(tools=openai_tools).pipe(JsonOutputToolsParser())\n        try:\n            return json.dumps(chat.invoke(messages)[0])\n        except IndexError:\n            return \"Input could not be mapped to the function!\"",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_99",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'chat' flows to 'self._query_function_call' on line 99 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 99,
      "code_snippet": "        if self.tools:\n            return self._query_function_call(chat, self.tools, messages)\n\n        chat_response = chat.invoke(messages)",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_106",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_query_function_call' on line 106 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 106,
      "code_snippet": "    def _query_function_call(\n        self,\n        chat: ChatOpenAI,\n        tools: Optional[Union[Dict[str, Any], Type[BaseModel], Callable[..., Any], BaseTool]],\n        messages: list[BaseMessage],\n    ) -> str:\n        from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n        from langchain_core.utils.function_calling import convert_to_openai_tool\n\n        openai_tools = [convert_to_openai_tool(tools)]\n        chat = chat.bind(tools=openai_tools).pipe(JsonOutputToolsParser())",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_106_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_query_function_call'",
      "description": "Function '_query_function_call' on line 106 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 106,
      "code_snippet": "        return chat_response.content\n\n    def _query_function_call(\n        self,\n        chat: ChatOpenAI,\n        tools: Optional[Union[Dict[str, Any], Type[BaseModel], Callable[..., Any], BaseTool]],",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_50_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_answer'",
      "description": "Function '_get_answer' on line 50 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 50,
      "code_snippet": "        return self._get_answer(prompt, self.config)\n\n    def _get_answer(self, prompt: str, config: BaseLlmConfig) -> str:\n        messages = []\n        if config.system_prompt:\n            messages.append(SystemMessage(content=config.system_prompt))",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_101",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 101,
      "code_snippet": "        if self.tools:\n            return self._query_function_call(chat, self.tools, messages)\n\n        chat_response = chat.invoke(messages)\n        if self.config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"token_usage\"]\n        return chat_response.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py_50",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/openai.py",
      "line_number": 50,
      "code_snippet": "            }\n            return response, response_token_info\n\n        return self._get_answer(prompt, self.config)\n\n    def _get_answer(self, prompt: str, config: BaseLlmConfig) -> str:\n        messages = []\n        if config.system_prompt:\n            messages.append(SystemMessage(content=config.system_prompt))\n        messages.append(HumanMessage(content=prompt))\n        kwargs = {",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/azure_openai.py_40",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input 'prompt' flows to LLM call via call in variable 'messages'. Function '_get_answer' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/azure_openai.py",
      "line_number": 40,
      "code_snippet": "\n        messages = BaseLlm._get_messages(prompt, system_prompt=config.system_prompt)\n\n        return chat.invoke(messages).content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/azure_openai.py_26_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'config.model or 'gpt-4o-mini'' is used without version pinning on line 26. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/azure_openai.py",
      "line_number": 26,
      "code_snippet": "            raise ValueError(\"Deployment name must be provided for Azure OpenAI\")\n\n        chat = AzureChatOpenAI(\n            deployment_name=config.deployment_name,\n            openai_api_version=str(config.api_version) if config.api_version else \"2024-02-01\",\n            model_name=config.model or \"gpt-4o-mini\",",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py_64",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input 'prompt' flows to LLM call via call in variable 'messages'. Function '_get_answer' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
      "line_number": 64,
      "code_snippet": "\n        messages = VertexAILlm._get_messages(prompt)\n        chat_response = llm.invoke(messages)\n        if config.token_usage:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py_62_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'config.model' is used without version pinning on line 62. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
      "line_number": 62,
      "code_snippet": "            )\n        else:\n            llm = ChatVertexAI(temperature=config.temperature, model=config.model)\n\n        messages = VertexAILlm._get_messages(prompt)\n        chat_response = llm.invoke(messages)",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py_65",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
      "line_number": 65,
      "code_snippet": "            llm = ChatVertexAI(temperature=config.temperature, model=config.model)\n\n        messages = VertexAILlm._get_messages(prompt)\n        chat_response = llm.invoke(messages)\n        if config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"usage_metadata\"]\n        return chat_response.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py_62",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/vertex_ai.py",
      "line_number": 62,
      "code_snippet": "                temperature=config.temperature, model=config.model, callbacks=callbacks, streaming=config.stream\n            )\n        else:\n            llm = ChatVertexAI(temperature=config.temperature, model=config.model)\n\n        messages = VertexAILlm._get_messages(prompt)\n        chat_response = llm.invoke(messages)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_69",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 69,
      "code_snippet": "            repo_id=model,\n            model_kwargs=model_kwargs,\n        )\n        return llm.invoke(prompt)\n\n    @staticmethod\n    def _from_endpoint(prompt: str, config: BaseLlmConfig) -> str:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_80",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 80,
      "code_snippet": "            task=\"text-generation\",\n            model_kwargs=config.model_kwargs,\n        )\n        return llm.invoke(prompt)\n\n    @staticmethod\n    def _from_pipeline(prompt: str, config: BaseLlmConfig) -> str:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_99",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 99,
      "code_snippet": "            task=\"text-generation\",\n            pipeline_kwargs=model_kwargs,\n        )\n        return llm.invoke(prompt)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_50_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_from_model'",
      "description": "Function '_from_model' on line 50 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 50,
      "code_snippet": "\n    @staticmethod\n    def _from_model(prompt: str, config: BaseLlmConfig) -> str:\n        model_kwargs = {\n            \"temperature\": config.temperature or 0.1,\n            \"max_new_tokens\": config.max_tokens,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_72_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_from_endpoint'",
      "description": "Function '_from_endpoint' on line 72 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 72,
      "code_snippet": "\n    @staticmethod\n    def _from_endpoint(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")\n        llm = HuggingFaceEndpoint(\n            huggingfacehub_api_token=api_key,",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_50",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 50,
      "code_snippet": "            return HuggingFaceLlm._from_endpoint(prompt=prompt, config=config)\n        else:\n            raise ValueError(\"Either `model` or `endpoint` must be set in config\")\n\n    @staticmethod\n    def _from_model(prompt: str, config: BaseLlmConfig) -> str:\n        model_kwargs = {\n            \"temperature\": config.temperature or 0.1,\n            \"max_new_tokens\": config.max_tokens,\n        }\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_72",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 72,
      "code_snippet": "            model_kwargs=model_kwargs,\n        )\n        return llm.invoke(prompt)\n\n    @staticmethod\n    def _from_endpoint(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\")\n        llm = HuggingFaceEndpoint(\n            huggingfacehub_api_token=api_key,\n            endpoint_url=config.endpoint,\n            task=\"text-generation\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py_83",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/huggingface.py",
      "line_number": 83,
      "code_snippet": "            model_kwargs=config.model_kwargs,\n        )\n        return llm.invoke(prompt)\n\n    @staticmethod\n    def _from_pipeline(prompt: str, config: BaseLlmConfig) -> str:\n        model_kwargs = {\n            \"temperature\": config.temperature or 0.1,\n            \"max_new_tokens\": config.max_tokens,\n        }\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py_54",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input 'prompt' flows to LLM call via call in variable 'messages'. Function '_get_answer' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
      "line_number": 54,
      "code_snippet": "\n        messages = BaseLlm._get_messages(prompt, system_prompt=config.system_prompt)\n\n        chat_response = chat.invoke(messages)\n        if config.token_usage:",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py_49_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'config.model' is used without version pinning on line 49. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
      "line_number": 49,
      "code_snippet": "    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n        chat = ChatAnthropic(anthropic_api_key=api_key, temperature=config.temperature, model_name=config.model)\n\n        if config.max_tokens and config.max_tokens != 1000:\n            logger.warning(\"Config option `max_tokens` is not supported by this model.\")",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py_49",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
      "line_number": 49,
      "code_snippet": "    @staticmethod\n    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n        chat = ChatAnthropic(anthropic_api_key=api_key, temperature=config.temperature, model_name=config.model)\n\n        if config.max_tokens and config.max_tokens != 1000:\n            logger.warning(\"Config option `max_tokens` is not supported by this model.\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py_56",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
      "line_number": 56,
      "code_snippet": "\n        messages = BaseLlm._get_messages(prompt, system_prompt=config.system_prompt)\n\n        chat_response = chat.invoke(messages)\n        if config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"token_usage\"]\n        return chat_response.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py_47",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/anthropic.py",
      "line_number": 47,
      "code_snippet": "            }\n            return response, response_token_info\n        return self._get_answer(prompt, self.config)\n\n    @staticmethod\n    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n        chat = ChatAnthropic(anthropic_api_key=api_key, temperature=config.temperature, model_name=config.model)\n\n        if config.max_tokens and config.max_tokens != 1000:\n            logger.warning(\"Config option `max_tokens` is not supported by this model.\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/together.py_68",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/together.py",
      "line_number": 68,
      "code_snippet": "        }\n\n        chat = ChatTogether(**kwargs)\n        chat_response = chat.invoke(prompt)\n        if config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"token_usage\"]\n        return chat_response.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/together.py_58",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/together.py",
      "line_number": 58,
      "code_snippet": "            }\n            return response, response_token_info\n        return self._get_answer(prompt, self.config)\n\n    @staticmethod\n    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.environ[\"TOGETHER_API_KEY\"]\n        kwargs = {\n            \"model_name\": config.model or \"mixtral-8x7b-32768\",\n            \"temperature\": config.temperature,\n            \"max_tokens\": config.max_tokens,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/mistralai.py_69",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/mistralai.py",
      "line_number": 69,
      "code_snippet": "                answer += chunk.content\n            return answer\n        else:\n            chat_response = client.invoke(**kwargs, input=messages)\n            if config.token_usage:\n                return chat_response.content, chat_response.response_metadata[\"token_usage\"]\n            return chat_response.content",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/mistralai.py_39",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/mistralai.py",
      "line_number": 39,
      "code_snippet": "            }\n            return response, response_token_info\n        return self._get_answer(prompt, self.config)\n\n    @staticmethod\n    def _get_answer(prompt: str, config: BaseLlmConfig):\n        try:\n            from langchain_core.messages import HumanMessage, SystemMessage\n            from langchain_mistralai.chat_models import ChatMistralAI\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/clarifai.py_38",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/clarifai.py",
      "line_number": 38,
      "code_snippet": "\n        try:\n            (params := {}) if config.model_kwargs is None else config.model_kwargs\n            predict_response = model.predict_by_bytes(\n                bytes(prompt, \"utf-8\"),\n                input_type=\"text\",\n                inference_params=params,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/clarifai.py_21",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/clarifai.py",
      "line_number": 21,
      "code_snippet": "\n    def get_llm_model_answer(self, prompt):\n        return self._get_answer(prompt=prompt, config=self.config)\n\n    @staticmethod\n    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:\n        try:\n            from clarifai.client.model import Model\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"The required dependencies for Clarifai are not installed.\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/cohere.py_63",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/cohere.py",
      "line_number": 63,
      "code_snippet": "        }\n\n        chat = ChatCohere(**kwargs)\n        chat_response = chat.invoke(prompt)\n        if config.token_usage:\n            return chat_response.content, chat_response.response_metadata[\"token_count\"]\n        return chat_response.content",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/cohere.py_53",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/cohere.py",
      "line_number": 53,
      "code_snippet": "            }\n            return response, response_token_info\n        return self._get_answer(prompt, self.config)\n\n    @staticmethod\n    def _get_answer(prompt: str, config: BaseLlmConfig) -> str:\n        api_key = config.api_key or os.environ[\"COHERE_API_KEY\"]\n        kwargs = {\n            \"model_name\": config.model or \"command-r\",\n            \"temperature\": config.temperature,\n            \"max_tokens\": config.max_tokens,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/llama2.py_53",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/llama2.py",
      "line_number": 53,
      "code_snippet": "        )\n        return llm.invoke(prompt)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/ollama.py_54",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'prompt' embedded in LLM prompt",
      "description": "User input parameter 'prompt' is directly passed to LLM API call 'llm.invoke'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/ollama.py",
      "line_number": 54,
      "code_snippet": "\n        return llm.invoke(prompt)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_251",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'contexts' embedded in LLM prompt",
      "description": "User input parameter 'contexts' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 251,
      "code_snippet": "            k[\"memories\"] = memories\n            prompt = self.generate_prompt(input_query, contexts, **k)\n            logger.info(f\"Prompt: {prompt}\")",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_314",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'contexts' embedded in LLM prompt",
      "description": "User input parameter 'contexts' is directly passed to LLM API call 'self.generate_prompt'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 314,
      "code_snippet": "\n            prompt = self.generate_prompt(input_query, contexts, **k)\n            logger.info(f\"Prompt: {prompt}\")",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_195",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'search.run' is used in 'run(' on line 195 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 195,
      "code_snippet": "        logger.info(f\"Access search to get answers for {input_query}\")\n        return search.run(input_query)\n\n    @staticmethod",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_178",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'access_search_and_get_results' on line 178 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 178,
      "code_snippet": "    def access_search_and_get_results(input_query: str):\n        \"\"\"\n        Search the internet for additional context\n\n        :param input_query: search query\n        :type input_query: str\n        :return: Search results\n        :rtype: Unknown\n        \"\"\"\n        try:\n            from langchain.tools import DuckDuckGoSearchRun",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_214",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 214 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 214,
      "code_snippet": "    def query(self, input_query: str, contexts: list[str], config: BaseLlmConfig = None, dry_run=False, memories=None):\n        \"\"\"\n        Queries the vector database based on the given input query.\n        Gets relevant doc based on the query and then passes it to an\n        LLM as context to get the answer.\n\n        :param input_query: The query to use.\n        :type input_query: str\n        :param contexts: Embeddings retrieved from the database to be used as context.\n        :type contexts: list[str]\n        :param config: The `BaseLlmConfig` instance to use as configuration options. This is used for one method call.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_274",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 274 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 274,
      "code_snippet": "    def chat(\n        self, input_query: str, contexts: list[str], config: BaseLlmConfig = None, dry_run=False, session_id: str = None\n    ):\n        \"\"\"\n        Queries the vector database on the given input query.\n        Gets relevant doc based on the query and then passes it to an\n        LLM as context to get the answer.\n\n        Maintains the whole conversation in memory.\n\n        :param input_query: The query to use.",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_178_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'access_search_and_get_results'",
      "description": "Function 'access_search_and_get_results' on line 178 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 178,
      "code_snippet": "\n    @staticmethod\n    def access_search_and_get_results(input_query: str):\n        \"\"\"\n        Search the internet for additional context\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_178_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'access_search_and_get_results'",
      "description": "Function 'access_search_and_get_results' on line 178 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 178,
      "code_snippet": "\n    @staticmethod\n    def access_search_and_get_results(input_query: str):\n        \"\"\"\n        Search the internet for additional context\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_214_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'query'",
      "description": "Function 'query' on line 214 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 214,
      "code_snippet": "            logger.info(f\"Token Info: {token_info}\")\n\n    def query(self, input_query: str, contexts: list[str], config: BaseLlmConfig = None, dry_run=False, memories=None):\n        \"\"\"\n        Queries the vector database based on the given input query.\n        Gets relevant doc based on the query and then passes it to an",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_274_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat'",
      "description": "Function 'chat' on line 274 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 274,
      "code_snippet": "                self.config: BaseLlmConfig = BaseLlmConfig.deserialize(prev_config)\n\n    def chat(\n        self, input_query: str, contexts: list[str], config: BaseLlmConfig = None, dry_run=False, session_id: str = None\n    ):\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py_195",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/llm/base.py",
      "line_number": 195,
      "code_snippet": "            ) from None\n        search = DuckDuckGoSearchRun()\n        logger.info(f\"Access search to get answers for {input_query}\")\n        return search.run(input_query)\n\n    @staticmethod\n    def _stream_response(answer: Any, token_info: Optional[dict[str, Any]] = None) -> Generator[Any, Any, None]:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/memory/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/memory/base.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport uuid\nfrom typing import Any, Optional\n\nfrom embedchain.core.db.database import get_session\nfrom embedchain.core.db.models import ChatHistory as ChatHistoryModel\nfrom embedchain.memory.message import ChatMessage\nfrom embedchain.memory.utils import merge_metadata_dict\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/chunkers/base_chunker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/chunkers/base_chunker.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nfrom typing import Any, Optional\n\nfrom embedchain.config.add_config import ChunkerConfig\nfrom embedchain.helpers.json_serializable import JSONSerializable\nfrom embedchain.models.data_type import DataType\n\nlogger = logging.getLogger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/opensearch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/opensearch.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nfrom typing import Any, Optional, Union\n\nfrom tqdm import tqdm\n\ntry:\n    from opensearchpy import OpenSearch\n    from opensearchpy.helpers import bulk\nexcept ImportError:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/qdrant.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/qdrant.py",
      "line_number": 1,
      "code_snippet": "import copy\nimport os\nfrom typing import Any, Optional, Union\n\ntry:\n    from qdrant_client import QdrantClient\n    from qdrant_client.http import models\n    from qdrant_client.http.models import Batch\n    from qdrant_client.models import Distance, VectorParams\nexcept ImportError:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/chroma.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/chroma.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Any, Optional, Union\n\nfrom chromadb import Collection, QueryResult\nfrom langchain.docstore.document import Document\nfrom tqdm import tqdm\n\nfrom embedchain.config import ChromaDbConfig\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.vectordb.base import BaseVectorDB",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/zilliz.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/vectordb/zilliz.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Any, Optional, Union\n\nfrom embedchain.config import ZillizDBConfig\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.vectordb.base import BaseVectorDB\n\ntry:\n    from pymilvus import (\n        Collection,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/data_formatter/data_formatter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/data_formatter/data_formatter.py",
      "line_number": 1,
      "code_snippet": "from importlib import import_module\nfrom typing import Any, Optional\n\nfrom embedchain.chunkers.base_chunker import BaseChunker\nfrom embedchain.config import AddConfig\nfrom embedchain.config.add_config import ChunkerConfig, LoaderConfig\nfrom embedchain.helpers.json_serializable import JSONSerializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.models.data_type import DataType\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/nvidia.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/nvidia.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom typing import Optional\n\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n\nfrom embedchain.config import BaseEmbedderConfig\nfrom embedchain.embedder.base import BaseEmbedder\nfrom embedchain.models import VectorDimensions\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/google.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/google.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional, Union\n\nimport google.generativeai as genai\nfrom chromadb import EmbeddingFunction, Embeddings\n\nfrom embedchain.config.embedder.google import GoogleAIEmbedderConfig\nfrom embedchain.embedder.base import BaseEmbedder\nfrom embedchain.models import VectorDimensions\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/gpt4all.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/gpt4all.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nfrom embedchain.config import BaseEmbedderConfig\nfrom embedchain.embedder.base import BaseEmbedder\nfrom embedchain.models import VectorDimensions\n\n\nclass GPT4AllEmbedder(BaseEmbedder):\n    def __init__(self, config: Optional[BaseEmbedderConfig] = None):\n        super().__init__(config=config)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/openai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/openai.py",
      "line_number": 1,
      "code_snippet": "import os\nimport warnings\nfrom typing import Optional\n\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\nfrom embedchain.config import BaseEmbedderConfig\nfrom embedchain.embedder.base import BaseEmbedder\nfrom embedchain.models import VectorDimensions\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/azure_openai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/azure_openai.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nfrom embedchain.config import BaseEmbedderConfig\nfrom embedchain.embedder.base import BaseEmbedder\nfrom embedchain.models import VectorDimensions\n\n\nclass AzureOpenAIEmbedder(BaseEmbedder):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/huggingface.py_20_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '__init__'",
      "description": "Function '__init__' on line 20 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/huggingface.py",
      "line_number": 20,
      "code_snippet": "\nclass HuggingFaceEmbedder(BaseEmbedder):\n    def __init__(self, config: Optional[BaseEmbedderConfig] = None):\n        super().__init__(config=config)\n\n        if self.config.endpoint:",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/huggingface.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/huggingface.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Optional\n\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\ntry:\n    from langchain_huggingface import HuggingFaceEndpointEmbeddings\nexcept ModuleNotFoundError:\n    raise ModuleNotFoundError(\n        \"The required dependencies for HuggingFaceHub are not installed.\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/mistralai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/mistralai.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom typing import Optional, Union\n\nfrom chromadb import EmbeddingFunction, Embeddings\n\nfrom embedchain.config import BaseEmbedderConfig\nfrom embedchain.embedder.base import BaseEmbedder\nfrom embedchain.models import VectorDimensions\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/clarifai.py_39",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/clarifai.py",
      "line_number": 39,
      "code_snippet": "                input_batch = [\n                    self.input_obj.get_text_input(input_id=str(id), raw_text=inp) for id, inp in enumerate(batch)\n                ]\n                response = self.model_obj.predict(input_batch)\n                embeddings.extend([list(output.data.embeddings[0].vector) for output in response.outputs])\n        except Exception as e:\n            print(f\"Predict failed, exception: {e}\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/clarifai.py_27",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/embedder/clarifai.py",
      "line_number": 27,
      "code_snippet": "        self.api_key = config.api_key or os.getenv(\"CLARIFAI_PAT\")\n        self.model = config.model\n        self.model_obj = Model(url=self.model, pat=self.api_key)\n        self.input_obj = Inputs(pat=self.api_key)\n\n    def __call__(self, input: Union[str, list[str]]) -> Embeddings:\n        if isinstance(input, str):\n            input = [input]\n\n        batch_size = 32\n        embeddings = []",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/misc.py_134_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [249]) and executes code (lines [134, 135, 517]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/misc.py",
      "line_number": 134,
      "code_snippet": "            import datetime\n            import subprocess\n            import sys\n\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", \"pysqlite3-binary\", \"--quiet\", \"--disable-pip-version-check\"]\n            )\n\n            __import__(\"pysqlite3\")\n            sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/misc.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/misc.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport itertools\nimport json\nimport logging\nimport os\nimport re\nimport string\nfrom typing import Any\n\nfrom schema import Optional, Or, Schema",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_100",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 100 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 100,
      "code_snippet": "def setup_hf_app():\n    subprocess.run([\"pip\", \"install\", \"huggingface_hub[cli]\"], check=True)\n    hf_setup_file = os.path.join(os.path.expanduser(\"~\"), \".cache/huggingface/token\")\n    if os.path.exists(hf_setup_file):",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_35",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 35 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 35,
      "code_snippet": "        shutil.move(\".env.example\", \".env\")\n        subprocess.run(fly_launch_command, check=True)\n        console.print(\"\u2705 [bold green]'fly launch' executed successfully.[/bold green]\")\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_55",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 55 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 55,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(modal_setup_cmd)}[/bold cyan]\")\n        subprocess.run(modal_setup_cmd, check=True)\n    shutil.move(\".env.example\", \".env\")\n    console.print(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_77",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 77 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 77,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(render_setup_cmd)}[/bold cyan]\")\n        subprocess.run(render_setup_cmd, check=True)\n    shutil.move(\".env.example\", \".env\")\n    console.print(",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_128",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 128 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 128,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(uvicorn_command)}[/bold cyan]\")\n        subprocess.run(uvicorn_command, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_139",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 139 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 139,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")\n        subprocess.run(modal_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_150",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 150 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 150,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running Streamlit app with command: {' '.join(streamlit_run_cmd)}[/bold cyan]\")\n        subprocess.run(streamlit_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_167",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 167 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 167,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(uvicorn_command)}[/bold cyan]\")\n        subprocess.run(uvicorn_command, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_178",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 178 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 178,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running Gradio app with command: {' '.join(gradio_run_cmd)}[/bold cyan]\")\n        subprocess.run(gradio_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_229",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 229 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 229,
      "code_snippet": "        console.print(f\"\ud83d\udd10 [bold cyan]Setting secrets for {app_name}[/bold cyan]\")\n        subprocess.run(secrets_command, check=True)\n\n        # Deploy application",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_233",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 233 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 233,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(deploy_command)}[/bold cyan]\")\n        subprocess.run(deploy_command, check=True)\n        console.print(\"\u2705 [bold green]'fly deploy' executed successfully.[/bold green]\")\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_248",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 248 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 248,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(modal_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(modal_deploy_cmd, check=True)\n        console.print(\"\u2705 [bold green]'modal deploy' executed successfully.[/bold green]\")\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_269",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 269 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 269,
      "code_snippet": "        )\n        subprocess.run(streamlit_deploy_cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        console.print(f\"\u274c [bold red]An error occurred: {e}[/bold red]\")",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_284",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 284 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 284,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(render_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(render_deploy_cmd, check=True)\n        console.print(\"\u2705 [bold green]'render blueprint launch' executed successfully.[/bold green]\")\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_299",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 299 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 299,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(gradio_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(gradio_deploy_cmd, check=True)\n        console.print(\"\u2705 [bold green]'gradio deploy' executed successfully.[/bold green]\")\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_317",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 317 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 317,
      "code_snippet": "        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(hf_spaces_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(hf_spaces_deploy_cmd, check=True)\n        console.print(\"\u2705 [bold green]'huggingface-cli upload' executed successfully.[/bold green]\")\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_258",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'deploy_streamlit' on line 258 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 258,
      "code_snippet": "def deploy_streamlit():\n    streamlit_deploy_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(streamlit_deploy_cmd)}[/bold cyan]\")\n        console.print(\n            \"\"\"\\n\\n\u2705 [bold yellow]To deploy a streamlit app, you can directly it from the UI.\\n\n        Click on the 'Deploy' button on the top right corner of the app.\\n\n        For more information, please refer to https://docs.embedchain.ai/deployment/streamlit_io\n        [/bold yellow]\n                      \\n\\n\"\"\"\n        )",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_30_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'setup_fly_io_app'",
      "description": "Function 'setup_fly_io_app' on line 30 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 30,
      "code_snippet": "\n    return src_path\n\n\ndef setup_fly_io_app(extra_args):\n    fly_launch_command = [\"fly\", \"launch\", \"--region\", \"sjc\", \"--no-deploy\"] + list(extra_args)\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(fly_launch_command)}[/bold cyan]\")\n        shutil.move(\".env.example\", \".env\")\n        subprocess.run(fly_launch_command, check=True)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_135_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_modal_com'",
      "description": "Function 'run_dev_modal_com' on line 135 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 135,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]FastAPI server stopped[/bold yellow]\")\n\n\ndef run_dev_modal_com():\n    modal_run_cmd = [\"modal\", \"serve\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")\n        subprocess.run(modal_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_146_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_streamlit_io'",
      "description": "Function 'run_dev_streamlit_io' on line 146 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 146,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]FastAPI server stopped[/bold yellow]\")\n\n\ndef run_dev_streamlit_io():\n    streamlit_run_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Streamlit app with command: {' '.join(streamlit_run_cmd)}[/bold cyan]\")\n        subprocess.run(streamlit_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_157_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_render_com'",
      "description": "Function 'run_dev_render_com' on line 157 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 157,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]Streamlit server stopped[/bold yellow]\")\n\n\ndef run_dev_render_com(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:\n        uvicorn_command.append(\"--reload\")\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_174_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'run_dev_gradio'",
      "description": "Function 'run_dev_gradio' on line 174 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 174,
      "code_snippet": "    except KeyboardInterrupt:\n        console.print(\"\\n\ud83d\uded1 [bold yellow]FastAPI server stopped[/bold yellow]\")\n\n\ndef run_dev_gradio():\n    gradio_run_cmd = [\"gradio\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Gradio app with command: {' '.join(gradio_run_cmd)}[/bold cyan]\")\n        subprocess.run(gradio_run_cmd, check=True)\n    except subprocess.CalledProcessError as e:",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_211_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'deploy_fly'",
      "description": "Function 'deploy_fly' on line 211 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 211,
      "code_snippet": "                    env_vars[key] = value\n    return env_vars\n\n\ndef deploy_fly():\n    app_name = \"\"\n    with open(\"fly.toml\", \"r\") as file:\n        for line in file:\n            if line.strip().startswith(\"app =\"):\n                app_name = line.split(\"=\")[1].strip().strip('\"')",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_244_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'deploy_modal'",
      "description": "Function 'deploy_modal' on line 244 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 244,
      "code_snippet": "            \"\u274c [bold red]'fly' command not found. Please ensure Fly CLI is installed and in your PATH.[/bold red]\"\n        )\n\n\ndef deploy_modal():\n    modal_deploy_cmd = [\"modal\", \"deploy\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(modal_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(modal_deploy_cmd, check=True)\n        console.print(\"\u2705 [bold green]'modal deploy' executed successfully.[/bold green]\")",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_258_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'deploy_streamlit'",
      "description": "Function 'deploy_streamlit' on line 258 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 258,
      "code_snippet": "            \"\u274c [bold red]'modal' command not found. Please ensure Modal CLI is installed and in your PATH.[/bold red]\"\n        )\n\n\ndef deploy_streamlit():\n    streamlit_deploy_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(streamlit_deploy_cmd)}[/bold cyan]\")\n        console.print(\n            \"\"\"\\n\\n\u2705 [bold yellow]To deploy a streamlit app, you can directly it from the UI.\\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_279_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'deploy_render'",
      "description": "Function 'deploy_render' on line 279 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 279,
      "code_snippet": "            Please ensure Streamlit CLI is installed and in your PATH.[/bold red]\"\"\"\n        )\n\n\ndef deploy_render():\n    render_deploy_cmd = [\"render\", \"blueprint\", \"launch\"]\n\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(render_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(render_deploy_cmd, check=True)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_294_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'deploy_gradio_app'",
      "description": "Function 'deploy_gradio_app' on line 294 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 294,
      "code_snippet": "            \"\u274c [bold red]'render' command not found. Please ensure Render CLI is installed and in your PATH.[/bold red]\"  # noqa:E501\n        )\n\n\ndef deploy_gradio_app():\n    gradio_deploy_cmd = [\"gradio\", \"deploy\"]\n\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(gradio_deploy_cmd)}[/bold cyan]\")\n        subprocess.run(gradio_deploy_cmd, check=True)",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_309_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'deploy_hf_spaces'",
      "description": "Function 'deploy_hf_spaces' on line 309 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 309,
      "code_snippet": "            \"\u274c [bold red]'gradio' command not found. Please ensure Gradio CLI is installed and in your PATH.[/bold red]\"  # noqa:E501\n        )\n\n\ndef deploy_hf_spaces(ec_app_name):\n    if not ec_app_name:\n        console.print(\"\u274c [bold red]'name' not found in embedchain.json[/bold red]\")\n        return\n    hf_spaces_deploy_cmd = [\"huggingface-cli\", \"upload\", ec_app_name, \".\", \".\", \"--repo-type=space\"]\n",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_30_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_fly_io_app'",
      "description": "Function 'setup_fly_io_app' on line 30 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 30,
      "code_snippet": "\n\ndef setup_fly_io_app(extra_args):\n    fly_launch_command = [\"fly\", \"launch\", \"--region\", \"sjc\", \"--no-deploy\"] + list(extra_args)\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(fly_launch_command)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_45_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_modal_com_app'",
      "description": "Function 'setup_modal_com_app' on line 45 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 45,
      "code_snippet": "\n\ndef setup_modal_com_app(extra_args):\n    modal_setup_file = os.path.join(os.path.expanduser(\"~\"), \".modal.toml\")\n    if os.path.exists(modal_setup_file):\n        console.print(",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_67_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_render_com_app'",
      "description": "Function 'setup_render_com_app' on line 67 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 67,
      "code_snippet": "\n\ndef setup_render_com_app():\n    render_setup_file = os.path.join(os.path.expanduser(\"~\"), \".render/config.yaml\")\n    if os.path.exists(render_setup_file):\n        console.print(",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_99_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'setup_hf_app'",
      "description": "Function 'setup_hf_app' on line 99 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 99,
      "code_snippet": "\n\ndef setup_hf_app():\n    subprocess.run([\"pip\", \"install\", \"huggingface_hub[cli]\"], check=True)\n    hf_setup_file = os.path.join(os.path.expanduser(\"~\"), \".cache/huggingface/token\")\n    if os.path.exists(hf_setup_file):",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_118_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_fly_io'",
      "description": "Function 'run_dev_fly_io' on line 118 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 118,
      "code_snippet": "\n\ndef run_dev_fly_io(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_135_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_modal_com'",
      "description": "Function 'run_dev_modal_com' on line 135 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 135,
      "code_snippet": "\n\ndef run_dev_modal_com():\n    modal_run_cmd = [\"modal\", \"serve\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_146_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_streamlit_io'",
      "description": "Function 'run_dev_streamlit_io' on line 146 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 146,
      "code_snippet": "\n\ndef run_dev_streamlit_io():\n    streamlit_run_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Streamlit app with command: {' '.join(streamlit_run_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_157_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_render_com'",
      "description": "Function 'run_dev_render_com' on line 157 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 157,
      "code_snippet": "\n\ndef run_dev_render_com(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_174_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'run_dev_gradio'",
      "description": "Function 'run_dev_gradio' on line 174 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 174,
      "code_snippet": "\n\ndef run_dev_gradio():\n    gradio_run_cmd = [\"gradio\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running Gradio app with command: {' '.join(gradio_run_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_211_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'deploy_fly'",
      "description": "Function 'deploy_fly' on line 211 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 211,
      "code_snippet": "\n\ndef deploy_fly():\n    app_name = \"\"\n    with open(\"fly.toml\", \"r\") as file:\n        for line in file:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_244_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'deploy_modal'",
      "description": "Function 'deploy_modal' on line 244 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 244,
      "code_snippet": "\n\ndef deploy_modal():\n    modal_deploy_cmd = [\"modal\", \"deploy\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(modal_deploy_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_258_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'deploy_streamlit'",
      "description": "Function 'deploy_streamlit' on line 258 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 258,
      "code_snippet": "\n\ndef deploy_streamlit():\n    streamlit_deploy_cmd = [\"streamlit\", \"run\", \"app.py\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running: {' '.join(streamlit_deploy_cmd)}[/bold cyan]\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_279_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'deploy_render'",
      "description": "Function 'deploy_render' on line 279 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 279,
      "code_snippet": "\n\ndef deploy_render():\n    render_deploy_cmd = [\"render\", \"blueprint\", \"launch\"]\n\n    try:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_294_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'deploy_gradio_app'",
      "description": "Function 'deploy_gradio_app' on line 294 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 294,
      "code_snippet": "\n\ndef deploy_gradio_app():\n    gradio_deploy_cmd = [\"gradio\", \"deploy\"]\n\n    try:",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_309_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'deploy_hf_spaces'",
      "description": "Function 'deploy_hf_spaces' on line 309 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 309,
      "code_snippet": "\n\ndef deploy_hf_spaces(ec_app_name):\n    if not ec_app_name:\n        console.print(\"\u274c [bold red]'name' not found in embedchain.json[/bold red]\")\n        return",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_118_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'run_dev_fly_io'",
      "description": "API endpoint 'run_dev_fly_io' on line 118 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 118,
      "code_snippet": "\n\ndef run_dev_fly_io(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_135_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'run_dev_modal_com'",
      "description": "API endpoint 'run_dev_modal_com' on line 135 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 135,
      "code_snippet": "\n\ndef run_dev_modal_com():\n    modal_run_cmd = [\"modal\", \"serve\", \"app\"]\n    try:\n        console.print(f\"\ud83d\ude80 [bold cyan]Running FastAPI app with command: {' '.join(modal_run_cmd)}[/bold cyan]\")",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_157_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'run_dev_render_com'",
      "description": "API endpoint 'run_dev_render_com' on line 157 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 157,
      "code_snippet": "\n\ndef run_dev_render_com(debug, host, port):\n    uvicorn_command = [\"uvicorn\", \"app:app\"]\n\n    if debug:",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py_100",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/utils/cli.py",
      "line_number": 100,
      "code_snippet": "\n\ndef setup_hf_app():\n    subprocess.run([\"pip\", \"install\", \"huggingface_hub[cli]\"], check=True)\n    hf_setup_file = os.path.join(os.path.expanduser(\"~\"), \".cache/huggingface/token\")\n    if os.path.exists(hf_setup_file):\n        console.print(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/vector_dimensions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/vector_dimensions.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\n# vector length created by embedding fn\nclass VectorDimensions(Enum):\n    GPT4ALL = 384\n    OPENAI = 1536\n    VERTEX_AI = 768\n    HUGGING_FACE = 384\n    GOOGLE_AI = 768",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/embedding_functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/embedding_functions.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass EmbeddingFunctions(Enum):\n    OPENAI = \"OPENAI\"\n    HUGGING_FACE = \"HUGGING_FACE\"\n    VERTEX_AI = \"VERTEX_AI\"\n    AWS_BEDROCK = \"AWS_BEDROCK\"\n    GPT4ALL = \"GPT4ALL\"\n    OLLAMA = \"OLLAMA\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/data_type.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/data_type.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass DirectDataType(Enum):\n    \"\"\"\n    DirectDataType enum contains data types that contain raw data directly.\n    \"\"\"\n\n    TEXT = \"text\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/providers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/models/providers.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass Providers(Enum):\n    OPENAI = \"OPENAI\"\n    ANTHROPHIC = \"ANTHPROPIC\"\n    VERTEX_AI = \"VERTEX_AI\"\n    GPT4ALL = \"GPT4ALL\"\n    OLLAMA = \"OLLAMA\"\n    AZURE_OPENAI = \"AZURE_OPENAI\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/telemetry/posthog.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Base64 High Entropy String",
      "description": "detect-secrets found a potential Base64 High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/telemetry/posthog.py",
      "line_number": 14,
      "code_snippet": "class AnonymousTelemetry:\n    def __init__(self, host=\"https://app.posthog.com\", enabled=True):\n        self.project_api_key = \"phc_PHQDA5KwztijnSojsxJ2c1DuJd52QCzJzT2xnSGvjN2\"\n        self.host = host\n        self.posthog = Posthog(project_api_key=self.project_api_key, host=self.host)",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/telemetry/posthog.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/telemetry/posthog.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nimport uuid\n\nfrom posthog import Posthog\n\nimport embedchain\nfrom embedchain.constants import CONFIG_DIR, CONFIG_FILE\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/pdf_file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/pdf_file.py",
      "line_number": 1,
      "code_snippet": "import hashlib\n\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string\n\n\n@register_deserializable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/substack.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/substack.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nimport time\nfrom xml.etree import ElementTree\n\nimport requests\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import is_readable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/openapi.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/openapi.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nfrom io import StringIO\nfrom urllib.parse import urlparse\n\nimport requests\nimport yaml\n\nfrom embedchain.loaders.base_loader import BaseLoader\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py_134",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'client.run' is used in 'run(' on line 134 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py",
      "line_number": 134,
      "code_snippet": "        client = DiscordClient(intents=intents)\n        client.run(self.token)\n\n        metadata = {",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py_99_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'load_data'",
      "description": "Function 'load_data' on line 99 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py",
      "line_number": 99,
      "code_snippet": "        }\n\n    def load_data(self, channel_id: str):\n        \"\"\"Load data from a Discord Channel ID.\"\"\"\n        import discord\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py_134",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discord.py",
      "line_number": 134,
      "code_snippet": "        intents = discord.Intents.default()\n        intents.message_content = True\n        client = DiscordClient(intents=intents)\n        client.run(self.token)\n\n        metadata = {\n            \"url\": channel_id,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/web_page.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/web_page.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nfrom typing import Any, Optional\n\nimport requests\n\ntry:\n    from bs4 import BeautifulSoup\nexcept ImportError:\n    raise ImportError(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/local_qna_pair.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/local_qna_pair.py",
      "line_number": 1,
      "code_snippet": "import hashlib\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\n\n\n@register_deserializable\nclass LocalQnaPairLoader(BaseLoader):\n    def load_data(self, content):\n        \"\"\"Load data from a local QnA pair.\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/docs_site_loader.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/docs_site_loader.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nfrom urllib.parse import urljoin, urlparse\n\nimport requests\n\ntry:\n    from bs4 import BeautifulSoup\nexcept ImportError:\n    raise ImportError(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/beehiiv.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/beehiiv.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nimport time\nfrom xml.etree import ElementTree\n\nimport requests\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import is_readable",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/dropbox.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/dropbox.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport os\n\nfrom dropbox.files import FileMetadata\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.loaders.directory_loader import DirectoryLoader\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py_57",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'flow.run_local_server' is used in 'execute(' on line 57 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py",
      "line_number": 57,
      "code_snippet": "                flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", GmailReader.SCOPES)\n                creds = flow.run_local_server(port=8080)\n            with open(\"token.json\", \"w\") as token:\n                token.write(creds.to_json())",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py_42_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in '_get_credentials'",
      "description": "Function '_get_credentials' on line 42 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py",
      "line_number": 42,
      "code_snippet": "\n    @staticmethod\n    def _get_credentials():\n        if not os.path.exists(\"credentials.json\"):\n            raise FileNotFoundError(\"Missing 'credentials.json'. Download it from your Google Developer account.\")\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py_42_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_credentials'",
      "description": "Function '_get_credentials' on line 42 makes critical security decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py",
      "line_number": 42,
      "code_snippet": "\n    @staticmethod\n    def _get_credentials():\n        if not os.path.exists(\"credentials.json\"):\n            raise FileNotFoundError(\"Missing 'credentials.json'. Download it from your Google Developer account.\")\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py_57",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/gmail.py",
      "line_number": 57,
      "code_snippet": "                creds.refresh(Request())\n            else:\n                flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", GmailReader.SCOPES)\n                creds = flow.run_local_server(port=8080)\n            with open(\"token.json\", \"w\") as token:\n                token.write(creds.to_json())\n        return creds",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discourse.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/discourse.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nimport time\nfrom typing import Any, Optional\n\nimport requests\n\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/csv.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/csv.py",
      "line_number": 1,
      "code_snippet": "import csv\nimport hashlib\nfrom io import StringIO\nfrom urllib.parse import urlparse\n\nimport requests\n\nfrom embedchain.loaders.base_loader import BaseLoader\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/postgres.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/postgres.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nfrom typing import Any, Optional\n\nfrom embedchain.loaders.base_loader import BaseLoader\n\nlogger = logging.getLogger(__name__)\n\n\nclass PostgresLoader(BaseLoader):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/directory_loader.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/directory_loader.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nfrom embedchain.config import AddConfig\nfrom embedchain.data_formatter.data_formatter import DataFormatter\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.loaders.text_file import TextFileLoader",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/audio.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/audio.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport os\n\nimport validators\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\n\ntry:\n    from deepgram import DeepgramClient, PrerecordedOptions",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/notion.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/notion.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nimport os\nfrom typing import Any, Optional\n\nimport requests\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/json.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/json.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport json\nimport os\nimport re\nfrom typing import Union\n\nimport requests\n\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string, is_valid_json_string",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py_29",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
      "line_number": 29,
      "code_snippet": "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    def _create_completion_request(self, content: str):\n        return self.client.chat.completions.create(\n            model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": content}], max_tokens=self.max_tokens\n        )\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py_29_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "INFO",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model ''gpt-4o'' is used without version pinning on line 29. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
      "line_number": 29,
      "code_snippet": "\n    def _create_completion_request(self, content: str):\n        return self.client.chat.completions.create(\n            model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": content}], max_tokens=self.max_tokens\n        )\n",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py_28_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in '_create_completion_request'",
      "description": "Function '_create_completion_request' on line 28 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
      "line_number": 28,
      "code_snippet": "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    def _create_completion_request(self, content: str):\n        return self.client.chat.completions.create(\n            model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": content}], max_tokens=self.max_tokens\n        )",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py_28_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_create_completion_request'",
      "description": "Function '_create_completion_request' on line 28 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
      "line_number": 28,
      "code_snippet": "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    def _create_completion_request(self, content: str):\n        return self.client.chat.completions.create(\n            model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": content}], max_tokens=self.max_tokens\n        )",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py_28",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/image.py",
      "line_number": 28,
      "code_snippet": "    @staticmethod\n    def _encode_image(image_path: str):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    def _create_completion_request(self, content: str):\n        return self.client.chat.completions.create(\n            model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": content}], max_tokens=self.max_tokens\n        )\n\n    def _process_url(self, url: str):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/slack.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/slack.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nimport os\nimport ssl\nfrom typing import Any, Optional\n\nimport certifi\n\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/rss_feed.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/rss_feed.py",
      "line_number": 1,
      "code_snippet": "import hashlib\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\n\n\n@register_deserializable\nclass RSSFeedLoader(BaseLoader):\n    \"\"\"Loader for RSS Feed.\"\"\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/mysql.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/mysql.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport logging\nfrom typing import Any, Optional\n\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string\n\nlogger = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/unstructured_file.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/loaders/unstructured_file.py",
      "line_number": 1,
      "code_snippet": "import hashlib\n\nfrom embedchain.helpers.json_serializable import register_deserializable\nfrom embedchain.loaders.base_loader import BaseLoader\nfrom embedchain.utils.misc import clean_string\n\n\n@register_deserializable\nclass UnstructuredLoader(BaseLoader):\n    def load_data(self, url):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/helpers/json_serializable.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/helpers/json_serializable.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom string import Template\nfrom typing import Any, Type, TypeVar, Union\n\nT = TypeVar(\"T\", bound=\"JSONSerializable\")\n\n# NOTE: Through inheritance, all of our classes should be children of JSONSerializable. (highest level)\n# NOTE: The @register_deserializable decorator should be added to all user facing child classes. (lowest level)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/helpers/callbacks.py_51_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Model API without rate limiting in 'generate'",
      "description": "API endpoint 'generate' on line 51 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/helpers/callbacks.py",
      "line_number": 51,
      "code_snippet": "\n\ndef generate(rq: queue.Queue):\n    \"\"\"\n    This is a generator that yields the items in the queue until it reaches the stop item.\n",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_105",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM API call 'self._client.beta.threads.messages.create'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 105,
      "code_snippet": "    def _send_message(self, message):\n        self._client.beta.threads.messages.create(thread_id=self.thread_id, role=\"user\", content=message)\n        self._wait_for_completion()",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_203",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'query' embedded in LLM prompt",
      "description": "User input parameter 'query' is directly passed to LLM API call 'self.pipeline.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 203,
      "code_snippet": "        }\n        return self.pipeline.chat(query, where=where)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_119",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "LLM output flows to command_injection sink",
      "description": "LLM output variable 'run_id' flows to 'self._client.beta.threads.runs.retrieve' on line 119 via direct flow. This creates a command_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 119,
      "code_snippet": "            time.sleep(0.1)  # Sleep before making the next API call to avoid hitting rate limits\n            run = self._client.beta.threads.runs.retrieve(thread_id=self.thread_id, run_id=run_id)\n            run_status = run.status\n            if run_status == \"failed\":",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_104",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_send_message' on line 104 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 104,
      "code_snippet": "    def _send_message(self, message):\n        self._client.beta.threads.messages.create(thread_id=self.thread_id, role=\"user\", content=message)\n        self._wait_for_completion()\n\n    def _wait_for_completion(self):\n        run = self._client.beta.threads.runs.create(\n            thread_id=self.thread_id,\n            assistant_id=self.assistant.id,\n            instructions=self.instructions,\n        )\n        run_id = run.id",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_108",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function '_wait_for_completion' on line 108 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 108,
      "code_snippet": "    def _wait_for_completion(self):\n        run = self._client.beta.threads.runs.create(\n            thread_id=self.thread_id,\n            assistant_id=self.assistant.id,\n            instructions=self.instructions,\n        )\n        run_id = run.id\n        run_status = run.status\n\n        while run_status in [\"queued\", \"in_progress\", \"requires_action\"]:\n            time.sleep(0.1)  # Sleep before making the next API call to avoid hitting rate limits",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_104_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in '_send_message'",
      "description": "Function '_send_message' on line 104 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 104,
      "code_snippet": "        ]\n\n    def _send_message(self, message):\n        self._client.beta.threads.messages.create(thread_id=self.thread_id, role=\"user\", content=message)\n        self._wait_for_completion()\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_108_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in '_wait_for_completion'",
      "description": "Function '_wait_for_completion' on line 108 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 108,
      "code_snippet": "        self._wait_for_completion()\n\n    def _wait_for_completion(self):\n        run = self._client.beta.threads.runs.create(\n            thread_id=self.thread_id,\n            assistant_id=self.assistant.id,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_104_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_send_message'",
      "description": "Function '_send_message' on line 104 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 104,
      "code_snippet": "        ]\n\n    def _send_message(self, message):\n        self._client.beta.threads.messages.create(thread_id=self.thread_id, role=\"user\", content=message)\n        self._wait_for_completion()\n",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_104",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 104,
      "code_snippet": "        return [\n            self._add_file_to_assistant(self._prepare_source_path(ds[\"source\"], ds.get(\"data_type\")))\n            for ds in data_sources\n        ]\n\n    def _send_message(self, message):\n        self._client.beta.threads.messages.create(thread_id=self.thread_id, role=\"user\", content=message)\n        self._wait_for_completion()\n\n    def _wait_for_completion(self):\n        run = self._client.beta.threads.runs.create(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py_196",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/store/assistants.py",
      "line_number": 196,
      "code_snippet": "            **self._telemetry_props,\n            \"data_type\": data_type or detect_datatype(source),\n        }\n        self.telemetry.capture(event_name=\"add\", properties=event_props)\n\n    def chat(self, query):\n        where = {\n            \"$and\": [\n                {\"assistant_id\": {\"$eq\": self.assistant_id}},\n                {\"thread_id\": {\"$in\": [self.thread_id, \"global_knowledge\"]}},\n            ]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_42",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 42,
      "code_snippet": "        \"\"\"\n        Get claim statements from the answer.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n        )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_63",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 63,
      "code_snippet": "        \"\"\"\n        Get verdicts for claim statements.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n        )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_42_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.config.model' is used without version pinning on line 42. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 42,
      "code_snippet": "        Get claim statements from the answer.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n        )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_63_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.config.model' is used without version pinning on line 63. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 63,
      "code_snippet": "        Get verdicts for claim statements.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n        )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_38_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_claim_statements'",
      "description": "Function '_get_claim_statements' on line 38 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 38,
      "code_snippet": "        return prompt\n\n    def _get_claim_statements(self, prompt: str) -> np.ndarray:\n        \"\"\"\n        Get claim statements from the answer.\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_59_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_get_claim_verdict_scores'",
      "description": "Function '_get_claim_verdict_scores' on line 59 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 59,
      "code_snippet": "        return prompt\n\n    def _get_claim_verdict_scores(self, prompt: str) -> np.ndarray:\n        \"\"\"\n        Get verdicts for claim statements.\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_38",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 38,
      "code_snippet": "        Generate the prompt for the given data.\n        \"\"\"\n        prompt = Template(self.config.answer_claims_prompt).substitute(question=data.question, answer=data.answer)\n        return prompt\n\n    def _get_claim_statements(self, prompt: str) -> np.ndarray:\n        \"\"\"\n        Get claim statements from the answer.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py_59",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/groundedness.py",
      "line_number": 59,
      "code_snippet": "        prompt = Template(self.config.claims_inference_prompt).substitute(\n            context=\"\\n\".join(data.contexts), claim_statements=\"\\n\".join(claim_statements)\n        )\n        return prompt\n\n    def _get_claim_verdict_scores(self, prompt: str) -> np.ndarray:\n        \"\"\"\n        Get verdicts for claim statements.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py_42_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.config.model' is used without version pinning on line 42. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py",
      "line_number": 42,
      "code_snippet": "        original_context = \"\\n\".join(data.contexts)\n        prompt = Template(self.config.prompt).substitute(context=original_context, question=data.question)\n        response = self.client.chat.completions.create(\n            model=self.config.model, messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        useful_context = response.choices[0].message.content.strip()",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py_36_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_compute_score'",
      "description": "Function '_compute_score' on line 36 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py",
      "line_number": 36,
      "code_snippet": "        return self._sbd.segment(text)\n\n    def _compute_score(self, data: EvalData) -> float:\n        \"\"\"\n        Computes the context relevance score for a given data item.\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py_42",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/context_relevancy.py",
      "line_number": 42,
      "code_snippet": "        \"\"\"\n        original_context = \"\\n\".join(data.contexts)\n        prompt = Template(self.config.prompt).substitute(context=original_context, question=data.question)\n        response = self.client.chat.completions.create(\n            model=self.config.model, messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        useful_context = response.choices[0].message.content.strip()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py_43",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py",
      "line_number": 43,
      "code_snippet": "        \"\"\"\n        Generates questions from the prompt.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py_43_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'self.config.model' is used without version pinning on line 43. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py",
      "line_number": 43,
      "code_snippet": "        Generates questions from the prompt.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py_39_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in '_generate_questions'",
      "description": "Function '_generate_questions' on line 39 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py",
      "line_number": 39,
      "code_snippet": "        )\n\n    def _generate_questions(self, prompt: str) -> list[str]:\n        \"\"\"\n        Generates questions from the prompt.\n        \"\"\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py_39",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/evaluation/metrics/answer_relevancy.py",
      "line_number": 39,
      "code_snippet": "        \"\"\"\n        return Template(self.config.prompt).substitute(\n            num_gen_questions=self.config.num_gen_questions, answer=data.answer\n        )\n\n    def _generate_questions(self, prompt: str) -> list[str]:\n        \"\"\"\n        Generates questions from the prompt.\n        \"\"\"\n        response = self.client.chat.completions.create(\n            model=self.config.model,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/render.com/app.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/render.com/app.py",
      "line_number": 1,
      "code_snippet": "from fastapi import FastAPI, responses\nfrom pydantic import BaseModel\n\nfrom embedchain import App\n\napp = FastAPI(title=\"Embedchain FastAPI App\")\nembedchain_app = App()\n\n\nclass SourceModel(BaseModel):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_29",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 29,
      "code_snippet": "    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"Ask me anything!\"):\n    app = embedchain_bot()\n\n    if prompt.startswith(\"/add\"):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_26",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 26,
      "code_snippet": "    ]\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"Ask me anything!\"):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_45",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 45,
      "code_snippet": "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"Added {prompt} to knowledge base!\"})\n            st.stop()\n\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_49",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 49,
      "code_snippet": "        st.markdown(prompt)\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n    with st.chat_message(\"assistant\"):\n        msg_placeholder = st.empty()\n        msg_placeholder.markdown(\"Thinking...\")\n        full_response = \"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_54",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 54,
      "code_snippet": "        msg_placeholder.markdown(\"Thinking...\")\n        full_response = \"\"\n\n        for response in app.chat(prompt):\n            msg_placeholder.empty()\n            full_response += response\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_33",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 33,
      "code_snippet": "    app = embedchain_bot()\n\n    if prompt.startswith(\"/add\"):\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        prompt = prompt.replace(\"/add\", \"\").strip()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py_37",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/streamlit.io/app.py",
      "line_number": 37,
      "code_snippet": "            st.markdown(prompt)\n            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        prompt = prompt.replace(\"/add\", \"\").strip()\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n            message_placeholder.markdown(\"Adding to knowledge base...\")\n            app.add(prompt)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/modal.com/app.py_74",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/modal.com/app.py",
      "line_number": 74,
      "code_snippet": "    \"\"\"\n    if not question:\n        return {\"message\": \"No question provided.\"}\n    response = embedchain_app.chat(question)\n    return {\"response\": response}\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/gradio.app/app.py_13",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.95,
      "title": "User input 'message' embedded in LLM prompt",
      "description": "User input parameter 'message' is directly passed to LLM API call 'app.chat'. This is a high-confidence prompt injection vector.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/gradio.app/app.py",
      "line_number": 13,
      "code_snippet": "def query(message, history):\n    return app.chat(message)\n",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/gradio.app/app.py_12",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'query' on line 12 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/gradio.app/app.py",
      "line_number": 12,
      "code_snippet": "def query(message, history):\n    return app.chat(message)\n\n\ndemo = gr.ChatInterface(query)\n\ndemo.launch()",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/fly.io/app.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/deployment/fly.io/app.py",
      "line_number": 1,
      "code_snippet": "from dotenv import load_dotenv\nfrom fastapi import FastAPI, responses\nfrom pydantic import BaseModel\n\nfrom embedchain import App\n\nload_dotenv(\".env\")\n\napp = FastAPI(title=\"Embedchain FastAPI App\")\nembedchain_app = App()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/llm/base.py_105_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [228]) and executes code (lines [105, 106, 107]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/llm/base.py",
      "line_number": 105,
      "code_snippet": "DEFAULT_PROMPT_TEMPLATE = Template(DEFAULT_PROMPT)\nDEFAULT_PROMPT_WITH_HISTORY_TEMPLATE = Template(DEFAULT_PROMPT_WITH_HISTORY)\nDEFAULT_PROMPT_WITH_MEM0_MEMORY_TEMPLATE = Template(DEFAULT_PROMPT_WITH_MEM0_MEMORY)\nDOCS_SITE_PROMPT_TEMPLATE = Template(DOCS_SITE_DEFAULT_PROMPT)\nquery_re = re.compile(r\"\\$\\{*query\\}*\")\ncontext_re = re.compile(r\"\\$\\{*context\\}*\")\nhistory_re = re.compile(r\"\\$\\{*history\\}*\")\n\n\n@register_deserializable",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/llm/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/llm/base.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport re\nfrom pathlib import Path\nfrom string import Template\nfrom typing import Any, Dict, Mapping, Optional, Union\n\nimport httpx\n\nfrom embedchain.config.base_config import BaseConfig",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/evaluation/base.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/config/evaluation/base.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\n\nfrom embedchain.config.base_config import BaseConfig\n\nANSWER_RELEVANCY_PROMPT = \"\"\"\nPlease provide $num_gen_questions questions from the provided answer.\nYou must provide the complete question, if are not able to provide the complete question, return empty string (\"\").\nPlease only provide one question per line without numbers or bullets to distinguish them.\nYou must only provide the questions and no other text.\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/core/db/models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/core/db/models.py",
      "line_number": 1,
      "code_snippet": "import uuid\n\nfrom sqlalchemy import TIMESTAMP, Column, Integer, String, Text, func\nfrom sqlalchemy.orm import declarative_base\n\nBase = declarative_base()\nmetadata = Base.metadata\n\n\nclass DataSource(Base):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/core/db/database.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/core/db/database.py",
      "line_number": 1,
      "code_snippet": "import os\n\nfrom alembic import command\nfrom alembic.config import Config\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine.base import Engine\nfrom sqlalchemy.orm import Session as SQLAlchemySession\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n\nfrom .models import Base",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/versions/40a327b3debd_create_initial_migrations.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp4m7hnnvv/embedchain/embedchain/embedchain/migrations/versions/40a327b3debd_create_initial_migrations.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"40a327b3debd\"\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 1195,
      "kept": 702,
      "filtered": 493,
      "reduction_pct": 41.3,
      "avg_tp_probability": 0.461,
      "filter_reasons": {
        "high severity with context": 429,
        "test file": 312,
        "build tool subprocess": 110,
        "example file": 50,
        "callback handler pattern": 15,
        "env variable reference": 14,
        "placeholder value": 10,
        "executor pool pattern": 9,
        "PyTorch model.eval": 5,
        "framework internal pattern": 3,
        "asyncio.run pattern": 1
      }
    }
  }
}