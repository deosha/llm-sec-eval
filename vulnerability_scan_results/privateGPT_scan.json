{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:15:48.573953Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT",
    "files_scanned": 88,
    "overall_score": 3.06,
    "confidence": 0.67,
    "duration_seconds": 1.208,
    "findings_count": 50,
    "severity_breakdown": {
      "CRITICAL": 35,
      "HIGH": 9,
      "MEDIUM": 2,
      "LOW": 2,
      "INFO": 2
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 16,
      "confidence": 0.48,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 0,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Token Limiting"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 1,
      "confidence": 0.43,
      "subscores": {
        "model_protection": 46,
        "extraction_defense": 60,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption in transit",
        "OAuth",
        "Signature verification"
      ],
      "gaps": []
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.45,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.38,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 5,
      "confidence": 0.46,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 0,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.93,
      "subscores": {
        "LLM01": 40,
        "LLM02": 72,
        "LLM03": 100,
        "LLM04": 36,
        "LLM05": 86,
        "LLM06": 100,
        "LLM07": 100,
        "LLM08": 65,
        "LLM09": 99,
        "LLM10": 0
      },
      "detected_controls": [
        "Training Data Poisoning (no vulnerabilities found)",
        "Sensitive Information Disclosure (no vulnerabilities found)",
        "Insecure Plugin Design (no vulnerabilities found)"
      ],
      "gaps": [
        "Prompt Injection: 2 critical",
        "Insecure Output Handling: 1 critical",
        "Model Denial of Service: 2 critical",
        "Supply Chain Vulnerabilities: 1 medium, 2 low",
        "Excessive Agency: 2 high, 1 medium",
        "Model Theft: 6 high",
        "ML: 30 critical",
        "SQLI: 1 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/ingest_folder.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/ingest_folder.py",
      "line_number": 1,
      "code_snippet": "#!/usr/bin/env python3\n\nimport argparse\nimport logging\nfrom pathlib import Path\n\nfrom private_gpt.di import global_injector\nfrom private_gpt.server.ingest.ingest_service import IngestService\nfrom private_gpt.server.ingest.ingest_watcher import IngestWatcher\nfrom private_gpt.settings.settings import Settings",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/utils.py_59",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 59 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/utils.py",
      "line_number": 59,
      "code_snippet": "        try:\n            for table in self.tables[storetype]:\n                sql = f\"DROP TABLE IF EXISTS {self.schema}.{table}\"\n                cur.execute(sql)\n                print(f\"Table {self.schema}.{table} dropped.\")",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/utils.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport os\nimport shutil\nfrom typing import Any, ClassVar\n\nfrom private_gpt.paths import local_data_path\nfrom private_gpt.settings.settings import settings\n\n\ndef wipe_file(file: str) -> None:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/extract_openapi.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/scripts/extract_openapi.py",
      "line_number": 1,
      "code_snippet": "import argparse\nimport json\nimport sys\n\nimport yaml\nfrom uvicorn.importer import import_from_string\n\nparser = argparse.ArgumentParser(prog=\"extract_openapi.py\")\nparser.add_argument(\"app\", help='App import string. Eg. \"main:app\"', default=\"main:app\")\nparser.add_argument(\"--app-dir\", help=\"Directory containing the app\", default=None)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/ui/ui.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/ui/ui.py",
      "line_number": 1,
      "code_snippet": "\"\"\"This file should be imported if and only if you want to run the UI locally.\"\"\"\n\nimport base64\nimport logging\nimport time\nfrom collections.abc import Iterable\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/settings/settings_loader.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/settings/settings_loader.py",
      "line_number": 1,
      "code_snippet": "import functools\nimport logging\nimport os\nimport sys\nfrom collections.abc import Iterable\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic.v1.utils import deep_update, unique_list\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/settings/yaml.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/settings/yaml.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nimport typing\nfrom typing import Any, TextIO\n\nfrom yaml import SafeLoader\n\n_env_replace_matcher = re.compile(r\"\\$\\{(\\w|_)+:?.*}\")\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/utils/eta.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/utils/eta.py",
      "line_number": 1,
      "code_snippet": "import datetime\nimport logging\nimport math\nimport time\nfrom collections import deque\nfrom typing import Any\n\nlogger = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/utils/ollama.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/utils/ollama.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom collections import deque\nfrom collections.abc import Iterator, Mapping\nfrom typing import Any\n\nfrom httpx import ConnectError\nfrom tqdm import tqdm  # type: ignore\n\nfrom private_gpt.utils.retry import retry\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/llm_component.py_204_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'azopenai_settings.llm_model' is used without version pinning on line 204. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/llm_component.py",
      "line_number": 204,
      "code_snippet": "\n                azopenai_settings = settings.azopenai\n                self.llm = AzureOpenAI(\n                    model=azopenai_settings.llm_model,\n                    deployment_name=azopenai_settings.llm_deployment_name,\n                    api_key=azopenai_settings.api_key,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/llm_component.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/llm_component.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom injector import inject, singleton\nfrom llama_index.core.llms import LLM, MockLLM\nfrom llama_index.core.settings import Settings as LlamaIndexSettings\nfrom llama_index.core.utils import set_global_tokenizer\nfrom transformers import AutoTokenizer  # type: ignore\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/prompt_helper.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/prompt_helper.py",
      "line_number": 1,
      "code_snippet": "import abc\nimport logging\nfrom collections.abc import Sequence\nfrom typing import Any, Literal\n\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\nlogger = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/ingest/ingest_component.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/ingest/ingest_component.py",
      "line_number": 1,
      "code_snippet": "import abc\nimport itertools\nimport logging\nimport multiprocessing\nimport multiprocessing.pool\nimport os\nimport threading\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Any",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/ingest/ingest_helper.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/ingest/ingest_helper.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom pathlib import Path\n\nfrom llama_index.core.readers import StringIterableReader\nfrom llama_index.core.readers.base import BaseReader\nfrom llama_index.core.readers.json import JSONReader\nfrom llama_index.core.schema import Document\n\nlogger = logging.getLogger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py_126_unpinned",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "LOW",
      "confidence": 0.75,
      "title": "Unpinned model version in API call",
      "description": "Model 'azopenai_settings.embedding_model' is used without version pinning on line 126. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. (Advisory: no dynamic execution detected in this file.)",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py",
      "line_number": 126,
      "code_snippet": "\n                azopenai_settings = settings.azopenai\n                self.embedding_model = AzureOpenAIEmbedding(\n                    model=azopenai_settings.embedding_model,\n                    deployment_name=azopenai_settings.embedding_deployment_name,\n                    api_key=azopenai_settings.api_key,",
      "recommendation": "Supply Chain Security Best Practices:\n1. Pin model versions explicitly (model='gpt-4-0613')\n2. Use model registries with version control\n3. Document model versions in requirements.txt or similar\n4. Implement model versioning in CI/CD pipelines"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py_17_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in '__init__'",
      "description": "Function '__init__' on line 17 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py",
      "line_number": 17,
      "code_snippet": "\n    @inject\n    def __init__(self, settings: Settings) -> None:\n        embedding_mode = settings.embedding.mode\n        logger.info(\"Initializing the embedding model in mode=%s\", embedding_mode)\n        match embedding_mode:",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/embedding_component.py",
      "line_number": 1,
      "code_snippet": "import logging\n\nfrom injector import inject, singleton\nfrom llama_index.core.embeddings import BaseEmbedding, MockEmbedding\n\nfrom private_gpt.paths import models_cache_path\nfrom private_gpt.settings.settings import Settings\n\nlogger = logging.getLogger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/vector_store/batched_chroma.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/vector_store/batched_chroma.py",
      "line_number": 1,
      "code_snippet": "from collections.abc import Generator, Sequence\nfrom typing import TYPE_CHECKING, Any\n\nfrom llama_index.core.schema import BaseNode, MetadataMode\nfrom llama_index.core.vector_stores.utils import node_to_metadata_dict\nfrom llama_index.vector_stores.chroma import ChromaVectorStore  # type: ignore\n\nif TYPE_CHECKING:\n    from collections.abc import Mapping\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/vector_store/vector_store_component.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/vector_store/vector_store_component.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport typing\n\nfrom injector import inject, singleton\nfrom llama_index.core.indices.vector_store import VectorIndexRetriever, VectorStoreIndex\nfrom llama_index.core.vector_stores.types import (\n    BasePydanticVectorStore,\n    FilterCondition,\n    MetadataFilter,\n    MetadataFilters,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/custom/sagemaker.py_45_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_embed'",
      "description": "Function '_embed' on line 45 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/custom/sagemaker.py",
      "line_number": 45,
      "code_snippet": "            self._async_not_implemented_warned = True\n\n    def _embed(self, sentences: list[str]) -> list[list[float]]:\n        request_params = {\n            \"inputs\": sentences,\n        }",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/custom/sagemaker.py_50",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/embedding/custom/sagemaker.py",
      "line_number": 50,
      "code_snippet": "            \"inputs\": sentences,\n        }\n\n        resp = self._boto_client.invoke_endpoint(\n            EndpointName=self.endpoint_name,\n            Body=json.dumps(request_params),\n            ContentType=\"application/json\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_266",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'messages' embedded in LLM prompt",
      "description": "User input 'messages' flows to LLM call via call in variable 'prompt'. Function 'chat' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 266,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_201",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "LLM output used in dangerous sql_injection sink",
      "description": "LLM output from 'self.generate_kwargs.update' is used in 'UPDATE' on line 201 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 201,
      "code_snippet": "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})\n\n        is_formatted = kwargs.pop(\"formatted\", False)",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)\n4. Apply strict input validation\n5. Use read-only database connections where possible"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_265",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 265 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 265,
      "code_snippet": "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_200_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'complete'",
      "description": "Function 'complete' on line 200 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 200,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})\n\n        is_formatted = kwargs.pop(\"formatted\", False)",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_201",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 201,
      "code_snippet": "\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})\n\n        is_formatted = kwargs.pop(\"formatted\", False)\n        if not is_formatted:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_213",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 213,
      "code_snippet": "            \"parameters\": self.inference_params,\n        }\n\n        resp = self._boto_client.invoke_endpoint(\n            EndpointName=self.endpoint_name,\n            Body=json.dumps(request_params),\n            ContentType=\"application/json\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_267",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 267,
      "code_snippet": "    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n\n    @llm_chat_callback()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_237",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 237,
      "code_snippet": "                \"stream\": True,\n                \"parameters\": self.inference_params,\n            }\n            resp = self._boto_client.invoke_endpoint_with_response_stream(\n                EndpointName=self.endpoint_name,\n                Body=json.dumps(request_params),\n                ContentType=\"application/json\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_200",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 200,
      "code_snippet": "            num_output=self.max_new_tokens,\n            model_name=\"Sagemaker LLama 2\",\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})\n\n        is_formatted = kwargs.pop(\"formatted\", False)\n        if not is_formatted:\n            prompt = self.completion_to_prompt(prompt)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py_228",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/components/llm/custom/sagemaker.py",
      "line_number": 228,
      "code_snippet": "        return CompletionResponse(\n            text=response_dict[0][\"generated_text\"][len(prompt) :], raw=resp\n        )\n\n    @llm_completion_callback()\n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        def get_stream():\n            text = \"\"\n\n            request_params = {\n                \"inputs\": prompt,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/embeddings/embeddings_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/embeddings/embeddings_router.py",
      "line_number": 1,
      "code_snippet": "from typing import Literal\n\nfrom fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel\n\nfrom private_gpt.server.embeddings.embeddings_service import (\n    Embedding,\n    EmbeddingsService,\n)\nfrom private_gpt.server.utils.auth import authenticated",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_service.py_185",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'chat' on line 185 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_service.py",
      "line_number": 185,
      "code_snippet": "    def chat(\n        self,\n        messages: list[ChatMessage],\n        use_context: bool = False,\n        context_filter: ContextFilter | None = None,\n    ) -> Completion:\n        chat_engine_input = ChatEngineInput.from_messages(messages)\n        last_message = (\n            chat_engine_input.last_message.content\n            if chat_engine_input.last_message\n            else None",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_service.py_211",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_service.py",
      "line_number": 211,
      "code_snippet": "            use_context=use_context,\n            context_filter=context_filter,\n        )\n        wrapped_response = chat_engine.chat(\n            message=last_message if last_message is not None else \"\",\n            chat_history=chat_history,\n        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py_91",
      "category": "LLM01: Prompt Injection",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "User input 'body' embedded in LLM prompt",
      "description": "User input 'body' flows to LLM call via assignment in variable 'all_messages'. Function 'chat_completion' may be vulnerable to prompt injection attacks.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
      "line_number": 91,
      "code_snippet": "    service = request.state.injector.get(ChatService)\n    all_messages = [\n        ChatMessage(content=m.content, role=MessageRole(m.role)) for m in body.messages\n    ]\n    if body.stream:\n        completion_gen = service.stream_chat(\n            messages=all_messages,\n            use_context=body.use_context,\n            context_filter=body.context_filter,\n        )\n        return StreamingResponse(\n            to_openai_sse_stream(\n                completion_gen.response,\n                completion_gen.sources if body.include_sources else None,\n            ),\n            media_type=\"text/event-stream\",\n        )\n    else:\n        completion = service.chat(\n            messages=all_messages,",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py_65_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute/network operation without confirmation in 'chat_completion'",
      "description": "Function 'chat_completion' on line 65 performs high-risk delete/write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
      "line_number": 65,
      "code_snippet": "    },\n)\ndef chat_completion(\n    request: Request, body: ChatBody\n) -> OpenAICompletion | StreamingResponse:\n    \"\"\"Given a list of messages comprising a conversation, return a response.",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py_65_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'chat_completion'",
      "description": "Function 'chat_completion' on line 65 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
      "line_number": 65,
      "code_snippet": "    },\n)\ndef chat_completion(\n    request: Request, body: ChatBody\n) -> OpenAICompletion | StreamingResponse:\n    \"\"\"Given a list of messages comprising a conversation, return a response.",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py_65_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'chat_completion'",
      "description": "API endpoint 'chat_completion' on line 65 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
      "line_number": 65,
      "code_snippet": "    },\n)\ndef chat_completion(\n    request: Request, body: ChatBody\n) -> OpenAICompletion | StreamingResponse:\n    \"\"\"Given a list of messages comprising a conversation, return a response.",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction\n\nAdd output protection mechanisms:\n\n1. Watermarking:\n   - Embed invisible markers in outputs\n   - Make responses traceable\n   - Detect if outputs are reused\n\n2. Output obfuscation:\n   - Add controlled noise to predictions\n   - Implement differential privacy\n   - Return confidence ranges not exact values\n\n3. Response filtering:\n   - Limit information in responses\n   - Remove sensitive metadata\n   - Aggregate results when possible\n\n4. Query restrictions:\n   - Limit types of allowed queries\n   - Block probing/extraction patterns\n   - Implement query complexity limits\n\n5. Legal notices:\n   - Include usage terms in responses\n   - Add copyright notices\n   - Document intended use only"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py_108",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chat/chat_router.py",
      "line_number": 108,
      "code_snippet": "            media_type=\"text/event-stream\",\n        )\n    else:\n        completion = service.chat(\n            messages=all_messages,\n            use_context=body.use_context,\n            context_filter=body.context_filter,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/completions/completions_router.py_53_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'prompt_completion'",
      "description": "API endpoint 'prompt_completion' on line 53 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/completions/completions_router.py",
      "line_number": 53,
      "code_snippet": "    },\n)\ndef prompt_completion(\n    request: Request, body: CompletionsBody\n) -> OpenAICompletion | StreamingResponse:\n    \"\"\"We recommend most users use our Chat completions API.",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/completions/completions_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/completions/completions_router.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel\nfrom starlette.responses import StreamingResponse\n\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.open_ai.openai_models import (\n    OpenAICompletion,\n    OpenAIMessage,\n)\nfrom private_gpt.server.chat.chat_router import ChatBody, chat_completion",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py_41_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'ingest_file'",
      "description": "API endpoint 'ingest_file' on line 41 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py",
      "line_number": 41,
      "code_snippet": "\n@ingest_router.post(\"/ingest/file\", tags=[\"Ingestion\"])\ndef ingest_file(request: Request, file: UploadFile) -> IngestResponse:\n    \"\"\"Ingests and processes a file, storing its chunks to be used as context.\n\n    The context obtained from files is later used in",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py_65_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'ingest_text'",
      "description": "API endpoint 'ingest_text' on line 65 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py",
      "line_number": 65,
      "code_snippet": "\n@ingest_router.post(\"/ingest/text\", tags=[\"Ingestion\"])\ndef ingest_text(request: Request, body: IngestTextBody) -> IngestResponse:\n    \"\"\"Ingests and processes a text, storing its chunks to be used as context.\n\n    The context obtained from files is later used in",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_router.py",
      "line_number": 1,
      "code_snippet": "from typing import Literal\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request, UploadFile\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.server.ingest.ingest_service import IngestService\nfrom private_gpt.server.ingest.model import IngestedDoc\nfrom private_gpt.server.utils.auth import authenticated\n\ningest_router = APIRouter(prefix=\"/v1\", dependencies=[Depends(authenticated)])",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/ingest/ingest_service.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport tempfile\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, AnyStr, BinaryIO\n\nfrom injector import inject, singleton\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\nfrom llama_index.core.storage import StorageContext\n\nfrom private_gpt.components.embedding.embedding_component import EmbeddingComponent",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_router.py_27_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 27. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_router.py",
      "line_number": 27,
      "code_snippet": "\n@chunks_router.post(\"/chunks\", tags=[\"Context Chunks\"])\ndef chunks_retrieval(request: Request, body: ChunksBody) -> ChunksResponse:\n    \"\"\"Given a `text`, returns the most relevant chunks from the ingested documents.\n\n    The returned information can be used to generate prompts that can be",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_router.py_27_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'chunks_retrieval'",
      "description": "API endpoint 'chunks_retrieval' on line 27 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_router.py",
      "line_number": 27,
      "code_snippet": "\n@chunks_router.post(\"/chunks\", tags=[\"Context Chunks\"])\ndef chunks_retrieval(request: Request, body: ChunksBody) -> ChunksResponse:\n    \"\"\"Given a `text`, returns the most relevant chunks from the ingested documents.\n\n    The returned information can be used to generate prompts that can be",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/chunks/chunks_service.py",
      "line_number": 1,
      "code_snippet": "from typing import TYPE_CHECKING, Literal\n\nfrom injector import inject, singleton\nfrom llama_index.core.indices import VectorStoreIndex\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.storage import StorageContext\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.components.embedding.embedding_component import EmbeddingComponent\nfrom private_gpt.components.llm.llm_component import LLMComponent",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/recipes/summarize/summarize_router.py_35_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Model API without rate limiting in 'summarize'",
      "description": "API endpoint 'summarize' on line 35 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/recipes/summarize/summarize_router.py",
      "line_number": 35,
      "code_snippet": "    tags=[\"Recipes\"],\n)\ndef summarize(\n    request: Request, body: SummarizeBody\n) -> SummarizeResponse | StreamingResponse:\n    \"\"\"Given a text, the model will return a summary.",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/recipes/summarize/summarize_service.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmpy_klsmsy/privateGPT/private_gpt/server/recipes/summarize/summarize_service.py",
      "line_number": 1,
      "code_snippet": "from itertools import chain\n\nfrom injector import inject, singleton\nfrom llama_index.core import (\n    Document,\n    StorageContext,\n    SummaryIndex,\n)\nfrom llama_index.core.base.response.schema import Response, StreamingResponse\nfrom llama_index.core.node_parser import SentenceSplitter",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 68,
      "kept": 50,
      "filtered": 18,
      "reduction_pct": 26.5,
      "avg_tp_probability": 0.555,
      "filter_reasons": {
        "high severity with context": 18,
        "test file": 8,
        "build tool subprocess": 2,
        "callback handler pattern": 1,
        "base64 image": 1
      }
    }
  }
}