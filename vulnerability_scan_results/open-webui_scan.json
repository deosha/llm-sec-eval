{
  "report_type": "static_scan",
  "generated_at": "2026-01-14T14:13:24.532055Z",
  "summary": {
    "target": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui",
    "files_scanned": 205,
    "overall_score": 5.94,
    "confidence": 0.68,
    "duration_seconds": 10.219,
    "findings_count": 310,
    "severity_breakdown": {
      "CRITICAL": 193,
      "HIGH": 54,
      "MEDIUM": 55,
      "LOW": 0,
      "INFO": 8
    }
  },
  "category_scores": [
    {
      "category_id": "1_prompt_security",
      "category_name": "Prompt Security",
      "score": 22,
      "confidence": 0.48,
      "subscores": {
        "input_validation": 0,
        "jailbreak_prevention": 30,
        "monitoring_response": 25,
        "prompt_injection_defense": 0,
        "context_isolation": 75,
        "prompt_templates": 60,
        "output_filtering": 0,
        "jailbreak_detection": 0
      },
      "detected_controls": [
        "Multi-layer input validation detected",
        "1 jailbreak prevention mechanisms active",
        "Context protection: Token Limiting"
      ],
      "gaps": [
        "Missing behavioral analysis/guardrails library",
        "No red team testing detected",
        "No attack detection tools found"
      ]
    },
    {
      "category_id": "2_model_security",
      "category_name": "Model Security & Integrity",
      "score": 2,
      "confidence": 0.52,
      "subscores": {
        "model_protection": 83,
        "extraction_defense": 67,
        "supply_chain_security": 62,
        "api_key_security": 0,
        "model_access_control": 0,
        "rate_limiting_defense": 0,
        "model_encryption": 0,
        "model_provenance": 0,
        "supply_chain_verification": 0
      },
      "detected_controls": [
        "Encryption at rest",
        "Encryption in transit",
        "RBAC",
        "OAuth",
        "Rate limiting",
        "Watermarking",
        "Checksum verification",
        "Signature verification"
      ],
      "gaps": []
    },
    {
      "category_id": "3_data_privacy",
      "category_name": "Data Privacy & PII Protection",
      "score": 1,
      "confidence": 0.55,
      "subscores": {
        "pii_handling": 0,
        "consent_management": 0,
        "compliance_auditing": 0,
        "pii_detection": 0,
        "pii_redaction": 0,
        "data_encryption": 0,
        "consent_management_comprehensive": 0,
        "data_lifecycle": 0,
        "gdpr_compliance": 0,
        "audit_logging": 30
      },
      "detected_controls": [
        "PII pattern matching",
        "NER models for PII",
        "PII encryption",
        "PII masking",
        "Consent withdrawal",
        "Right to delete",
        "Audit logging"
      ],
      "gaps": [
        "PII detection and handling needs improvement",
        "Consider implementing PII detection tools (Presidio, spaCy NER)",
        "Consent management is weak",
        "Implement user consent mechanisms and user rights endpoints",
        "Regulatory compliance controls missing",
        "Add audit logging and compliance documentation"
      ]
    },
    {
      "category_id": "4_hallucination_mitigation",
      "category_name": "Hallucination Detection & Mitigation",
      "score": 0,
      "confidence": 0.38,
      "subscores": {
        "factual_consistency": 0,
        "output_validation": 0,
        "user_feedback": 0,
        "rag_integration": 0,
        "citation_tracking": 0,
        "confidence_scoring": 0,
        "fact_checking": 0,
        "consistency_checks": 0,
        "grounding": 0
      },
      "detected_controls": [],
      "gaps": [
        "Missing controls: Citation tracking / RAG, Confidence scoring, Consistency checks, Constraint checking, Cross-validation",
        "Plus 9 more controls need implementation"
      ]
    },
    {
      "category_id": "5_ethical_ai",
      "category_name": "Ethical AI & Bias Detection",
      "score": 0,
      "confidence": 0.4,
      "subscores": {
        "bias_testing": 0,
        "fairness_metrics": 0,
        "explainability": 0,
        "harmful_content_filtering": 0,
        "diverse_training": 0,
        "transparency": 0,
        "legacy_bias_detection": 0,
        "legacy_fairness": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "6_governance",
      "category_name": "Governance & Compliance",
      "score": 20,
      "confidence": 0.51,
      "subscores": {
        "plugin_security": 0,
        "authorization_controls": 75,
        "audit_logging": 0,
        "incident_response": 0,
        "model_documentation": 50,
        "risk_assessment": 0,
        "user_consent": 0,
        "legacy_governance": 0
      },
      "detected_controls": [],
      "gaps": []
    },
    {
      "category_id": "0_owasp_vulnerabilities",
      "category_name": "OWASP LLM Vulnerabilities",
      "score": 0,
      "confidence": 0.92,
      "subscores": {
        "LLM01": 100,
        "LLM02": 0,
        "LLM03": 100,
        "LLM04": 0,
        "LLM05": 2,
        "LLM06": 0,
        "LLM07": 53,
        "LLM08": 0,
        "LLM09": 10,
        "LLM10": 63
      },
      "detected_controls": [
        "Prompt Injection (no vulnerabilities found)",
        "Training Data Poisoning (no vulnerabilities found)"
      ],
      "gaps": [
        "Insecure Output Handling: 4 critical",
        "Model Denial of Service: 7 critical",
        "Supply Chain Vulnerabilities: 2 critical, 3 high",
        "Sensitive Information Disclosure: 7 critical, 29 high, 23 medium",
        "Insecure Plugin Design: 3 high",
        "Excessive Agency: 1 critical, 6 high, 5 medium",
        "Overreliance: 2 critical, 1 high",
        "Model Theft: 2 high, 1 medium",
        "ML: 170 critical, 26 medium",
        "SQLI: 10 high"
      ]
    }
  ],
  "findings": [
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/contribution_stats.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/contribution_stats.py",
      "line_number": 1,
      "code_snippet": "import os\nimport subprocess\nfrom collections import Counter\n\nCONFIG_FILE_EXTENSIONS = (\".json\", \".yml\", \".yaml\", \".ini\", \".conf\", \".toml\")\n\n\ndef is_text_file(filepath):\n    # Check for binary file by scanning for null bytes.\n    try:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py_23",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'subprocess.run' is used in 'run(' on line 23 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
      "line_number": 23,
      "code_snippet": "        os.environ[\"APP_BUILD_HASH\"] = version\n        subprocess.run([npm, \"run\", \"build\"], check=True)  # noqa: S603",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py_11_exec",
      "category": "LLM08: Excessive Agency",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM-generated code in 'initialize'",
      "description": "Function 'initialize' on line 11 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
      "line_number": 11,
      "code_snippet": "from hatchling.builders.hooks.plugin.interface import BuildHookInterface\n\n\nclass CustomBuildHook(BuildHookInterface):\n    def initialize(self, version, build_data):\n        super().initialize(version, build_data)\n        stderr.write(\">>> Building Open Webui frontend\\n\")\n        npm = shutil.which(\"npm\")\n        if npm is None:\n            raise RuntimeError(",
      "recommendation": "Code Execution Security:\n1. NEVER execute LLM-generated code directly with exec()/eval()\n2. If code execution is necessary, use sandboxed environments (Docker, VM)\n3. Implement strict code validation and static analysis before execution\n4. Use allowlists for permitted functions/modules\n5. Set resource limits (CPU, memory, time) for execution\n6. Parse and validate code structure before running\n7. Consider using safer alternatives (JSON, declarative configs)\n8. Log all code execution attempts with full context\n9. Require human review for generated code\n10. Use tools like RestrictedPython for safer Python execution\n\nHigh-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py_11_direct_execution",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Direct execution of LLM output in 'initialize'",
      "description": "Function 'initialize' on line 11 directly executes LLM-generated code using subprocess.run. This is extremely dangerous and allows arbitrary code execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
      "line_number": 11,
      "code_snippet": "\nclass CustomBuildHook(BuildHookInterface):\n    def initialize(self, version, build_data):\n        super().initialize(version, build_data)\n        stderr.write(\">>> Building Open Webui frontend\\n\")\n        npm = shutil.which(\"npm\")",
      "recommendation": "NEVER directly execute LLM-generated code:\n\n1. Remove direct execution:\n   - Do not use eval(), exec(), or os.system()\n   - Avoid dynamic code execution\n   - Use safer alternatives (allow-lists)\n\n2. If code generation is required:\n   - Generate code for review only\n   - Require human approval before execution\n   - Use sandboxing (containers, VMs)\n   - Implement strict security policies\n\n3. Use structured outputs:\n   - Return data, not code\n   - Use JSON schemas\n   - Define clear interfaces\n\n4. Add safeguards:\n   - Static code analysis before execution\n   - Whitelist allowed operations\n   - Rate limiting and monitoring"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/hatch_build.py",
      "line_number": 1,
      "code_snippet": "# noqa: INP001\nimport os\nimport shutil\nimport subprocess\nfrom sys import stderr\n\nfrom hatchling.builders.hooks.plugin.interface import BuildHookInterface\n\n\nclass CustomBuildHook(BuildHookInterface):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/functions.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport sys\nimport inspect\nimport json\nimport asyncio\n\nfrom pydantic import BaseModel\nfrom typing import AsyncGenerator, Generator, Iterator\nfrom fastapi import (\n    Depends,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tasks.py",
      "line_number": 1,
      "code_snippet": "# tasks.py\nimport asyncio\nfrom typing import Dict\nfrom uuid import uuid4\nimport json\nimport logging\nfrom redis.asyncio import Redis\nfrom fastapi import Request\nfrom typing import Dict, List, Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/config.py_2461_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 2461. File fetches external content - HIGH RISK.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/config.py",
      "line_number": 2461,
      "code_snippet": "\n####################################\n# Information Retrieval (RAG)\n####################################\n\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/config.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/config.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport os\nimport shutil\nimport base64\nimport redis\n\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Generic, Union, Optional, TypeVar",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/env.py_453_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'PASSWORD_VALIDATION_REGEX_PATTERN' containing sensitive data is being logged on line 453. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/env.py",
      "line_number": 453,
      "code_snippet": "    PASSWORD_VALIDATION_REGEX_PATTERN = re.compile(PASSWORD_VALIDATION_REGEX_PATTERN)\nexcept Exception as e:\n    log.error(f\"Invalid PASSWORD_VALIDATION_REGEX_PATTERN: {e}\")\n    PASSWORD_VALIDATION_REGEX_PATTERN = re.compile(\n        r\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[^\\w\\s]).{8,}$\"",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/env.py",
      "line_number": 1,
      "code_snippet": "import importlib.metadata\nimport json\nimport logging\nimport os\nimport pkgutil\nimport sys\nimport shutil\nfrom uuid import uuid4\nfrom pathlib import Path\nfrom cryptography.hazmat.primitives import serialization",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py_34",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 34. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
      "line_number": 34,
      "code_snippet": "        \"Uh-oh! This username is already registered. Please choose another username.\"\n    )\n    PASSWORD_TOO_LONG = \"Uh-oh! The password you entered is too long. Please make sure your password is less than 72 bytes long.\"\n    COMMAND_TAKEN = \"Uh-oh! This command is already registered. Please choose another command string.\"\n    FILE_EXISTS = \"Uh-oh! This file is already registered. Please choose another file.\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py_67",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 67. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
      "line_number": 67,
      "code_snippet": "    USER_NOT_FOUND = \"We could not find what you're looking for :/\"\n    API_KEY_NOT_FOUND = \"Oops! It looks like there's a hiccup. The API key is missing. Please make sure to provide a valid API key to access this feature.\"\n    API_KEY_NOT_ALLOWED = \"Use of API key is not enabled in the environment.\"\n\n    MALICIOUS = \"Unusual activities detected, please try again in a few minutes.\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py_80",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 80. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
      "line_number": 80,
      "code_snippet": "    OPENAI_NOT_FOUND = lambda name=\"\": \"OpenAI API was not found\"\n    OLLAMA_NOT_FOUND = \"WebUI could not connect to Ollama\"\n    CREATE_API_KEY_ERROR = \"Oops! Something went wrong while creating your API key. Please try again later. If the issue persists, contact support for assistance.\"\n    API_KEY_CREATION_NOT_ALLOWED = \"API key creation is not allowed in the environment.\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py_81",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 81. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
      "line_number": 81,
      "code_snippet": "    OLLAMA_NOT_FOUND = \"WebUI could not connect to Ollama\"\n    CREATE_API_KEY_ERROR = \"Oops! Something went wrong while creating your API key. Please try again later. If the issue persists, contact support for assistance.\"\n    API_KEY_CREATION_NOT_ALLOWED = \"API key creation is not allowed in the environment.\"\n\n    EMPTY_CONTENT = \"The content provided is empty. Please ensure that there is text or data present before proceeding.\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/constants.py",
      "line_number": 1,
      "code_snippet": "from enum import Enum\n\n\nclass MESSAGES(str, Enum):\n    DEFAULT = lambda msg=\"\": f\"{msg if msg else ''}\"\n    MODEL_ADDED = lambda model=\"\": f\"The model '{model}' has been added successfully.\"\n    MODEL_DELETED = (\n        lambda model=\"\": f\"The model '{model}' has been deleted successfully.\"\n    )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py_34_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute operation without confirmation in 'serve'",
      "description": "Function 'serve' on line 34 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py",
      "line_number": 34,
      "code_snippet": "\n@app.command()\ndef serve(\n    host: str = \"0.0.0.0\",\n    port: int = 8080,\n):",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py_88_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute operation without confirmation in 'dev'",
      "description": "Function 'dev' on line 88 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py",
      "line_number": 88,
      "code_snippet": "\n@app.command()\ndef dev(\n    host: str = \"0.0.0.0\",\n    port: int = 8080,\n    reload: bool = True,",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/__init__.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport os\nimport random\nfrom pathlib import Path\n\nimport typer\nimport uvicorn\nfrom typing import Optional\nfrom typing_extensions import Annotated\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/main.py_1516",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/main.py",
      "line_number": 1516,
      "code_snippet": "    models = get_filtered_models(models, user)\n\n    log.debug(\n        f\"/api/models returned filtered models accessible to the user: {json.dumps([model.get('id') for model in models])}\"\n    )\n    return {\"data\": models}\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/functions.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\n\nimport logging\nimport aiohttp\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom open_webui.env import AIOHTTP_CLIENT_TIMEOUT\nfrom open_webui.models.functions import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/files.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport uuid\nimport json\nfrom pathlib import Path\nfrom typing import Optional\nfrom urllib.parse import quote\nimport asyncio\n\nfrom fastapi import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/tasks.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/tasks.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter, Depends, HTTPException, Response, status, Request\nfrom fastapi.responses import JSONResponse, RedirectResponse\n\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport logging\nimport re\n\nfrom open_webui.utils.chat import generate_chat_completion\nfrom open_webui.utils.task import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/configs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/configs.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport copy\nfrom fastapi import APIRouter, Depends, Request, HTTPException\nfrom pydantic import BaseModel, ConfigDict\nimport aiohttp\n\nfrom typing import Optional\n\nfrom open_webui.env import AIOHTTP_CLIENT_TIMEOUT\nfrom open_webui.utils.auth import get_admin_user, get_verified_user",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py_346_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'get_model_profile_image'",
      "description": "Function 'get_model_profile_image' on line 346 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py",
      "line_number": 346,
      "code_snippet": "\n@router.get(\"/model/profile/image\")\ndef get_model_profile_image(\n    id: str, user=Depends(get_verified_user), db: Session = Depends(get_session)\n):\n    model = Models.get_model_by_id(id, db=db)",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py_346_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Model API without rate limiting in 'get_model_profile_image'",
      "description": "API endpoint 'get_model_profile_image' on line 346 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py",
      "line_number": 346,
      "code_snippet": "\n@router.get(\"/model/profile/image\")\ndef get_model_profile_image(\n    id: str, user=Depends(get_verified_user), db: Session = Depends(get_session)\n):\n    model = Models.get_model_by_id(id, db=db)",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py_321",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py",
      "line_number": 321,
      "code_snippet": "            or has_access(user.id, \"read\", model.access_control, db=db)\n        ):\n            return ModelAccessResponse(\n                **model.model_dump(),\n                write_access=(\n                    (user.role == \"admin\" and BYPASS_ADMIN_ACCESS_CONTROL)\n                    or user.id == model.user_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py_97",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/models.py",
      "line_number": 97,
      "code_snippet": "    return ModelAccessListResponse(\n        items=[\n            ModelAccessResponse(\n                **model.model_dump(),\n                write_access=(\n                    (user.role == \"admin\" and BYPASS_ADMIN_ACCESS_CONTROL)\n                    or user.id == model.user_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/users.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\nfrom sqlalchemy.orm import Session\nimport base64\nimport io\n\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request, status\nfrom fastapi.responses import Response, StreamingResponse, FileResponse\nfrom pydantic import BaseModel, ConfigDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/tools.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom pathlib import Path\nfrom typing import Optional\nimport time\nimport re\nimport aiohttp\nfrom open_webui.env import AIOHTTP_CLIENT_TIMEOUT\nfrom open_webui.models.groups import Groups\nfrom pydantic import BaseModel, HttpUrl\nfrom fastapi import APIRouter, Depends, HTTPException, Request, status",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/groups.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/groups.py",
      "line_number": 1,
      "code_snippet": "import os\nfrom pathlib import Path\nfrom typing import Optional\nimport logging\n\nfrom open_webui.models.users import Users, UserInfoResponse\nfrom open_webui.models.groups import (\n    Groups,\n    GroupForm,\n    GroupUpdateForm,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py_156",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function '_compute_similarities' on line 156 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py",
      "line_number": 156,
      "code_snippet": "def _compute_similarities(feedbacks: list[LeaderboardFeedbackData], query: str) -> dict:\n    \"\"\"\n    Compute how relevant each feedback is to a search query.\n\n    Uses embeddings to find semantic similarity between the query and\n    each feedback's tags. Higher similarity means the feedback is more\n    relevant to what the user searched for.\n\n    This is used to weight Elo calculations - feedbacks matching the\n    query have more influence on the final rankings.\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py_156_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in '_compute_similarities'",
      "description": "Function '_compute_similarities' on line 156 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py",
      "line_number": 156,
      "code_snippet": "\n\ndef _compute_similarities(feedbacks: list[LeaderboardFeedbackData], query: str) -> dict:\n    \"\"\"\n    Compute how relevant each feedback is to a search query.\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py_187",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py",
      "line_number": 187,
      "code_snippet": "        return {}\n\n    try:\n        tag_embeddings = embedding_model.encode(all_tags)\n        query_embedding = embedding_model.encode([query])[0]\n    except Exception as e:\n        log.error(f\"Embedding error: {e}\")",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py_188",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py",
      "line_number": 188,
      "code_snippet": "\n    try:\n        tag_embeddings = embedding_model.encode(all_tags)\n        query_embedding = embedding_model.encode([query])[0]\n    except Exception as e:\n        log.error(f\"Embedding error: {e}\")\n        return {}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py_156",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** Function with user input parameter and LLM call\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/evaluations.py",
      "line_number": 156,
      "code_snippet": "        ]\n        for model_id, tags in tag_counts.items()\n    }\n\n\ndef _compute_similarities(feedbacks: list[LeaderboardFeedbackData], query: str) -> dict:\n    \"\"\"\n    Compute how relevant each feedback is to a search query.\n\n    Uses embeddings to find semantic similarity between the query and\n    each feedback's tags. Higher similarity means the feedback is more",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/notes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/notes.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom typing import Optional\n\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request, status, BackgroundTasks\nfrom pydantic import BaseModel\n\nfrom open_webui.socket.main import sio\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/prompts.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\nfrom fastapi import APIRouter, Depends, HTTPException, status, Request\n\nfrom open_webui.models.prompts import (\n    PromptForm,\n    PromptUserResponse,\n    PromptAccessResponse,\n    PromptModel,\n    Prompts,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/auths.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/auths.py",
      "line_number": 1,
      "code_snippet": "import re\nimport uuid\nimport time\nimport datetime\nimport logging\nfrom aiohttp import ClientSession\nimport urllib\n\n\nfrom open_webui.models.auths import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py_506",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'get_merged_models' on line 506 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py",
      "line_number": 506,
      "code_snippet": "    def get_merged_models(model_lists):\n        log.debug(f\"merge_models_lists {model_lists}\")\n        models = {}\n\n        for idx, model_list in enumerate(model_lists):\n            if model_list is not None and \"error\" not in model_list:\n                for model in model_list:\n                    model_id = model.get(\"id\") or model.get(\"name\")\n\n                    if (\n                        \"api.openai.com\"",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py_169_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 169. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py",
      "line_number": 169,
      "code_snippet": "                )\n        except Exception as e:\n            log.error(f\"Error getting OAuth token: {e}\")\n\n        if oauth_token:",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py_197_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 197. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py",
      "line_number": 197,
      "code_snippet": "        return token_provider()\n    except Exception as e:\n        log.error(f\"Error getting Microsoft Entra ID access token: {e}\")\n        return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py_506_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk network operation without confirmation in 'get_merged_models'",
      "description": "Function 'get_merged_models' on line 506 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py",
      "line_number": 506,
      "code_snippet": "        return True\n\n    def get_merged_models(model_lists):\n        log.debug(f\"merge_models_lists {model_lists}\")\n        models = {}\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py_882",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/openai.py",
      "line_number": 882,
      "code_snippet": "        payload[\"model\"] = payload[\"model\"].replace(f\"{prefix_id}.\", \"\")\n\n    # Add user info to the payload if the model is a pipeline\n    if \"pipeline\" in model and model.get(\"pipeline\"):\n        payload[\"user\"] = {\n            \"name\": user.name,\n            \"id\": user.id,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/scim.py_281_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'scim_token' containing sensitive data is being logged on line 281. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/scim.py",
      "line_number": 281,
      "code_snippet": "        if hasattr(scim_token, \"value\"):\n            scim_token = scim_token.value\n        log.debug(f\"SCIM token configured: {bool(scim_token)}\")\n        if not scim_token or token != scim_token:\n            raise HTTPException(",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/scim.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/scim.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nExperimental SCIM 2.0 Implementation for Open WebUI\nProvides System for Cross-domain Identity Management endpoints for users and groups\n\nNOTE: This is an experimental implementation and may not fully comply with SCIM 2.0 standards, and is subject to change.\n\"\"\"\n\nimport logging\nimport uuid\nimport time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/utils.py_94",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/utils.py",
      "line_number": 94,
      "code_snippet": "    form_data: ChatTitleMessagesForm, user=Depends(get_verified_user)\n):\n    try:\n        pdf_bytes = PDFGenerator(form_data).generate_chat_pdf()\n\n        return Response(\n            content=pdf_bytes,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py_1562",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'items' flows to 'VECTOR_DB_CLIENT.insert' on line 1562 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py",
      "line_number": 1562,
      "code_snippet": "        log.info(f\"adding to collection {collection_name}\")\n        VECTOR_DB_CLIENT.insert(\n            collection_name=collection_name,\n            items=items,",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py_153_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'get_rf'",
      "description": "Function 'get_rf' on line 153 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py",
      "line_number": 153,
      "code_snippet": "\n\ndef get_rf(\n    engine: str = \"\",\n    reranking_model: Optional[str] = None,\n    external_reranker_url: str = \"\",",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/retrieval.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport mimetypes\nimport os\nimport shutil\nimport asyncio\n\nimport re\nimport uuid\nfrom datetime import datetime",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/images.py_380_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "MEDIUM",
      "confidence": 0.75,
      "title": "Model API without rate limiting in 'get_models'",
      "description": "API endpoint 'get_models' on line 380 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/images.py",
      "line_number": 380,
      "code_snippet": "\n@router.get(\"/models\")\ndef get_models(request: Request, user=Depends(get_verified_user)):\n    try:\n        if request.app.state.config.IMAGE_GENERATION_ENGINE == \"openai\":\n            return [",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/images.py_699",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/images.py",
      "line_number": 699,
      "code_snippet": "                        request, image_data, content_type, {**data, **metadata}, user\n                    )\n                    images.append({\"url\": url})\n            elif model.endswith(\":generateContent\"):\n                for image in res[\"candidates\"]:\n                    for part in image[\"content\"][\"parts\"]:\n                        if part.get(\"inlineData\", {}).get(\"data\"):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py_90_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in 'get_session_user_chat_usage_stats'",
      "description": "Function 'get_session_user_chat_usage_stats' on line 90 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
      "line_number": 90,
      "code_snippet": "\n@router.get(\"/stats/usage\", response_model=ChatUsageStatsListResponse)\ndef get_session_user_chat_usage_stats(\n    items_per_page: Optional[int] = 50,\n    page: Optional[int] = 1,\n    user=Depends(get_verified_user),",
      "recommendation": "Critical security, data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "LLM10_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py_90_unrestricted_api",
      "category": "LLM10: Model Theft",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Model API without rate limiting in 'get_session_user_chat_usage_stats'",
      "description": "API endpoint 'get_session_user_chat_usage_stats' on line 90 provides model access without rate limiting. This allows attackers to make unlimited queries to extract model knowledge, potentially stealing intellectual property or sensitive training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
      "line_number": 90,
      "code_snippet": "\n@router.get(\"/stats/usage\", response_model=ChatUsageStatsListResponse)\ndef get_session_user_chat_usage_stats(\n    items_per_page: Optional[int] = 50,\n    page: Optional[int] = 1,\n    user=Depends(get_verified_user),",
      "recommendation": "Implement rate limiting to prevent model extraction:\n\n1. Add rate limiting:\n   - Use Flask-Limiter, fastapi-limiter, or similar\n   - Set per-IP and per-user limits\n   - Example: @limiter.limit(\"100/hour\")\n\n2. Implement request quotas:\n   - Track API usage per user\n   - Enforce monthly/daily quotas\n   - Alert on suspicious patterns\n\n3. Add authentication:\n   - Require API keys for access\n   - Track usage by authenticated user\n   - Revoke keys for abuse\n\n4. Monitor for extraction attempts:\n   - Log all requests with metadata\n   - Detect unusual query patterns\n   - Alert on high-frequency access\n\n5. Implement CAPTCHA:\n   - For public endpoints\n   - Triggered after N requests\n   - Prevents automated extraction"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py_240",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
      "line_number": 240,
      "code_snippet": "                )\n            return 0\n\n        messages_map = chat.chat.get(\"history\", {}).get(\"messages\", {})\n        message_id = chat.chat.get(\"history\", {}).get(\"currentId\")\n\n        history_models = {}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py_241",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
      "line_number": 241,
      "code_snippet": "            return 0\n\n        messages_map = chat.chat.get(\"history\", {}).get(\"messages\", {})\n        message_id = chat.chat.get(\"history\", {}).get(\"currentId\")\n\n        history_models = {}\n        history_message_count = len(messages_map)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py_107",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
      "line_number": 107,
      "code_snippet": "\n        chat_stats = []\n        for chat in chats:\n            messages_map = chat.chat.get(\"history\", {}).get(\"messages\", {})\n            message_id = chat.chat.get(\"history\", {}).get(\"currentId\")\n\n            if messages_map and message_id:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py_108",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/chats.py",
      "line_number": 108,
      "code_snippet": "        chat_stats = []\n        for chat in chats:\n            messages_map = chat.chat.get(\"history\", {}).get(\"messages\", {})\n            message_id = chat.chat.get(\"history\", {}).get(\"currentId\")\n\n            if messages_map and message_id:\n                try:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/memories.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/memories.py",
      "line_number": 1,
      "code_snippet": "from fastapi import APIRouter, Depends, HTTPException, Request, status\nfrom pydantic import BaseModel\nimport logging\nimport asyncio\nfrom typing import Optional\n\nfrom open_webui.models.memories import Memories, MemoryModel\nfrom open_webui.retrieval.vector.factory import VECTOR_DB_CLIENT\nfrom open_webui.utils.auth import get_verified_user\nfrom open_webui.internal.db import get_session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py_954_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'api_key' containing sensitive data is included in a prompt string on line 954. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py",
      "line_number": 954,
      "code_snippet": "                    json=payload,\n                    headers={\n                        \"Authorization\": f\"Bearer {api_key}\",\n                        \"Content-Type\": \"application/json\",\n                    },",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py_998_secret_in_prompt",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Sensitive data passed into LLM prompt",
      "description": "Variable 'api_key' containing sensitive data is included in a prompt string on line 998. This can lead to data leakage through model outputs, logs, or training data.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py",
      "line_number": 998,
      "code_snippet": "                        data=data_form,\n                        headers={\n                            \"Authorization\": f\"Bearer {api_key}\",\n                        },\n                    )",
      "recommendation": "Sensitive Data in Prompts:\n1. Never include PII (SSN, credit cards, emails) in prompts\n2. Redact or anonymize sensitive data before sending to LLM\n3. Use data masking: 'SSN: ***-**-1234'\n4. Consider using PII detection libraries before LLM calls\n5. Implement data classification to identify sensitive fields"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py_603",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/audio.py",
      "line_number": 603,
      "code_snippet": "            )\n\n        model = request.app.state.faster_whisper_model\n        segments, info = model.transcribe(\n            file_path,\n            beam_size=5,\n            vad_filter=WHISPER_VAD_FILTER,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/channels.py_1047",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/channels.py",
      "line_number": 1047,
      "code_snippet": "                        message_model_id = thread_message.meta.get(\"model_id\", None)\n                        message_model = MODELS.get(message_model_id, None)\n                        username = (\n                            message_model.get(\"name\", message_model_id)\n                            if message_model\n                            else message_model_id\n                        )",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/channels.py_1070",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/channels.py",
      "line_number": 1070,
      "code_snippet": "                thread_history_string = \"\\n\\n\".join(thread_history)\n                system_message = {\n                    \"role\": \"system\",\n                    \"content\": f\"You are {model.get('name', model_id)}, participating in a threaded conversation. Be concise and conversational.\"\n                    + (\n                        f\"Here's the thread history:\\n\\n\\n{thread_history_string}\\n\\n\\nContinue the conversation naturally as {model.get('name', model_id)}, addressing the most recent message while being aware of the full context.\"\n                        if thread_history",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/channels.py_1072",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.8,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/channels.py",
      "line_number": 1072,
      "code_snippet": "                    \"role\": \"system\",\n                    \"content\": f\"You are {model.get('name', model_id)}, participating in a threaded conversation. Be concise and conversational.\"\n                    + (\n                        f\"Here's the thread history:\\n\\n\\n{thread_history_string}\\n\\n\\nContinue the conversation naturally as {model.get('name', model_id)}, addressing the most recent message while being aware of the full context.\"\n                        if thread_history\n                        else \"\"\n                    ),",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/folders.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/folders.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nimport shutil\nimport uuid\nfrom pathlib import Path\nfrom typing import Optional\nfrom pydantic import BaseModel\nimport mimetypes\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/knowledge.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/knowledge.py",
      "line_number": 1,
      "code_snippet": "from typing import List, Optional\nfrom pydantic import BaseModel\nfrom fastapi import APIRouter, Depends, HTTPException, status, Request, Query\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.concurrency import run_in_threadpool\nimport logging\nimport io\nimport zipfile\n\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/ollama.py_309",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits",
      "description": "Function 'merge_ollama_models_lists' on line 309 has 4 DoS risk(s): LLM calls in loops, No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/ollama.py",
      "line_number": 309,
      "code_snippet": "def merge_ollama_models_lists(model_lists):\n    merged_models = {}\n\n    for idx, model_list in enumerate(model_lists):\n        if model_list is not None:\n            for model in model_list:\n                id = model.get(\"model\")\n                if id is not None:\n                    if id not in merged_models:\n                        model[\"urls\"] = [idx]\n                        merged_models[id] = model",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/ollama.py_1194",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/ollama.py",
      "line_number": 1194,
      "code_snippet": "\n    prefix_id = api_config.get(\"prefix_id\", None)\n    if prefix_id:\n        form_data.model = form_data.model.replace(f\"{prefix_id}.\", \"\")\n\n    return await send_post_request(\n        url=f\"{url}/api/generate\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/pipelines.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/routers/pipelines.py",
      "line_number": 1,
      "code_snippet": "from fastapi import (\n    Depends,\n    FastAPI,\n    File,\n    Form,\n    HTTPException,\n    Request,\n    UploadFile,\n    status,\n    APIRouter,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py_34_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 34 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
      "line_number": 34,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py_58_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk delete/write/execute operation without confirmation in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 58 performs high-risk delete/write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
      "line_number": 58,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py_34_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'run_migrations_offline'",
      "description": "Function 'run_migrations_offline' on line 34 makes critical financial decisions based on LLM output without human oversight or verification. Action edges detected (HTTP/file/DB/subprocess) - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
      "line_number": 34,
      "code_snippet": "\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL",
      "recommendation": "Critical financial decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py_58_automated_action",
      "category": "LLM09: Overreliance",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Automated action without confidence threshold in 'run_migrations_online'",
      "description": "Function 'run_migrations_online' on line 58 automatically executes actions based on LLM output without checking confidence thresholds or validating output. Action edges detected - risk of automated execution.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
      "line_number": 58,
      "code_snippet": "\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine",
      "recommendation": "Implement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/env.py",
      "line_number": 1,
      "code_snippet": "from logging.config import fileConfig\n\nfrom alembic import context\nfrom open_webui.models.auths import Auth\nfrom open_webui.env import DATABASE_URL, DATABASE_PASSWORD\nfrom sqlalchemy import engine_from_config, pool, create_engine\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tools/builtin.py_836",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tools/builtin.py",
      "line_number": 836,
      "code_snippet": "\n        # Extract messages from history\n        messages = []\n        history = chat.chat.get(\"history\", {})\n        msg_dict = history.get(\"messages\", {})\n\n        # Build message chain from currentId",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tools/builtin.py_772",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/tools/builtin.py",
      "line_number": 772,
      "code_snippet": "\n            # Find a matching message snippet\n            snippet = \"\"\n            messages = chat.chat.get(\"history\", {}).get(\"messages\", {})\n            lower_query = query.lower()\n\n            for msg_id, msg in messages.items():",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py_62",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "LLM output used in dangerous command_injection sink",
      "description": "LLM output from 'router.run' is used in 'run(' on line 62 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py",
      "line_number": 62,
      "code_snippet": "        router = Router(db, logger=log, migrate_dir=migrate_dir)\n        router.run()\n        db.close()\n",
      "recommendation": "Mitigations for Command Injection:\n1. Never pass LLM output to shell commands\n2. Use subprocess with shell=False and list arguments\n3. Apply allowlist validation for expected values\n4. Use shlex.quote() if shell execution is unavoidable\n5. Consider alternative APIs that don't use shell"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py_55_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk execute/network operation without confirmation in 'handle_peewee_migration'",
      "description": "Function 'handle_peewee_migration' on line 55 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py",
      "line_number": 55,
      "code_snippet": "# Workaround to handle the peewee migration\n# This is required to ensure the peewee migration is handled before the alembic migration\ndef handle_peewee_migration(DATABASE_URL):\n    # db = None\n    try:\n        # Replace the postgresql:// with postgres:// to handle the peewee migration",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/db.py",
      "line_number": 1,
      "code_snippet": "import os\nimport json\nimport logging\nfrom contextlib import contextmanager\nfrom typing import Any, Optional\n\nfrom open_webui.internal.wrappers import register_connection\nfrom open_webui.env import (\n    OPEN_WEBUI_DIR,\n    DATABASE_URL,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/wrappers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/wrappers.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom contextvars import ContextVar\n\nfrom peewee import *\nfrom peewee import InterfaceError as PeeWeeInterfaceError\nfrom peewee import PostgresqlDatabase\nfrom playhouse.db_url import connect, parse\nfrom playhouse.shortcuts import ReconnectMixin\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/auth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/auth.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport uuid\nimport jwt\nimport base64\nimport hmac\nimport hashlib\nimport requests\nimport os\nimport bcrypt\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/misc.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/misc.py",
      "line_number": 1,
      "code_snippet": "import hashlib\nimport re\nimport threading\nimport time\nimport uuid\nimport logging\nfrom datetime import timedelta\nfrom pathlib import Path\nfrom typing import Callable, Optional, Sequence, Union\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/files.py",
      "line_number": 1,
      "code_snippet": "from open_webui.routers.images import (\n    get_image_data,\n    upload_image,\n)\n\nfrom fastapi import (\n    APIRouter,\n    Depends,\n    HTTPException,\n    Request,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/task.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/task.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport math\nimport re\nfrom datetime import datetime\nfrom typing import Optional, Any\nimport uuid\n\n\nfrom open_webui.utils.misc import get_last_user_message, get_messages_content\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/code_interpreter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/code_interpreter.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport json\nimport logging\nimport uuid\nfrom typing import Optional\n\nimport aiohttp\nimport websockets\nfrom pydantic import BaseModel\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/audit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/audit.py",
      "line_number": 1,
      "code_snippet": "from contextlib import asynccontextmanager\nfrom dataclasses import asdict, dataclass\nfrom enum import Enum\nimport re\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Dict,\n    MutableMapping,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_340",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'check_model_access' on line 340 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 340,
      "code_snippet": "def check_model_access(user, model, db=None):\n    if model.get(\"arena\"):\n        if not has_access(\n            user.id,\n            type=\"read\",\n            access_control=model.get(\"info\", {})\n            .get(\"meta\", {})\n            .get(\"access_control\", {}),\n            db=db,\n        ):\n            raise Exception(\"Model not found\")",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_364",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_filtered_models' on line 364 has 5 DoS risk(s): LLM calls in loops, No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 364,
      "code_snippet": "def get_filtered_models(models, user, db=None):\n    # Filter out models that the user does not have access to\n    if (\n        user.role == \"user\"\n        or (user.role == \"admin\" and not BYPASS_ADMIN_ACCESS_CONTROL)\n    ) and not BYPASS_MODEL_ACCESS_CONTROL:\n        model_ids = [model[\"id\"] for model in models if not model.get(\"arena\")]\n        model_infos = {\n            model_info.id: model_info\n            for model_info in Models.get_models_by_ids(model_ids)\n        }",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_364_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk admin operation without confirmation in 'get_filtered_models'",
      "description": "Function 'get_filtered_models' on line 364 performs high-risk admin operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 364,
      "code_snippet": "\n\ndef get_filtered_models(models, user, db=None):\n    # Filter out models that the user does not have access to\n    if (\n        user.role == \"user\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_364_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_filtered_models'",
      "description": "Function 'get_filtered_models' on line 364 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 364,
      "code_snippet": "\n\ndef get_filtered_models(models, user, db=None):\n    # Filter out models that the user does not have access to\n    if (\n        user.role == \"user\"",
      "recommendation": "Critical security decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_341",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 341,
      "code_snippet": "\n\ndef check_model_access(user, model, db=None):\n    if model.get(\"arena\"):\n        if not has_access(\n            user.id,\n            type=\"read\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_40",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 40,
      "code_snippet": "\n\nasync def fetch_ollama_models(request: Request, user: UserModel = None):\n    raw_ollama_models = await ollama.get_all_models(request, user=user)\n    return [\n        {\n            \"id\": model[\"model\"],",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_57",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 57,
      "code_snippet": "\n\nasync def fetch_openai_models(request: Request, user: UserModel = None):\n    openai_response = await openai.get_all_models(request, user=user)\n    return openai_response[\"data\"]\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_93",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 93,
      "code_snippet": "        request.app.state.BASE_MODELS = base_models\n\n    # deep copy the base models to avoid modifying the original list\n    models = [model.copy() for model in base_models]\n\n    # If there are no models, return an empty list\n    if len(models) == 0:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_381",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 381,
      "code_snippet": "            group.id for group in Groups.get_groups_by_member_id(user.id, db=db)\n        }\n        for model in models:\n            if model.get(\"arena\"):\n                if has_access(\n                    user.id,\n                    type=\"read\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_345",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 345,
      "code_snippet": "        if not has_access(\n            user.id,\n            type=\"read\",\n            access_control=model.get(\"info\", {})\n            .get(\"meta\", {})\n            .get(\"access_control\", {}),\n            db=db,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_370",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 370,
      "code_snippet": "        user.role == \"user\"\n        or (user.role == \"admin\" and not BYPASS_ADMIN_ACCESS_CONTROL)\n    ) and not BYPASS_MODEL_ACCESS_CONTROL:\n        model_ids = [model[\"id\"] for model in models if not model.get(\"arena\")]\n        model_infos = {\n            model_info.id: model_info\n            for model_info in Models.get_models_by_ids(model_ids)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py_385",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/models.py",
      "line_number": 385,
      "code_snippet": "                if has_access(\n                    user.id,\n                    type=\"read\",\n                    access_control=model.get(\"info\", {})\n                    .get(\"meta\", {})\n                    .get(\"access_control\", {}),\n                    user_group_ids=user_group_ids,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/webhook.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/webhook.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport aiohttp\n\nfrom open_webui.config import WEBUI_FAVICON_URL\nfrom open_webui.env import AIOHTTP_CLIENT_TIMEOUT, VERSION\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/pdf_generator.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/pdf_generator.py",
      "line_number": 1,
      "code_snippet": "from datetime import datetime\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nfrom html import escape\n\nfrom markdown import markdown\n\nimport site\nfrom fpdf import FPDF",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/security_headers.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/security_headers.py",
      "line_number": 1,
      "code_snippet": "import re\nimport os\n\nfrom fastapi import Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom typing import Dict\n\n\nclass SecurityHeadersMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py_341_fetch_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "CRITICAL",
      "confidence": 0.7,
      "title": "Network fetch combined with code execution",
      "description": "This file downloads external content (lines [773, 970]) and executes code (lines [341, 536, 867]). This pattern enables remote code execution attacks if the fetched content is not properly validated.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py",
      "line_number": 341,
      "code_snippet": "                        def make_tool_function(\n                            function_name, tool_server_data, headers\n                        ):\n                            async def tool_function(**kwargs):\n                                return await execute_tool_server(\n                                    url=tool_server_data[\"url\"],\n                                    headers=headers,\n                                    cookies=cookies,\n                                    name=function_name,\n                                    params=kwargs,",
      "recommendation": "Secure Remote Code Patterns:\n1. NEVER execute code fetched from network without verification\n2. Use cryptographic signatures to verify downloaded code\n3. Pin URLs and verify checksums\n4. Use package managers instead of direct downloads\n5. Sandbox execution in isolated environments"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py_385_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'get_builtin_tools'",
      "description": "Function 'get_builtin_tools' on line 385 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py",
      "line_number": 385,
      "code_snippet": "\n\ndef get_builtin_tools(\n    request: Request, extra_params: dict, features: dict = None, model: dict = None\n) -> dict[str, dict]:\n    \"\"\"",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py_385_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_builtin_tools'",
      "description": "Function 'get_builtin_tools' on line 385 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py",
      "line_number": 385,
      "code_snippet": "\n\ndef get_builtin_tools(\n    request: Request, extra_params: dict, features: dict = None, model: dict = None\n) -> dict[str, dict]:\n    \"\"\"",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py_412",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/tools.py",
      "line_number": 412,
      "code_snippet": "    # Knowledge base tools - conditional injection based on model knowledge\n    # If model has attached knowledge (any type), only provide query_knowledge_files\n    # Otherwise, provide all KB browsing tools\n    model_knowledge = model.get(\"info\", {}).get(\"meta\", {}).get(\"knowledge\", [])\n    if model_knowledge:\n        # Model has attached knowledge - only allow semantic search within it\n        builtin_functions.append(query_knowledge_files)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/response.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/response.py",
      "line_number": 1,
      "code_snippet": "import json\nfrom uuid import uuid4\nfrom open_webui.utils.misc import (\n    openai_chat_chunk_message_template,\n    openai_chat_completion_message_template,\n)\n\n\ndef convert_ollama_tool_call_to_openai(tool_calls: list) -> list:\n    openai_tool_calls = []",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/logger.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/logger.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport sys\nfrom typing import TYPE_CHECKING\n\nfrom loguru import logger\nfrom opentelemetry import trace\nfrom open_webui.env import (\n    ENABLE_AUDIT_STDOUT,\n    ENABLE_AUDIT_LOGS_FILE,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/payload.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/payload.py",
      "line_number": 1,
      "code_snippet": "from open_webui.utils.task import prompt_template, prompt_variables_template\nfrom open_webui.utils.misc import (\n    deep_update,\n    add_or_update_system_message,\n    replace_system_message_content,\n)\n\nfrom typing import Callable, Optional\nimport copy\nimport json",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py_266",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
      "line_number": 266,
      "code_snippet": "                    \"selected_model_id\": selected_model_id,\n                }\n\n        if model.get(\"pipe\"):\n            # Below does not require bypass_filter because this is the only route the uses this function and it is already bypassing the filter\n            return await generate_function_chat_completion(\n                request, form_data, user=user, models=models",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py_271",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
      "line_number": 271,
      "code_snippet": "            return await generate_function_chat_completion(\n                request, form_data, user=user, models=models\n            )\n        if model.get(\"owned_by\") == \"ollama\":\n            # Using /ollama/api/chat endpoint\n            form_data = convert_payload_openai_to_ollama(form_data)\n            response = await generate_ollama_chat_completion(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py_227",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
      "line_number": 227,
      "code_snippet": "                model_ids = [\n                    model[\"id\"]\n                    for model in list(request.app.state.MODELS.values())\n                    if model.get(\"owned_by\") != \"arena\"\n                ]\n                selected_model_id = random.choice(model_ids)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py_217",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/chat.py",
      "line_number": 217,
      "code_snippet": "                model_ids = [\n                    model[\"id\"]\n                    for model in list(request.app.state.MODELS.values())\n                    if model.get(\"owned_by\") != \"arena\" and model[\"id\"] not in model_ids\n                ]\n\n            selected_model_id = None",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/embeddings.py_75",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/embeddings.py",
      "line_number": 75,
      "code_snippet": "            check_model_access(user, model)\n\n    # Ollama backend\n    if model.get(\"owned_by\") == \"ollama\":\n        ollama_payload = convert_embedding_payload_openai_to_ollama(form_data)\n        response = await ollama_embeddings(\n            request=request,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/redis.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/redis.py",
      "line_number": 1,
      "code_snippet": "import inspect\nfrom urllib.parse import urlparse\n\nimport logging\n\nimport redis\n\nfrom open_webui.env import (\n    REDIS_CLUSTER,\n    REDIS_SOCKET_CONNECT_TIMEOUT,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/rate_limit.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/rate_limit.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Optional, Dict\nfrom open_webui.env import REDIS_KEY_PREFIX\n\n\nclass RateLimiter:\n    \"\"\"\n    General-purpose rate limiter using Redis with a rolling window strategy.\n    Falls back to in-memory storage if Redis is not available.\n    \"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py_100_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 100. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py",
      "line_number": 100,
      "code_snippet": "\n        # Executing the modified content in the created module's namespace\n        exec(content, module.__dict__)\n        frontmatter = extract_frontmatter(content)\n        log.info(f\"Loaded module: {module.__name__}\")\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py_144_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "exec() on non-literal content on line 144. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py",
      "line_number": 144,
      "code_snippet": "\n        # Execute the modified content in the created module's namespace\n        exec(content, module.__dict__)\n        frontmatter = extract_frontmatter(content)\n        log.info(f\"Loaded module: {module.__name__}\")\n",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py_70_auth",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Plugin management function without authentication in 'load_tool_module_by_id'",
      "description": "Function 'load_tool_module_by_id' on line 70 manages plugins (install/enable/execute) without requiring authentication. This allows unauthorized users to manipulate the plugin system, potentially installing malicious plugins or executing arbitrary code.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py",
      "line_number": 70,
      "code_snippet": "\n\ndef load_tool_module_by_id(tool_id, content=None):\n\n    if content is None:\n        tool = Tools.get_tool_by_id(tool_id)",
      "recommendation": "Plugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py_117_auth",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.75,
      "title": "Plugin management function without authentication in 'load_function_module_by_id'",
      "description": "Function 'load_function_module_by_id' on line 117 manages plugins (install/enable/execute) without requiring authentication. This allows unauthorized users to manipulate the plugin system, potentially installing malicious plugins or executing arbitrary code.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py",
      "line_number": 117,
      "code_snippet": "\n\ndef load_function_module_by_id(function_id: str, content: str | None = None):\n    if content is None:\n        function = Functions.get_function_by_id(function_id)\n        if not function:",
      "recommendation": "Plugin Authentication & Authorization:\n1. Require authentication for all plugin management operations\n2. Implement role-based access control (RBAC)\n3. Use API keys or OAuth tokens for plugin operations\n4. Verify user permissions before plugin actions\n5. Audit all plugin management operations\n6. Implement rate limiting per user/API key\n7. Use multi-factor authentication for sensitive operations\n8. Validate plugin ownership before modifications\n9. Implement plugin signing with developer keys\n10. Log all authentication attempts and failures"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/plugin.py",
      "line_number": 1,
      "code_snippet": "import os\nimport re\nimport subprocess\nimport sys\nfrom importlib import util\nimport types\nimport tempfile\nimport logging\n\nfrom open_webui.env import PIP_OPTIONS, PIP_PACKAGE_INDEX_OPTIONS, OFFLINE_MODE",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/filter.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/filter.py",
      "line_number": 1,
      "code_snippet": "import inspect\nimport logging\n\nfrom open_webui.utils.plugin import (\n    load_function_module_by_id,\n    get_function_module_from_cache,\n)\nfrom open_webui.models.functions import Functions\n\nlog = logging.getLogger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/channels.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/channels.py",
      "line_number": 1,
      "code_snippet": "import re\n\n\ndef extract_mentions(message: str, triggerChar: str = \"@\"):\n    # Escape triggerChar in case it's a regex special character\n    triggerChar = re.escape(triggerChar)\n    pattern = rf\"<{triggerChar}([A-Z]):([^|>]+)\"\n\n    matches = re.findall(pattern, message)\n    return [{\"id_type\": id_type, \"id\": id_value} for id_type, id_value in matches]",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/access_control.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/access_control.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional, Set, Union, List, Dict, Any\nfrom open_webui.models.users import Users, UserModel\nfrom open_webui.models.groups import Groups\n\n\nfrom open_webui.config import DEFAULT_USER_PERMISSIONS\nimport json\n\n\ndef fill_missing_permissions(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py_953",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'add_file_context' on line 953 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py",
      "line_number": 953,
      "code_snippet": "def add_file_context(messages: list, chat_id: str, user) -> list:\n    \"\"\"\n    Add file URLs to messages for native function calling.\n    \"\"\"\n    if not chat_id or chat_id.startswith(\"local:\"):\n        return messages\n\n    chat = Chats.get_chat_by_id_and_user_id(chat_id, user.id)\n    if not chat:\n        return messages\n",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "LLM05_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py_3246_code_exec",
      "category": "LLM05: Supply Chain Vulnerabilities",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "Code execution on external content",
      "description": "eval() on non-literal content on line 3246. ",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py",
      "line_number": 3246,
      "code_snippet": "                        try:\n                            # json.loads cannot be used because some models do not produce valid JSON\n                            tool_function_params = ast.literal_eval(tool_args)\n                        except Exception as e:\n                            log.debug(e)\n                            # Fallback to JSON parsing",
      "recommendation": "Secure Code Execution:\n1. NEVER use eval/exec on untrusted input\n2. Use safe alternatives (json.loads, ast.literal_eval)\n3. Validate and sanitize all external content\n4. Use sandboxed execution environments"
    },
    {
      "id": "LLM07_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py_339_tool",
      "category": "LLM07: Insecure Plugin Design",
      "severity": "HIGH",
      "confidence": 0.85,
      "title": "Insecure tool function 'process_tool_result' executes dangerous operations",
      "description": "Tool function 'process_tool_result' on line 339 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py",
      "line_number": 339,
      "code_snippet": "            append=False,\n        )\n\n\ndef process_tool_result(\n    request,\n    tool_function_name,\n    tool_result,\n    tool_type,\n    direct_tool=False,",
      "recommendation": "Secure Tool/Plugin Implementation:\n1. NEVER execute shell commands from LLM output directly\n2. Use allowlists for permitted commands/operations\n3. Validate all file paths against allowed directories\n4. Use parameterized queries - never raw SQL from LLM\n5. Validate URLs against allowlist before HTTP requests\n6. Implement strict input schemas (JSON Schema, Pydantic)\n7. Add rate limiting and request throttling\n8. Log all tool invocations for audit\n9. Use principle of least privilege\n10. Implement human-in-the-loop for destructive operations"
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py_1312_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "HIGH",
      "confidence": 0.7,
      "title": "High-risk write/execute/network operation without confirmation in 'apply_params_to_form_data'",
      "description": "Function 'apply_params_to_form_data' on line 1312 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py",
      "line_number": 1312,
      "code_snippet": "\n\ndef apply_params_to_form_data(form_data, model):\n    params = form_data.pop(\"params\", {})\n    custom_params = params.pop(\"custom_params\", {})\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py_1312_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'apply_params_to_form_data'",
      "description": "Function 'apply_params_to_form_data' on line 1312 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/middleware.py",
      "line_number": 1312,
      "code_snippet": "\n\ndef apply_params_to_form_data(form_data, model):\n    params = form_data.pop(\"params\", {})\n    custom_params = params.pop(\"custom_params\", {})\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards\n\nImplement confidence thresholds for automated actions:\n\n1. Add confidence scoring:\n   - Request confidence scores from LLM\n   - Calculate custom confidence metrics\n   - Track historical accuracy\n\n2. Set thresholds:\n   - High confidence (>0.9): Auto-execute\n   - Medium confidence (0.7-0.9): Human review\n   - Low confidence (<0.7): Reject or escalate\n\n3. Validate output:\n   - Use schema validation (Pydantic)\n   - Check output format and constraints\n   - Verify against expected patterns\n\n4. Implement fallbacks:\n   - Have backup strategies for low confidence\n   - Use simpler/safer alternatives\n   - Escalate to human operators"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_703_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 703. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 703,
      "code_snippet": "\n        except Exception as e:\n            log.error(f\"Error getting OAuth token for user {user_id}: {e}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_725_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 725. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 725,
      "code_snippet": "                    session.id, refreshed_token\n                )\n                log.info(f\"Successfully refreshed token for session {session.id}\")\n                return session.token\n            else:",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_728_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 728. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 728,
      "code_snippet": "                return session.token\n            else:\n                log.error(f\"Failed to refresh token for session {session.id}\")\n                return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_732_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 732. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 732,
      "code_snippet": "\n        except Exception as e:\n            log.error(f\"Error refreshing token for session {session.id}: {e}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_749_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 749. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 749,
      "code_snippet": "\n        if not token_data.get(\"refresh_token\"):\n            log.warning(f\"No refresh token available for session {session.id}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_771_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 771. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 771,
      "code_snippet": "                        )\n            if not token_endpoint:\n                log.error(f\"No token endpoint found for client_id {client_id}\")\n                return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_823_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 823. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 823,
      "code_snippet": "\n        except Exception as e:\n            log.error(f\"Exception during token refresh for client_id {client_id}: {e}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_865_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 865. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 865,
      "code_snippet": "                )\n                error_message = f\"Token exchange failed: {error_desc}\"\n                log.error(f\"Invalid token response for client_id {client_id}: {token}\")\n                token = None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_997_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 997. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 997,
      "code_snippet": "\n        except Exception as e:\n            log.error(f\"Error getting OAuth token for user {user_id}: {e}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1019_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1019. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1019,
      "code_snippet": "                    session.id, refreshed_token\n                )\n                log.info(f\"Successfully refreshed token for session {session.id}\")\n                return session.token\n            else:",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1022_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1022. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1022,
      "code_snippet": "                return session.token\n            else:\n                log.error(f\"Failed to refresh token for session {session.id}\")\n                return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1026_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1026. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1026,
      "code_snippet": "\n        except Exception as e:\n            log.error(f\"Error refreshing token for session {session.id}: {e}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1043_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1043. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1043,
      "code_snippet": "\n        if not token_data.get(\"refresh_token\"):\n            log.warning(f\"No refresh token available for session {session.id}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1064_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1064. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1064,
      "code_snippet": "                        )\n            if not token_endpoint:\n                log.error(f\"No token endpoint found for provider {provider}\")\n                return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1117_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 1117. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1117,
      "code_snippet": "\n        except Exception as e:\n            log.error(f\"Exception during token refresh for provider {provider}: {e}\")\n            return None\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1456_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'user_data' containing sensitive data is being logged on line 1456. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1456,
      "code_snippet": "                sub = user_data.get(OAUTH_PROVIDERS[provider].get(\"sub_claim\", \"sub\"))\n            if not sub:\n                log.warning(f\"OAuth callback failed, sub is missing: {user_data}\")\n                raise HTTPException(400, detail=ERROR_MESSAGES.INVALID_CRED)\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1511_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'user_data' containing sensitive data is being logged on line 1511. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1511,
      "code_snippet": "                    email = f\"{provider}@{sub}.local\"\n                else:\n                    log.warning(f\"OAuth callback failed, email is missing: {user_data}\")\n                    raise HTTPException(400, detail=ERROR_MESSAGES.INVALID_CRED)\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/oauth.py",
      "line_number": 1,
      "code_snippet": "import base64\nimport copy\nimport hashlib\nimport logging\nimport mimetypes\nimport sys\nimport urllib\nimport uuid\nimport json\nfrom datetime import datetime, timedelta",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/functions.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom open_webui.models.users import Users, UserModel\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import BigInteger, Boolean, Column, String, Text, Index\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/tags.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/tags.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nimport uuid\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\n\n\nfrom pydantic import BaseModel, ConfigDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/files.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import BigInteger, Column, String, Text, JSON\n\nlog = logging.getLogger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM02_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py_415",
      "category": "LLM02: Insecure Output Handling",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "LLM output flows to sql_injection sink",
      "description": "LLM output variable 'data' flows to 'update' on line 415 via direct flow. This creates a sql_injection vulnerability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
      "line_number": 415,
      "code_snippet": "                data = model.model_dump(exclude={\"id\"})\n                result = db.query(Model).filter_by(id=id).update(data)\n\n                db.commit()",
      "recommendation": "Mitigations for SQL Injection:\n1. Use parameterized queries: cursor.execute(query, (param,))\n2. Never concatenate LLM output into SQL\n3. Use ORM query builders (SQLAlchemy, Django ORM)"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py_414",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
      "line_number": 414,
      "code_snippet": "        try:\n            with get_db_context(db) as db:\n                # update only the fields that are present in the model\n                data = model.model_dump(exclude={\"id\"})\n                result = db.query(Model).filter_by(id=id).update(data)\n\n                db.commit()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py_461",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
      "line_number": 461,
      "code_snippet": "                # Update or insert models\n                for model in models:\n                    if model.id in existing_ids:\n                        db.query(Model).filter_by(id=model.id).update(\n                            {\n                                **model.model_dump(),\n                                \"user_id\": user_id,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py_463",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
      "line_number": 463,
      "code_snippet": "                    if model.id in existing_ids:\n                        db.query(Model).filter_by(id=model.id).update(\n                            {\n                                **model.model_dump(),\n                                \"user_id\": user_id,\n                                \"updated_at\": int(time.time()),\n                            }",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py_471",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/models.py",
      "line_number": 471,
      "code_snippet": "                    else:\n                        new_model = Model(\n                            **{\n                                **model.model_dump(),\n                                \"user_id\": user_id,\n                                \"updated_at\": int(time.time()),\n                            }",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/users.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/users.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\n\n\nfrom open_webui.env import DATABASE_USER_ACTIVE_STATUS_UPDATE_INTERVAL\n\nfrom open_webui.models.chats import Chats",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/tools.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom open_webui.models.users import Users, UserResponse\nfrom open_webui.models.groups import Groups\n\nfrom pydantic import BaseModel, ConfigDict",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py_96_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 96. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py",
      "line_number": 96,
      "code_snippet": "            return encrypted\n        except Exception as e:\n            log.error(f\"Error encrypting tokens: {e}\")\n            raise\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "LLM06_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py_105_sensitive_logging",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "Sensitive data in log statement",
      "description": "Variable 'token' containing sensitive data is being logged on line 105. Log files often lack proper access controls and can expose PII, secrets, or prompt content.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py",
      "line_number": 105,
      "code_snippet": "            return json.loads(decrypted)\n        except Exception as e:\n            log.error(f\"Error decrypting tokens: {e}\")\n            raise\n",
      "recommendation": "Secure Logging:\n1. Never log PII, credentials, or sensitive user data\n2. Implement log redaction for sensitive fields\n3. Use structured logging with data classification\n4. Set appropriate log levels (avoid DEBUG in prod)\n5. Ensure log storage has proper access controls"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/oauth_sessions.py",
      "line_number": 1,
      "code_snippet": "import time\nimport logging\nimport uuid\nfrom typing import Optional, List\nimport base64\nimport hashlib\nimport json\n\nfrom cryptography.fernet import Fernet\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/groups.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/groups.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport time\nfrom typing import Optional\nimport uuid\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\n\nfrom open_webui.models.files import FileMetadataResponse",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/notes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/notes.py",
      "line_number": 1,
      "code_snippet": "import json\nimport time\nimport uuid\nfrom typing import Optional\nfrom functools import lru_cache\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, get_db, get_db_context\nfrom open_webui.models.groups import Groups\nfrom open_webui.utils.access_control import has_access",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/prompts.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/prompts.py",
      "line_number": 1,
      "code_snippet": "import time\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom open_webui.models.groups import Groups\nfrom open_webui.models.users import Users, UserResponse\n\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import BigInteger, Column, String, Text, JSON",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/auths.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/auths.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport uuid\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom open_webui.models.users import UserModel, UserProfileImageResponse, Users\nfrom pydantic import BaseModel\nfrom sqlalchemy import Boolean, Column, String, Text\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/messages.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/messages.py",
      "line_number": 1,
      "code_snippet": "import json\nimport time\nimport uuid\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom open_webui.models.tags import TagModel, Tag, Tags\nfrom open_webui.models.users import Users, User, UserNameResponse\nfrom open_webui.models.channels import Channels, ChannelMember",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/feedbacks.py_257",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/feedbacks.py",
      "line_number": 257,
      "code_snippet": "                feedback_model = FeedbackModel.model_validate(feedback)\n                user_model = UserResponse.model_validate(user)\n                feedbacks.append(\n                    FeedbackUserResponse(**feedback_model.model_dump(), user=user_model)\n                )\n\n            return FeedbackListResponse(items=feedbacks, total=total)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_426",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 426,
      "code_snippet": "        if chat is None:\n            return None\n\n        return chat.chat.get(\"history\", {}).get(\"messages\", {}).get(message_id, {})\n\n    def upsert_message_to_chat_by_id_and_message_id(\n        self, id: str, message_id: str, message: dict",
      "recommendation": "Mitigations:\n1. Use structured prompt templates (e.g., LangChain PromptTemplate)\n2. Implement input sanitization to remove prompt injection patterns\n3. Use separate 'user' and 'system' message roles (ChatML format)\n4. Apply input validation and length limits\n5. Use allowlists for expected input formats\n6. Consider prompt injection detection libraries\n\n1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM04_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_419",
      "category": "LLM04: Model Denial of Service",
      "severity": "CRITICAL",
      "confidence": 0.8,
      "title": "Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits",
      "description": "Function 'get_message_by_id_and_message_id' on line 419 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 419,
      "code_snippet": "    def get_message_by_id_and_message_id(\n        self, id: str, message_id: str\n    ) -> Optional[dict]:\n        chat = self.get_chat_by_id(id)\n        if chat is None:\n            return None\n\n        return chat.chat.get(\"history\", {}).get(\"messages\", {}).get(message_id, {})\n\n    def upsert_message_to_chat_by_id_and_message_id(\n        self, id: str, message_id: str, message: dict",
      "recommendation": "Model DoS Mitigations:\n1. Implement rate limiting per user/IP (@limiter.limit('10/minute'))\n2. Validate and limit input length (max 1000 chars)\n3. Set token limits (max_tokens=500)\n4. Configure timeouts (timeout=30 seconds)\n5. Avoid LLM calls in unbounded loops\n6. Implement circuit breakers for cascading failures\n7. Monitor and alert on resource usage\n8. Use queuing for batch processing\n9. Implement cost controls and budgets"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_1031",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 1031 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 1031,
      "code_snippet": "                            *[\n                                text(\n                                    f\"\"\"\n                                    EXISTS (\n                                        SELECT 1",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_1089",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 1089 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 1089,
      "code_snippet": "                            *[\n                                text(\n                                    f\"\"\"\n                                    EXISTS (\n                                        SELECT 1",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_410",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 410,
      "code_snippet": "        if chat is None:\n            return None\n\n        return chat.chat.get(\"title\", \"New Chat\")\n\n    def get_messages_map_by_chat_id(self, id: str) -> Optional[dict]:\n        chat = self.get_chat_by_id(id)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_417",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 417,
      "code_snippet": "        if chat is None:\n            return None\n\n        return chat.chat.get(\"history\", {}).get(\"messages\", {}) or {}\n\n    def get_message_by_id_and_message_id(\n        self, id: str, message_id: str",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py_1489",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/chats.py",
      "line_number": 1489,
      "code_snippet": "        with get_db_context(db) as db:\n            # Join Chat and ChatFile tables to get shared chats associated with the file_id\n            all_chats = (\n                db.query(Chat)\n                .join(ChatFile, Chat.id == ChatFile.chat_id)\n                .filter(ChatFile.file_id == file_id, Chat.share_id.isnot(None))\n                .all()",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/memories.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/memories.py",
      "line_number": 1,
      "code_snippet": "import time\nimport uuid\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, get_db, get_db_context\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import BigInteger, Column, String, Text\n\n####################",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/channels.py_277_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.8,
      "title": "Critical decision without oversight in '_create_membership_models'",
      "description": "Function '_create_membership_models' on line 277 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/channels.py",
      "line_number": 277,
      "code_snippet": "        return users\n\n    def _create_membership_models(\n        self,\n        channel_id: str,\n        invited_by: str,",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/channels.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/channels.py",
      "line_number": 1,
      "code_snippet": "import json\nimport secrets\nimport time\nimport uuid\nfrom typing import Optional\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\nfrom open_webui.models.groups import Groups\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/folders.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/folders.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nimport uuid\nfrom typing import Optional\nimport re\n\n\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import BigInteger, Column, Text, JSON, Boolean, func\nfrom sqlalchemy.orm import Session",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/knowledge.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.9,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/models/knowledge.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nimport time\nfrom typing import Optional\nimport uuid\n\nfrom sqlalchemy.orm import Session\nfrom open_webui.internal.db import Base, JSONField, get_db, get_db_context\n\nfrom open_webui.models.files import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/storage/provider.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/storage/provider.py",
      "line_number": 1,
      "code_snippet": "import os\nimport shutil\nimport json\nimport logging\nimport re\nfrom abc import ABC, abstractmethod\nfrom typing import BinaryIO, Tuple, Dict\n\nimport boto3\nfrom botocore.config import Config",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM08_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_1211_risk",
      "category": "LLM08: Excessive Agency",
      "severity": "MEDIUM",
      "confidence": 0.7,
      "title": "High-risk write/network operation without confirmation in 'get_model_path'",
      "description": "Function 'get_model_path' on line 1211 performs high-risk write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 1211,
      "code_snippet": "\n\ndef get_model_path(model: str, update_model: bool = False):\n    # Construct huggingface_hub kwargs with local_files_only to return the snapshot path\n    cache_dir = os.getenv(\"SENTENCE_TRANSFORMERS_HOME\")\n",
      "recommendation": "High-Risk Operation Safety:\n1. Require explicit user confirmation for destructive actions\n2. Display clear preview of what will be changed/deleted\n3. Implement \"undo\" functionality where possible\n4. Use transaction rollback for database operations\n5. Add time delays before executing irreversible actions\n6. Send notifications for critical operations\n7. Implement approval workflows for sensitive operations\n8. Maintain detailed audit logs of all actions\n9. Use \"dry-run\" mode to show what would happen\n10. Consider implementing operation quotas/limits"
    },
    {
      "id": "LLM09_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_1211_critical_decision",
      "category": "LLM09: Overreliance",
      "severity": "INFO",
      "confidence": 0.9,
      "title": "Critical decision without oversight in 'get_model_path'",
      "description": "Function 'get_model_path' on line 1211 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 1211,
      "code_snippet": "\n\ndef get_model_path(model: str, update_model: bool = False):\n    # Construct huggingface_hub kwargs with local_files_only to return the snapshot path\n    cache_dir = os.getenv(\"SENTENCE_TRANSFORMERS_HOME\")\n",
      "recommendation": "Critical data_modification decision requires human oversight:\n\n1. Implement human-in-the-loop review:\n   - Add review queue for high-stakes decisions\n   - Require explicit human approval before execution\n   - Log all decisions for audit trail\n\n2. Add verification mechanisms:\n   - Cross-reference with trusted sources\n   - Implement multi-step verification\n   - Use confidence thresholds\n\n3. Include safety checks:\n   - Set limits on transaction amounts\n   - Require secondary confirmation\n   - Implement rollback mechanisms\n\n4. Add disclaimers:\n   - Inform users output may be incorrect\n   - Recommend professional consultation\n   - Document limitations clearly\n\n5. Monitor and review:\n   - Track decision outcomes\n   - Review failures and near-misses\n   - Continuously improve safeguards"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_925",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 925,
      "code_snippet": "    if reranking_function is None:\n        return None\n    if reranking_engine == \"external\":\n        return lambda query, documents, user=None: reranking_function.predict(\n            [(query, doc.page_content) for doc in documents], user=user\n        )\n    else:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_929",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 929,
      "code_snippet": "            [(query, doc.page_content) for doc in documents], user=user\n        )\n    else:\n        return lambda query, documents, user=None: reranking_function.predict(\n            [(query, doc.page_content) for doc in documents]\n        )\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_1231",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 1231,
      "code_snippet": "    # Inspiration from upstream sentence_transformers\n    if (\n        os.path.exists(model)\n        or (\"\\\\\" in model or model.count(\"/\") > 1)\n        and local_files_only\n    ):\n        # If fully qualified path exists, return input, else set repo_id",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_1015",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 1015,
      "code_snippet": "            chat = Chats.get_chat_by_id(item.get(\"id\"))\n\n            if chat and (user.role == \"admin\" or chat.user_id == user.id):\n                messages_map = chat.chat.get(\"history\", {}).get(\"messages\", {})\n                message_id = chat.chat.get(\"history\", {}).get(\"currentId\")\n\n                if messages_map and message_id:",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py_1016",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** LLM call with potential user input\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/utils.py",
      "line_number": 1016,
      "code_snippet": "\n            if chat and (user.role == \"admin\" or chat.user_id == user.id):\n                messages_map = chat.chat.get(\"history\", {}).get(\"messages\", {})\n                message_id = chat.chat.get(\"history\", {}).get(\"currentId\")\n\n                if messages_map and message_id:\n                    # Reconstruct the message list in order",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/socket/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/socket/utils.py",
      "line_number": 1,
      "code_snippet": "import json\nimport uuid\nfrom open_webui.utils.redis import get_redis_connection\nfrom open_webui.env import REDIS_KEY_PREFIX\nfrom typing import Optional, List, Tuple\nimport pycrdt as Y\n\n\nclass RedisLock:\n    def __init__(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/socket/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/socket/main.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport random\n\nimport socketio\nimport logging\nimport sys\nimport time\nfrom typing import Dict, Set\nfrom redis import asyncio as aioredis\nimport pycrdt as Y",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serpapi.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serpapi.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\nfrom urllib.parse import urlencode\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/bocha.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/bocha.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nimport json\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/searchapi.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/searchapi.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\nfrom urllib.parse import urlencode\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/mojeek.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/mojeek.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\ndef search_mojeek(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/tavily.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/tavily.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\ndef search_tavily(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/brave.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/brave.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport time\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serper.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serper.py",
      "line_number": 1,
      "code_snippet": "import json\nimport logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/exa.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/exa.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult\n\nlog = logging.getLogger(__name__)\n\nEXA_API_BASE = \"https://api.exa.ai\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/perplexity_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/perplexity_search.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional, Literal\nimport requests\n\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\nfrom open_webui.utils.headers import include_user_info_headers\n\n\nlog = logging.getLogger(__name__)\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/azure.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/azure.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\"\"\"\nAzure AI Search integration for Open WebUI.\nDocumentation: https://learn.microsoft.com/en-us/python/api/overview/azure/search-documents-readme?view=azure-python\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/searxng.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/searxng.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\ndef search_searxng(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/perplexity.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/perplexity.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional, Literal\nimport requests\n\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nMODELS = Literal[\n    \"sonar\",\n    \"sonar-pro\",\n    \"sonar-reasoning\",",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/bing.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/bing.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport os\nfrom pprint import pprint\nfrom typing import Optional\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\nimport argparse\n\nlog = logging.getLogger(__name__)\n\"\"\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/sougou.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/sougou.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport json\nfrom typing import Optional, List\n\n\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/google_pse.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/google_pse.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\ndef search_google_pse(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/utils.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/utils.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport logging\nimport socket\nimport ssl\nimport urllib.parse\nimport urllib.request\nfrom datetime import datetime, time, timedelta\nfrom typing import (\n    Any,\n    AsyncIterator,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serpstack.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serpstack.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\ndef search_serpstack(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/jina_search.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/jina_search.py",
      "line_number": 1,
      "code_snippet": "import logging\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult\nfrom yarl import URL\n\nlog = logging.getLogger(__name__)\n\n\ndef search_jina(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/main.py",
      "line_number": 1,
      "code_snippet": "import validators\n\nfrom typing import Optional\nfrom urllib.parse import urlparse\n\nfrom pydantic import BaseModel\n\nfrom open_webui.retrieval.web.utils import resolve_hostname\nfrom open_webui.utils.misc import is_string_allowed\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/yacy.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/yacy.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom requests.auth import HTTPDigestAuth\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/external.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/external.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional, List\n\nimport requests\n\nfrom fastapi import Request\n\n\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\nfrom open_webui.utils.headers import include_user_info_headers",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/duckduckgo.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/duckduckgo.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\nfrom ddgs import DDGS\nfrom ddgs.exceptions import RatelimitException\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/ollama.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/ollama.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/kagi.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/kagi.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n\ndef search_kagi(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/firecrawl.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/firecrawl.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional, List\n\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\n\nlog = logging.getLogger(__name__)\n\n\ndef search_firecrawl(",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serply.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/web/serply.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional\nfrom urllib.parse import urlencode\n\nimport requests\nfrom open_webui.retrieval.web.main import SearchResult, get_filtered_results\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/models/colbert.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/models/colbert.py",
      "line_number": 1,
      "code_snippet": "import os\nimport logging\nimport torch\nimport numpy as np\nfrom colbert.infra import ColBERTConfig\nfrom colbert.modeling.checkpoint import Checkpoint\n\n\nfrom open_webui.retrieval.models.base_reranker import BaseReranker\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/models/external.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/models/external.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport requests\nfrom typing import Optional, List, Tuple\nfrom urllib.parse import quote\n\n\nfrom open_webui.env import ENABLE_FORWARD_USER_INFO_HEADERS, REQUESTS_VERIFY\nfrom open_webui.retrieval.models.base_reranker import BaseReranker\nfrom open_webui.utils.headers import include_user_info_headers\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/type.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.85,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/type.py",
      "line_number": 1,
      "code_snippet": "from enum import StrEnum\n\n\nclass VectorType(StrEnum):\n    MILVUS = \"milvus\"\n    QDRANT = \"qdrant\"\n    CHROMA = \"chroma\"\n    PINECONE = \"pinecone\"\n    ELASTICSEARCH = \"elasticsearch\"\n    OPENSEARCH = \"opensearch\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/mistral.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/mistral.py",
      "line_number": 1,
      "code_snippet": "import requests\nimport aiohttp\nimport asyncio\nimport logging\nimport os\nimport sys\nimport time\nfrom typing import List, Dict, Any\nfrom contextlib import asynccontextmanager\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/youtube.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/youtube.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom xml.etree.ElementTree import ParseError\n\nfrom typing import Any, Dict, Generator, List, Optional, Sequence, Union\nfrom urllib.parse import parse_qs, urlparse\nfrom langchain_core.documents import Document\n\nlog = logging.getLogger(__name__)\n\nALLOWED_SCHEMES = {\"http\", \"https\"}",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/tavily.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/tavily.py",
      "line_number": 1,
      "code_snippet": "import requests\nimport logging\nfrom typing import Iterator, List, Literal, Union\n\nfrom langchain_core.document_loaders import BaseLoader\nfrom langchain_core.documents import Document\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/external_web.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/external_web.py",
      "line_number": 1,
      "code_snippet": "import requests\nimport logging\nfrom typing import Iterator, List, Union\n\nfrom langchain_core.document_loaders import BaseLoader\nfrom langchain_core.documents import Document\n\nlog = logging.getLogger(__name__)\n\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/mineru.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/mineru.py",
      "line_number": 1,
      "code_snippet": "import os\nimport time\nimport requests\nimport logging\nimport tempfile\nimport zipfile\nfrom typing import List, Optional\nfrom langchain_core.documents import Document\nfrom fastapi import HTTPException, status\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/external_document.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/external_document.py",
      "line_number": 1,
      "code_snippet": "import requests\nimport logging, os\nfrom typing import Iterator, List, Union\nfrom urllib.parse import quote\n\nfrom langchain_core.document_loaders import BaseLoader\nfrom langchain_core.documents import Document\nfrom open_webui.utils.headers import include_user_info_headers\n\nlog = logging.getLogger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/main.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/main.py",
      "line_number": 1,
      "code_snippet": "import requests\nimport logging\nimport ftfy\nimport sys\nimport json\n\nfrom azure.identity import DefaultAzureCredential\nfrom langchain_community.document_loaders import (\n    AzureAIDocumentIntelligenceLoader,\n    BSHTMLLoader,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/datalab_marker.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/loaders/datalab_marker.py",
      "line_number": 1,
      "code_snippet": "import os\nimport time\nimport requests\nimport logging\nimport json\nfrom typing import List, Optional\nfrom langchain_core.documents import Document\nfrom fastapi import HTTPException, status\n\nlog = logging.getLogger(__name__)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/elasticsearch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/elasticsearch.py",
      "line_number": 1,
      "code_snippet": "from elasticsearch import Elasticsearch, BadRequestError\nfrom typing import Optional\nimport ssl\nfrom elasticsearch.helpers import bulk, scan\n\nfrom open_webui.retrieval.vector.utils import process_metadata\nfrom open_webui.retrieval.vector.main import (\n    VectorDBBase,\n    VectorItem,\n    SearchResult,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/qdrant_multitenancy.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/qdrant_multitenancy.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional, Tuple, List, Dict, Any\nfrom urllib.parse import urlparse\n\nimport grpc\nfrom open_webui.config import (\n    QDRANT_API_KEY,\n    QDRANT_GRPC_PORT,\n    QDRANT_ON_DISK,\n    QDRANT_PREFER_GRPC,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/milvus.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/milvus.py",
      "line_number": 1,
      "code_snippet": "from pymilvus import MilvusClient as Client\nfrom pymilvus import FieldSchema, DataType\nfrom pymilvus import connections, Collection\n\nimport json\nimport logging\nfrom typing import Optional\n\nfrom open_webui.retrieval.vector.utils import process_metadata\nfrom open_webui.retrieval.vector.main import (",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pinecone.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "MEDIUM",
      "confidence": 0.9,
      "title": "Potential prompt injection vulnerability (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** No attack vector detected\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pinecone.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional, List, Dict, Any, Union\nimport logging\nimport time  # for measuring elapsed time\nfrom pinecone import Pinecone, ServerlessSpec\n\n# Add gRPC support for better performance (Pinecone best practice)\ntry:\n    from pinecone.grpc import PineconeGRPC\n\n    GRPC_AVAILABLE = True",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Review and secure all prompt construction paths."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/opensearch.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/opensearch.py",
      "line_number": 1,
      "code_snippet": "from opensearchpy import OpenSearch\nfrom opensearchpy.helpers import bulk\nfrom typing import Optional\n\nfrom open_webui.retrieval.vector.utils import process_metadata\nfrom open_webui.retrieval.vector.main import (\n    VectorDBBase,\n    VectorItem,\n    SearchResult,\n    GetResult,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/qdrant.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/qdrant.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional\nimport logging\nfrom urllib.parse import urlparse\n\nfrom qdrant_client import QdrantClient as Qclient\nfrom qdrant_client.http.models import PointStruct\nfrom qdrant_client.models import models\n\nfrom open_webui.retrieval.vector.main import (\n    VectorDBBase,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/s3vector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/s3vector.py",
      "line_number": 1,
      "code_snippet": "from open_webui.retrieval.vector.utils import process_metadata\nfrom open_webui.retrieval.vector.main import (\n    VectorDBBase,\n    VectorItem,\n    GetResult,\n    SearchResult,\n)\nfrom open_webui.config import S3_VECTOR_BUCKET_NAME, S3_VECTOR_REGION\nfrom typing import List, Optional, Dict, Any, Union\nimport logging",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pgvector.py_242",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 242 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pgvector.py",
      "line_number": 242,
      "code_snippet": "        if not existing_index_def:\n            index_sql = (\n                f\"CREATE INDEX IF NOT EXISTS {index_name} \"\n                f\"ON document_chunk USING {index_method} (vector {VECTOR_OPCLASS})\"\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pgvector.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/pgvector.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional, List, Dict, Any, Tuple\nimport logging\nimport json\nfrom sqlalchemy import (\n    func,\n    literal,\n    cast,\n    column,\n    create_engine,\n    Column,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/opengauss.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/opengauss.py",
      "line_number": 1,
      "code_snippet": "from typing import Optional, List, Dict, Any\nimport logging\nimport re\nimport json\nfrom sqlalchemy import (\n    func,\n    literal,\n    cast,\n    column,\n    create_engine,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/weaviate.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/weaviate.py",
      "line_number": 1,
      "code_snippet": "import weaviate\nimport re\nimport uuid\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom open_webui.retrieval.vector.main import (\n    VectorDBBase,\n    VectorItem,\n    SearchResult,\n    GetResult,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/milvus_multitenancy.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/milvus_multitenancy.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom typing import Optional, Tuple, List, Dict, Any\n\nfrom open_webui.config import (\n    MILVUS_URI,\n    MILVUS_TOKEN,\n    MILVUS_DB,\n    MILVUS_COLLECTION_PREFIX,\n    MILVUS_INDEX_TYPE,\n    MILVUS_METRIC_TYPE,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py_10",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 10. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
      "line_number": 10,
      "code_snippet": "ORACLE_DB_USE_WALLET =  false\nORACLE_DB_USER = \"DEMOUSER\"\nORACLE_DB_PASSWORD = \"Welcome123456\"\nORACLE_DB_DSN = \"localhost:1521/FREEPDB1\"\n",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
      "line_number": 16,
      "code_snippet": "# ORACLE_DB_USE_WALLET =  true\n# ORACLE_DB_USER = \"DEMOUSER\"\n# ORACLE_DB_PASSWORD = \"Welcome123456\"\n# ORACLE_DB_DSN = \"medium\"\n# ORACLE_DB_DSN = \"(description=  (retry_count=3)(retry_delay=3)(address=(protocol=tcps)(port=1522)(host=xx.oraclecloud.com))(connect_data=(service_name=yy.adb.oraclecloud.com))(security=(ssl_server_dn_match=no)))\"",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Hardcoded secret detected: Secret Keyword",
      "description": "detect-secrets found a potential Secret Keyword on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
      "line_number": 20,
      "code_snippet": "# ORACLE_DB_DSN = \"(description=  (retry_count=3)(retry_delay=3)(address=(protocol=tcps)(port=1522)(host=xx.oraclecloud.com))(connect_data=(service_name=yy.adb.oraclecloud.com))(security=(ssl_server_dn_match=no)))\"\n# ORACLE_WALLET_DIR = \"/home/opc/adb_wallet\"\n# ORACLE_WALLET_PASSWORD = \"Welcome1\"\n\nORACLE_VECTOR_LENGTH = 768",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/oracle23ai.py",
      "line_number": 1,
      "code_snippet": "\"\"\"\nOracle 23ai Vector Database Client - Fixed Version\n\n# .env\nVECTOR_DB = \"oracle23ai\"\n\n## DBCS or oracle 23ai free\nORACLE_DB_USE_WALLET =  false\nORACLE_DB_USER = \"DEMOUSER\"\nORACLE_DB_PASSWORD = \"Welcome123456\"",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/chroma.py_195",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 195 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/chroma.py",
      "line_number": 195,
      "code_snippet": "            # If collection doesn't exist, that's fine - nothing to delete\n            log.debug(\n                f\"Attempted to delete from non-existent collection {collection_name}. Ignoring.\"\n            )\n            pass",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/chroma.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/retrieval/vector/dbs/chroma.py",
      "line_number": 1,
      "code_snippet": "import chromadb\nimport logging\nfrom chromadb import Settings\nfrom chromadb.utils.batch_utils import create_batches\n\nfrom typing import Optional\n\nfrom open_webui.retrieval.vector.main import (\n    VectorDBBase,\n    VectorItem,",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/images/comfyui.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/images/comfyui.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nimport json\nimport logging\nimport random\nimport requests\nimport aiohttp\nimport urllib.parse\nimport urllib.request\nfrom typing import Optional\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/mcp/client.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/mcp/client.py",
      "line_number": 1,
      "code_snippet": "import asyncio\nfrom typing import Optional\nfrom contextlib import AsyncExitStack\n\nimport anyio\n\nfrom mcp import ClientSession\nfrom mcp.client.auth import OAuthClientProvider, TokenStorage\nfrom mcp.client.streamable_http import streamablehttp_client\nfrom mcp.shared.auth import OAuthClientInformationFull, OAuthClientMetadata, OAuthToken",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/db/access_control.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 0.75,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/db/access_control.py",
      "line_number": 1,
      "code_snippet": "from pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import BigInteger, Boolean, Column, String, Text, JSON\nfrom sqlalchemy.dialects.postgresql import JSONB\n\n\nfrom sqlalchemy import or_, func, select, and_, text, cast, or_, and_, func\n\n\ndef has_permission(db, DocumentModel, query, filter: dict, permission: str = \"read\"):\n    group_ids = filter.get(\"group_ids\", [])",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/metrics.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/metrics.py",
      "line_number": 1,
      "code_snippet": "\"\"\"OpenTelemetry metrics bootstrap for Open WebUI.\n\nThis module initialises a MeterProvider that sends metrics to an OTLP\ncollector. The collector is responsible for exposing a Prometheus\n`/metrics` endpoint \u2013 WebUI does **not** expose it directly.\n\nMetrics collected:\n\n* http.server.requests (counter)\n* http.server.duration (histogram, milliseconds)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/constants.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/constants.py",
      "line_number": 1,
      "code_snippet": "from opentelemetry.semconv.trace import SpanAttributes as _SpanAttributes\n\n# Span Tags\nSPAN_DB_TYPE = \"mysql\"\nSPAN_REDIS_TYPE = \"redis\"\nSPAN_DURATION = \"duration\"\nSPAN_SQL_STR = \"sql\"\nSPAN_SQL_EXPLAIN = \"explain\"\nSPAN_ERROR_TYPE = \"error\"\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/setup.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/setup.py",
      "line_number": 1,
      "code_snippet": "from fastapi import FastAPI\nfrom opentelemetry import trace\n\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import (\n    OTLPSpanExporter as HttpOTLPSpanExporter,\n)\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/instrumentors.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/instrumentors.py",
      "line_number": 1,
      "code_snippet": "import logging\nimport traceback\nfrom typing import Collection, Union\n\nfrom aiohttp import (\n    TraceRequestStartParams,\n    TraceRequestEndParams,\n    TraceRequestExceptionParams,\n)\nfrom chromadb.telemetry.opentelemetry.fastapi import instrument_fastapi",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/logs.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/utils/telemetry/logs.py",
      "line_number": 1,
      "code_snippet": "import logging\nfrom base64 import b64encode\nfrom opentelemetry.sdk._logs import (\n    LoggingHandler,\n    LoggerProvider,\n)\nfrom opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter\nfrom opentelemetry.exporter.otlp.proto.http._log_exporter import (\n    OTLPLogExporter as HttpOTLPLogExporter,\n)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/010_migrate_modelfiles_to_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/010_migrate_modelfiles_to_models.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 009_add_models.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/012_add_tools.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/012_add_tools.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 009_add_models.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/014_add_files.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/014_add_files.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 009_add_models.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/001_initial_schema.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/001_initial_schema.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 001_initial_schema.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/008_add_memory.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/008_add_memory.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 002_add_local_sharing.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/015_add_functions.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/015_add_functions.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 009_add_models.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/009_add_models.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/internal/migrations/009_add_models.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Peewee migrations -- 009_add_models.py.\n\nSome examples (model - class or model name)::\n\n    > Model = migrator.orm['table_name']            # Return model in current state by name\n    > Model = migrator.ModelClass                   # Return model in current state by name\n\n    > migrator.sql(sql)                             # Run custom SQL\n    > migrator.run(func, *args, **kwargs)           # Run python function with the given args\n    > migrator.create_model(Model)                  # Create a model (could be used as decorator)",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py",
      "line_number": 16,
      "code_snippet": "import json\n\nrevision = \"3ab32c4b8f59\"\ndown_revision = \"1af9b942657b\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py",
      "line_number": 17,
      "code_snippet": "\nrevision = \"3ab32c4b8f59\"\ndown_revision = \"1af9b942657b\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3ab32c4b8f59_update_tags.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update tags\n\nRevision ID: 3ab32c4b8f59\nRevises: 1af9b942657b\nCreate Date: 2024-10-09 21:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py",
      "line_number": 16,
      "code_snippet": "\n\nrevision = \"c29facfe716b\"\ndown_revision = \"c69f45358db4\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py",
      "line_number": 17,
      "code_snippet": "\nrevision = \"c29facfe716b\"\ndown_revision = \"c69f45358db4\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c29facfe716b_update_file_table_path.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update file table path\n\nRevision ID: c29facfe716b\nRevises: c69f45358db4\nCreate Date: 2024-10-20 17:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_20",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 20. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 20,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"b10670c03dd5\"\ndown_revision: Union[str, None] = \"2f1211949ecc\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_42",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 42 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 42,
      "code_snippet": "        indexed_cols = [row[2] for row in idx_info]  # col names\n        if column_name in indexed_cols:\n            conn.execute(sa.text(f\"DROP INDEX IF EXISTS {index_name}\"))\n\n",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_175",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 175 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 175,
      "code_snippet": "    for uid, api_key in users_with_keys:\n        if api_key:\n            conn.execute(\n                sa.text(\n                    \"\"\"",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_55",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 55 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 55,
      "code_snippet": "\n        # 2. Load old data\n        rows = conn.execute(sa.text(f'SELECT id, {column} FROM \"{table}\"')).fetchall()\n\n        for row in rows:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_68",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 68 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 68,
      "code_snippet": "\n            conn.execute(\n                sa.text(f'UPDATE \"{table}\" SET {column}_json = :val WHERE id = :id'),\n                {\"val\": json.dumps(parsed) if parsed else None, \"id\": uid},\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_95",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 95 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 95,
      "code_snippet": "        op.add_column(table, sa.Column(f\"{column}_text\", sa.Text(), nullable=True))\n\n        rows = conn.execute(sa.text(f'SELECT id, {column} FROM \"{table}\"')).fetchall()\n\n        for uid, raw in rows:",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "SQLI_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_99",
      "category": "SQL Injection",
      "severity": "HIGH",
      "confidence": 0.9,
      "title": "SQL injection: f-string formatting",
      "description": "SQL query on line 99 uses f-string formatting. This allows attackers to modify query logic, access unauthorized data, or execute arbitrary SQL commands.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 99,
      "code_snippet": "        for uid, raw in rows:\n            conn.execute(\n                sa.text(f'UPDATE \"{table}\" SET {column}_text = :val WHERE id = :id'),\n                {\"val\": json.dumps(raw) if raw else None, \"id\": uid},\n            )",
      "recommendation": "Use parameterized queries:\n1. cursor.execute('SELECT * FROM t WHERE id = %s', (user_id,))\n2. cursor.execute('SELECT * FROM t WHERE id = ?', (user_id,))\n3. Use ORM (SQLAlchemy, Django ORM)\n4. Never use f-strings or .format() in SQL"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/b10670c03dd5_update_user_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update user table\n\nRevision ID: b10670c03dd5\nRevises: 2f1211949ecc\nCreate Date: 2025-11-28 04:55:31.737538\n\n\"\"\"\n\nfrom typing import Sequence, Union\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/38d63c18f30f_add_oauth_session_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/38d63c18f30f_add_oauth_session_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add oauth_session table\n\nRevision ID: 38d63c18f30f\nRevises: 3af16a1c9fb6\nCreate Date: 2025-09-08 14:19:59.583921\n\n\"\"\"\n\nfrom typing import Sequence, Union\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"3781e22d8b01\"\ndown_revision = \"7826ab40b532\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"3781e22d8b01\"\ndown_revision = \"7826ab40b532\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3781e22d8b01_update_message_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update message & channel tables\n\nRevision ID: 3781e22d8b01\nRevises: 7826ab40b532\nCreate Date: 2024-12-30 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/57c599a3cb57_add_channel_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/57c599a3cb57_add_channel_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add channel table\n\nRevision ID: 57c599a3cb57\nRevises: 922e7a387820\nCreate Date: 2024-12-22 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/a5c220713937_add_reply_to_id_column_to_message.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/a5c220713937_add_reply_to_id_column_to_message.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"a5c220713937\"\ndown_revision: Union[str, None] = \"38d63c18f30f\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/1af9b942657b_migrate_tags.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/1af9b942657b_migrate_tags.py",
      "line_number": 16,
      "code_snippet": "import json\n\nrevision = \"1af9b942657b\"\ndown_revision = \"242a2047eae0\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/1af9b942657b_migrate_tags.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/1af9b942657b_migrate_tags.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Migrate tags\n\nRevision ID: 1af9b942657b\nRevises: 242a2047eae0\nCreate Date: 2024-10-09 21:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/37f288994c47_add_group_member_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/37f288994c47_add_group_member_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"add_group_member_table\n\nRevision ID: 37f288994c47\nRevises: a5c220713937\nCreate Date: 2025-11-17 03:45:25.123939\n\n\"\"\"\n\nimport uuid\nimport time",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6a39f3d8e55c_add_knowledge_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6a39f3d8e55c_add_knowledge_table.py",
      "line_number": 15,
      "code_snippet": "\n\nrevision = \"6a39f3d8e55c\"\ndown_revision = \"c0fbf31ca0db\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6a39f3d8e55c_add_knowledge_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6a39f3d8e55c_add_knowledge_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add knowledge table\n\nRevision ID: 6a39f3d8e55c\nRevises: c0fbf31ca0db\nCreate Date: 2024-10-01 14:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"018012973d35\"\ndown_revision = \"d31026856c01\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"018012973d35\"\ndown_revision = \"d31026856c01\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/018012973d35_add_indexes.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add indexes\n\nRevision ID: 018012973d35\nRevises: d31026856c01\nCreate Date: 2025-08-13 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7e5b5dc7342b_init.py_19",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 19. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7e5b5dc7342b_init.py",
      "line_number": 19,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"7e5b5dc7342b\"\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"4ace53fd72c8\"\ndown_revision = \"af906e964978\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"4ace53fd72c8\"\ndown_revision = \"af906e964978\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/4ace53fd72c8_update_folder_table_datetime.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update folder table and change DateTime to BigInteger for timestamp fields\n\nRevision ID: 4ace53fd72c8\nRevises: af906e964978\nCreate Date: 2024-10-23 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/ca81bd47c050_add_config_table.py_15",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 15. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/ca81bd47c050_add_config_table.py",
      "line_number": 15,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"ca81bd47c050\"\ndown_revision: Union[str, None] = \"7e5b5dc7342b\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7826ab40b532_update_file_table.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7826ab40b532_update_file_table.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"7826ab40b532\"\ndown_revision = \"57c599a3cb57\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7826ab40b532_update_file_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/7826ab40b532_update_file_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update file table\n\nRevision ID: 7826ab40b532\nRevises: 57c599a3cb57\nCreate Date: 2024-12-23 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6283dc0e4d8d_add_channel_file_table.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/6283dc0e4d8d_add_channel_file_table.py",
      "line_number": 17,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"6283dc0e4d8d\"\ndown_revision: Union[str, None] = \"3e0e00844bb0\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py",
      "line_number": 13,
      "code_snippet": "\n# Revision identifiers, used by Alembic.\nrevision = \"af906e964978\"\ndown_revision = \"c29facfe716b\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py_14",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 14. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py",
      "line_number": 14,
      "code_snippet": "# Revision identifiers, used by Alembic.\nrevision = \"af906e964978\"\ndown_revision = \"c29facfe716b\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/af906e964978_add_feedback_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add feedback table\n\nRevision ID: af906e964978\nRevises: c29facfe716b\nCreate Date: 2024-10-20 17:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"c69f45358db4\"\ndown_revision = \"3ab32c4b8f59\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"c69f45358db4\"\ndown_revision = \"3ab32c4b8f59\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/c69f45358db4_add_folder_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add folder table\n\nRevision ID: c69f45358db4\nRevises: 3ab32c4b8f59\nCreate Date: 2024-10-16 02:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/242a2047eae0_update_chat_table.py_16",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 16. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/242a2047eae0_update_chat_table.py",
      "line_number": 16,
      "code_snippet": "\nrevision = \"242a2047eae0\"\ndown_revision = \"6a39f3d8e55c\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/242a2047eae0_update_chat_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/242a2047eae0_update_chat_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update chat table\n\nRevision ID: 242a2047eae0\nRevises: 6a39f3d8e55c\nCreate Date: 2024-10-09 21:02:35.241684\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3e0e00844bb0_add_knowledge_file_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/3e0e00844bb0_add_knowledge_file_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add knowledge_file table\n\nRevision ID: 3e0e00844bb0\nRevises: 90ef40d4714e\nCreate Date: 2025-12-02 06:54:19.401334\n\n\"\"\"\n\nfrom typing import Sequence, Union\n",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/d31026856c01_update_folder_table_data.py_12",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 12. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/d31026856c01_update_folder_table_data.py",
      "line_number": 12,
      "code_snippet": "import sqlalchemy as sa\n\nrevision = \"d31026856c01\"\ndown_revision = \"9f0c9cd09105\"\nbranch_labels = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/d31026856c01_update_folder_table_data.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/d31026856c01_update_folder_table_data.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Update folder table data\n\nRevision ID: d31026856c01\nRevises: 9f0c9cd09105\nCreate Date: 2025-07-13 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/922e7a387820_add_group_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/922e7a387820_add_group_table.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"922e7a387820\"\ndown_revision = \"4ace53fd72c8\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/922e7a387820_add_group_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/922e7a387820_add_group_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add group table\n\nRevision ID: 922e7a387820\nRevises: 4ace53fd72c8\nCreate Date: 2024-11-14 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/81cc2ce44d79_update_channel_file_and_knowledge_table.py_17",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 17. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/81cc2ce44d79_update_channel_file_and_knowledge_table.py",
      "line_number": 17,
      "code_snippet": "\n# revision identifiers, used by Alembic.\nrevision: str = \"81cc2ce44d79\"\ndown_revision: Union[str, None] = \"6283dc0e4d8d\"\nbranch_labels: Union[str, Sequence[str], None] = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "LLM06_DS_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/9f0c9cd09105_add_note_table.py_13",
      "category": "LLM06: Sensitive Information Disclosure",
      "severity": "HIGH",
      "confidence": 0.8,
      "title": "Hardcoded secret detected: Hex High Entropy String",
      "description": "detect-secrets found a potential Hex High Entropy String on line 13. Hardcoded secrets in source code can be extracted from version control, compiled binaries, or by anyone with code access.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/9f0c9cd09105_add_note_table.py",
      "line_number": 13,
      "code_snippet": "\nrevision = \"9f0c9cd09105\"\ndown_revision = \"3781e22d8b01\"\nbranch_labels = None\ndepends_on = None",
      "recommendation": "Remove hardcoded secrets:\n1. Use environment variables: os.getenv('SECRET_KEY')\n2. Use secret management (AWS Secrets Manager, HashiCorp Vault)\n3. Use .env files (not committed to git)\n4. Rotate the exposed credential immediately"
    },
    {
      "id": "ML_LLM01_/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/9f0c9cd09105_add_note_table.py_1",
      "category": "LLM01: Prompt Injection (ML-Detected)",
      "severity": "CRITICAL",
      "confidence": 1.0,
      "title": "Potential direct prompt injection (ML-detected)",
      "description": "ML model detected prompt injection vulnerability with 100% confidence.\n\n**Attack Vector:** Direct prompt injection: User input is directly included in the LLM prompt\n\n**Location:** ML model detected vulnerability patterns in file\n\n**Top Contributing Factors:**\n  - Llm Call Count: 12100%\n  - Variable Reuse Count: 11100%\n  - Has Input Call: 10900%\n  - Llm Output To Sink: 10100%\n  - Cross Function Flow: 10000%\n\n**Risk:** Direct injection allows attackers to manipulate LLM behavior by crafting malicious input that becomes part of the prompt.",
      "file_path": "/private/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/tmp00zpu59_/open-webui/backend/open_webui/migrations/versions/9f0c9cd09105_add_note_table.py",
      "line_number": 1,
      "code_snippet": "\"\"\"Add note table\n\nRevision ID: 9f0c9cd09105\nRevises: 3781e22d8b01\nCreate Date: 2025-05-03 03:00:00.000000\n\n\"\"\"\n\nfrom alembic import op\nimport sqlalchemy as sa",
      "recommendation": "1. Use structured prompt templates (e.g., LangChain PromptTemplate) to separate user input from instructions.\n2. Implement input validation and sanitization.\n3. Use system prompts to establish boundaries.\n4. Consider using input length limits.\n5. Implement output validation to detect instruction following.\n6. Use content filtering on both input and output."
    }
  ],
  "metadata": {
    "fp_reduction": {
      "total": 339,
      "kept": 310,
      "filtered": 29,
      "reduction_pct": 8.6,
      "avg_tp_probability": 0.621,
      "filter_reasons": {
        "high severity with context": 29,
        "test file": 6,
        "build tool subprocess": 4,
        "server runner": 3,
        "env variable reference": 2,
        "base64 image": 1
      }
    }
  }
}